# Comparing `tmp/paasta-tools-0.92.1.tar.gz` & `tmp/paasta-tools-1.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/paasta-tools-0.92.1.tar", last modified: Fri Jan  3 01:32:14 2020, max compression
+gzip compressed data, was "dist/paasta-tools-1.0.0.tar", last modified: Thu May 16 19:44:12 2024, max compression
```

## Comparing `paasta-tools-0.92.1.tar` & `paasta-tools-1.0.0.tar`

### file list

```diff
@@ -1,254 +1,343 @@
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/
--rw-rw-r--   0 travis    (2000) travis    (2000)     1043 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/requirements-minimal.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)      194 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/MANIFEST.in
--rw-rw-r--   0 travis    (2000) travis    (2000)      248 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/PKG-INFO
--rw-rw-r--   0 travis    (2000) travis    (2000)     4990 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/README.md
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/
--rw-rw-r--   0 travis    (2000) travis    (2000)     1550 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_kubernetes_api.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2748 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/hacheck.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5710 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/marathon_dashboard.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)      248 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/generate_all_deployments
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1697 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscale_all_services.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6614 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_oom_events.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5169 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/secret_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    16184 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/firewall.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     6411 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/list_marathon_service_instances.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    72590 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4765 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/remote_git.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7754 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cassandracluster_tools.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    25316 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_remote_run.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3855 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/spark_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    19754 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/long_running_service_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5335 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/setup_kubernetes_crd.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12175 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/setup_kubernetes_cr.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2962 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cleanup_tron_namespaces.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1100 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/am_i_mesos_leader.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    19511 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_metastatus.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    35579 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4613 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cleanup_kubernetes_cr.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      870 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_native_serviceinit.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6773 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/flink_tools.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1140 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_cassandracluster_services_replication.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/
--rw-rw-r--   0 travis    (2000) travis    (2000)     4159 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/forecasting.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7807 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/load_boost.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1088 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/utils.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1916 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/ec2_fitness.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2333 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/pause_service_autoscaler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    63346 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/autoscaling_cluster_lib.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    38259 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscaling/autoscaling_service_lib.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/monitoring/
--rwxrwxr-x   0 travis    (2000) travis    (2000)     2128 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/kill_orphaned_docker_containers.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     6026 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/check_capacity.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     5574 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/check_synapse_replication.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1494 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/check_marathon_has_apps.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)      995 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/check_mesos_quorum.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1785 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/check_mesos_duplicate_frameworks.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5455 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/check_mesos_outdated_tasks.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1813 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/check_mesos_active_frameworks.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      578 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      106 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_deploy_tron_jobs
--rw-rw-r--   0 travis    (2000) travis    (2000)     7352 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_service_config_loader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5348 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/firewall_update.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     9448 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_maintenance.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     4668 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_kubernetes_services_replication.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5708 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/oom_logger.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     5758 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/native_mesos_scheduler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6475 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cleanup_kubernetes_jobs.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/contrib/
--rwxrwxr-x   0 travis    (2000) travis    (2000)     2575 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/mock_patch_checker.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1575 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/mass-deploy-tag.sh
--rw-rw-r--   0 travis    (2000) travis    (2000)     1884 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/emit_allocated_cpu_metrics.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     5803 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/graceful_container_drain.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     2342 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/bounce_log_latency_parser.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1181 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/create_dynamodb_table.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     3155 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/delete_old_marathon_deployments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      138 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/README.md
--rwxrwxr-x   0 travis    (2000) travis    (2000)      525 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/check_deployd_leader_election.sh
--rwxrwxr-x   0 travis    (2000) travis    (2000)     2108 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/utilization_check.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2941 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/check_registered_slaves_aws.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2317 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/get_running_task_allocation.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     2480 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/shared_ip_check.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7036 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/check_orphans.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     3861 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/kill_bad_containers.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1315 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/is_pod_healthy_in_smartstack.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     9948 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/paasta_update_soa_memcpu.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)      715 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/contrib/paasta_get_num_deployments.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4190 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cleanup_kubernetes_crd.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     4994 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_cluster_boost.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/mesos/
--rw-rw-r--   0 travis    (2000) travis    (2000)     2199 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/util.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1453 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/cfg.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2985 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/task.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1355 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/exceptions.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1949 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/parallel.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1158 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/zookeeper.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2029 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/framework.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10338 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/master.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3993 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/slave.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5366 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/mesos_file.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2201 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/cluster.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1488 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos/log.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     4054 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/paasta_execute_docker_command.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/api/
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/api/views/
--rw-rw-r--   0 travis    (2000) travis    (2000)     1204 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/marathon_dashboard.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1367 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/service.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    39896 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/instance.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2755 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/resources.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1457 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/exception.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3944 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/autoscaler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      841 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/version.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      578 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2278 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/pause_autoscaler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      975 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/views/metastatus.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4975 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/auth_decorator.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/api/api_docs/
--rw-rw-r--   0 travis    (2000) travis    (2000)    52273 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/api_docs/swagger.json
--rw-rw-r--   0 travis    (2000) travis    (2000)     6780 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/api.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      578 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1348 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/settings.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4796 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/api/client.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)      886 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/get_mesos_leader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)   115069 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/utils.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    25198 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/tron_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1331 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mac_address.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     4593 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/generate_services_file.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1448 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/synapse_srv_namespaces_fact.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1927 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/dump_locally_running_services.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)      896 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/generate_services_yaml.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1586 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/run-paasta-api-in-dev-mode.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    30677 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/mesos_maintenance.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     3946 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cleanup_maintenance.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/deployd/
--rwxrwxr-x   0 travis    (2000) travis    (2000)    17046 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/watchers.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8619 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/queue.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8316 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/common.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4472 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/metrics.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2591 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/leader.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6655 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/workers.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    10497 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/master.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployd/__init__.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     8573 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_services_replication_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15942 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/bounce_lib.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    20156 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/monitoring_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3484 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/async_utils.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/tron/
--rw-rw-r--   0 travis    (2000) travis    (2000)     5385 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/tron/tron_command_context.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/tron/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3063 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/tron/tron_timeutils.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3960 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/tron/client.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/frameworks/
--rw-rw-r--   0 travis    (2000) travis    (2000)     9731 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/frameworks/native_service_config.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    24434 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/frameworks/native_scheduler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2821 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/frameworks/adhoc_scheduler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/frameworks/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8537 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/frameworks/task_store.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3035 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/frameworks/constraints.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     4813 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_marathon_services_replication.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/cli/
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/
--rw-rw-r--   0 travis    (2000) travis    (2000)    32079 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/spark_run.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2435 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/security_check.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    40152 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/local_run.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    13130 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/check.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2526 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/get_latest_deployment.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2972 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/performance_check.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3931 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/cook_image.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    16268 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/validate.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    10202 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/remote_run.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7186 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/push_to_registry.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8776 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/secret.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    12007 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/start_stop_restart.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    48361 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/logs.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1964 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/list.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7879 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/wait_for_deployment.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1601 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/list_clusters.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    53227 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/mark_for_deployment.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     9300 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/rollback.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3188 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/autoscale.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      578 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3733 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/itest.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     3558 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/pause_service_autoscaler.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2735 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/get_docker_image.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5945 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/info.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4549 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/sysdig.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    52952 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/status.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4644 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/boost.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     8274 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cmds/metastatus.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      929 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/paasta_tabcomplete.sh
--rwxrwxr-x   0 travis    (2000) travis    (2000)     5623 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/cli.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    35926 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/utils.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4249 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm_cmd.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      578 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/cli/schemas/
--rw-rw-r--   0 travis    (2000) travis    (2000)    12744 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/schemas/marathon_schema.json
--rw-rw-r--   0 travis    (2000) travis    (2000)     3271 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/schemas/adhoc_schema.json
--rw-rw-r--   0 travis    (2000) travis    (2000)    19818 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/schemas/kubernetes_schema.json
--rw-rw-r--   0 travis    (2000) travis    (2000)     9907 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/schemas/tron_schema.json
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/
--rw-rw-r--   0 travis    (2000) travis    (2000)     2757 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/autosuggest.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      578 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/
--rw-rw-r--   0 travis    (2000) travis    (2000)      739 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/monitoring.yaml
--rw-rw-r--   0 travis    (2000) travis    (2000)     4069 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/marathon-PROD.yaml
--rw-rw-r--   0 travis    (2000) travis    (2000)      319 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/service.yaml
--rw-rw-r--   0 travis    (2000) travis    (2000)       52 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/smartstack.yaml
--rw-rw-r--   0 travis    (2000) travis    (2000)      277 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/README.md
--rw-rw-r--   0 travis    (2000) travis    (2000)      144 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cli/fsm/template/cookiecutter.json
--rwxrwxr-x   0 travis    (2000) travis    (2000)    10833 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/docker_wrapper.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1201 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deployment_utils.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)      108 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/deploy_marathon_services
--rwxrwxr-x   0 travis    (2000) travis    (2000)    10357 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/generate_deployments_for_service.py
--rw-rw-r--   0 travis    (2000) travis    (2000)       66 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/py.typed
--rw-rw-r--   0 travis    (2000) travis    (2000)    63073 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/marathon_tools.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/secret_providers/
--rw-rw-r--   0 travis    (2000) travis    (2000)     6205 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/secret_providers/vault.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     1531 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/secret_providers/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      865 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     5542 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kafkacluster_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     2700 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/slack.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6571 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/setup_kubernetes_job.py
--rw-rw-r--   0 travis    (2000) travis    (2000)      690 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/clusterman.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     7519 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/cleanup_marathon_jobs.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)    40479 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/setup_marathon_job.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7518 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/iptables.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4909 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/adhoc_tools.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     2191 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/autoscale_cluster.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/application/
--rw-rw-r--   0 travis    (2000) travis    (2000)     2783 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/application/tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    15783 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/application/controller_wrappers.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/application/__init__.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/bin/
--rwxrwxr-x   0 travis    (2000) travis    (2000)     7219 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/bin/paasta_secrets_sync.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/bin/__init__.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     5259 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/bin/kubernetes_remove_evicted_pods.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     6310 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/bin/paasta_cleanup_stale_nodes.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/kubernetes/__init__.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    24963 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/smartstack_tools.py
--rw-rw-r--   0 travis    (2000) travis    (2000)    12724 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/drain_lib.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     7250 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_spark_jobs.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4183 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/firewall_logging.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     2399 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/list_kubernetes_service_instances.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools/metrics/
--rwxrwxr-x   0 travis    (2000) travis    (2000)    38811 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/metrics/metastatus_lib.py
--rw-rw-r--   0 travis    (2000) travis    (2000)     4063 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/metrics/metrics_lib.py
--rw-rw-r--   0 travis    (2000) travis    (2000)        0 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/metrics/__init__.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     4690 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/setup_tron_namespace.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     1450 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/list_tron_namespaces.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     4355 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/graceful_app_drain.py
--rwxrwxr-x   0 travis    (2000) travis    (2000)     5835 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/paasta_tools/check_flink_services_health.py
--rw-rw-r--   0 travis    (2000) travis    (2000)       38 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/setup.cfg
--rw-rw-r--   0 travis    (2000) travis    (2000)      315 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/pyproject.toml
--rw-rw-r--   0 travis    (2000) travis    (2000)     5084 2020-01-03 01:10:29.000000 paasta-tools-0.92.1/setup.py
-drwxrwxr-x   0 travis    (2000) travis    (2000)        0 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools.egg-info/
--rw-rw-r--   0 travis    (2000) travis    (2000)      248 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools.egg-info/PKG-INFO
--rw-rw-r--   0 travis    (2000) travis    (2000)        1 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools.egg-info/dependency_links.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)       13 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools.egg-info/top_level.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)     8635 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools.egg-info/SOURCES.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)      864 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools.egg-info/requires.txt
--rw-rw-r--   0 travis    (2000) travis    (2000)     1110 2020-01-03 01:32:14.000000 paasta-tools-0.92.1/paasta_tools.egg-info/entry_points.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/
+-rw-r--r--   0 runner    (1001) docker     (127)      204 2024-05-16 19:44:10.000000 paasta-tools-1.0.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)      298 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     6582 2024-05-16 19:44:10.000000 paasta-tools-1.0.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/k8s_itests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/k8s_itests/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      536 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/k8s_itests/test_autoscaling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      966 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/k8s_itests/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/
+-rw-r--r--   0 runner    (1001) docker     (127)      864 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4751 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/adhoc_tools.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1042 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/am_i_mesos_leader.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/api/
+-rw-r--r--   0 runner    (1001) docker     (127)      578 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8588 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/api.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/api/api_docs/
+-rw-r--r--   0 runner    (1001) docker     (127)    77060 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/api_docs/swagger.json
+-rw-r--r--   0 runner    (1001) docker     (127)     3305 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1303 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/settings.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/api/tweens/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/tweens/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3756 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/tweens/profiling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3826 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/tweens/request_logger.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/api/views/
+-rw-r--r--   0 runner    (1001) docker     (127)      578 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4065 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/autoscaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1457 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/exception.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/flink.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13390 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/instance.py
+-rw-r--r--   0 runner    (1001) docker     (127)      975 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/metastatus.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2277 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/pause_autoscaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2755 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/resources.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1367 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/service.py
+-rw-r--r--   0 runner    (1001) docker     (127)      841 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/api/views/version.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3375 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/apply_external_resources.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3514 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/async_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1700 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/autoscaling_service_lib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4320 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/forecasting.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7819 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/load_boost.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/max_all_k8s_services.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2318 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/pause_service_autoscaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1671 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/autoscaling/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6217 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/bounce_lib.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2042 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/broadcast_log_to_services.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7288 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cassandracluster_tools.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8435 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_autoscaler_max_instances.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1140 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_cassandracluster_services_replication.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     7211 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_flink_services_health.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1550 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_kubernetes_api.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5761 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_kubernetes_services_replication.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7801 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_oom_events.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9471 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_services_replication_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7330 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/check_spark_jobs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4672 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cleanup_kubernetes_cr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4491 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cleanup_kubernetes_crd.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12446 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cleanup_kubernetes_jobs.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3946 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cleanup_maintenance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2868 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cleanup_tron_namespaces.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)      578 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     8480 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cli.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/
+-rw-r--r--   0 runner    (1001) docker     (127)      578 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5119 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/autoscale.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4588 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/boost.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12968 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/check.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5273 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/cook_image.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2677 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/get_docker_image.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5682 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/get_image_version.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3094 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/get_latest_deployment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5922 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/info.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3997 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/itest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2219 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/list.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1550 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/list_clusters.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5577 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/list_deploy_queue.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    49329 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/local_run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    52420 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/logs.py
+-rw-r--r--   0 runner    (1001) docker     (127)    76217 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/mark_for_deployment.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6125 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/mesh_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8226 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/metastatus.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3494 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/pause_service_autoscaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9736 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/push_to_registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10137 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/remote_run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13829 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/rollback.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19709 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/secret.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2377 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/security_check.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45133 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/spark_run.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    14600 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/start_stop_restart.py
+-rw-r--r--   0 runner    (1001) docker     (127)    84455 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/status.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39825 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/validate.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9298 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/cmds/wait_for_deployment.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/
+-rw-r--r--   0 runner    (1001) docker     (127)      578 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2757 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/autosuggest.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/
+-rw-r--r--   0 runner    (1001) docker     (127)      277 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/README.md
+-rw-r--r--   0 runner    (1001) docker     (127)      144 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/cookiecutter.json
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/
+-rw-r--r--   0 runner    (1001) docker     (127)     4075 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/kubernetes-PROD.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      739 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/monitoring.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      319 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/service.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/smartstack.yaml
+-rw-r--r--   0 runner    (1001) docker     (127)     4135 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/fsm_cmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)      929 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/paasta_tabcomplete.sh
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/
+-rw-r--r--   0 runner    (1001) docker     (127)     6460 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/adhoc_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)     5570 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/autoscaling_schema.json
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/autotuned_defaults/
+-rw-r--r--   0 runner    (1001) docker     (127)     1158 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/autotuned_defaults/cassandracluster_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)     3612 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/autotuned_defaults/kubernetes_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)     5301 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/deploy_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)    39538 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/eks_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)    39538 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/kubernetes_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)     6442 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/rollback_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)      729 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/service_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)     7359 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/smartstack_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)    26509 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/schemas/tron_schema.json
+-rw-r--r--   0 runner    (1001) docker     (127)    40542 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/cli/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      690 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/clusterman.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14188 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/config_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/contrib/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2241 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/bounce_log_latency_parser.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      879 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/check_manual_oapi_changes.sh
+-rw-r--r--   0 runner    (1001) docker     (127)     9966 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/check_orphans.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1181 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/create_dynamodb_table.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3239 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/create_paasta_playground.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1872 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/emit_allocated_cpu_metrics.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10918 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/get_running_task_allocation.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2638 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/habitat_fixer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12334 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/ide_helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4334 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/is_pod_healthy_in_proxy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1710 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/is_pod_healthy_in_smartstack.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3861 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/kill_bad_containers.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1578 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/mass-deploy-tag.sh
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2575 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/mock_patch_checker.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    17885 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/paasta_update_soa_memcpu.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3959 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/render_template.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10493 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/rightsizer_soaconfigs_update.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4927 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/service_shard_remove.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9383 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/service_shard_update.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2480 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/shared_ip_check.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2603 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/timeouts_metrics_prom.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2057 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/contrib/utilization_check.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3137 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/delete_kubernetes_deployments.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1801 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/deployment_utils.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6935 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/docker_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      744 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/docker_wrapper_imports.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12725 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/drain_lib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2220 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/dump_locally_running_services.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4633 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/eks_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14276 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/envoy_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16304 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/firewall.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4183 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/firewall_logging.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5388 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/firewall_update.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12157 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/flink_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3070 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/flinkeks_tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/frameworks/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/frameworks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2770 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/frameworks/adhoc_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2969 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/frameworks/constraints.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24535 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/frameworks/native_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9181 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/frameworks/native_service_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8536 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/frameworks/task_store.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      248 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/generate_all_deployments
+-rwxr-xr-x   0 runner    (1001) docker     (127)     9674 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/generate_deployments_for_service.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4617 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/generate_services_file.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      896 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/generate_services_yaml.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      835 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/get_mesos_leader.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2748 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/hacheck.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/instance/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/instance/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4371 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/instance/hpa_metrics_parser.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46810 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/instance/kubernetes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7518 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/iptables.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4992 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kafkacluster_tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/application/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/application/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16009 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/application/controller_wrappers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3618 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/application/tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/bin/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/bin/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5287 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/bin/kubernetes_remove_evicted_pods.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6310 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/bin/paasta_cleanup_stale_nodes.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    28779 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes/bin/paasta_secrets_sync.py
+-rw-r--r--   0 runner    (1001) docker     (127)   166865 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/kubernetes_tools.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     3910 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/list_kubernetes_service_instances.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1679 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/list_tron_namespaces.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8904 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/log_task_lifecycle_events.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25773 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/long_running_service_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1324 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mac_address.py
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/marathon_dashboard.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/mesos/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1453 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/cfg.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2143 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/cluster.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1355 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2029 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/framework.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1436 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/log.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10338 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/master.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5366 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/mesos_file.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1949 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/parallel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3993 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/slave.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2985 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/task.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2199 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/util.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1158 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos/zookeeper.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    31450 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos_maintenance.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35836 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/mesos_tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/metrics/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/metrics/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    39707 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/metrics/metastatus_lib.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6402 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/metrics/metrics_lib.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/monitoring/
+-rw-r--r--   0 runner    (1001) docker     (127)      578 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5883 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring/check_capacity.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2997 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring/check_k8s_api_performance.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1748 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring/check_mesos_active_frameworks.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1720 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring/check_mesos_duplicate_frameworks.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)      937 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring/check_mesos_quorum.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     2070 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring/kill_orphaned_docker_containers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24665 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monitoring_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5092 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/monkrelaycluster_tools.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5758 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/native_mesos_scheduler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5037 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/nrtsearchservice_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2593 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/nrtsearchserviceeks_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7646 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/oom_logger.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4375 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_cluster_boost.py
+-rw-r--r--   0 runner    (1001) docker     (127)      106 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_deploy_tron_jobs
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4099 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_execute_docker_command.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4571 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_maintenance.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    17402 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_metastatus.py
+-rw-r--r--   0 runner    (1001) docker     (127)      819 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_native_serviceinit.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    25707 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_remote_run.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8517 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paasta_service_config_loader.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/
+-rw-r--r--   0 runner    (1001) docker     (127)      850 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/api/
+-rw-r--r--   0 runner    (1001) docker     (127)      229 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/api/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10628 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/api/autoscaler_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24615 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/api/default_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5468 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/api/resources_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    63296 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/api/service_api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35200 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/api_client.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/apis/
+-rw-r--r--   0 runner    (1001) docker     (127)      693 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/apis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16583 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/configuration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4368 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/exceptions.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/
+-rw-r--r--   0 runner    (1001) docker     (127)      348 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6944 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/adhoc_launch_history.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6937 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/autoscaler_count_msg.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7292 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/deploy_queue.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8370 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/deploy_queue_service_instance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7659 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/envoy_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7459 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/envoy_location.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7285 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/envoy_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7825 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/flink_cluster_overview.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6746 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/flink_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7129 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/flink_job.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7504 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/flink_job_details.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6706 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/flink_jobs.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6718 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/float_and_error.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6993 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/hpa_metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6555 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/inline_object.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6691 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/inline_response200.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6579 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/inline_response2001.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8444 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_bounce_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7357 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_mesh_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9761 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7045 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_adhoc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7052 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_cassandracluster.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7022 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_flink.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7036 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_kafkacluster.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12277 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_kubernetes.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7647 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_kubernetes_autoscaling_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8454 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_kubernetes_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8564 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_status_tron.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6825 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/instance_tasks.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6718 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/integer_and_error.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6917 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_container.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9899 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_container_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6913 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_healthcheck.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8476 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_pod.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6874 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_pod_event.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9276 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_pod_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7628 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_replica_set.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8613 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/kubernetes_version.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6801 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/meta_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6912 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/resource.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7450 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/resource_item.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6819 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/resource_value.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8029 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/smartstack_backend.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7241 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/smartstack_location.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7325 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/smartstack_status.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7025 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model/task_tail_lines.py
+-rw-r--r--   0 runner    (1001) docker     (127)    74015 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/model_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/models/
+-rw-r--r--   0 runner    (1001) docker     (127)     4000 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12197 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/paastaapi/rest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7742 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/prune_completed_pods.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/puppet_service_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)       66 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/py.typed
+-rw-r--r--   0 runner    (1001) docker     (127)     4837 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/remote_git.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1971 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/run-paasta-api-in-dev-mode.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1616 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/run-paasta-api-playground.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/secret_providers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1975 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/secret_providers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7517 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/secret_providers/vault.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9060 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/secret_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11573 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/setup_istio_mesh.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14675 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/setup_kubernetes_cr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5279 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/setup_kubernetes_crd.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     4642 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/setup_kubernetes_internal_crd.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    12646 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/setup_kubernetes_job.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    37673 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/setup_prometheus_adapter_config.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5028 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/setup_tron_namespace.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2700 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/slack.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25551 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/smartstack_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8557 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/spark_tools.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     1421 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/synapse_srv_namespaces_fact.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools/tron/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/tron/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3960 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/tron/client.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5379 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/tron/tron_command_context.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3054 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/tron/tron_timeutils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    48900 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/tron_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)   145790 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3897 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/paasta_tools/vitesscluster_tools.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)      298 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    12432 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      910 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       24 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/paasta_tools.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      344 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (127)     1545 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/requirements-minimal.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-16 19:44:12.000000 paasta-tools-1.0.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     5067 2024-05-16 19:44:11.000000 paasta-tools-1.0.0/setup.py
```

### Comparing `paasta-tools-0.92.1/paasta_tools/check_kubernetes_api.py` & `paasta-tools-1.0.0/paasta_tools/check_kubernetes_api.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/hacheck.py` & `paasta-tools-1.0.0/paasta_tools/hacheck.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/autoscale_all_services.py` & `paasta-tools-1.0.0/paasta_tools/dump_locally_running_services.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,55 +1,71 @@
 #!/usr/bin/env python
-# Copyright 2015-2016 Yelp Inc.
+# Copyright 2015-2019 Yelp Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import argparse
-import logging
+"""
+Usage: ./paasta_dump_locally_running_services.py [options]
 
-from paasta_tools.autoscaling.autoscaling_service_lib import autoscale_services
-from paasta_tools.marathon_tools import DEFAULT_SOA_DIR
+Outputs a JSON-encoded list of services that are running on this host along
+with the host port that each service is listening on.
 
+Command line options:
 
-def parse_args():
-    parser = argparse.ArgumentParser(description="Autoscales marathon jobs")
+- -d <SOA_DIR>, --soa-dir <SOA_DIR>: Specify a SOA config dir to read from
+"""
+import argparse
+import json
+import sys
+from typing import List
+from typing import Optional
+from typing import Sequence
+from typing import Tuple
+
+from paasta_tools.kubernetes_tools import get_kubernetes_services_running_here_for_nerve
+from paasta_tools.long_running_service_tools import ServiceNamespaceConfig
+from paasta_tools.puppet_service_tools import get_puppet_services_running_here_for_nerve
+from paasta_tools.utils import DEFAULT_SOA_DIR
+
+
+def parse_args(argv: Optional[Sequence[str]]) -> argparse.Namespace:
+    parser = argparse.ArgumentParser(
+        description="Dumps information about locally running services."
+    )
     parser.add_argument(
         "-d",
         "--soa-dir",
         dest="soa_dir",
         metavar="SOA_DIR",
         default=DEFAULT_SOA_DIR,
         help="define a different soa config directory",
     )
-    parser.add_argument(
-        "-v", "--verbose", action="store_true", help="Increase logging verboseness"
-    )
-    parser.add_argument(
-        "services",
-        type=str,
-        nargs="*",
-        help="name of services to scale (optional defaults to all autoscaling enabled services)",
-    )
-    args = parser.parse_args()
-    return args
+    return parser.parse_args(argv)
 
 
-def main():
-    args = parse_args()
-    if args.verbose:
-        logging.basicConfig(level=logging.DEBUG)
-    else:
-        logging.basicConfig(level=logging.WARNING)
-    autoscale_services(soa_dir=args.soa_dir, services=args.services)
+def main(argv: Optional[Sequence[str]] = None) -> None:
+    args = parse_args(argv)
+    soa_dir = args.soa_dir
+
+    service_dump: List[
+        Tuple[str, ServiceNamespaceConfig]
+    ] = get_puppet_services_running_here_for_nerve(
+        soa_dir=soa_dir
+    ) + get_kubernetes_services_running_here_for_nerve(
+        cluster=None, soa_dir=soa_dir
+    )
+
+    print(json.dumps(service_dump))
+    sys.exit(0)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/check_oom_events.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/mesh_status.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,204 +8,167 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import argparse
-import json
-import sys
-import time
-from collections import namedtuple
-from random import choice
+from typing import List
+from typing import Tuple
 
-from pysensu_yelp import Status
-
-from paasta_tools import monitoring_tools
-from paasta_tools.cli.utils import get_instance_config
+from paasta_tools.api.client import get_paasta_oapi_client
+from paasta_tools.cli.cmds.status import get_envoy_status_human
+from paasta_tools.cli.cmds.status import get_smartstack_status_human
+from paasta_tools.cli.utils import figure_out_service_name
+from paasta_tools.cli.utils import get_instance_configs_for_service
+from paasta_tools.cli.utils import get_paasta_oapi_api_clustername
+from paasta_tools.cli.utils import lazy_choices_completer
+from paasta_tools.cli.utils import verify_instances
+from paasta_tools.eks_tools import EksDeploymentConfig
 from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import get_services_for_cluster
+from paasta_tools.utils import list_clusters
+from paasta_tools.utils import list_services
 from paasta_tools.utils import load_system_paasta_config
+from paasta_tools.utils import PaastaColors
+from paasta_tools.utils import SystemPaastaConfig
 
-try:
-    from scribereader import scribereader
-except ImportError:
-    scribereader = None
-
-
-OOM_EVENTS_STREAM = "tmp_paasta_oom_events"
-
-OOMEvent = namedtuple("OOMEvent", ["hostname", "container_id", "process_name"])
 
-
-def compose_check_name_for_service_instance(check_name, service, instance):
-    return f"{check_name}.{service}.{instance}"
-
-
-def parse_args(args):
-    parser = argparse.ArgumentParser(
+def add_subparser(subparsers) -> None:
+    mesh_status_parser = subparsers.add_parser(
+        "mesh-status",
+        help="Display the mesh status of a PaaSTA service.",
         description=(
-            "Check the %s stream and report to Sensu if"
-            " there are any OOM events." % OOM_EVENTS_STREAM
-        )
+            "'paasta mesh-status' queries the PaaSTA API in order to report "
+            "on the health of a PaaSTA service in the mesh."
+        ),
     )
-    parser.add_argument(
+    mesh_status_parser.add_argument(
+        "-s",
+        "--service",
+        type=str,
+        help="The name of the service you wish to inspect",
+        required=True,
+    ).completer = lazy_choices_completer(list_services)
+    mesh_status_parser.add_argument(
+        "-i",
+        "--instance",
+        type=str,
+        help="The name of the instance of the service you wish to inspect",
+        required=True,
+    )  # No completer because we need to know service first and we can't until some other stuff has happened
+    mesh_status_parser.add_argument(
+        "-c",
+        "--cluster",
+        type=str,
+        help="The name of the cluster in which the instance runs",
+        required=True,
+    ).completer = lazy_choices_completer(list_clusters)
+    mesh_status_parser.add_argument(
         "-d",
         "--soa-dir",
         dest="soa_dir",
+        metavar="SOA_DIR",
         default=DEFAULT_SOA_DIR,
         help="define a different soa config directory",
     )
-    parser.add_argument(
-        "-r",
-        "--realert-every",
-        dest="realert_every",
-        type=int,
-        default=1,
-        help="Sensu 'realert_every' to use.",
-    )
-    parser.add_argument(
-        "-s",
-        "--superregion",
-        dest="superregion",
-        required=True,
-        help="The superregion to read OOM events from.",
-    )
-    return parser.parse_args(args)
+    mesh_status_parser.set_defaults(command=paasta_mesh_status)
 
 
-def read_oom_events_from_scribe(cluster, superregion, num_lines=1000):
-    """Read the latest 'num_lines' lines from OOM_EVENTS_STREAM and iterate over them."""
-    host_port = choice(scribereader.get_default_scribe_hosts(tail=True))
-    stream = scribereader.get_stream_tailer(
-        stream_name=OOM_EVENTS_STREAM,
-        tailing_host=host_port["host"],
-        tailing_port=host_port["port"],
-        use_kafka=True,
-        lines=num_lines,
-        superregion=superregion,
-    )
-    for line in stream:
-        try:
-            j = json.loads(line)
-            if j.get("cluster", "") == cluster:
-                yield j
-        except json.decoder.JSONDecodeError:
-            pass
-
-
-def latest_oom_events(cluster, superregion, interval=60):
-    """
-    :returns: {(service, instance): [OOMEvent, OOMEvent,...] }
-              if the number of events > 0
-    """
-    start_timestamp = int(time.time()) - interval
-    res = {}
-    for e in read_oom_events_from_scribe(cluster, superregion):
-        if e["timestamp"] > start_timestamp:
-            key = (e["service"], e["instance"])
-            res.setdefault(key, []).append(
-                OOMEvent(
-                    hostname=e.get("hostname", ""),
-                    container_id=e.get("container_id", ""),
-                    process_name=e.get("process_name", ""),
-                )
-            )
-    return res
-
-
-def compose_sensu_status(instance, oom_events, is_check_enabled):
-    """
-    :param instance: InstanceConfig
-    :param oom_events: a list of OOMEvents
-    :param is_check_enabled: boolean to indicate whether the check enabled for the instance
-    """
-    if not is_check_enabled:
-        return (
-            Status.OK,
-            "This check is disabled for {}.{}.".format(
-                instance.service, instance.instance
-            ),
+def paasta_mesh_status_on_api_endpoint(
+    cluster: str,
+    service: str,
+    instance: str,
+    system_paasta_config: SystemPaastaConfig,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> Tuple[int, List[str]]:
+    instance_config = next(
+        get_instance_configs_for_service(
+            service=service,
+            soa_dir=soa_dir,
+            clusters=[cluster],
+            instances=[instance],
+        ),
+        None,
+    )
+    if not instance_config:
+        print(
+            "ERROR: Could not find config files for this service instance in soaconfigs. Maybe you mispelled an argument?"
+        )
+        exit(1)
+
+    client = get_paasta_oapi_client(
+        cluster=get_paasta_oapi_api_clustername(
+            cluster,
+            is_eks=(instance_config.__class__ == EksDeploymentConfig),
+        ),
+        system_paasta_config=system_paasta_config,
+    )
+    if not client:
+        print("ERROR: Cannot get a paasta-api client")
+        exit(1)
+
+    try:
+        mesh_status = client.service.mesh_instance(
+            service=service,
+            instance=instance,
         )
-    if len(oom_events) == 0:
+    except client.api_error as exc:
+        # 405 (method not allowed) is returned for instances that are not configured
+        # for the mesh, or for which getting mesh status is not supported
         return (
-            Status.OK,
-            "No oom events for %s.%s in the last minute."
-            % (instance.service, instance.instance),
+            exc.status,
+            [PaastaColors.red(exc.body if exc.status == 405 else exc.reason)],
         )
-    else:
+    except (client.connection_error, client.timeout_error) as exc:
         return (
-            Status.CRITICAL,
-            "The Out Of Memory killer killed %d processes (%s) "
-            "in the last minute in %s.%s containers."
-            % (
-                len(oom_events),
-                ",".join(
-                    sorted({e.process_name for e in oom_events if e.process_name})
-                ),
-                instance.service,
-                instance.instance,
-            ),
+            1,
+            [PaastaColors.red(f"Could not connect to API: {exc.__class__.__name__}")],
+        )
+    except Exception as e:
+        output = [PaastaColors.red(f"Exception when talking to the API:")]
+        output.extend(str(e).split("\n"))
+        return 1, output
+
+    output = []
+    if mesh_status.smartstack is not None:
+        smartstack_status_human = get_smartstack_status_human(
+            mesh_status.smartstack.registration,
+            mesh_status.smartstack.expected_backends_per_location,
+            mesh_status.smartstack.locations,
+        )
+        output.extend(smartstack_status_human)
+    if mesh_status.envoy is not None:
+        envoy_status_human = get_envoy_status_human(
+            mesh_status.envoy.registration,
+            mesh_status.envoy.expected_backends_per_location,
+            mesh_status.envoy.locations,
         )
+        output.extend(envoy_status_human)
 
+    return 0, output
 
-def send_sensu_event(instance, oom_events, args):
-    """
-    :param instance: InstanceConfig
-    :param oom_events: a list of OOMEvents
-    """
-    check_name = compose_check_name_for_service_instance(
-        "oom-killer", instance.service, instance.instance
-    )
-    monitoring_overrides = instance.get_monitoring()
-    status = compose_sensu_status(
-        instance=instance,
-        oom_events=oom_events,
-        is_check_enabled=monitoring_overrides.get("check_oom_events", True),
-    )
-    memory_limit = instance.get_mem()
-    try:
-        memory_limit_str = f"{int(memory_limit)}MB"
-    except ValueError:
-        memory_limit_str = memory_limit
-
-    monitoring_overrides.update(
-        {
-            "page": False,
-            "alert_after": "0m",
-            "realert_every": args.realert_every,
-            "runbook": "y/check-oom-events",
-            "tip": "Try bumping the memory limit past %s" % memory_limit_str,
-        }
-    )
-    return monitoring_tools.send_event(
-        service=instance.service,
-        check_name=check_name,
-        overrides=monitoring_overrides,
-        status=status[0],
-        output=status[1],
-        soa_dir=instance.soa_dir,
-    )
 
+def paasta_mesh_status(args) -> int:
+    system_paasta_config = load_system_paasta_config()
+
+    # validate args, funcs have their own error output
+    service = figure_out_service_name(args, args.soa_dir)
+    if verify_instances(args.instance, service, [args.cluster], args.soa_dir):
+        return 1
 
-def main(sys_argv):
-    args = parse_args(sys_argv[1:])
-    cluster = load_system_paasta_config().get_cluster()
-    victims = latest_oom_events(cluster, args.superregion)
-    for (service, instance) in get_services_for_cluster(cluster, soa_dir=args.soa_dir):
-        try:
-            instance_config = get_instance_config(
-                service=service,
-                instance=instance,
-                cluster=cluster,
-                load_deployments=False,
-                soa_dir=args.soa_dir,
-            )
-            oom_events = victims.get((service, instance), [])
-            send_sensu_event(instance_config, oom_events, args)
-        except NotImplementedError:  # When instance_type is not supported by get_instance_config
-            pass
+    return_code, mesh_output = paasta_mesh_status_on_api_endpoint(
+        cluster=args.cluster,
+        service=service,
+        instance=args.instance,
+        system_paasta_config=system_paasta_config,
+        soa_dir=args.soa_dir,
+    )
 
+    output = [
+        f"service: {service}",
+        f"cluster: {args.cluster}",
+        f"instance: {PaastaColors.cyan(args.instance)}",
+    ]
+    output.extend(["  " + line for line in mesh_output])
+    print("\n".join(output))
 
-if __name__ == "__main__":
-    main(sys.argv)
+    return return_code
```

### Comparing `paasta-tools-0.92.1/paasta_tools/firewall.py` & `paasta-tools-1.0.0/paasta_tools/firewall.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,22 +7,26 @@
 import logging
 import os.path
 import re
 from contextlib import contextmanager
 
 from paasta_tools import iptables
 from paasta_tools.cli.utils import get_instance_config
-from paasta_tools.marathon_tools import get_all_namespaces_for_service
+from paasta_tools.long_running_service_tools import get_all_namespaces_for_service
 from paasta_tools.utils import get_running_mesos_docker_containers
 from paasta_tools.utils import load_system_paasta_config
 from paasta_tools.utils import NoConfigurationForServiceError
 from paasta_tools.utils import timed_flock
 
 
-PRIVATE_IP_RANGES = (
+INBOUND_PRIVATE_IP_RANGES = (
+    "127.0.0.0/255.0.0.0",
+    "169.254.0.0/255.255.0.0",
+)
+OUTBOUND_PRIVATE_IP_RANGES = (
     "127.0.0.0/255.0.0.0",
     "10.0.0.0/255.0.0.0",
     "172.16.0.0/255.240.0.0",
     "192.168.0.0/255.255.0.0",
     "169.254.0.0/255.255.0.0",
 )
 DEFAULT_SYNAPSE_SERVICE_DIR = "/var/run/synapse/services"
@@ -73,21 +77,22 @@
             # we don't want to break all of the firewall infrastructure when that happens
             return ()
         except NoConfigurationForServiceError:
             # PAASTA-12050: a deleted service may still have containers running on PaaSTA hosts
             # for several minutes after the directory disappears from soa-configs.
             return ()
 
-        if not conf.get_outbound_firewall():
-            return ()
+        rules = list()
+
+        if conf.get_outbound_firewall():
+            rules.extend(_default_rules(conf, self.log_prefix))
+            rules.extend(_well_known_rules(conf))
+            rules.extend(_smartstack_rules(conf, soa_dir, synapse_service_dir))
+            rules.extend(_cidr_rules(conf))
 
-        rules = list(_default_rules(conf, self.log_prefix))
-        rules.extend(_well_known_rules(conf))
-        rules.extend(_smartstack_rules(conf, soa_dir, synapse_service_dir))
-        rules.extend(_cidr_rules(conf))
         return tuple(rules)
 
     def update_rules(self, soa_dir, synapse_service_dir):
         iptables.ensure_chain(
             self.chain_name, self.get_rules(soa_dir, synapse_service_dir)
         )
         iptables.reorder_chain(self.chain_name)
@@ -283,15 +288,15 @@
                     ),
                     target_parameters=(),
                 )
 
 
 def services_running_here():
     """Generator helper that yields (service, instance, mac address) of both
-    marathon tasks.
+    mesos tasks.
     """
     for container in get_running_mesos_docker_containers():
         if container["HostConfig"]["NetworkMode"] != "bridge":
             continue
 
         service = container["Labels"].get("paasta_service")
         instance = container["Labels"].get("paasta_instance")
@@ -410,15 +415,15 @@
                 protocol="ip",
                 src="0.0.0.0/0.0.0.0",
                 dst=ip_range,
                 target="RETURN",
                 matches=(),
                 target_parameters=(),
             )
-            for ip_range in PRIVATE_IP_RANGES
+            for ip_range in OUTBOUND_PRIVATE_IP_RANGES
         ),
     )
 
 
 def ensure_service_chains(service_groups, soa_dir, synapse_service_dir):
     """Ensure service chains exist and have the right rules.
 
@@ -480,22 +485,20 @@
         active_service_groups(), soa_dir, synapse_service_dir
     )
     ensure_dispatch_chains(service_chains)
     garbage_collect_old_service_chains(service_chains)
 
 
 def prepare_new_container(soa_dir, synapse_service_dir, service, instance, mac):
-    """Update iptables to include rules for a new (not yet running) MAC address
-    """
+    """Update iptables to include rules for a new (not yet running) MAC address"""
     ensure_shared_chains()  # probably already set, but just to be safe
     service_group = ServiceGroup(service, instance)
     service_group.update_rules(soa_dir, synapse_service_dir)
     iptables.insert_rule("PAASTA", dispatch_rule(service_group.chain_name, mac))
 
 
 @contextmanager
 def firewall_flock(flock_path=DEFAULT_FIREWALL_FLOCK_PATH):
-    """ Grab an exclusive flock to avoid concurrent iptables updates
-    """
+    """Grab an exclusive flock to avoid concurrent iptables updates"""
     with io.FileIO(flock_path, "w") as f:
         with timed_flock(f, seconds=DEFAULT_FIREWALL_FLOCK_TIMEOUT_SECS):
             yield
```

### Comparing `paasta-tools-0.92.1/paasta_tools/remote_git.py` & `paasta-tools-1.0.0/paasta_tools/remote_git.py`

 * *Files 1% similar despite different names*

```diff
@@ -94,17 +94,17 @@
         refs = client.fetch_pack(path, lambda refs: [], None, lambda data: None)
         return {k.decode("UTF-8"): v.decode("UTF-8") for k, v in refs.items()}
     except dulwich.errors.HangupException as e:
         raise LSRemoteException(f"Unable to fetch remote refs from {git_url}: {e}")
 
 
 def get_authors(git_url, from_sha, to_sha):
-    """ Gets the list of authors who contributed to a git changeset.
+    """Gets the list of authors who contributed to a git changeset.
     Currently only supports fetching this in a very "yelpy" way by
-    executing a gitolite command """
+    executing a gitolite command"""
     matches = re.match("(?P<git_server>.*):(?P<git_repo>.*)", git_url)
     if matches is None:
         return (1, f"could not understand the git url {git_url} for authors detection")
     git_server = matches.group("git_server")
     git_repo = matches.group("git_repo")
     if git_server is None:
         return (
@@ -119,8 +119,9 @@
 
     if "git.yelpcorp.com" in git_server:
         ssh_command = (
             f"ssh {git_server} authors-of-changeset {git_repo} {from_sha} {to_sha}"
         )
         return _run(command=ssh_command, timeout=5.0)
     else:
+        # TODO: PAASTA-16927: support getting authors for services on GHE
         return 1, f"Fetching authors not supported for {git_server}"
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cassandracluster_tools.py` & `paasta-tools-1.0.0/paasta_tools/cassandracluster_tools.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,25 +13,25 @@
 import logging
 from typing import List
 from typing import Mapping
 from typing import Optional
 
 import service_configuration_lib
 
-from paasta_tools.kubernetes_tools import InvalidJobNameError
-from paasta_tools.kubernetes_tools import NoConfigurationForServiceError
 from paasta_tools.kubernetes_tools import sanitise_kubernetes_name
 from paasta_tools.kubernetes_tools import sanitised_cr_name
 from paasta_tools.long_running_service_tools import LongRunningServiceConfig
 from paasta_tools.long_running_service_tools import LongRunningServiceConfigDict
 from paasta_tools.utils import BranchDictV2
 from paasta_tools.utils import compose_job_id
 from paasta_tools.utils import decompose_job_id
 from paasta_tools.utils import deep_merge_dictionaries
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import InvalidJobNameError
+from paasta_tools.utils import load_service_instance_config
 from paasta_tools.utils import load_v2_deployments_json
 
 KUBERNETES_NAMESPACE = "paasta-cassandraclusters"
 
 log = logging.getLogger(__name__)
 log.addHandler(logging.NullHandler())
 
@@ -96,14 +96,18 @@
         return registrations or [
             compose_job_id(self.get_service_name_smartstack(), "main")
         ]
 
     def get_kubernetes_namespace(self) -> str:
         return KUBERNETES_NAMESPACE
 
+    def get_namespace(self) -> str:
+        """Get namespace from config, default to 'paasta'"""
+        return self.config_dict.get("namespace", KUBERNETES_NAMESPACE)
+
     def get_instances(self, with_limit: bool = True) -> int:
         return self.config_dict.get("replicas", 1)
 
     def get_bounce_method(self) -> str:
         """
         This isn't really true since we use the StatefulSet RollingUpdate strategy
         However for the paasta-api we need to map to a paasta bounce method and
@@ -114,18 +118,15 @@
     def get_sanitised_service_name(self) -> str:
         return sanitise_kubernetes_name(self.get_service())
 
     def get_sanitised_instance_name(self) -> str:
         return sanitise_kubernetes_name(self.get_instance())
 
     def get_sanitised_deployment_name(self) -> str:
-        return "{service}-{instance}".format(
-            service=self.get_sanitised_service_name(),
-            instance=self.get_sanitised_instance_name(),
-        )
+        return self.get_sanitised_instance_name()
 
     def validate(
         self,
         params: List[str] = [
             "cpus",
             "security",
             "dependencies_reference",
@@ -162,30 +163,19 @@
     :param load_deployments: A boolean indicating if the corresponding deployments.json for this service
                              should also be loaded
     :param soa_dir: The SOA configuration directory to read from
     :returns: A dictionary of whatever was in the config for the service instance"""
     general_config = service_configuration_lib.read_service_configuration(
         service, soa_dir=soa_dir
     )
-    cassandracluster_conf_file = "cassandracluster-%s" % cluster
-    instance_configs = service_configuration_lib.read_extra_service_information(
-        service, cassandracluster_conf_file, soa_dir=soa_dir
+    instance_config = load_service_instance_config(
+        service, instance, "cassandracluster", cluster, soa_dir=soa_dir
     )
-
-    if instance.startswith("_"):
-        raise InvalidJobNameError(
-            f"Unable to load kubernetes job config for {service}.{instance} as instance name starts with '_'"
-        )
-    if instance not in instance_configs:
-        raise NoConfigurationForServiceError(
-            f"{instance} not found in config file {soa_dir}/{service}/{cassandracluster_conf_file}.yaml."
-        )
-
     general_config = deep_merge_dictionaries(
-        overrides=instance_configs[instance], defaults=general_config
+        overrides=instance_config, defaults=general_config
     )
 
     branch_dict: Optional[BranchDictV2] = None
     if load_deployments:
         deployments_json = load_v2_deployments_json(service, soa_dir=soa_dir)
         temp_instance_config = CassandraClusterDeploymentConfig(
             service=service,
@@ -210,11 +200,11 @@
 
 
 # TODO: read this from CRD in service configs
 def cr_id(service: str, instance: str) -> Mapping[str, str]:
     return dict(
         group="yelp.com",
         version="v1alpha1",
-        namespace="paasta-cassandraclusters",
+        namespace=KUBERNETES_NAMESPACE,
         plural="cassandraclusters",
         name=sanitised_cr_name(service, instance),
     )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/paasta_remote_run.py` & `paasta-tools-1.0.0/paasta_tools/paasta_remote_run.py`

 * *Files 5% similar despite different names*

```diff
@@ -33,31 +33,32 @@
 from task_processing.metrics import create_counter  # noreorder
 from task_processing.metrics import get_metric  # noreorder
 from task_processing.plugins.persistence.dynamodb_persistence import (
     DynamoDBPersister,
 )  # noreorder
 from task_processing.runners.sync import Sync  # noreorder
 from task_processing.task_processor import TaskProcessor  # noreorder
+from typing import Tuple
 
 from paasta_tools import mesos_tools
 from paasta_tools.cli.cmds.remote_run import add_list_parser
 from paasta_tools.cli.cmds.remote_run import add_start_parser
 from paasta_tools.cli.cmds.remote_run import add_stop_parser
 from paasta_tools.cli.cmds.remote_run import get_system_paasta_config
 from paasta_tools.cli.cmds.remote_run import split_constraints
 from paasta_tools.cli.utils import figure_out_service_name
 from paasta_tools.frameworks.native_service_config import load_paasta_native_job_config
 from paasta_tools.mesos_tools import get_all_frameworks
 from paasta_tools.mesos_tools import get_mesos_master
 from paasta_tools.utils import compose_job_id
+from paasta_tools.utils import decompose_job_id
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import get_code_sha_from_dockerurl
 from paasta_tools.utils import get_config_hash
 from paasta_tools.utils import NoConfigurationForServiceError
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import validate_service_instance
 
 MESOS_TASK_SPACER = "."
 TASKPROC_OFFER_TIMEOUT_RAW = "Failed due to offer timeout"
 
 
@@ -108,15 +109,15 @@
     soa_dir = args.yelpsoa_config_root
     service = figure_out_service_name(args, soa_dir=args.yelpsoa_config_root)
 
     cluster = args.cluster or system_paasta_config.get_remote_run_config().get(
         "default_cluster", None
     )
     if not cluster:
-        paasta_print(
+        print(
             PaastaColors.red(
                 "PaaSTA on this machine has not been configured with a default cluster."
                 "Please pass one using '-c'."
             )
         )
         emit_counter_metric(
             "paasta.remote_run." + args.action + ".failed", service, "UNKNOWN"
@@ -129,50 +130,50 @@
         instance = "remote"
     else:
         try:
             instance_type = validate_service_instance(
                 service, instance, cluster, soa_dir
             )
         except NoConfigurationForServiceError as e:
-            paasta_print(e)
+            print(e)
             emit_counter_metric(
                 "paasta.remote_run." + args.action + ".failed", service, instance
             )
             sys.exit(1)
 
         if instance_type != "adhoc":
-            paasta_print(
+            print(
                 PaastaColors.red(
                     "Please use instance declared in adhoc.yaml for use "
                     f"with remote-run, {instance} is declared as {instance_type}"
                 )
             )
             emit_counter_metric(
                 "paasta.remote_run." + args.action + ".failed", service, instance
             )
             sys.exit(1)
 
     return (system_paasta_config, service, cluster, soa_dir, instance, instance_type)
 
 
 def accumulate_config_overrides(args, service, instance):
-    """ Although task configs come with defaults values, certain args can
+    """Although task configs come with defaults values, certain args can
     override them. We accumulate them in a dict here and return them.
     """
     overrides = {}
 
     # constraint overrides
     constraints = []
     try:
         if args.constraints_json:
             constraints.extend(json.loads(args.constraints_json))
         if args.constraint:
             constraints.extend(split_constraints(args.constraint))
     except Exception as e:
-        paasta_print(f"Error while parsing constraints: {e}")
+        print(f"Error while parsing constraints: {e}")
         emit_counter_metric("paasta.remote_run.start.failed", service, instance)
         sys.exit(1)
     if constraints:
         overrides["constraints"] = constraints
     # cmd overrides
     if args.cmd:
         overrides["cmd"] = args.cmd
@@ -180,26 +181,26 @@
     if args.instances:
         overrides["instances"] = args.instances
 
     return overrides
 
 
 def generate_run_id(length=8):
-    """ Generates a random string of uppercase letters and digits for use as
+    """Generates a random string of uppercase letters and digits for use as
     a run identifier
     """
     run_id = "".join(
         random.choice(string.ascii_uppercase + string.digits) for _ in range(length)
     )
-    paasta_print(f"Generated random run identifier: {run_id}")
+    print(f"Generated random run identifier: {run_id}")
     return run_id
 
 
 def create_framework_name(service, instance, run_id):
-    """ Creates a framework name for our task """
+    """Creates a framework name for our task"""
     return "paasta-remote {} {} {}".format(
         compose_job_id(service, instance),
         datetime.utcnow().strftime("%Y%m%d%H%M%S%f"),
         run_id,
     )
 
 
@@ -209,15 +210,15 @@
     taskproc_config,
     cluster,
     framework_name,
     framework_staging_timeout,
     role="*",
     pool="default",
 ):
-    """ Create a Mesos executor specific to our cluster """
+    """Create a Mesos executor specific to our cluster"""
     MesosExecutor = processor.executor_cls("mesos_task")
 
     mesos_address = mesos_tools.find_mesos_leader(cluster)
 
     return MesosExecutor(
         role=role,
         pool=pool,
@@ -288,48 +289,46 @@
         )
     )
 
     return kwargs
 
 
 def create_mesos_task_config(processor, service, instance, *args, **kwargs):
-    """ Creates a Mesos task configuration """
+    """Creates a Mesos task configuration"""
     MesosExecutor = processor.executor_cls("mesos_task")
     try:
         return MesosExecutor.TASK_CONFIG_INTERFACE(
             **paasta_to_task_config_kwargs(service, instance, *args, **kwargs)
         )
     except InvariantException as e:
         if len(e.missing_fields) > 0:
-            paasta_print(
+            print(
                 PaastaColors.red(
                     "Mesos task config is missing following fields: "
                     f"{', '.join(e.missing_fields)}"
                 )
             )
         elif len(e.invariant_errors) > 0:
-            paasta_print(
+            print(
                 PaastaColors.red(
                     "Mesos task config is failing following checks: "
                     f"{', '.join(str(ie) for ie in e.invariant_errors)}"
                 )
             )
         else:
-            paasta_print(PaastaColors.red(f"Mesos task config error: {e}"))
+            print(PaastaColors.red(f"Mesos task config error: {e}"))
     except PTypeError as e:
-        paasta_print(
-            PaastaColors.red(f"Mesos task config is failing a type check: {e}")
-        )
+        print(PaastaColors.red(f"Mesos task config is failing a type check: {e}"))
     traceback.print_exc()
     emit_counter_metric("paasta.remote_run.start.failed", service, instance)
     sys.exit(1)
 
 
 def task_config_to_dict(task_config):
-    """ Convert a task config to a dict and remove all empty keys """
+    """Convert a task config to a dict and remove all empty keys"""
     dconf = dict(task_config)
     for k in list(dconf.keys()):
         if not dconf[k]:
             del dconf[k]
     return dconf
 
 
@@ -348,15 +347,15 @@
         aws_access_key_id=credentials["accessKeyId"],
         aws_secret_access_key=credentials["secretAccessKey"],
     )
 
 
 # TODO: rename to registry?
 def build_executor_stack(processor, cluster_executor, taskproc_config, cluster, region):
-    """ Executor stack consists of:
+    """Executor stack consists of:
     1. Cluster Executor (e.g. MesosExecutor)
     2. LoggingExecutor
     3. StatefulExecutor
     """
     # logging executor
     task_logging_executor = processor.executor_from_config(
         provider="logging", provider_config={"downstream_executor": cluster_executor}
@@ -372,25 +371,25 @@
         ),
     )
     return stateful_executor
 
 
 def set_runner_signal_handlers(runner):
     def handle_interrupt(_signum, _frame):
-        paasta_print(PaastaColors.red("Signal received, shutting down scheduler."))
+        print(PaastaColors.red("Signal received, shutting down scheduler."))
         if runner is not None:
             runner.stop()
         sys.exit(143 if _signum == signal.SIGTERM else 1)
 
     signal.signal(signal.SIGINT, handle_interrupt)
     signal.signal(signal.SIGTERM, handle_interrupt)
 
 
 def run_task(executor, task_config):
-    """ Runs a task until a terminal event is received, which is returned. """
+    """Runs a task until a terminal event is received, which is returned."""
     runner = Sync(executor)
     set_runner_signal_handlers(runner)
     terminal_event = runner.run(task_config)
     if getattr(terminal_event, "platform_type", None) == "lost":
         runner.kill(task_config.task_id)
     runner.stop()
     return terminal_event
@@ -422,38 +421,38 @@
 
 def run_tasks_with_retries(executor_factory, task_config_factory, retries=0):
     # use max in case retries is negative, +1 for initial try
     tries_left = max(retries, 0) + 1
     terminals = []
 
     while tries_left > 0:
-        paasta_print(
+        print(
             PaastaColors.yellow(f"Scheduling task on Mesos (tries left: {tries_left})")
         )
 
         try:
             executor = executor_factory()
             task_config = task_config_factory()
             terminal_event = run_task(executor, task_config)
         except (Exception, ValueError) as e:
             # implies an error with our code, and not with mesos, so just return
             # immediately
-            paasta_print(f"Except while running executor stack: {e}")
+            print(f"Except while running executor stack: {e}")
             traceback.print_exc()
             terminals.append((None, task_config))
             return terminals
 
         terminals.append((terminal_event, task_config))
         if terminal_event.success:
-            paasta_print(PaastaColors.green("Task finished successfully"))
+            print(PaastaColors.green("Task finished successfully"))
             break
         else:
             # TODO: add reconciliation and other more specific behavior
             error_msg = get_terminal_event_error_message(terminal_event)
-            paasta_print(PaastaColors.red(f"Task failed:\n{error_msg}"))
+            print(PaastaColors.red(f"Task failed:\n{error_msg}"))
 
         tries_left -= 1
 
     return terminals
 
 
 def send_notification_email(
@@ -494,15 +493,15 @@
     service,
     instance,
     run_id,
     email_address=None,
     framework_config=None,
     task_config=None,
 ):
-    """ Given a terminal event:
+    """Given a terminal event:
     1. Emit metrics
     2. Notify users
     3. Produce exit code
     """
     if event and event.success:
         exit_code = 0
         error_message = None
@@ -528,15 +527,15 @@
             success=event.success,
             error_message=error_message,
         )
     return exit_code
 
 
 def remote_run_start(args):
-    """ Start a task in Mesos
+    """Start a task in Mesos
     Steps:
     1. Accumulate overrides
     2. Create task configuration
     3. Build executor stack
     4. Run the task on the executor stack
     """
     # accumulate all configuration needed to build what we need to run a task
@@ -569,15 +568,15 @@
     role = native_job_config.get_role() or default_role
     pool = native_job_config.get_pool()
     processor = TaskProcessor()
     processor.load_plugin(provider_module="task_processing.plugins.stateful")
     processor.load_plugin(provider_module="task_processing.plugins.mesos")
 
     if args.detach:
-        paasta_print("Running in background")
+        print("Running in background")
         if os.fork() > 0:
             return
         os.setsid()
         if os.fork() > 0:
             return
         sys.stdout = open("/dev/null", "w")
         sys.stderr = open("/dev/null", "w")
@@ -614,15 +613,15 @@
         return build_executor_stack(
             processor, mesos_executor, taskproc_config, cluster, region
         )
 
     if args.dry_run:
         task_config_dict = task_config_to_dict(task_config_factory())
         pp = pprint.PrettyPrinter(indent=2)
-        paasta_print(
+        print(
             PaastaColors.green("Would have run task with:"),
             PaastaColors.green("Framework config:"),
             pp.pformat(framework_config),
             PaastaColors.green("Task config:"),
             pp.pformat(task_config_dict),
             sep="\n",
         )
@@ -644,62 +643,58 @@
     sys.exit(exit_code)
 
 
 # TODO: reimplement using build_executor_stack and task uuid instead of run_id
 def remote_run_stop(args):
     _, service, cluster, _, instance, _ = extract_args(args)
     if args.framework_id is None and args.run_id is None:
-        paasta_print(
-            PaastaColors.red("Must provide either run id or framework id to stop.")
-        )
+        print(PaastaColors.red("Must provide either run id or framework id to stop."))
         emit_counter_metric("paasta.remote_run.stop.failed", service, instance)
         sys.exit(1)
 
     frameworks = [
         f
         for f in get_all_frameworks(active_only=True)
         if re.search(f"^paasta-remote {service}.{instance}", f.name)
     ]
     framework_id = args.framework_id
     if framework_id is None:
         if re.match(r"\s", args.run_id):
-            paasta_print(PaastaColors.red("Run id must not contain whitespace."))
+            print(PaastaColors.red("Run id must not contain whitespace."))
             emit_counter_metric("paasta.remote_run.stop.failed", service, instance)
             sys.exit(1)
 
         found = [
             f for f in frameworks if re.search(" %s$" % args.run_id, f.name) is not None
         ]
         if len(found) > 0:
             framework_id = found[0].id
         else:
-            paasta_print(
-                PaastaColors.red("Framework with run id %s not found." % args.run_id)
-            )
+            print(PaastaColors.red("Framework with run id %s not found." % args.run_id))
             emit_counter_metric("paasta.remote_run.stop.failed", service, instance)
             sys.exit(1)
     else:
         found = [f for f in frameworks if f.id == framework_id]
         if len(found) == 0:
-            paasta_print(
+            print(
                 PaastaColors.red(
                     "Framework id %s does not match any %s.%s remote-run. Check status to find the correct id."
                     % (framework_id, service, instance)
                 )
             )
             emit_counter_metric("paasta.remote_run.stop.failed", service, instance)
             sys.exit(1)
 
-    paasta_print("Tearing down framework %s." % framework_id)
+    print("Tearing down framework %s." % framework_id)
     mesos_master = get_mesos_master()
     teardown = mesos_master.teardown(framework_id)
     if teardown.status_code == 200:
-        paasta_print(PaastaColors.green("OK"))
+        print(PaastaColors.green("OK"))
     else:
-        paasta_print(teardown.text)
+        print(teardown.text)
 
 
 def remote_run_frameworks():
     return get_all_frameworks(active_only=True)
 
 
 def remote_run_filter_frameworks(service, instance, frameworks=None):
@@ -713,36 +708,53 @@
 def remote_run_list_report(service, instance, cluster, frameworks=None):
     filtered = remote_run_filter_frameworks(service, instance, frameworks=frameworks)
     filtered.sort(key=lambda x: x.name)
     for f in filtered:
         launch_time, run_id = re.match(
             r"paasta-remote [^\s]+ (\w+) (\w+)", f.name
         ).groups()
-        paasta_print(
+        print(
             "Launch time: %s, run id: %s, framework id: %s"
             % (launch_time, run_id, f.id)
         )
     if len(filtered) > 0:
-        paasta_print(
+        print(
             (
                 "Use `paasta remote-run stop -s {} -c {} -i {} [-R <run id> "
                 "| -F <framework id>]` to stop."
             ).format(service, cluster, instance)
         )
     else:
-        paasta_print("Nothing found.")
+        print("Nothing found.")
 
 
 def remote_run_list(args, frameworks=None):
     _, service, cluster, _, instance, _ = extract_args(args)
     return remote_run_list_report(
         service=service, instance=instance, cluster=cluster, frameworks=frameworks
     )
 
 
+def deformat_job_id(job_id: str) -> Tuple[str, str, str, str]:
+    job_id = job_id.replace("--", "_")
+    return decompose_job_id(job_id)
+
+
+def get_app_id_and_task_uuid_from_executor_id(executor_id: str) -> Tuple[str, str]:
+    """Parse the marathon executor ID and return the (app id, task uuid)"""
+    app_id, task_uuid = executor_id.rsplit(".", 1)
+    return app_id, task_uuid
+
+
+def parse_service_instance_from_executor_id(task_id: str) -> Tuple[str, str]:
+    app_id, task_uuid = get_app_id_and_task_uuid_from_executor_id(task_id)
+    (srv_name, srv_instance, _, __) = deformat_job_id(app_id)
+    return srv_name, srv_instance
+
+
 def main(argv):
     args = parse_args(argv)
 
     if args.debug:
         logging.basicConfig(level=logging.DEBUG)
     elif args.verbose:
         logging.basicConfig(level=logging.INFO)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/setup_kubernetes_crd.py` & `paasta-tools-1.0.0/paasta_tools/setup_kubernetes_crd.py`

 * *Files 14% similar despite different names*

```diff
@@ -24,17 +24,20 @@
 import argparse
 import logging
 import sys
 from typing import Sequence
 
 import service_configuration_lib
 from kubernetes.client import V1beta1CustomResourceDefinition
-from kubernetes.client.rest import ApiException
+from kubernetes.client import V1CustomResourceDefinition
+from kubernetes.client.exceptions import ApiException
 
 from paasta_tools.kubernetes_tools import KubeClient
+from paasta_tools.kubernetes_tools import paasta_prefixed
+from paasta_tools.kubernetes_tools import update_crds
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import load_system_paasta_config
 
 log = logging.getLogger(__name__)
 
 
 def parse_args() -> argparse.Namespace:
@@ -96,72 +99,70 @@
 def setup_kube_crd(
     kube_client: KubeClient,
     cluster: str,
     services: Sequence[str],
     soa_dir: str = DEFAULT_SOA_DIR,
 ) -> bool:
     existing_crds = kube_client.apiextensions.list_custom_resource_definition(
-        label_selector="paasta.yelp.com/service"
+        label_selector=paasta_prefixed("service")
     )
 
-    success = True
+    # This step can fail in k8s 1.22 since this version is not existing anymore
+    # we need to support this for the transition
+    try:
+        existing_crds_v1_beta1 = (
+            kube_client.apiextensions_v1_beta1.list_custom_resource_definition(
+                label_selector=paasta_prefixed("service")
+            )
+        )
+    except ApiException:
+        existing_crds_v1_beta1 = []
+        log.debug(
+            "Listing CRDs with apiextensions/v1beta1 not supported on this cluster, falling back to v1"
+        )
+
+    desired_crds = []
+    desired_crds_v1_beta1 = []
     for service in services:
         crd_config = service_configuration_lib.read_extra_service_information(
             service, f"crd-{cluster}", soa_dir=soa_dir
         )
         if not crd_config:
             log.info("nothing to deploy")
             continue
 
         metadata = crd_config.get("metadata", {})
         if "labels" not in metadata:
             metadata["labels"] = {}
         metadata["labels"]["yelp.com/paasta_service"] = service
-        metadata["labels"]["paasta.yelp.com/service"] = service
-        desired_crd = V1beta1CustomResourceDefinition(
-            api_version=crd_config.get("apiVersion"),
-            kind=crd_config.get("kind"),
-            metadata=metadata,
-            spec=crd_config.get("spec"),
-        )
+        metadata["labels"][paasta_prefixed("service")] = service
 
-        existing_crd = None
-        for crd in existing_crds.items:
-            if crd.metadata.name == desired_crd.metadata["name"]:
-                existing_crd = crd
-                break
-
-        try:
-            if existing_crd:
-                desired_crd.metadata[
-                    "resourceVersion"
-                ] = existing_crd.metadata.resource_version
-                kube_client.apiextensions.replace_custom_resource_definition(
-                    name=desired_crd.metadata["name"], body=desired_crd
-                )
-            else:
-                try:
-                    kube_client.apiextensions.create_custom_resource_definition(
-                        body=desired_crd
-                    )
-                except ValueError as err:
-                    # TODO: kubernetes server will sometimes reply with conditions:null,
-                    # figure out how to deal with this correctly, for more details:
-                    # https://github.com/kubernetes/kubernetes/pull/64996
-                    if "`conditions`, must not be `None`" in str(err):
-                        pass
-                    else:
-                        raise err
-            log.info(f"deployed {desired_crd.metadata['name']} for {cluster}:{service}")
-        except ApiException as exc:
-            log.error(
-                f"error deploying crd for {cluster}:{service}, "
-                f"status: {exc.status}, reason: {exc.reason}"
+        if "apiextensions.k8s.io/v1beta1" == crd_config["apiVersion"]:
+            desired_crd = V1beta1CustomResourceDefinition(
+                api_version=crd_config.get("apiVersion"),
+                kind=crd_config.get("kind"),
+                metadata=metadata,
+                spec=crd_config.get("spec"),
             )
-            log.debug(exc.body)
-            success = False
+            desired_crds_v1_beta1.append(desired_crd)
+        else:
+            desired_crd = V1CustomResourceDefinition(
+                api_version=crd_config.get("apiVersion"),
+                kind=crd_config.get("kind"),
+                metadata=metadata,
+                spec=crd_config.get("spec"),
+            )
+            desired_crds.append(desired_crd)
 
-    return success
+    return update_crds(
+        kube_client=kube_client,
+        desired_crds=desired_crds,
+        existing_crds=existing_crds,
+    ) and update_crds(
+        kube_client=kube_client,
+        desired_crds=desired_crds_v1_beta1,
+        existing_crds=existing_crds_v1_beta1,
+    )
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/setup_kubernetes_cr.py` & `paasta-tools-1.0.0/paasta_tools/setup_kubernetes_cr.py`

 * *Files 11% similar despite different names*

```diff
@@ -25,32 +25,36 @@
 import sys
 from typing import Any
 from typing import Mapping
 from typing import Optional
 from typing import Sequence
 
 import yaml
+from kubernetes.client.exceptions import ApiException
 
+from paasta_tools.cli.utils import LONG_RUNNING_INSTANCE_TYPE_HANDLERS
 from paasta_tools.flink_tools import get_flink_ingress_url_root
 from paasta_tools.kubernetes_tools import create_custom_resource
 from paasta_tools.kubernetes_tools import CustomResourceDefinition
 from paasta_tools.kubernetes_tools import ensure_namespace
 from paasta_tools.kubernetes_tools import KubeClient
 from paasta_tools.kubernetes_tools import KubeCustomResource
 from paasta_tools.kubernetes_tools import KubeKind
 from paasta_tools.kubernetes_tools import list_custom_resources
 from paasta_tools.kubernetes_tools import load_custom_resource_definitions
+from paasta_tools.kubernetes_tools import paasta_prefixed
 from paasta_tools.kubernetes_tools import sanitise_kubernetes_name
-from paasta_tools.kubernetes_tools import sanitised_cr_name
 from paasta_tools.kubernetes_tools import update_custom_resource
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import get_config_hash
+from paasta_tools.utils import get_git_sha_from_dockerurl
 from paasta_tools.utils import load_all_configs
 from paasta_tools.utils import load_system_paasta_config
 
+
 log = logging.getLogger(__name__)
 
 
 class StdoutKubeClient:
     """Replace all destructive operations in Kubernetes APIs with
     writing out YAML to stdout."""
 
@@ -145,55 +149,83 @@
     kube_client: KubeClient,
     soa_dir: str,
     cluster: str,
     custom_resource_definitions: Sequence[CustomResourceDefinition],
     service: str = None,
     instance: str = None,
 ) -> bool:
-    cluster_crds = {
-        crd.spec.names.kind
-        for crd in kube_client.apiextensions.list_custom_resource_definition(
-            label_selector="paasta.yelp.com/service"
-        ).items
-    }
-    log.debug(f"CRDs found: {cluster_crds}")
-    results = []
-    for crd in custom_resource_definitions:
-        if crd.kube_kind.singular not in cluster_crds:
-            # TODO: kube_kind.singular seems to correspond to `crd.names.kind`
-            # and not `crd.names.singular`
-            log.warning(f"CRD {crd.kube_kind.singular} " f"not found in {cluster}")
-            continue
-        config_dicts = load_all_configs(
-            cluster=cluster, file_prefix=crd.file_prefix, soa_dir=soa_dir
-        )
-        if not config_dicts:
-            continue
-        ensure_namespace(
-            kube_client=kube_client, namespace=f"paasta-{crd.kube_kind.plural}"
-        )
-        results.append(
-            setup_custom_resources(
-                kube_client=kube_client,
-                kind=crd.kube_kind,
-                config_dicts=config_dicts,
-                version=crd.version,
-                group=crd.group,
-                cluster=cluster,
-                service=service,
-                instance=instance,
+
+    got_results = False
+    succeeded = False
+    # We support two versions due to our upgrade to 1.22
+    # this functions runs succefully when any of the two apiextensions
+    # succeed to update the CRDs as the cluster could be in any version
+    # we need to try both possibilities
+    for apiextension in [
+        kube_client.apiextensions,
+        kube_client.apiextensions_v1_beta1,
+    ]:
+
+        try:
+            crds_list = apiextension.list_custom_resource_definition(
+                label_selector=paasta_prefixed("service")
+            ).items
+        except ApiException:
+            log.debug(
+                "Listing CRDs with apiextensions/v1 not supported on this cluster, falling back to v1beta1"
             )
-        )
-    return all(results) if results else True
+            crds_list = []
+
+        cluster_crds = {crd.spec.names.kind for crd in crds_list}
+        log.debug(f"CRDs found: {cluster_crds}")
+        results = []
+        for crd in custom_resource_definitions:
+            if crd.kube_kind.singular not in cluster_crds:
+                # TODO: kube_kind.singular seems to correspond to `crd.names.kind`
+                # and not `crd.names.singular`
+                log.warning(f"CRD {crd.kube_kind.singular} " f"not found in {cluster}")
+                continue
+
+            # by convention, entries where key begins with _ are used as templates
+            # and will be filter out here
+            config_dicts = load_all_configs(
+                cluster=cluster, file_prefix=crd.file_prefix, soa_dir=soa_dir
+            )
+
+            ensure_namespace(
+                kube_client=kube_client, namespace=f"paasta-{crd.kube_kind.plural}"
+            )
+            results.append(
+                setup_custom_resources(
+                    kube_client=kube_client,
+                    kind=crd.kube_kind,
+                    crd=crd,
+                    config_dicts=config_dicts,
+                    version=crd.version,
+                    group=crd.group,
+                    cluster=cluster,
+                    service=service,
+                    instance=instance,
+                )
+            )
+        if results:
+            got_results = True
+            if any(results):
+                succeeded = True
+    # we want to return True if we never called `setup_custom_resources`
+    # (i.e., we noop'd) or if any call to `setup_custom_resources`
+    # succeed (handled above) - otherwise, we want to return False
+    return succeeded or not got_results
 
 
 def setup_custom_resources(
     kube_client: KubeClient,
     kind: KubeKind,
     version: str,
+    crd: CustomResourceDefinition,
     config_dicts: Mapping[str, Mapping[str, Any]],
     group: str,
     cluster: str,
     service: str = None,
     instance: str = None,
 ) -> bool:
     succeded = True
@@ -210,115 +242,146 @@
             instance=instance,
             instance_configs=config,
             kind=kind,
             custom_resources=crs,
             version=version,
             group=group,
             cluster=cluster,
+            crd=crd,
         ):
             succeded = False
     return succeded
 
 
-def get_dashboard_url(
-    kind: str, service: str, instance: str, cluster: str
-) -> Optional[str]:
+def get_dashboard_base_url(kind: str, cluster: str, is_eks: bool) -> Optional[str]:
     system_paasta_config = load_system_paasta_config()
     dashboard_links = system_paasta_config.get_dashboard_links()
     if kind.lower() == "flink":
         flink_link = dashboard_links.get(cluster, {}).get("Flink")
         if flink_link is None:
-            flink_link = get_flink_ingress_url_root(cluster)
+            flink_link = get_flink_ingress_url_root(cluster, is_eks)
         if flink_link[-1:] != "/":
             flink_link += "/"
-        flink_link += sanitised_cr_name(service, instance)
         return flink_link
     return None
 
 
+def get_cr_owner(kind: str) -> Optional[str]:
+    system_paasta_config = load_system_paasta_config()
+    owners = system_paasta_config.get_cr_owners()
+    return owners.get(kind.lower())
+
+
 def format_custom_resource(
     instance_config: Mapping[str, Any],
     service: str,
     instance: str,
     cluster: str,
     kind: str,
     version: str,
     group: str,
     namespace: str,
+    git_sha: str,
+    is_eks: bool,
 ) -> Mapping[str, Any]:
     sanitised_service = sanitise_kubernetes_name(service)
     sanitised_instance = sanitise_kubernetes_name(instance)
     resource: Mapping[str, Any] = {
         "apiVersion": f"{group}/{version}",
         "kind": kind,
         "metadata": {
             "name": f"{sanitised_service}-{sanitised_instance}",
             "namespace": namespace,
             "labels": {
                 "yelp.com/paasta_service": service,
                 "yelp.com/paasta_instance": instance,
                 "yelp.com/paasta_cluster": cluster,
-                "paasta.yelp.com/service": service,
-                "paasta.yelp.com/instance": instance,
-                "paasta.yelp.com/cluster": cluster,
+                paasta_prefixed("service"): service,
+                paasta_prefixed("instance"): instance,
+                paasta_prefixed("cluster"): cluster,
             },
             "annotations": {},
         },
         "spec": instance_config,
     }
-    url = get_dashboard_url(kind, service, instance, cluster)
+    if is_eks:
+        resource["metadata"]["labels"][paasta_prefixed("eks")] = str(is_eks)
+
+    url = get_dashboard_base_url(kind, cluster, is_eks)
     if url:
-        resource["metadata"]["annotations"]["yelp.com/dashboard_url"] = url
-        resource["metadata"]["annotations"]["paasta.yelp.com/dashboard_url"] = url
+        resource["metadata"]["annotations"][paasta_prefixed("dashboard_base_url")] = url
+    owner = get_cr_owner(kind)
+    if owner:
+        resource["metadata"]["labels"]["yelp.com/owner"] = owner
     config_hash = get_config_hash(resource)
+
     resource["metadata"]["annotations"]["yelp.com/desired_state"] = "running"
-    resource["metadata"]["annotations"]["paasta.yelp.com/desired_state"] = "running"
+    resource["metadata"]["annotations"][paasta_prefixed("desired_state")] = "running"
     resource["metadata"]["labels"]["yelp.com/paasta_config_sha"] = config_hash
-    resource["metadata"]["labels"]["paasta.yelp.com/config_sha"] = config_hash
+    resource["metadata"]["labels"][paasta_prefixed("config_sha")] = config_hash
+    resource["metadata"]["labels"][paasta_prefixed("git_sha")] = git_sha
     return resource
 
 
 def reconcile_kubernetes_resource(
     kube_client: KubeClient,
     service: str,
     instance_configs: Mapping[str, Any],
     custom_resources: Sequence[KubeCustomResource],
     kind: KubeKind,
     version: str,
     group: str,
+    crd: CustomResourceDefinition,
     cluster: str,
     instance: str = None,
 ) -> bool:
+    succeeded = True
+    config_handler = LONG_RUNNING_INSTANCE_TYPE_HANDLERS[crd.file_prefix]
+
+    is_eks = False
+    if crd.file_prefix.endswith("eks"):
+        is_eks = True
 
-    results = []
     for inst, config in instance_configs.items():
         if instance is not None and instance != inst:
             continue
-        formatted_resource = format_custom_resource(
-            instance_config=config,
-            service=service,
-            instance=inst,
-            cluster=cluster,
-            kind=kind.singular,
-            version=version,
-            group=group,
-            namespace=f"paasta-{kind.plural}",
-        )
-        desired_resource = KubeCustomResource(
-            service=service,
-            instance=inst,
-            config_sha=formatted_resource["metadata"]["labels"][
-                "paasta.yelp.com/config_sha"
-            ],
-            kind=kind.singular,
-            name=formatted_resource["metadata"]["name"],
-            namespace=f"paasta-{kind.plural}",
-        )
-
         try:
+            soa_config = config_handler.loader(
+                service=service,
+                instance=inst,
+                cluster=cluster,
+                load_deployments=True,
+                soa_dir=DEFAULT_SOA_DIR,
+            )
+            git_sha = get_git_sha_from_dockerurl(soa_config.get_docker_url(), long=True)
+            formatted_resource = format_custom_resource(
+                instance_config=config,
+                service=service,
+                instance=inst,
+                cluster=cluster,
+                kind=kind.singular,
+                version=version,
+                group=group,
+                namespace=f"paasta-{kind.plural}",
+                git_sha=git_sha,
+                is_eks=is_eks,
+            )
+            desired_resource = KubeCustomResource(
+                service=service,
+                instance=inst,
+                config_sha=formatted_resource["metadata"]["labels"][
+                    paasta_prefixed("config_sha")
+                ],
+                git_sha=formatted_resource["metadata"]["labels"].get(
+                    paasta_prefixed("git_sha")
+                ),
+                kind=kind.singular,
+                name=formatted_resource["metadata"]["name"],
+                namespace=f"paasta-{kind.plural}",
+            )
             if not (service, inst, kind.singular) in [
                 (c.service, c.instance, c.kind) for c in custom_resources
             ]:
                 log.info(f"{desired_resource} does not exist so creating")
                 create_custom_resource(
                     kube_client=kube_client,
                     version=version,
@@ -338,14 +401,13 @@
                     formatted_resource=formatted_resource,
                     group=group,
                 )
             else:
                 log.info(f"{desired_resource} is up to date, no action taken")
         except Exception as e:
             log.error(str(e))
-            results.append(False)
-        results.append(True)
-    return all(results) if results else True
+            succeeded = False
+    return succeeded
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cleanup_tron_namespaces.py` & `paasta-tools-1.0.0/paasta_tools/cleanup_tron_namespaces.py`

 * *Files 21% similar despite different names*

```diff
@@ -22,15 +22,14 @@
 - -d <SOA_DIR>, --soa-dir <SOA_DIR>: Specify a SOA config dir to read from
 - --dry-run: Print namespaces to be deleted instead of deleting them
 """
 import argparse
 import sys
 
 from paasta_tools import tron_tools
-from paasta_tools.utils import paasta_print
 
 
 def parse_args():
     parser = argparse.ArgumentParser(description="Cleans up stale Tron namespaces.")
     parser.add_argument(
         "-d",
         "--soa-dir",
@@ -57,37 +56,35 @@
     namespaces = client.list_namespaces()
     expected_namespaces = tron_tools.get_tron_namespaces(
         cluster=cluster, soa_dir=args.soa_dir
     )
     to_delete = set(namespaces) - set(expected_namespaces) - {"MASTER"}
 
     if not to_delete:
-        paasta_print("No Tron namespaces to remove")
+        print("No Tron namespaces to remove")
         sys.exit(0)
 
     if args.dry_run:
-        paasta_print(
-            "Dry run, would have removed namespaces:\n  " + "\n  ".join(to_delete)
-        )
+        print("Dry run, would have removed namespaces:\n  " + "\n  ".join(to_delete))
         sys.exit(0)
 
     successes = []
     errors = []
     for namespace in to_delete:
         try:
             client.update_namespace(namespace, "")
             successes.append(namespace)
         except Exception as e:
             errors.append((namespace, e))
 
     if successes:
-        paasta_print("Successfully removed namespaces:\n", "\n  ".join(successes))
+        print("Successfully removed namespaces:\n", "\n  ".join(successes))
 
     if errors:
-        paasta_print(
+        print(
             "Failed to remove namespaces:\n  "
             + "\n  ".join(
                 [
                     "{namespace}: {error}".format(namespace=namespace, error=str(error))
                     for namespace, error in errors
                 ]
             )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/am_i_mesos_leader.py` & `paasta-tools-1.0.0/paasta_tools/am_i_mesos_leader.py`

 * *Files 17% similar despite different names*

```diff
@@ -18,21 +18,20 @@
 Check if this host is the current mesos-master leader.
 This is done by simply calling mesos_tools.is_mesos_leader.
 Exits 0 if this is the leader, and 1 if it isn't.
 """
 from sys import exit
 
 from paasta_tools.mesos_tools import is_mesos_leader
-from paasta_tools.utils import paasta_print
 
 
 def main():
     if is_mesos_leader():
-        paasta_print(True)
+        print(True)
         exit(0)
     else:
-        paasta_print(False)
+        print(False)
         exit(1)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/paasta_metastatus.py` & `paasta-tools-1.0.0/paasta_tools/paasta_metastatus.py`

 * *Files 15% similar despite different names*

```diff
@@ -10,66 +10,54 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import argparse
 import io
-import itertools
 import logging
 import sys
+from contextlib import redirect_stdout
 from typing import Mapping
 from typing import MutableSequence
 from typing import Optional
 from typing import Sequence
 from typing import Tuple
 
 import a_sync
-from marathon.exceptions import MarathonError
 from mypy_extensions import TypedDict
 
 from paasta_tools import __version__
-from paasta_tools.autoscaling.autoscaling_cluster_lib import AutoscalingInfo
-from paasta_tools.autoscaling.autoscaling_cluster_lib import (
-    get_autoscaling_info_for_all_resources,
-)
 from paasta_tools.cli.utils import get_instance_config
 from paasta_tools.kubernetes_tools import is_kubernetes_available
 from paasta_tools.kubernetes_tools import KubeClient
-from paasta_tools.marathon_tools import get_marathon_clients
-from paasta_tools.marathon_tools import get_marathon_servers
-from paasta_tools.marathon_tools import MarathonClient
-from paasta_tools.marathon_tools import MarathonClients
+from paasta_tools.kubernetes_tools import load_kubernetes_service_config
 from paasta_tools.mesos.exceptions import MasterNotAvailableException
 from paasta_tools.mesos.master import MesosMaster
 from paasta_tools.mesos.master import MesosState
+from paasta_tools.mesos_tools import get_mesos_config_path
 from paasta_tools.mesos_tools import get_mesos_leader
 from paasta_tools.mesos_tools import get_mesos_master
 from paasta_tools.mesos_tools import is_mesos_available
 from paasta_tools.metrics import metastatus_lib
 from paasta_tools.metrics.metastatus_lib import _GenericNodeGroupingFunctionT
 from paasta_tools.metrics.metastatus_lib import _KeyFuncRetT
 from paasta_tools.metrics.metastatus_lib import HealthCheckResult
 from paasta_tools.metrics.metastatus_lib import ResourceUtilization
 from paasta_tools.metrics.metastatus_lib import ResourceUtilizationDict
 from paasta_tools.utils import format_table
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import print_with_indent
-from paasta_tools.utils import set_paasta_print_file
 
 
 log = logging.getLogger("paasta_metastatus")
 logging.basicConfig()
 # kazoo can be really noisy - turn it down
 logging.getLogger("kazoo").setLevel(logging.CRITICAL)
-logging.getLogger("paasta_tools.autoscaling.autoscaling_cluster_lib").setLevel(
-    logging.ERROR
-)
 
 ServiceInstanceStats = TypedDict(
     "ServiceInstanceStats", {"mem": float, "cpus": float, "disk": float, "gpus": int}
 )
 
 
 class FatalError(Exception):
@@ -88,86 +76,54 @@
             "Group resource information of slaves grouped by attribute."
             "Note: This is only effective with -vv"
         ),
     )
     parser.add_argument("-t", "--threshold", type=int, default=90)
     parser.add_argument("--use-mesos-cache", action="store_true", default=False)
     parser.add_argument(
-        "-a",
-        "--autoscaling-info",
-        action="store_true",
-        default=False,
-        dest="autoscaling_info",
-    )
-    parser.add_argument(
         "-v",
         "--verbose",
         action="count",
         dest="verbose",
         default=0,
         help="Print out more output regarding the state of the cluster",
     )
     parser.add_argument(
         "-s",
         "--service",
+        dest="service_name",
         help=(
             "Show how many of a given service instance can be run on a cluster slave."
             "Note: This is only effective with -vvv and --instance must also be specified"
         ),
     )
     parser.add_argument(
         "-i",
         "--instance",
+        dest="instance_name",
         help=(
             "Show how many of a given service instance can be run on a cluster slave."
             "Note: This is only effective with -vvv and --service must also be specified"
         ),
     )
     return parser.parse_args(argv)
 
 
-def get_marathon_framework_ids(
-    marathon_clients: Sequence[MarathonClient],
-) -> Sequence[str]:
-    return [client.get_info().framework_id for client in marathon_clients]
-
-
 def _run_mesos_checks(
     mesos_master: MesosMaster, mesos_state: MesosState
 ) -> Sequence[HealthCheckResult]:
     mesos_state_status = metastatus_lib.get_mesos_state_status(mesos_state)
 
     metrics = a_sync.block(mesos_master.metrics_snapshot)
     mesos_metrics_status = metastatus_lib.get_mesos_resource_utilization_health(
         mesos_metrics=metrics, mesos_state=mesos_state
     )
     return mesos_state_status + mesos_metrics_status  # type: ignore
 
 
-def _run_marathon_checks(
-    marathon_clients: Sequence[MarathonClient],
-) -> Sequence[HealthCheckResult]:
-    try:
-        marathon_results = metastatus_lib.get_marathon_status(marathon_clients)
-        return marathon_results
-    except (MarathonError, ValueError) as e:
-        paasta_print(
-            PaastaColors.red(f"CRITICAL: Unable to contact Marathon cluster: {e}")
-        )
-        raise FatalError(2)
-
-
-def all_marathon_clients(
-    marathon_clients: MarathonClients,
-) -> Sequence[MarathonClient]:
-    return [
-        c for c in itertools.chain(marathon_clients.current, marathon_clients.previous)
-    ]
-
-
 def utilization_table_by_grouping(
     groupings: Sequence[str],
     grouping_function: _GenericNodeGroupingFunctionT,
     resource_info_dict_grouped: Mapping[_KeyFuncRetT, ResourceUtilizationDict],
     threshold: float,
     service_instance_stats: Optional[ServiceInstanceStats] = None,
 ) -> Tuple[Sequence[MutableSequence[str]], bool]:
@@ -183,16 +139,18 @@
         # Insert so agent count is still last
         static_headers.insert(-1, "Slots + Limiting Resource")
 
     all_rows = [[grouping.capitalize() for grouping in groupings] + static_headers]
     table_rows = []
 
     for grouping_values, resource_info_dict in resource_info_dict_grouped.items():
-        resource_utilizations = metastatus_lib.resource_utillizations_from_resource_info(
-            total=resource_info_dict["total"], free=resource_info_dict["free"]
+        resource_utilizations = (
+            metastatus_lib.resource_utillizations_from_resource_info(
+                total=resource_info_dict["total"], free=resource_info_dict["free"]
+            )
         )
         healthcheck_utilization_pairs = [
             metastatus_lib.healthcheck_result_resource_utilization_pair_for_resource_utilization(
                 utilization, threshold
             )
             for utilization in resource_utilizations
         ]
@@ -237,19 +195,26 @@
     )
 
 
 def utilization_table_by_grouping_from_kube(
     groupings: Sequence[str],
     threshold: float,
     kube_client: KubeClient,
+    *,
+    namespace: str,
     service_instance_stats: Optional[ServiceInstanceStats] = None,
 ) -> Tuple[Sequence[MutableSequence[str]], bool]:
     grouping_function = metastatus_lib.key_func_for_attribute_multi_kube(groupings)
-    resource_info_dict_grouped = metastatus_lib.get_resource_utilization_by_grouping_kube(
-        grouping_function, kube_client
+
+    resource_info_dict_grouped = (
+        metastatus_lib.get_resource_utilization_by_grouping_kube(
+            grouping_func=grouping_function,
+            kube_client=kube_client,
+            namespace=namespace,
+        )
     )
 
     return utilization_table_by_grouping(
         groupings,
         grouping_function,
         resource_info_dict_grouped,
         threshold,
@@ -317,115 +282,99 @@
     except Exception as e:
         log.error(
             f"Failed to get stats for service {service} instance {instance}: {str(e)}"
         )
         return None
 
 
-def _run_kube_checks(kube_client: KubeClient,) -> Sequence[HealthCheckResult]:
-    kube_status = metastatus_lib.get_kube_status(kube_client)
+def _run_kube_checks(
+    kube_client: KubeClient, namespace: str
+) -> Sequence[HealthCheckResult]:
+    kube_status = metastatus_lib.get_kube_status(kube_client, namespace)
     kube_metrics_status = metastatus_lib.get_kube_resource_utilization_health(
         kube_client=kube_client
     )
     return kube_status + kube_metrics_status  # type: ignore
 
 
 def print_output(argv: Optional[Sequence[str]] = None) -> None:
     mesos_available = is_mesos_available()
     kube_available = is_kubernetes_available()
 
     args = parse_args(argv)
 
     system_paasta_config = load_system_paasta_config()
-
+    service_config_dict = load_kubernetes_service_config(
+        service=args.service_name,
+        instance=args.instance_name,
+        cluster=system_paasta_config.get_cluster(),
+    )
     if mesos_available:
         master_kwargs = {}
         # we don't want to be passing False to not override a possible True
         # value from system config
         if args.use_mesos_cache:
             master_kwargs["use_mesos_cache"] = True
 
-        master = get_mesos_master(**master_kwargs)
-
-        marathon_servers = get_marathon_servers(system_paasta_config)
-        marathon_clients = all_marathon_clients(get_marathon_clients(marathon_servers))
+        master = get_mesos_master(
+            mesos_config_path=get_mesos_config_path(system_paasta_config),
+            **master_kwargs,
+        )
 
         try:
             mesos_state = a_sync.block(master.state)
             all_mesos_results = _run_mesos_checks(
                 mesos_master=master, mesos_state=mesos_state
             )
         except MasterNotAvailableException as e:
             # if we can't connect to master at all,
             # then bomb out early
-            paasta_print(PaastaColors.red("CRITICAL:  %s" % "\n".join(e.args)))
+            print(PaastaColors.red("CRITICAL:  %s" % "\n".join(e.args)))
             raise FatalError(2)
 
-        marathon_results = _run_marathon_checks(marathon_clients)
     else:
-        marathon_results = [
-            metastatus_lib.HealthCheckResult(
-                message="Marathon is not configured to run here", healthy=True
-            )
-        ]
         all_mesos_results = [
             metastatus_lib.HealthCheckResult(
                 message="Mesos is not configured to run here", healthy=True
             )
         ]
 
     if kube_available:
         kube_client = KubeClient()
-        kube_results = _run_kube_checks(kube_client)
+        kube_results = _run_kube_checks(
+            kube_client, service_config_dict.get_namespace()
+        )
     else:
         kube_results = [
             metastatus_lib.HealthCheckResult(
                 message="Kubernetes is not configured to run here", healthy=True
             )
         ]
 
     mesos_ok = all(metastatus_lib.status_for_results(all_mesos_results))
-    marathon_ok = all(metastatus_lib.status_for_results(marathon_results))
     kube_ok = all(metastatus_lib.status_for_results(kube_results))
 
     mesos_summary = metastatus_lib.generate_summary_for_check("Mesos", mesos_ok)
-    marathon_summary = metastatus_lib.generate_summary_for_check(
-        "Marathon", marathon_ok
-    )
     kube_summary = metastatus_lib.generate_summary_for_check("Kubernetes", kube_ok)
 
-    healthy_exit = True if all([mesos_ok, marathon_ok]) else False
+    healthy_exit = mesos_ok
 
-    paasta_print(f"Master paasta_tools version: {__version__}")
-    paasta_print("Mesos leader: %s" % get_mesos_leader())
+    print(f"Master paasta_tools version: {__version__}")
+    print("Mesos leader: %s" % get_mesos_leader())
     metastatus_lib.print_results_for_healthchecks(
         mesos_summary, mesos_ok, all_mesos_results, args.verbose
     )
     if args.verbose > 1 and mesos_available:
         print_with_indent("Resources Grouped by %s" % ", ".join(args.groupings), 2)
         all_rows, healthy_exit = utilization_table_by_grouping_from_mesos_state(
             groupings=args.groupings, threshold=args.threshold, mesos_state=mesos_state
         )
         for line in format_table(all_rows):
             print_with_indent(line, 4)
 
-        if args.autoscaling_info:
-            print_with_indent("Autoscaling resources:", 2)
-            headers = [
-                field.replace("_", " ").capitalize()
-                for field in AutoscalingInfo._fields
-            ]
-            table = [headers] + [
-                [str(x) for x in asi]
-                for asi in get_autoscaling_info_for_all_resources(mesos_state)
-            ]
-
-            for line in format_table(table):
-                print_with_indent(line, 4)
-
         if args.verbose >= 3:
             print_with_indent("Per Slave Utilization", 2)
             cluster = system_paasta_config.get_cluster()
             service_instance_stats = get_service_instance_stats(
                 args.service, args.instance, cluster
             )
             if service_instance_stats:
@@ -445,30 +394,27 @@
             # 1 for per-slave resources, so delete it.
             for row in all_rows:
                 row.pop()
 
             for line in format_table(all_rows):
                 print_with_indent(line, 4)
     metastatus_lib.print_results_for_healthchecks(
-        marathon_summary, marathon_ok, marathon_results, args.verbose
-    )
-    metastatus_lib.print_results_for_healthchecks(
         kube_summary, kube_ok, kube_results, args.verbose
     )
     if args.verbose > 1 and kube_available:
         print_with_indent("Resources Grouped by %s" % ", ".join(args.groupings), 2)
         all_rows, healthy_exit = utilization_table_by_grouping_from_kube(
-            groupings=args.groupings, threshold=args.threshold, kube_client=kube_client
+            groupings=args.groupings,
+            threshold=args.threshold,
+            kube_client=kube_client,
+            namespace=service_config_dict.get_namespace(),
         )
         for line in format_table(all_rows):
             print_with_indent(line, 4)
 
-        if args.autoscaling_info:
-            print_with_indent("No autoscaling resources for Kubernetes", 2)
-
         if args.verbose >= 3:
             print_with_indent("Per Node Utilization", 2)
             cluster = system_paasta_config.get_cluster()
             service_instance_stats = get_service_instance_stats(
                 args.service, args.instance, cluster
             )
             if service_instance_stats:
@@ -479,14 +425,15 @@
             # modifications to the healthy_exit variable here, because we don't
             # care about a single node having high usage.
             all_rows, _ = utilization_table_by_grouping_from_kube(
                 groupings=args.groupings + ["hostname"],
                 threshold=args.threshold,
                 kube_client=kube_client,
                 service_instance_stats=service_instance_stats,
+                namespace=service_config_dict.get_namespace(),
             )
             # The last column from utilization_table_by_grouping_from_kube is "Agent count", which will always be
             # 1 for per-node resources, so delete it.
             for row in all_rows:
                 row.pop()
 
             for line in format_table(all_rows):
@@ -495,15 +442,15 @@
     if not healthy_exit:
         raise FatalError(2)
 
 
 def get_output(argv: Optional[Sequence[str]] = None) -> Tuple[str, int]:
     output = io.StringIO()
     exit_code = 1
-    with set_paasta_print_file(output):
+    with redirect_stdout(output):
         exit_code = 0
         try:
             print_output(argv)
         except FatalError as e:
             exit_code = e.exit_code
     ret = output.getvalue()
     return ret, exit_code
```

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos_tools.py` & `paasta-tools-1.0.0/paasta_tools/mesos_tools.py`

 * *Files 3% similar despite different names*

```diff
@@ -55,44 +55,60 @@
 from paasta_tools.mesos.task import Task
 from paasta_tools.utils import DeployBlacklist
 from paasta_tools.utils import DeployWhitelist
 from paasta_tools.utils import format_table
 from paasta_tools.utils import get_user_agent
 from paasta_tools.utils import load_system_paasta_config
 from paasta_tools.utils import PaastaColors
+from paasta_tools.utils import SystemPaastaConfig
 from paasta_tools.utils import TimeoutError
 
-MARATHON_FRAMEWORK_NAME_PREFIX = "marathon"
-
 ZookeeperHostPath = namedtuple("ZookeeperHostPath", ["host", "path"])
-SlaveTaskCount = namedtuple("SlaveTaskCount", ["count", "batch_count", "slave"])
+SlaveTaskCount = namedtuple("SlaveTaskCount", ["count", "slave"])
 
 DEFAULT_MESOS_CLI_CONFIG_LOCATION = "/nail/etc/mesos-cli.json"
 
+TERMINAL_STATES = (
+    "TASK_ERROR",
+    "TASK_KILLED",
+    "TASK_FAILED",
+    "TASK_FINISHED",
+    "TASK_DROPPED",
+    "TASK_GONE",
+    "TASK_GONE_BY_OPERATOR",
+)
+
 log = logging.getLogger(__name__)
 log.addHandler(logging.NullHandler())
 
 
-def get_mesos_config_path():
+def get_mesos_config_path(
+    system_paasta_config: Optional[SystemPaastaConfig] = None,
+) -> str:
     """
     Determine where to find the configuration for mesos-cli.
     """
-    return (
-        load_system_paasta_config()
-        .get_mesos_cli_config()
-        .get("path", DEFAULT_MESOS_CLI_CONFIG_LOCATION)
+    if system_paasta_config is None:
+        system_paasta_config = load_system_paasta_config()
+
+    return system_paasta_config.get_mesos_cli_config().get(
+        "path", DEFAULT_MESOS_CLI_CONFIG_LOCATION
     )
 
 
-def get_mesos_config():
-    return load_mesos_config(get_mesos_config_path())
+def get_mesos_config(mesos_config_path: Optional[str] = None) -> Dict:
+    if mesos_config_path is None:
+        mesos_config_path = get_mesos_config_path()
+    return load_mesos_config(mesos_config_path)
 
 
-def get_mesos_master(**overrides: Any) -> MesosMaster:
-    config = get_mesos_config()
+def get_mesos_master(
+    mesos_config_path: Optional[str] = None, **overrides: Any
+) -> MesosMaster:
+    config = get_mesos_config(mesos_config_path)
     for k, v in overrides.items():
         config[k] = v
     return MesosMaster(config)
 
 
 MY_HOSTNAME = socket.getfqdn()
 MESOS_MASTER_PORT = 5050
@@ -105,21 +121,21 @@
 
 class MesosTailLines(NamedTuple):
     stdout: List[str]
     stderr: List[str]
     error_message: str
 
 
-def get_mesos_leader() -> str:
+def get_mesos_leader(mesos_config_path: Optional[str] = None) -> str:
     """Get the current mesos-master leader's hostname.
     Attempts to determine this by using mesos.cli to query ZooKeeper.
 
     :returns: The current mesos-master hostname"""
     try:
-        url = get_mesos_master().host
+        url = get_mesos_master(mesos_config_path).host
     except mesos_exceptions.MasterNotAvailableException:
         log.debug("mesos.cli failed to provide the master host")
         raise
     log.debug("mesos.cli thinks the master host is: %s" % url)
     hostname = urlparse(url).hostname
     log.debug("The parsed master hostname is: %s" % hostname)
     # This check is necessary, as if we parse a value such as 'localhost:5050',
@@ -141,71 +157,77 @@
     """Check if a hostname is the current mesos leader.
 
     :param hostname: The hostname to query mesos-master on
     :returns: True if hostname is the mesos-master leader, False otherwise"""
     return get_mesos_leader() == hostname
 
 
+class MesosLeaderUnavailable(Exception):
+    pass
+
+
 def find_mesos_leader(cluster):
-    """ Find the leader with redirect given one mesos master.
-    """
+    """Find the leader with redirect given one mesos master."""
     master = (
         load_system_paasta_config().get_cluster_fqdn_format().format(cluster=cluster)
     )
     if master is None:
         raise ValueError("Mesos master is required to find leader")
 
     url = f"http://{master}:{MESOS_MASTER_PORT}/redirect"
-    response = requests.get(url)
+    try:
+        # Timeouts here are for connect, read
+        response = requests.get(url, timeout=(5, 30))
+    except Exception as e:
+        raise MesosLeaderUnavailable(e)
     hostname = urlparse(response.url).hostname
     return f"{hostname}:{MESOS_MASTER_PORT}"
 
 
 async def get_current_tasks(job_id: str) -> List[Task]:
-    """ Returns a list of all the tasks with a given job id.
+    """Returns a list of all the tasks with a given job id.
     :param job_id: the job id of the tasks.
     :return tasks: a list of mesos.cli.Task.
     """
     mesos_master = get_mesos_master()
     framework_tasks = await mesos_master.tasks(fltr=job_id, active_only=False)
     return framework_tasks
 
 
 def is_task_running(task: Task) -> bool:
     return task["state"] == "TASK_RUNNING"
 
 
 def filter_running_tasks(tasks: Collection[Task]) -> List[Task]:
-    """ Filters those tasks where it's state is TASK_RUNNING.
+    """Filters those tasks where it's state is TASK_RUNNING.
     :param tasks: a list of mesos.cli.Task
     :return filtered: a list of running tasks
     """
     return [task for task in tasks if is_task_running(task)]
 
 
 def filter_not_running_tasks(tasks: Collection[Task]) -> List[Task]:
-    """ Filters those tasks where it's state is *not* TASK_RUNNING.
+    """Filters those tasks where it's state is *not* TASK_RUNNING.
     :param tasks: a list of mesos.cli.Task
     :return filtered: a list of tasks *not* running
     """
     return [task for task in tasks if not is_task_running(task)]
 
 
 async def get_running_tasks_from_frameworks(job_id=""):
-    """ Will include tasks from active and completed frameworks
+    """Will include tasks from active and completed frameworks
     but NOT orphaned tasks
     """
     active_framework_tasks = await get_current_tasks(job_id)
     running_tasks = filter_running_tasks(active_framework_tasks)
     return running_tasks
 
 
 async def get_all_running_tasks() -> Collection[Task]:
-    """ Will include all running tasks; for now orphans are not included
-    """
+    """Will include all running tasks; for now orphans are not included"""
     framework_tasks = await get_current_tasks("")
     mesos_master = get_mesos_master()
     framework_tasks += await mesos_master.orphan_tasks()
     running_tasks = filter_running_tasks(framework_tasks)
     return running_tasks
 
 
@@ -260,15 +282,15 @@
     :param job_id: the job id.
     :return tasks: a list of mesos.Task.
     """
     return [task for task in tasks if job_id in task["id"]]
 
 
 async def get_non_running_tasks_from_frameworks(job_id: str = "") -> List[Task]:
-    """ Will include tasks from active and completed frameworks
+    """Will include tasks from active and completed frameworks
     but NOT orphaned tasks
     """
     active_framework_tasks = await get_current_tasks(job_id)
     not_running_tasks = filter_not_running_tasks(active_framework_tasks)
     return not_running_tasks
 
 
@@ -833,39 +855,36 @@
 
 
 async def get_mesos_task_count_by_slave(
     mesos_state: MesosState,
     slaves_list: Sequence[Dict] = None,
     pool: Optional[str] = None,
 ) -> List[Dict]:
-    """Get counts of running tasks per mesos slave. Also include separate count of batch tasks
+    """Get counts of running tasks per mesos slave.
 
     :param mesos_state: mesos state dict
     :param slaves_list: a list of slave dicts to count running tasks for.
     :param pool: pool of slaves to return (None means all)
     :returns: list of slave dicts {'task_count': SlaveTaskCount}
     """
     all_mesos_tasks = await get_all_running_tasks()  # empty string = all app ids
     slaves = {
-        slave["id"]: {"count": 0, "slave": slave, "batch_count": 0}
+        slave["id"]: {"count": 0, "slave": slave}
         for slave in mesos_state.get("slaves", [])
     }
     for task in all_mesos_tasks:
         try:
             task_slave = await task.slave()
             if task_slave["id"] not in slaves:
                 log.debug("Slave {} not found for task".format(task_slave["id"]))
                 continue
             else:
                 slaves[task_slave["id"]]["count"] += 1
                 task_framework = await task.framework()
                 log.debug(f"Task framework: {task_framework.name}")
-                # Marathon is only framework that runs service. Others are batch.
-                if not task_framework.name.startswith(MARATHON_FRAMEWORK_NAME_PREFIX):
-                    slaves[task_slave["id"]]["batch_count"] += 1
         except SlaveDoesNotExist:
             log.debug(
                 "Tried to get mesos slaves for task {}, but none existed.".format(
                     task["id"]
                 )
             )
             continue
@@ -884,19 +903,17 @@
     else:
         slaves_with_counts = [
             {"task_counts": SlaveTaskCount(**slave_counts)}
             for slave_counts in slaves.values()
         ]
     for slave in slaves_with_counts:
         log.debug(
-            "Slave: {}, running {} tasks, "
-            "including {} batch tasks".format(
+            "Slave: {}, running {} tasks".format(
                 slave["task_counts"].slave["hostname"],
                 slave["task_counts"].count,
-                slave["task_counts"].batch_count,
             )
         )
     return slaves_with_counts
 
 
 def get_count_running_tasks_on_slave(hostname: str) -> int:
     """Return the number of tasks running on a particular slave
@@ -1020,26 +1037,23 @@
             srv_port = int(re.findall("[0-9]+", executor["resources"]["ports"])[0])
         else:
             srv_port = None
         srv_list.append((srv_name, srv_instance, srv_port))
     return srv_list
 
 
-def is_task_terminal(task: MesosTask,) -> bool:
+def is_task_terminal(
+    task: MesosTask,
+) -> bool:
     """Return whether a given mesos task is terminal.
 
     Terminal states are documented in
     http://mesos.apache.org/api/latest/java/org/apache/mesos/Protos.TaskState.html
 
     :param task: the task to be inspected
     :returns: a boolean indicating if the task is considered to be in a terminal state
     """
-    return task["state"] in [
-        "TASK_ERROR",
-        "TASK_KILLED",
-        "TASK_FAILED",
-        "TASK_FINISHED",
-    ]
+    return task["state"] in TERMINAL_STATES
 
 
 def is_mesos_available() -> bool:
     return Path(get_mesos_config_path()).exists()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cleanup_kubernetes_cr.py` & `paasta-tools-1.0.0/paasta_tools/cleanup_kubernetes_cr.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,14 +26,15 @@
 from typing import Sequence
 
 from paasta_tools.kubernetes_tools import CustomResourceDefinition
 from paasta_tools.kubernetes_tools import delete_custom_resource
 from paasta_tools.kubernetes_tools import KubeClient
 from paasta_tools.kubernetes_tools import list_custom_resources
 from paasta_tools.kubernetes_tools import load_custom_resource_definitions
+from paasta_tools.kubernetes_tools import paasta_prefixed
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import load_all_configs
 from paasta_tools.utils import load_system_paasta_config
 
 log = logging.getLogger(__name__)
 
 
@@ -84,15 +85,15 @@
     soa_dir: str,
     cluster: str,
     custom_resource_definitions: Sequence[CustomResourceDefinition],
 ) -> bool:
     cluster_crds = {
         crd.spec.names.kind
         for crd in kube_client.apiextensions.list_custom_resource_definition(
-            label_selector="paasta.yelp.com/service"
+            label_selector=paasta_prefixed("service")
         ).items
     }
     log.debug(f"CRDs found: {cluster_crds}")
     results = []
     for crd in custom_resource_definitions:
         if crd.kube_kind.singular not in cluster_crds:
             # TODO: kube_kind.singular seems to correspond to `crd.names.kind`
```

### Comparing `paasta-tools-0.92.1/paasta_tools/paasta_native_serviceinit.py` & `paasta-tools-1.0.0/paasta_tools/paasta_native_serviceinit.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,22 +1,21 @@
 from paasta_tools.frameworks.native_scheduler import MESOS_TASK_SPACER
 from paasta_tools.mesos_tools import status_mesos_tasks_verbose
 from paasta_tools.utils import calculate_tail_lines
 from paasta_tools.utils import compose_job_id
-from paasta_tools.utils import paasta_print
 
 
 def perform_command(command, service, instance, cluster, verbose, soa_dir):
     tail_lines = calculate_tail_lines(verbose_level=verbose)
 
     # We have to add a spacer at the end to make sure we only return
     # things for service.main and not service.main_foo
     task_id_prefix = "{}{}".format(compose_job_id(service, instance), MESOS_TASK_SPACER)
 
     if command == "status":
-        paasta_print(
+        print(
             status_mesos_tasks_verbose(
                 job_id=task_id_prefix,
                 get_short_task_id=lambda x: x,
                 tail_lines=tail_lines,
             )
         )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/flink_tools.py` & `paasta-tools-1.0.0/paasta_tools/monkrelaycluster_tools.py`

 * *Files 21% similar despite different names*

```diff
@@ -6,191 +6,141 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import json
-from typing import Any
 from typing import List
 from typing import Mapping
 from typing import Optional
 
-import requests
 import service_configuration_lib
-from mypy_extensions import TypedDict
 
-from paasta_tools.kubernetes_tools import InvalidJobNameError
-from paasta_tools.kubernetes_tools import NoConfigurationForServiceError
 from paasta_tools.kubernetes_tools import sanitised_cr_name
 from paasta_tools.long_running_service_tools import LongRunningServiceConfig
 from paasta_tools.long_running_service_tools import LongRunningServiceConfigDict
 from paasta_tools.utils import BranchDictV2
 from paasta_tools.utils import deep_merge_dictionaries
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import load_service_instance_config
 from paasta_tools.utils import load_v2_deployments_json
 
 
-FLINK_INGRESS_PORT = 31080
-FLINK_DASHBOARD_TIMEOUT_SECONDS = 5
+class MonkRelayClusterDeploymentConfigDict(LongRunningServiceConfigDict, total=False):
+    replicas: int
 
 
-class TaskManagerConfig(TypedDict, total=False):
-    instances: int
+class MonkRelayClusterDeploymentConfig(LongRunningServiceConfig):
+    config_dict: MonkRelayClusterDeploymentConfigDict
 
-
-class FlinkDeploymentConfigDict(LongRunningServiceConfigDict, total=False):
-    taskmanager: TaskManagerConfig
-
-
-class FlinkDeploymentConfig(LongRunningServiceConfig):
-    config_dict: FlinkDeploymentConfigDict
-
-    config_filename_prefix = "flink"
+    config_filename_prefix = "monkrelays"
 
     def __init__(
         self,
         service: str,
         cluster: str,
         instance: str,
-        config_dict: FlinkDeploymentConfigDict,
+        config_dict: MonkRelayClusterDeploymentConfigDict,
         branch_dict: Optional[BranchDictV2],
         soa_dir: str = DEFAULT_SOA_DIR,
     ) -> None:
 
         super().__init__(
             cluster=cluster,
             instance=instance,
             service=service,
             soa_dir=soa_dir,
             config_dict=config_dict,
             branch_dict=branch_dict,
         )
 
+    def get_instances(self, with_limit: bool = True) -> int:
+        return self.config_dict.get("replicas", 1)
+
     def validate(
         self,
-        params: List[str] = [
-            "cpus",
-            "mem",
-            "security",
-            "dependencies_reference",
-            "deploy_group",
-        ],
+        params: List[str] = None,
     ) -> List[str]:
         # Use InstanceConfig to validate shared config keys like cpus and mem
+        # TODO: add mem back to this list once we fix PAASTA-15582 and
+        # move to using the same units as flink/marathon etc.
+        if params is None:
+            params = [
+                "cpus",
+                "security",
+                "dependencies_reference",
+                "deploy_group",
+            ]
+
         error_msgs = super().validate(params=params)
 
         if error_msgs:
             name = self.get_instance()
             return [f"{name}: {msg}" for msg in error_msgs]
         else:
             return []
 
-    # Since Flink services are stateful, losing capacity is not transparent to the users
-    def get_replication_crit_percentage(self) -> int:
-        return self.config_dict.get("replication_threshold", 100)
 
-
-def load_flink_instance_config(
+def load_monkrelaycluster_instance_config(
     service: str,
     instance: str,
     cluster: str,
     load_deployments: bool = True,
     soa_dir: str = DEFAULT_SOA_DIR,
-) -> FlinkDeploymentConfig:
-    """Read a service instance's configuration for Flink.
+) -> MonkRelayClusterDeploymentConfig:
+    """Read a service instance's configuration for MonkRelayCluster.
 
     If a branch isn't specified for a config, the 'branch' key defaults to
     paasta-${cluster}.${instance}.
 
     :param service: The service name
     :param instance: The instance of the service to retrieve
     :param cluster: The cluster to read the configuration for
     :param load_deployments: A boolean indicating if the corresponding deployments.json for this service
                              should also be loaded
     :param soa_dir: The SOA configuration directory to read from
     :returns: A dictionary of whatever was in the config for the service instance"""
     general_config = service_configuration_lib.read_service_configuration(
         service, soa_dir=soa_dir
     )
-    flink_conf_file = "flink-%s" % cluster
-    instance_configs = service_configuration_lib.read_extra_service_information(
-        service, flink_conf_file, soa_dir=soa_dir
+    instance_config = load_service_instance_config(
+        service, instance, "monkrelays", cluster, soa_dir=soa_dir
     )
-
-    if instance.startswith("_"):
-        raise InvalidJobNameError(
-            f"Unable to load kubernetes job config for {service}.{instance} as instance name starts with '_'"
-        )
-    if instance not in instance_configs:
-        raise NoConfigurationForServiceError(
-            f"{instance} not found in config file {soa_dir}/{service}/{flink_conf_file}.yaml."
-        )
-
     general_config = deep_merge_dictionaries(
-        overrides=instance_configs[instance], defaults=general_config
+        overrides=instance_config, defaults=general_config
     )
 
     branch_dict: Optional[BranchDictV2] = None
     if load_deployments:
         deployments_json = load_v2_deployments_json(service, soa_dir=soa_dir)
-        temp_instance_config = FlinkDeploymentConfig(
+        temp_instance_config = MonkRelayClusterDeploymentConfig(
             service=service,
             cluster=cluster,
             instance=instance,
             config_dict=general_config,
             branch_dict=None,
             soa_dir=soa_dir,
         )
         branch = temp_instance_config.get_branch()
         deploy_group = temp_instance_config.get_deploy_group()
         branch_dict = deployments_json.get_branch_dict(service, branch, deploy_group)
 
-    return FlinkDeploymentConfig(
+    return MonkRelayClusterDeploymentConfig(
         service=service,
         cluster=cluster,
         instance=instance,
         config_dict=general_config,
         branch_dict=branch_dict,
         soa_dir=soa_dir,
     )
 
 
 # TODO: read this from CRD in service configs
 def cr_id(service: str, instance: str) -> Mapping[str, str]:
     return dict(
         group="yelp.com",
         version="v1alpha1",
-        namespace="paasta-flinks",
-        plural="flinks",
+        namespace="paasta-monkrelays",
+        plural="monkrelays",
         name=sanitised_cr_name(service, instance),
     )
-
-
-def get_flink_ingress_url_root(cluster: str) -> str:
-    return f"http://flink.k8s.paasta-{cluster}.yelp:{FLINK_INGRESS_PORT}/"
-
-
-def _dashboard_get(service: str, instance: str, cluster: str, path: str) -> str:
-    root = get_flink_ingress_url_root(cluster)
-    name = sanitised_cr_name(service, instance)
-    url = f"{root}{name}/{path}"
-    response = requests.get(url, timeout=FLINK_DASHBOARD_TIMEOUT_SECONDS)
-    response.raise_for_status()
-    return response.text
-
-
-def get_flink_jobmanager_overview(
-    service: str, instance: str, cluster: str
-) -> Mapping[str, Any]:
-    try:
-        response = _dashboard_get(service, instance, cluster, "overview")
-        return json.loads(response)
-    except requests.RequestException as e:
-        url = e.request.url
-        err = e.response or str(e)
-        raise ValueError(f"failed HTTP request to Jobmanager dashboard {url}: {err}")
-    except json.JSONDecodeError as e:
-        raise ValueError(f"JSON decoding error from Jobmanager dashboard: {e}")
-    except ConnectionError as e:
-        raise ValueError(f"failed HTTP request to Jobmanager dashboard: {e}")
```

### Comparing `paasta-tools-0.92.1/paasta_tools/check_cassandracluster_services_replication.py` & `paasta-tools-1.0.0/paasta_tools/check_cassandracluster_services_replication.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/autoscaling/forecasting.py` & `paasta-tools-1.0.0/paasta_tools/autoscaling/forecasting.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,12 @@
 from paasta_tools.autoscaling.utils import get_autoscaling_component
 from paasta_tools.autoscaling.utils import register_autoscaling_component
+from paasta_tools.long_running_service_tools import (
+    DEFAULT_UWSGI_AUTOSCALING_MOVING_AVERAGE_WINDOW,
+)
 
 
 FORECAST_POLICY_KEY = "forecast_policy"
 
 
 def get_forecast_policy(name):
     """
@@ -35,15 +38,17 @@
     window_end, _ = historical_load[-1]
     window_begin = window_end - window_size
     return window_historical_load(historical_load, window_begin, window_end)
 
 
 @register_autoscaling_component("moving_average", FORECAST_POLICY_KEY)
 def moving_average_forecast_policy(
-    historical_load, moving_average_window_seconds=1800, **kwargs
+    historical_load,
+    moving_average_window_seconds=DEFAULT_UWSGI_AUTOSCALING_MOVING_AVERAGE_WINDOW,
+    **kwargs,
 ):
     """Does a simple average of all historical load data points within the moving average window. Weights all data
     points within the window equally."""
 
     windowed_data = trailing_window_historical_load(
         historical_load, moving_average_window_seconds
     )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/autoscaling/load_boost.py` & `paasta-tools-1.0.0/paasta_tools/autoscaling/load_boost.py`

 * *Files 1% similar despite different names*

```diff
@@ -85,16 +85,15 @@
     except Exception as e:
         # Fail gracefully in the face of ANY error
         log.error(f"get_boost failed with: {e}")
         return current_load
 
 
 def get_boost_factor(zk_boost_path: str) -> float:
-    """This function returns the boost factor value if a boost is active
-    """
+    """This function returns the boost factor value if a boost is active"""
     current_time = get_time()
 
     with ZookeeperPool() as zk:
         boost_values = get_boost_values(zk_boost_path, zk)
         if current_time < boost_values.end_time:
             return boost_values.boost_factor
         else:
@@ -141,15 +140,15 @@
 
     otherwise just zk_boost_path is enough.
     """
     if factor < MIN_BOOST_FACTOR:
         log.error(f"Cannot set a boost factor smaller than {MIN_BOOST_FACTOR}")
         return False
 
-    if factor > MAX_BOOST_FACTOR:
+    if not override and factor > MAX_BOOST_FACTOR:
         log.warning(
             "Boost factor {} does not sound reasonable. Defaulting to {}".format(
                 factor, MAX_BOOST_FACTOR
             )
         )
         factor = MAX_BOOST_FACTOR
```

### Comparing `paasta-tools-0.92.1/paasta_tools/autoscaling/pause_service_autoscaler.py` & `paasta-tools-1.0.0/paasta_tools/autoscaling/pause_service_autoscaler.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,76 +1,77 @@
 import os
 import time
 from datetime import datetime
 
 import pytz
 from tzlocal import get_localzone as tzlocal_get_localzone
 
+import paasta_tools.paastaapi.models as paastamodels
 from paasta_tools.api import client
-from paasta_tools.utils import paasta_print
 
 
 def get_localzone():
     if "TZ" in os.environ:
         return pytz.timezone(os.environ["TZ"])
     else:
         return tzlocal_get_localzone()
 
 
+def print_paused_message(pause_time):
+    local_tz = get_localzone()
+    paused_readable = local_tz.localize(datetime.fromtimestamp(pause_time)).strftime(
+        "%F %H:%M:%S %Z"
+    )
+    print(f"Service autoscaler is paused until {paused_readable}")
+
+
 def get_service_autoscale_pause_time(cluster):
-    api = client.get_paasta_api_client(cluster=cluster, http_res=True)
+    api = client.get_paasta_oapi_client(cluster=cluster, http_res=True)
     if not api:
-        paasta_print(
-            "Could not connect to paasta api. Maybe you misspelled the cluster?"
-        )
+        print("Could not connect to paasta api. Maybe you misspelled the cluster?")
         return 1
-    pause_time, http = api.service_autoscaler.get_service_autoscaler_pause().result()
-    if http.status_code == 500:
-        paasta_print("Could not connect to zookeeper server")
+    pause_time, status, _ = api.default.get_service_autoscaler_pause(
+        _return_http_data_only=False
+    )
+    if status == 500:
+        print("Could not connect to zookeeper server")
         return 2
 
     pause_time = float(pause_time)
     if pause_time < time.time():
-        paasta_print("Service autoscaler is not paused")
+        print("Service autoscaler is not paused")
     else:
-        local_tz = get_localzone()
-        paused_readable = local_tz.localize(
-            datetime.fromtimestamp(pause_time)
-        ).strftime("%F %H:%M:%S %Z")
-        paasta_print(f"Service autoscaler is paused until {paused_readable}")
+        print_paused_message(pause_time)
 
     return 0
 
 
 def update_service_autoscale_pause_time(cluster, mins):
-    api = client.get_paasta_api_client(cluster=cluster, http_res=True)
+    api = client.get_paasta_oapi_client(cluster=cluster, http_res=True)
     if not api:
-        paasta_print(
-            "Could not connect to paasta api. Maybe you misspelled the cluster?"
-        )
+        print("Could not connect to paasta api. Maybe you misspelled the cluster?")
         return 1
-    body = {"minutes": mins}
-    res, http = api.service_autoscaler.update_service_autoscaler_pause(
-        json_body=body
-    ).result()
-    if http.status_code == 500:
-        paasta_print("Could not connect to zookeeper server")
+    res, status, _ = api.default.update_service_autoscaler_pause(
+        paastamodels.InlineObject(minutes=int(mins)), _return_http_data_only=False
+    )
+    if status == 500:
+        print("Could not connect to zookeeper server")
         return 2
 
-    paasta_print(f"Service autoscaler is paused for {mins}")
+    print(f"Service autoscaler is paused for {mins} minutes")
     return 0
 
 
 def delete_service_autoscale_pause_time(cluster):
-    api = client.get_paasta_api_client(cluster=cluster, http_res=True)
+    api = client.get_paasta_oapi_client(cluster=cluster, http_res=True)
     if not api:
-        paasta_print(
-            "Could not connect to paasta api. Maybe you misspelled the cluster?"
-        )
+        print("Could not connect to paasta api. Maybe you misspelled the cluster?")
         return 1
-    res, http = api.service_autoscaler.delete_service_autoscaler_pause().result()
-    if http.status_code == 500:
-        paasta_print("Could not connect to zookeeper server")
+    res, status, _ = api.default.delete_service_autoscaler_pause(
+        _return_http_data_only=False
+    )
+    if status == 500:
+        print("Could not connect to zookeeper server")
         return 2
 
-    paasta_print("Service autoscaler is unpaused")
+    print("Service autoscaler is unpaused")
     return 0
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring/kill_orphaned_docker_containers.py` & `paasta-tools-1.0.0/paasta_tools/monitoring/kill_orphaned_docker_containers.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 import sys
 
 import a_sync
 
 from paasta_tools import mesos_tools
 from paasta_tools.utils import get_docker_client
 from paasta_tools.utils import get_running_mesos_docker_containers
-from paasta_tools.utils import paasta_print
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         description=(
             "Cross references running containers with task ids from the mesos slave"
             " and optionally kills them."
@@ -47,27 +46,27 @@
         )
         if mesos_task_id not in running_mesos_task_ids:
             orphaned_containers.append(
                 (container["Names"][0].strip("/"), mesos_task_id)
             )
 
     if orphaned_containers:
-        paasta_print(
+        print(
             "CRIT: Docker containers are orphaned: {}{}".format(
                 ", ".join(
                     f"{container_name} ({mesos_task_id})"
                     for container_name, mesos_task_id in orphaned_containers
                 ),
                 " and will be killed" if args.force else "",
             )
         )
         if args.force:
             for container_name, mesos_task_id in orphaned_containers:
                 docker_client.kill(container_name)
         sys.exit(1)
     else:
-        paasta_print("OK: All mesos task IDs accounted for")
+        print("OK: All mesos task IDs accounted for")
         sys.exit(0)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring/check_capacity.py` & `paasta-tools-1.0.0/paasta_tools/monitoring/check_capacity.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,19 +13,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import argparse
 import json
 import sys
 from collections import defaultdict
 
-from bravado.exception import HTTPError
-
-from paasta_tools.api.client import get_paasta_api_client
+from paasta_tools.api.client import get_paasta_oapi_client
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 
 
 def parse_capacity_check_options():
     parser = argparse.ArgumentParser(
         formatter_class=argparse.ArgumentDefaultsHelpFormatter
     )
 
@@ -104,15 +101,15 @@
     checks = [o for o in overrides if o["groupings"] == groupings]
     if len(checks) == 0:
         return default_check
     elif len(checks) == 1:
         return checks[0]
     else:
         group_string = ", ".join([f"{k}: {v}" for k, v in groupings.items()])
-        paasta_print("UNKNOWN Multiple overrides specified for %s" % group_string)
+        print("UNKNOWN Multiple overrides specified for %s" % group_string)
         sys.exit(3)
 
 
 def read_overrides(override_file):
     if override_file:
         with open(override_file, "r") as f:
             return json.loads(f.read())
@@ -126,36 +123,36 @@
     cluster = (
         options.cluster
         if options.cluster is not None
         else system_paasta_config.get_cluster()
     )
     value_to_check = options.type
 
-    client = get_paasta_api_client(cluster=cluster)
+    client = get_paasta_oapi_client(cluster=cluster)
     if client is None:
-        paasta_print("UNKNOWN Failed to load paasta api client")
+        print("UNKNOWN Failed to load paasta api client")
         sys.exit(3)
 
     overrides = read_overrides(options.overrides)
 
     attributes = options.attributes.split(",")
 
     try:
-        resource_use = client.resources.resources(groupings=attributes).result()
-    except HTTPError as e:
-        paasta_print("UNKNOWN received exception from paasta api:\n\t%s" % e)
+        resource_use = client.resources.resources(groupings=attributes)
+    except client.api_error as e:
+        print(f"UNKNOWN received exception from paasta api:\n\t%s{e}")
         sys.exit(3)
 
     default_check = {
         "warn": {"cpus": options.warn, "mem": options.warn, "disk": options.warn},
         "crit": {"cpus": options.crit, "mem": options.crit, "disk": options.crit},
     }
 
     failures = defaultdict(list)
-    for usage_value in resource_use:
+    for usage_value in resource_use.value:
         check = get_check_from_overrides(
             overrides, default_check, usage_value["groupings"]
         )
         usage_percent = calc_percent_usage(usage_value, value_to_check)
         for c in ["crit", "warn"]:
             if usage_percent > check[c][value_to_check]:
                 failures[c].append(
@@ -169,24 +166,22 @@
                     }
                 )
                 break
 
     return_value = [0]
     if len(failures["crit"]) > 0:
         result = error_message(failures["crit"], "CRITICAL", cluster, value_to_check)
-        paasta_print(result)
+        print(result)
         return_value.append(2)
     if len(failures["warn"]) > 0:
         result = error_message(failures["warn"], "WARNING", cluster, value_to_check)
-        paasta_print(result)
+        print(result)
         return_value.append(1)
 
     if max(return_value) == 0:
-        paasta_print(
-            f"OK cluster {cluster} is below critical capacity in {value_to_check}"
-        )
+        print(f"OK cluster {cluster} is below critical capacity in {value_to_check}")
 
     sys.exit(max(return_value))
 
 
 if __name__ == "__main__":
     run_capacity_check()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring/check_marathon_has_apps.py` & `paasta-tools-1.0.0/paasta_tools/mesos/zookeeper.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,46 +1,37 @@
-#!/usr/bin/env python
-# Copyright 2015-2016 Yelp Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import sys
-
-from marathon.exceptions import InternalServerError
-from marathon.exceptions import MarathonError
+import contextlib
 
-from paasta_tools import marathon_tools
-from paasta_tools.metrics.metastatus_lib import assert_marathon_apps
-from paasta_tools.utils import paasta_print
+import kazoo.client
+import kazoo.exceptions
+import kazoo.handlers.threading
 
 
-def check_marathon_apps():
-    clients = marathon_tools.get_list_of_marathon_clients()
-    if not clients:
-        paasta_print("UNKNOWN: Failed to load marathon clients.")
-        sys.exit(3)
+TIMEOUT = 1
 
-    try:
-        result = assert_marathon_apps(clients)
-    except (MarathonError, InternalServerError, ValueError) as e:
-        paasta_print("CRITICAL: Unable to connect to Marathon cluster: %s" % e)
-        sys.exit(2)
-
-    if result.healthy:
-        paasta_print("OK: " + result.message)
-        sys.exit(0)
-    else:
-        paasta_print(result.message)
-        sys.exit(2)
+# Helper for testing
+client_class = kazoo.client.KazooClient
 
 
-if __name__ == "__main__":
-    check_marathon_apps()
+@contextlib.contextmanager
+def client(*args, **kwargs):
+    zk = client_class(*args, **kwargs)
+    zk.start(timeout=TIMEOUT)
+    try:
+        yield zk
+    finally:
+        zk.stop()
+        zk.close()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring/check_mesos_quorum.py` & `paasta-tools-1.0.0/paasta_tools/monitoring/check_mesos_quorum.py`

 * *Files 24% similar despite different names*

```diff
@@ -11,22 +11,21 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import sys
 
 from paasta_tools.metrics.metastatus_lib import assert_quorum_size
-from paasta_tools.utils import paasta_print
 
 
 def check_mesos_quorum():
     result = assert_quorum_size()
     if result.healthy:
-        paasta_print("OK: " + result.message)
+        print("OK: " + result.message)
         sys.exit(0)
     else:
-        paasta_print(result.message)
+        print(result.message)
         sys.exit(2)
 
 
 if __name__ == "__main__":
     check_mesos_quorum()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring/check_mesos_duplicate_frameworks.py` & `paasta-tools-1.0.0/paasta_tools/monitoring/check_mesos_duplicate_frameworks.py`

 * *Files 7% similar despite different names*

```diff
@@ -16,15 +16,14 @@
 import sys
 
 from a_sync import block
 
 from paasta_tools.mesos.exceptions import MasterNotAvailableException
 from paasta_tools.mesos_tools import get_mesos_master
 from paasta_tools.metrics.metastatus_lib import assert_no_duplicate_frameworks
-from paasta_tools.utils import paasta_print
 
 
 def parse_args():
     parser = argparse.ArgumentParser()
 
     parser.add_argument(
         "--check",
@@ -40,21 +39,21 @@
 def check_mesos_no_duplicate_frameworks() -> None:
     options = parse_args()
     check = options.check.split(",")
     master = get_mesos_master()
     try:
         state = block(master.state)
     except MasterNotAvailableException as e:
-        paasta_print("CRITICAL: %s" % e.args[0])
+        print("CRITICAL: %s" % e.args[0])
         sys.exit(2)
 
     result = assert_no_duplicate_frameworks(state, check)
     if result.healthy:
-        paasta_print("OK: " + result.message)
+        print("OK: " + result.message)
         sys.exit(0)
     else:
-        paasta_print(result.message)
+        print(result.message)
         sys.exit(2)
 
 
 if __name__ == "__main__":
     check_mesos_no_duplicate_frameworks()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring/check_mesos_active_frameworks.py` & `paasta-tools-1.0.0/paasta_tools/monitoring/check_mesos_active_frameworks.py`

 * *Files 10% similar despite different names*

```diff
@@ -16,15 +16,14 @@
 import sys
 
 from a_sync import block
 
 from paasta_tools.mesos.exceptions import MasterNotAvailableException
 from paasta_tools.mesos_tools import get_mesos_master
 from paasta_tools.metrics.metastatus_lib import assert_frameworks_exist
-from paasta_tools.utils import paasta_print
 
 
 def parse_args():
     parser = argparse.ArgumentParser()
 
     parser.add_argument(
         "--expected",
@@ -41,21 +40,21 @@
 def check_mesos_active_frameworks() -> None:
     options = parse_args()
     expected = options.expected.split(",")
     master = get_mesos_master()
     try:
         state = block(master.state)
     except MasterNotAvailableException as e:
-        paasta_print("CRITICAL: %s" % e.args[0])
+        print("CRITICAL: %s" % e.args[0])
         sys.exit(2)
 
     result = assert_frameworks_exist(state, expected)
     if result.healthy:
-        paasta_print("OK: " + result.message)
+        print("OK: " + result.message)
         sys.exit(0)
     else:
-        paasta_print(result.message)
+        print(result.message)
         sys.exit(2)
 
 
 if __name__ == "__main__":
     check_mesos_active_frameworks()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring/__init__.py` & `paasta-tools-1.0.0/paasta_tools/api/__init__.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/paasta_service_config_loader.py` & `paasta-tools-1.0.0/paasta_tools/paasta_service_config_loader.py`

 * *Files 23% similar despite different names*

```diff
@@ -7,29 +7,32 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import copy
 import logging
+from typing import Any
 from typing import Dict
 from typing import Iterable
 from typing import List
 from typing import Tuple
 from typing import Type
 
-from service_configuration_lib import read_extra_service_information
 from service_configuration_lib import read_service_configuration
 
 from paasta_tools import utils
+from paasta_tools.autoscaling.utils import AutoscalingParamsDict
 from paasta_tools.utils import deep_merge_dictionaries
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import InstanceConfig_T
 from paasta_tools.utils import list_clusters
+from paasta_tools.utils import load_service_instance_configs
 from paasta_tools.utils import load_v2_deployments_json
 from paasta_tools.utils import NoDeploymentsAvailable
 
 
 log = logging.getLogger(__name__)
 log.addHandler(logging.NullHandler())
 
@@ -41,21 +44,21 @@
     :Example:
 
     >>> from paasta_tools.paasta_service_config_loader import PaastaServiceConfigLoader
     >>> from paasta_tools.utils import DEFAULT_SOA_DIR
     >>>
     >>> sc = PaastaServiceConfigLoader(service='fake_service', soa_dir=DEFAULT_SOA_DIR)
     >>>
-    >>> for instance in sc.instances(cluster='fake_cluster', instance_type='marathon'):
+    >>> for instance in sc.instances(cluster='fake_cluster', instance_type_class=KubernetesDeploymentConfig):
     ...     print(instance)
     ...
     main
     canary
     >>>
-    >>> for instance_config in sc.instance_configs(cluster='fake_cluster', instance_type='marathon'):
+    >>> for instance_config in sc.instance_configs(cluster='fake_cluster', instance_type_class=KubernetesDeploymentConfig):
     ...     print(instance_config.get_instance())
     ...
     main
     canary
     >>>
     """
 
@@ -90,31 +93,31 @@
 
     def instances(
         self, cluster: str, instance_type_class: Type[InstanceConfig_T]
     ) -> Iterable[str]:
         """Returns an iterator that yields instance names as strings.
 
         :param cluster: The cluster name
-        :param instance_type: One of paasta_tools.utils.INSTANCE_TYPES
+        :param instance_type_class: a subclass of InstanceConfig
         :returns: an iterator that yields instance names
         """
         if (cluster, instance_type_class) not in self._framework_configs:
             self._refresh_framework_config(cluster, instance_type_class)
         for instance in self._framework_configs.get((cluster, instance_type_class), []):
             yield instance
 
     def instance_configs(
         self, cluster: str, instance_type_class: Type[InstanceConfig_T]
     ) -> Iterable[InstanceConfig_T]:
         """Returns an iterator that yields InstanceConfig objects.
 
         :param cluster: The cluster name
-        :param instance_type: One of paasta_tools.utils.INSTANCE_TYPES
-        :returns: an iterator that yields instances of MarathonServiceConfig, etc.
-        :raises NotImplementedError: when it doesn't know how to create a config for instance_type
+        :param instance_type_class: a subclass of InstanceConfig
+        :returns: an iterator that yields instances of KubernetesDeploymentConfig, etc.
+        :raises NotImplementedError: when it doesn't know how to create a config for instance_type_class
         """
         if (cluster, instance_type_class) not in self._framework_configs:
             self._refresh_framework_config(cluster, instance_type_class)
         for instance, config in self._framework_configs.get(
             (cluster, instance_type_class), {}
         ).items():
             try:
@@ -128,18 +131,19 @@
         self, cluster: str, instance_type_class: Type[InstanceConfig_T]
     ):
         return f"{instance_type_class.config_filename_prefix}-{cluster}"
 
     def _refresh_framework_config(
         self, cluster: str, instance_type_class: Type[InstanceConfig_T]
     ):
-        conf_name = self._framework_config_filename(cluster, instance_type_class)
-        log.info("Reading configuration file: %s.yaml", conf_name)
-        instances = read_extra_service_information(
-            service_name=self._service, extra_info=conf_name, soa_dir=self._soa_dir
+        instances = load_service_instance_configs(
+            service=self._service,
+            instance_type=instance_type_class.config_filename_prefix,
+            cluster=cluster,
+            soa_dir=self._soa_dir,
         )
         self._framework_configs[(cluster, instance_type_class)] = instances
 
     def _get_branch_dict(
         self, cluster: str, instance: str, config: utils.InstanceConfig
     ) -> utils.BranchDictV2:
         if self._deployments_json is None:
@@ -165,24 +169,33 @@
     def _create_service_config(
         self,
         cluster: str,
         instance: str,
         config: utils.InstanceConfigDict,
         config_class: Type[InstanceConfig_T],
     ) -> InstanceConfig_T:
-        """Create a service instance's configuration for marathon.
+        """Create a service instance's configuration for kubernetes.
 
         :param cluster: The cluster to read the configuration for
         :param instance: The instance of the service to retrieve
         :param config: the framework instance config.
         :returns: An instance of config_class
         """
 
         merged_config = self._get_merged_config(config)
 
+        # These type: ignore annotations will go away once yelpsoa-configs is migrated (COREJAVA-1339)
+        if (
+            "autoscaling" in merged_config
+            and "metrics_providers" not in merged_config["autoscaling"]  # type: ignore
+        ):
+            merged_config["autoscaling"] = transform_autoscaling_params_dict(  # type: ignore
+                copy.deepcopy(merged_config["autoscaling"])  # type: ignore
+            )
+
         temp_instance_config = config_class(
             service=self._service,
             cluster=cluster,
             instance=instance,
             config_dict=merged_config,
             branch_dict=None,
             soa_dir=self._soa_dir,
@@ -194,7 +207,22 @@
             service=self._service,
             cluster=cluster,
             instance=instance,
             config_dict=merged_config,
             branch_dict=branch_dict,
             soa_dir=self._soa_dir,
         )
+
+
+# Remove this once yelpsoa-configs is using the new format (COREJAVA-1339)
+def transform_autoscaling_params_dict(
+    old_autoscaling_params: Dict[str, Any]
+) -> AutoscalingParamsDict:
+    metrics_provider_type = old_autoscaling_params.pop("metrics_provider", "cpu")
+    old_autoscaling_params["type"] = metrics_provider_type
+    scaledown_policies = old_autoscaling_params.pop("scaledown_policies", None)
+
+    new_autoscaling_params = {"metrics_providers": [old_autoscaling_params]}
+    if scaledown_policies is not None:
+        new_autoscaling_params["scaledown_policies"] = scaledown_policies
+
+    return new_autoscaling_params  # type: ignore
```

### Comparing `paasta-tools-0.92.1/paasta_tools/firewall_update.py` & `paasta-tools-1.0.0/paasta_tools/firewall_update.py`

 * *Files 1% similar despite different names*

```diff
@@ -79,16 +79,18 @@
     # Main loop waiting on inotify file events
     inotify = Inotify(block_duration_s=1)  # event_gen blocks for 1 second
     inotify.add_watch(args.synapse_service_dir.encode(), IN_MOVED_TO | IN_MODIFY)
     services_by_dependencies_time = 0
 
     for event in inotify.event_gen():  # blocks for only up to 1 second at a time
         if services_by_dependencies_time + args.update_secs < time.time():
-            services_by_dependencies = smartstack_dependencies_of_running_firewalled_services(
-                soa_dir=args.soa_dir
+            services_by_dependencies = (
+                smartstack_dependencies_of_running_firewalled_services(
+                    soa_dir=args.soa_dir
+                )
             )
             services_by_dependencies_time = time.time()
 
         if event is None:
             continue
 
         process_inotify_event(
```

### Comparing `paasta-tools-0.92.1/paasta_tools/paasta_maintenance.py` & `paasta-tools-1.0.0/paasta_tools/check_services_replication_tools.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-#!/usr/bin/env python
-# Copyright 2015-2016 Yelp Inc.
+# Copyright 2015-2019 Yelp Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,257 +10,268 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import argparse
 import logging
 import sys
-import traceback
-from socket import getfqdn
-from socket import gethostbyname
-from socket import gethostname
-
-from paasta_tools import mesos_maintenance
-from paasta_tools import utils
-from paasta_tools.marathon_tools import get_expected_instance_count_for_namespace
-from paasta_tools.marathon_tools import load_marathon_service_config
-from paasta_tools.marathon_tools import marathon_services_running_here
-from paasta_tools.smartstack_tools import backend_is_up
-from paasta_tools.smartstack_tools import get_backends
-from paasta_tools.smartstack_tools import get_replication_for_services
-from paasta_tools.smartstack_tools import ip_port_hostname_from_svname
-from paasta_tools.smartstack_tools import load_smartstack_info_for_service
-from paasta_tools.utils import paasta_print
+from typing import Any
+from typing import Callable
+from typing import Container
+from typing import List
+from typing import Optional
+from typing import Sequence
+from typing import Tuple
+from typing import Type
+
+from mypy_extensions import Arg
+from mypy_extensions import NamedArg
+
+from paasta_tools.kubernetes_tools import get_all_managed_namespaces
+from paasta_tools.kubernetes_tools import get_all_nodes
+from paasta_tools.kubernetes_tools import get_all_pods
+from paasta_tools.kubernetes_tools import KubeClient
+from paasta_tools.kubernetes_tools import V1Node
+from paasta_tools.kubernetes_tools import V1Pod
+from paasta_tools.metrics import metrics_lib
+from paasta_tools.monitoring_tools import ReplicationChecker
+from paasta_tools.paasta_service_config_loader import PaastaServiceConfigLoader
+from paasta_tools.smartstack_tools import KubeSmartstackEnvoyReplicationChecker
+from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import InstanceConfig_T
+from paasta_tools.utils import list_services
+from paasta_tools.utils import load_system_paasta_config
+from paasta_tools.utils import SPACER
+
+try:
+    import yelp_meteorite
+except ImportError:
+    yelp_meteorite = None
 
 log = logging.getLogger(__name__)
 
+CheckServiceReplication = Callable[
+    [
+        Arg(InstanceConfig_T, "instance_config"),
+        Arg(Sequence[V1Pod], "all_pods"),
+        Arg(Any, "replication_checker"),
+        NamedArg(bool, "dry_run"),
+    ],
+    Optional[bool],
+]
 
-def parse_args():
-    """Parses the command line arguments passed to this script"""
+
+def parse_args() -> argparse.Namespace:
     parser = argparse.ArgumentParser()
     parser.add_argument(
         "-d",
-        "--duration",
-        type=mesos_maintenance.parse_timedelta,
-        default="1h",
-        help="Duration of the maintenance window. Any pytimeparse unit is supported.",
+        "--soa-dir",
+        dest="soa_dir",
+        metavar="SOA_DIR",
+        default=DEFAULT_SOA_DIR,
+        help="define a different soa config directory",
     )
     parser.add_argument(
-        "-s",
-        "--start",
-        type=mesos_maintenance.parse_datetime,
-        default=str(mesos_maintenance.now()),
-        help="Time to start the maintenance window. Defaults to now.",
+        "--crit",
+        dest="under_replicated_crit_pct",
+        type=float,
+        default=10,
+        help="The percentage of under replicated service instances past which "
+        "the script will return a critical status",
     )
     parser.add_argument(
-        "action",
-        choices=[
-            "cluster_status",
-            "down",
-            "drain",
-            "is_host_down",
-            "is_host_drained",
-            "is_host_draining",
-            "is_hosts_past_maintenance_end",
-            "is_hosts_past_maintenance_start",
-            "is_safe_to_drain",
-            "is_safe_to_kill",
-            "schedule",
-            "status",
-            "undrain",
-            "up",
-        ],
-        help="Action to perform on the specified hosts",
+        "--min-count-critical",
+        dest="min_count_critical",
+        type=int,
+        default=5,
+        help="The script will not return a critical status if the number of "
+        "under replicated service instances is below this number, even if the "
+        "percentage is above the critical percentage.",
     )
     parser.add_argument(
-        "hostname",
+        "service_instance_list",
         nargs="*",
-        default=[getfqdn()],
-        help="Hostname(s) of machine(s) to start draining. "
-        "You can specify <hostname>|<ip> to avoid querying DNS to determine the corresponding IP.",
+        help="The list of service instances to check",
+        metavar="SERVICE%sINSTANCE" % SPACER,
     )
     parser.add_argument(
-        "-v",
-        "--verbose",
-        action="count",
-        dest="verbose",
-        default=0,
-        help="Print out more output.",
-    )
-    return parser.parse_args()
-
-
-def is_safe_to_kill(hostname):
-    """Checks if a host has drained or reached its maintenance window
-    :param hostname: hostname to check
-    :returns: True or False
-    """
-    return mesos_maintenance.is_host_drained(
-        hostname
-    ) or mesos_maintenance.is_host_past_maintenance_start(hostname)
-
-
-def is_hostname_local(hostname):
-    return hostname == "localhost" or hostname == getfqdn() or hostname == gethostname()
-
-
-def is_safe_to_drain(hostname):
-    """Checks if a host has healthy tasks running locally that have low
-    replication in other places
-    :param hostname: hostname to check
-    :returns: True or False
-    """
-    if not is_hostname_local(hostname):
-        paasta_print(
-            "Due to the way is_safe_to_drain is implemented, it can only work on localhost."
-        )
-        return False
-    return not are_local_tasks_in_danger()
-
+        "-v", "--verbose", action="store_true", dest="verbose", default=False
+    )
+    parser.add_argument(
+        "--dry-run",
+        action="store_true",
+        dest="dry_run",
+        help="Print Sensu alert events and metrics instead of sending them",
+    )
+    parser.add_argument(
+        "--additional-namespaces",
+        help="full names of namespaces to check services replication for that don't match --namespace-prefix"
+        "Used only when service is kubernetes",
+        dest="additional_namespaces",
+        nargs="+",
+        # we default this to paasta since we always want to run this check on paasta namespace
+        # to avoid having two cron jobs running with two different namespace-prefix
+        default=["paasta"],
+    )
+    parser.add_argument(
+        "--eks",
+        help="This flag checks k8 services running on EKS",
+        dest="eks",
+        action="store_true",
+        default=False,
+    )
+    options = parser.parse_args()
+
+    return options
+
+
+def check_services_replication(
+    soa_dir: str,
+    cluster: str,
+    service_instances: Sequence[str],
+    instance_type_class: Type[InstanceConfig_T],
+    check_service_replication: CheckServiceReplication,
+    replication_checker: ReplicationChecker,
+    all_pods: Sequence[V1Pod],
+    dry_run: bool = False,
+) -> Tuple[int, int]:
+    service_instances_set = set(service_instances)
+    replication_statuses: List[bool] = []
+
+    for service in list_services(soa_dir=soa_dir):
+        service_config = PaastaServiceConfigLoader(service=service, soa_dir=soa_dir)
+        for instance_config in service_config.instance_configs(
+            cluster=cluster, instance_type_class=instance_type_class
+        ):
+            if (
+                service_instances_set
+                and f"{service}{SPACER}{instance_config.instance}"
+                not in service_instances_set
+            ):
+                continue
+            if instance_config.get_docker_image():
+                is_well_replicated = check_service_replication(
+                    instance_config=instance_config,
+                    all_pods=all_pods,
+                    replication_checker=replication_checker,
+                    dry_run=dry_run,
+                )
+                if is_well_replicated is not None:
+                    replication_statuses.append(is_well_replicated)
 
-def is_healthy_in_haproxy(local_port, backends):
-    local_ip = gethostbyname(gethostname())
-    for backend in backends:
-        ip, port, _ = ip_port_hostname_from_svname(backend["svname"])
-        if ip == local_ip and port == local_port:
-            if backend_is_up(backend):
-                log.debug("Found a healthy local backend: %s" % backend)
-                return True
             else:
-                log.debug("Found a unhealthy local backend: %s" % backend)
-                return False
-    log.debug("Couldn't find any haproxy backend listening on %s" % local_port)
-    return False
-
+                log.debug(
+                    "%s is not deployed. Skipping replication monitoring."
+                    % instance_config.job_id
+                )
 
-def synapse_replication_is_low(service, instance, system_paasta_config, local_backends):
-    crit_threshold = 80
-    cluster = system_paasta_config.get_cluster()
-    marathon_service_config = load_marathon_service_config(
-        service=service, instance=instance, cluster=cluster, load_deployments=False
-    )
-    reg_svc, reg_namespace, _, __ = utils.decompose_job_id(
-        marathon_service_config.get_registrations()
+    num_under_replicated = len(
+        [status for status in replication_statuses if status is False]
     )
-    # We only actually care about the replication of where we're registering
-    service, namespace = reg_svc, reg_namespace
+    return num_under_replicated, len(replication_statuses)
 
-    smartstack_replication_info = load_smartstack_info_for_service(
-        service=service,
-        namespace=namespace,
-        blacklist=[],
-        system_paasta_config=system_paasta_config,
-    )
-    expected_count = get_expected_instance_count_for_namespace(
-        service=service, namespace=namespace
-    )
-    expected_count_per_location = int(expected_count / len(smartstack_replication_info))
-
-    synapse_name = utils.compose_job_id(service, namespace)
-    local_replication = get_replication_for_services(
-        synapse_host=system_paasta_config.get_default_synapse_host(),
-        synapse_port=system_paasta_config.get_synapse_port(),
-        synapse_haproxy_url_format=system_paasta_config.get_synapse_haproxy_url_format(),
-        services=[synapse_name],
-    )
-    num_available = local_replication.get(synapse_name, 0)
-    under_replicated, ratio = utils.is_under_replicated(
-        num_available, expected_count_per_location, crit_threshold
-    )
-    log.info(
-        "Service %s.%s has %d out of %d expected instances"
-        % (service, instance, num_available, expected_count_per_location)
-    )
-    return under_replicated
-
-
-def are_local_tasks_in_danger():
-    try:
-        system_paasta_config = utils.load_system_paasta_config()
-        local_services = marathon_services_running_here()
-        local_backends = get_backends(
-            service=None,
-            synapse_host=system_paasta_config.get_default_synapse_host(),
-            synapse_port=system_paasta_config.get_synapse_port(),
-            synapse_haproxy_url_format=system_paasta_config.get_synapse_haproxy_url_format(),
-        )
-        for service, instance, port in local_services:
-            log.info(f"Inspecting {service}.{instance} on {port}")
-            if is_healthy_in_haproxy(
-                port, local_backends
-            ) and synapse_replication_is_low(
-                service, instance, system_paasta_config, local_backends=local_backends
-            ):
-                log.warning(
-                    "{}.{} on port {} is healthy but the service is in danger!".format(
-                        service, instance, port
-                    )
-                )
-                return True
-        return False
-    except Exception:
-        log.warning(traceback.format_exc())
-        return False
 
+def emit_cluster_replication_metrics(
+    pct_under_replicated: float,
+    cluster: str,
+    scheduler: str,
+    dry_run: bool = False,
+) -> None:
+    metric_name = "paasta.pct_services_under_replicated"
+    if dry_run:
+        print(f"Would've sent value {pct_under_replicated} for metric '{metric_name}'")
+    else:
+        meteorite_dims = {"paasta_cluster": cluster, "scheduler": scheduler}
+        gauge = yelp_meteorite.create_gauge(metric_name, meteorite_dims)
+        gauge.set(pct_under_replicated)
 
-def paasta_maintenance():
-    """Manipulate the maintenance state of a PaaSTA host.
-    :returns: None
-    """
-    args = parse_args()
 
-    if args.verbose >= 2:
+def main(
+    instance_type_class: Type[InstanceConfig_T],
+    check_service_replication: CheckServiceReplication,
+    namespace: str = None,
+) -> None:
+    args = parse_args()
+    if args.verbose:
         logging.basicConfig(level=logging.DEBUG)
-    elif args.verbose == 1:
-        logging.basicConfig(level=logging.INFO)
     else:
         logging.basicConfig(level=logging.WARNING)
 
-    action = args.action
-    hostnames = args.hostname
+    system_paasta_config = load_system_paasta_config()
+    cluster = system_paasta_config.get_cluster()
+    replication_checker: ReplicationChecker
+
+    timer = metrics_lib.system_timer(dimensions=dict(eks=args.eks, cluster=cluster))
+
+    timer.start()
+
+    if namespace:
+        pods, nodes = get_kubernetes_pods_and_nodes(namespace=namespace)
+        replication_checker = KubeSmartstackEnvoyReplicationChecker(
+            nodes=nodes,
+            system_paasta_config=system_paasta_config,
+        )
+    else:
+        pods, nodes = get_kubernetes_pods_and_nodes(
+            additional_namespaces=args.additional_namespaces,
+        )
+        replication_checker = KubeSmartstackEnvoyReplicationChecker(
+            nodes=nodes,
+            system_paasta_config=system_paasta_config,
+        )
+
+    count_under_replicated, total = check_services_replication(
+        soa_dir=args.soa_dir,
+        cluster=cluster,
+        service_instances=args.service_instance_list,
+        instance_type_class=instance_type_class,
+        check_service_replication=check_service_replication,
+        replication_checker=replication_checker,
+        all_pods=pods,
+        dry_run=args.dry_run,
+    )
+    pct_under_replicated = 0 if total == 0 else 100 * count_under_replicated / total
+    if yelp_meteorite is not None:
+        emit_cluster_replication_metrics(
+            pct_under_replicated,
+            cluster,
+            scheduler="kubernetes",
+            dry_run=args.dry_run,
+        )
+
+    exit_code = 0
+    if (
+        pct_under_replicated >= args.under_replicated_crit_pct
+        and count_under_replicated >= args.min_count_critical
+    ):
+        log.critical(
+            f"{pct_under_replicated}% of instances ({count_under_replicated}/{total}) "
+            f"are under replicated (past {args.under_replicated_crit_pct} is critical)!"
+        )
+        exit_code = 2
+
+    timer.stop(tmp_dimensions={"result": exit_code})
+    logging.info(
+        f"Stopping timer for {cluster} (eks={args.eks}) with result {exit_code}: {timer()}ms elapsed"
+    )
+    sys.exit(exit_code)
+
+
+def get_kubernetes_pods_and_nodes(
+    namespace: Optional[str] = None,
+    additional_namespaces: Optional[Container[str]] = None,
+) -> Tuple[List[V1Pod], List[V1Node]]:
+    kube_client = KubeClient()
 
-    if action != "status" and not hostnames:
-        paasta_print("You must specify one or more hostnames")
-        return
-
-    start = args.start
-    duration = args.duration
-
-    ret = "Done"
-    if action == "drain":
-        mesos_maintenance.drain(hostnames, start, duration)
-    elif action == "undrain":
-        mesos_maintenance.undrain(hostnames)
-    elif action == "down":
-        mesos_maintenance.down(hostnames)
-    elif action == "up":
-        mesos_maintenance.up(hostnames)
-    elif action == "status":
-        ret = mesos_maintenance.friendly_status()
-    elif action == "cluster_status":
-        ret = mesos_maintenance.status()
-    elif action == "schedule":
-        ret = mesos_maintenance.schedule()
-    elif action == "is_safe_to_drain":
-        ret = is_safe_to_drain(hostnames[0])
-    elif action == "is_safe_to_kill":
-        ret = is_safe_to_kill(hostnames[0])
-    elif action == "is_host_drained":
-        ret = mesos_maintenance.is_host_drained(hostnames[0])
-    elif action == "is_host_down":
-        ret = mesos_maintenance.is_host_down(hostnames[0])
-    elif action == "is_host_draining":
-        ret = mesos_maintenance.is_host_draining(hostnames[0])
-    elif action == "is_host_past_maintenance_start":
-        ret = mesos_maintenance.is_host_past_maintenance_start(hostnames[0])
-    elif action == "is_host_past_maintenance_end":
-        ret = mesos_maintenance.is_host_past_maintenance_end(hostnames[0])
+    all_pods: List[V1Pod] = []
+    if namespace:
+        all_pods = get_all_pods(kube_client=kube_client, namespace=namespace)
     else:
-        raise NotImplementedError("Action: '%s' is not implemented." % action)
-    paasta_print(ret)
-    return ret
+        all_managed_namespaces = get_all_managed_namespaces(kube_client)
+        for managed_namespace in all_managed_namespaces:
+            all_pods.extend(
+                get_all_pods(kube_client=kube_client, namespace=managed_namespace)
+            )
 
+    all_nodes = get_all_nodes(kube_client)
 
-if __name__ == "__main__":
-    if paasta_maintenance():
-        sys.exit(0)
-    sys.exit(1)
+    return all_pods, all_nodes
```

### Comparing `paasta-tools-0.92.1/paasta_tools/check_kubernetes_services_replication.py` & `paasta-tools-1.0.0/paasta_tools/check_kubernetes_services_replication.py`

 * *Files 15% similar despite different names*

```diff
@@ -29,85 +29,114 @@
 50, meaning if less than 50% of a service's backends are available, the script sends
 CRITICAL. If replication_threshold is defined in the yelpsoa config for a service
 instance then it will be used instead.
 """
 import logging
 from typing import Optional
 from typing import Sequence
+from typing import Union
 
+from paasta_tools import eks_tools
 from paasta_tools import kubernetes_tools
 from paasta_tools import monitoring_tools
 from paasta_tools.check_services_replication_tools import main
+from paasta_tools.check_services_replication_tools import parse_args
+from paasta_tools.eks_tools import EksDeploymentConfig
 from paasta_tools.kubernetes_tools import filter_pods_by_service_instance
 from paasta_tools.kubernetes_tools import is_pod_ready
 from paasta_tools.kubernetes_tools import KubernetesDeploymentConfig
 from paasta_tools.kubernetes_tools import V1Pod
 from paasta_tools.long_running_service_tools import get_proxy_port_for_instance
-from paasta_tools.smartstack_tools import KubeSmartstackReplicationChecker
+from paasta_tools.smartstack_tools import KubeSmartstackEnvoyReplicationChecker
 
 
 log = logging.getLogger(__name__)
+DEFAULT_ALERT_AFTER = "10m"
 
 
 def check_healthy_kubernetes_tasks_for_service_instance(
-    instance_config: KubernetesDeploymentConfig,
+    instance_config: Union[KubernetesDeploymentConfig, EksDeploymentConfig],
     expected_count: int,
     all_pods: Sequence[V1Pod],
+    dry_run: bool = False,
 ) -> None:
     si_pods = filter_pods_by_service_instance(
         pod_list=all_pods,
         service=instance_config.service,
         instance=instance_config.instance,
     )
     num_healthy_tasks = len([pod for pod in si_pods if is_pod_ready(pod)])
     log.info(
         f"Checking {instance_config.service}.{instance_config.instance} in kubernetes as it is not in smartstack"
     )
     monitoring_tools.send_replication_event_if_under_replication(
         instance_config=instance_config,
         expected_count=expected_count,
         num_available=num_healthy_tasks,
+        dry_run=dry_run,
     )
 
 
 def check_kubernetes_pod_replication(
-    instance_config: KubernetesDeploymentConfig,
-    all_tasks_or_pods: Sequence[V1Pod],
-    smartstack_replication_checker: KubeSmartstackReplicationChecker,
+    instance_config: Union[KubernetesDeploymentConfig, EksDeploymentConfig],
+    all_pods: Sequence[V1Pod],
+    replication_checker: KubeSmartstackEnvoyReplicationChecker,
+    dry_run: bool = False,
 ) -> Optional[bool]:
     """Checks a service's replication levels based on how the service's replication
-    should be monitored. (smartstack or k8s)
+    should be monitored. (smartstack/envoy or k8s)
 
-    :param instance_config: an instance of KubernetesDeploymentConfig
-    :param smartstack_replication_checker: an instance of KubeSmartstackReplicationChecker
+    :param instance_config: an instance of KubernetesDeploymentConfig or EksDeploymentConfig
+    :param replication_checker: an instance of KubeSmartstackEnvoyReplicationChecker
     """
+    default_alert_after = DEFAULT_ALERT_AFTER
     expected_count = instance_config.get_instances()
     log.info(
         "Expecting %d total tasks for %s" % (expected_count, instance_config.job_id)
     )
     proxy_port = get_proxy_port_for_instance(instance_config)
 
     registrations = instance_config.get_registrations()
+
+    # If this instance does not autoscale and only has 1 instance, set alert after to 20m.
+    # Otherwise, set it to 10 min.
+    if (
+        not instance_config.is_autoscaling_enabled()
+        and instance_config.get_instances() == 1
+    ):
+        default_alert_after = "20m"
+    if "monitoring" not in instance_config.config_dict:
+        instance_config.config_dict["monitoring"] = {}
+    instance_config.config_dict["monitoring"][
+        "alert_after"
+    ] = instance_config.config_dict["monitoring"].get(
+        "alert_after", default_alert_after
+    )
+
     # if the primary registration does not match the service_instance name then
     # the best we can do is check k8s for replication (for now).
     if proxy_port is not None and registrations[0] == instance_config.job_id:
-        is_well_replicated = monitoring_tools.check_smartstack_replication_for_instance(
+        is_well_replicated = monitoring_tools.check_replication_for_instance(
             instance_config=instance_config,
             expected_count=expected_count,
-            smartstack_replication_checker=smartstack_replication_checker,
+            replication_checker=replication_checker,
+            dry_run=dry_run,
         )
         return is_well_replicated
     else:
         check_healthy_kubernetes_tasks_for_service_instance(
             instance_config=instance_config,
             expected_count=expected_count,
-            all_pods=all_tasks_or_pods,
+            all_pods=all_pods,
+            dry_run=dry_run,
         )
         return None
 
 
 if __name__ == "__main__":
+    args = parse_args()
     main(
-        kubernetes_tools.KubernetesDeploymentConfig,
-        check_kubernetes_pod_replication,
-        namespace="paasta",
+        instance_type_class=eks_tools.EksDeploymentConfig
+        if args.eks
+        else kubernetes_tools.KubernetesDeploymentConfig,
+        check_service_replication=check_kubernetes_pod_replication,
     )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/oom_logger.py` & `paasta-tools-1.0.0/paasta_tools/oom_logger.py`

 * *Files 19% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
 paasta_oom_logger is supposed to be used as a syslog-ng destination.
 It looks for OOM events in the log, adds PaaSTA service and instance names
-and send JSON-encoded messages the Scribe stream 'tmp_paasta_oom_events'.
+and send JSON-encoded messages the stream 'tmp_paasta_oom_events'.
 
 syslog-ng.conf:
 
 destination paasta_oom_logger {
   program("exec /usr/bin/paasta_oom_logger" template("${UNIXTIME} ${HOST} ${MESSAGE}\n") );
 };
 
@@ -42,86 +42,134 @@
 from paasta_tools.cli.utils import get_instance_config
 from paasta_tools.utils import _log
 from paasta_tools.utils import DEFAULT_LOGLEVEL
 from paasta_tools.utils import get_docker_client
 from paasta_tools.utils import load_system_paasta_config
 
 
+# Sorry to any non-yelpers but this won't
+# do much as our metrics and logging libs
+# are not open source
 try:
     import yelp_meteorite
 except ImportError:
-    # Sorry to any non-yelpers but you won't
-    # get metrics emitted as our metrics lib
-    # is currently not open source
     yelp_meteorite = None
 
+try:
+    import clog
+except ImportError:
+    clog = None
+
 
 LogLine = namedtuple(
     "LogLine",
     [
         "timestamp",
         "hostname",
         "container_id",
         "cluster",
         "service",
         "instance",
         "process_name",
+        "mesos_container_id",
+        "mem_limit",
     ],
 )
 
 
 def capture_oom_events_from_stdin():
     process_name_regex = re.compile(
         r"^\d+\s[a-zA-Z0-9\-]+\s.*\]\s(.+)\sinvoked\soom-killer:"
     )
-    oom_regex = re.compile(
+    oom_regex_docker = re.compile(
         r"^(\d+)\s([a-zA-Z0-9\-]+)\s.*Task in /docker/(\w{12})\w+ killed as a"
     )
-    process_name = ""
+    oom_regex_kubernetes = re.compile(
+        r"""
+        ^(\d+)\s # timestamp
+        ([a-zA-Z0-9\-]+) # hostname
+        \s.*Task\sin\s/kubepods/(?:[a-zA-Z]+/)? # start of message; non capturing, optional group for the qos cgroup
+        pod[-\w]+/(\w{12})\w+\s # containerid
+        killed\sas\sa*  # eom
+        """,
+        re.VERBOSE,
+    )
+    oom_regex_kubernetes_structured = re.compile(
+        r"""
+        ^(\d+)\s # timestamp
+        ([a-zA-Z0-9\-]+) # hostname
+        \s.*oom-kill:.*task_memcg=/kubepods/(?:[a-zA-Z]+/)? # start of message; non-capturing, optional group for the qos cgroup
+        pod[-\w]+/(\w{12})\w+,.*$ # containerid
+        """,
+        re.VERBOSE,
+    )
+    oom_regex_kubernetes_systemd_cgroup = re.compile(
+        r"""
+        ^(\d+)\s # timestamp
+        ([a-zA-Z0-9\-]+) # hostname
+        \s.*oom-kill:.*task_memcg=/kubepods\.slice/[^,]+docker-(\w{12})\w+\.scope,.*$ # loosely match systemd slice and containerid
+        """,
+        re.VERBOSE,
+    )
+    event_detail_regexes = [
+        oom_regex_docker,
+        oom_regex_kubernetes,
+        oom_regex_kubernetes_structured,
+        oom_regex_kubernetes_systemd_cgroup,
+    ]
 
+    process_name = ""
     while True:
-        syslog = sys.stdin.readline()
+        try:
+            syslog = sys.stdin.readline()
+        except StopIteration:
+            break
         if not syslog:
             break
         r = process_name_regex.search(syslog)
         if r:
             process_name = r.group(1)
-        r = oom_regex.search(syslog)
-        if r:
-            yield (int(r.group(1)), r.group(2), r.group(3), process_name)
-            process_name = ""
+        for expression in event_detail_regexes:
+            r = expression.search(syslog)
+            if r:
+                yield (int(r.group(1)), r.group(2), r.group(3), process_name)
+                process_name = ""
+                break
 
 
 def get_container_env_as_dict(docker_inspect):
     env_vars = {}
     config = docker_inspect.get("Config")
     if config is not None:
         env = config.get("Env", [])
         for i in env:
             name, _, value = i.partition("=")
             env_vars[name] = value
     return env_vars
 
 
-def log_to_scribe(logger, log_line):
+def log_to_clog(log_line):
     """Send the event to 'tmp_paasta_oom_events'."""
     line = (
         '{"timestamp": %d, "hostname": "%s", "container_id": "%s", "cluster": "%s", '
-        '"service": "%s", "instance": "%s", "process_name": "%s"}'
+        '"service": "%s", "instance": "%s", "process_name": "%s", '
+        '"mesos_container_id": "%s", "mem_limit": "%s"}'
         % (
             log_line.timestamp,
             log_line.hostname,
             log_line.container_id,
             log_line.cluster,
             log_line.service,
             log_line.instance,
             log_line.process_name,
+            log_line.mesos_container_id,
+            log_line.mem_limit,
         )
     )
-    logger.log_line("tmp_paasta_oom_events", line)
+    clog.log_line("tmp_paasta_oom_events", line)
 
 
 def log_to_paasta(log_line):
     """Add the event to the standard PaaSTA logging backend."""
     line = "oom-killer killed {} on {} (container_id: {}).".format(
         "a %s process" % log_line.process_name
         if log_line.process_name
@@ -140,32 +188,43 @@
 
 
 def send_sfx_event(service, instance, cluster):
     if yelp_meteorite:
         service_instance_config = get_instance_config(
             service=service, instance=instance, cluster=cluster
         )
+        dimensions = {
+            "paasta_cluster": cluster,
+            "paasta_instance": instance,
+            "paasta_service": service,
+            "paasta_pool": service_instance_config.get_pool(),
+        }
         yelp_meteorite.events.emit_event(
             "paasta.service.oom_events",
-            dimensions={
-                "paasta_cluster": cluster,
-                "paasta_instance": instance,
-                "paasta_service": service,
-                "paasta_pool": service_instance_config.get_pool(),
-            },
+            dimensions=dimensions,
+        )
+        counter = yelp_meteorite.create_counter(
+            "paasta.service.oom_count",
+            default_dimensions=dimensions,
         )
+        counter.count()
 
 
 def main():
-    try:
-        from clog.loggers import ScribeLogger
-    except ImportError:
-        print("Scribe logger unavailable, exiting.", file=sys.stderr)
+    if clog is None:
+        print("CLog logger unavailable, exiting.", file=sys.stderr)
         sys.exit(1)
-    scribe_logger = ScribeLogger(host="169.254.255.254", port=1463, retry_interval=5)
+
+    clog.config.configure(
+        scribe_host="169.254.255.254",
+        scribe_port=1463,
+        monk_disable=False,
+        scribe_disable=False,
+    )
+
     cluster = load_system_paasta_config().get_cluster()
     client = get_docker_client()
     for (
         timestamp,
         hostname,
         container_id,
         process_name,
@@ -173,23 +232,27 @@
         try:
             docker_inspect = client.inspect_container(resource_id=container_id)
         except (APIError):
             continue
         env_vars = get_container_env_as_dict(docker_inspect)
         service = env_vars.get("PAASTA_SERVICE", "unknown")
         instance = env_vars.get("PAASTA_INSTANCE", "unknown")
+        mesos_container_id = env_vars.get("MESOS_CONTAINER_NAME", "mesos-null")
+        mem_limit = env_vars.get("PAASTA_RESOURCE_MEM", "unknown")
         log_line = LogLine(
             timestamp=timestamp,
             hostname=hostname,
             container_id=container_id,
             cluster=cluster,
             service=service,
             instance=instance,
             process_name=process_name,
+            mesos_container_id=mesos_container_id,
+            mem_limit=mem_limit,
         )
-        log_to_scribe(scribe_logger, log_line)
+        log_to_clog(log_line)
         log_to_paasta(log_line)
         send_sfx_event(service, instance, cluster)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/native_mesos_scheduler.py` & `paasta-tools-1.0.0/paasta_tools/native_mesos_scheduler.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/mock_patch_checker.py` & `paasta-tools-1.0.0/paasta_tools/contrib/mock_patch_checker.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/env python3.6
+#!/usr/bin/env python3.8
 import ast
 import sys
 
 
 class MockChecker(ast.NodeVisitor):
     def __init__(self):
         self.errors = 0
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/mass-deploy-tag.sh` & `paasta-tools-1.0.0/paasta_tools/contrib/mass-deploy-tag.sh`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 	jq_output=$(jq -r '.v1 | to_entries | .[] | .key + " " + .value.docker_image' /nail/etc/services/${service}/deployments.json)
 	if [ -z "$jq_output" ] ; then
 		echo "${service} has no deployments. Skipping."
 		continue
 	fi
 	# git_repo=$(paasta info -s ${service} | grep -oP 'Git Repo: \K.*$')
 	git_repo=$(script -qc "paasta info -s ${service}" | sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" | grep 'Git Repo: ' | cut -d' ' -f3)
-	default_git_repo=git@git.yelpcorp.com:services/${service}.git
+	default_git_repo=git@github.yelpcorp.com:services/${service}.git
 	echo git clone ${git_repo-${default_git_repo}} ${service}
 	git clone ${git_repo} ${service}
 	unset git_repo
 	cd ${service}
 	while read -r deploy_group sha; do
 		deploy_group=$(echo ${deploy_group} | sed 's/^.*paasta-//')
 		sha=$(echo ${sha} | sed 's/^.*paasta-//')
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/emit_allocated_cpu_metrics.py` & `paasta-tools-1.0.0/paasta_tools/contrib/emit_allocated_cpu_metrics.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,13 +38,13 @@
                     "paasta.service.instances", dimensions
                 )
                 gauge.set(service_instance_config.get_instances())
 
 
 def main():
     logging.basicConfig(level=logging.INFO)
-    for thing in ["marathon", "adhoc"]:
+    for thing in ["adhoc"]:
         emit_metrics_for_type(thing)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/graceful_container_drain.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/list_deploy_queue.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,183 +1,171 @@
 #!/usr/bin/env python
-import errno
+# Copyright 2015-2020 Yelp Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import datetime
 import json
-import os.path
-import random
-import shlex
 import sys
-import threading
-from subprocess import PIPE
-from subprocess import Popen
-from subprocess import STDOUT
-
-import yaml
-
-from paasta_tools.utils import paasta_print
-
-
-def _timeout(process):
-    """Helper function for _run. It terminates the process.
-    Doesn't raise OSError, if we try to terminate a non-existing
-    process as there can be a very small window between poll() and kill()
-    """
-    if process.poll() is None:
-        try:
-            # sending SIGKILL to the process
-            process.kill()
-        except OSError as e:
-            # No such process error
-            # The process could have been terminated meanwhile
-            if e.errno != errno.ESRCH:
-                raise
-
-
-def cmd(command):
-    stream = False
-    timeout = 60
-    output = []
+import time
+import traceback
+from typing import List
+from typing import Union
+
+from paasta_tools.api.client import get_paasta_oapi_client
+from paasta_tools.cli.utils import lazy_choices_completer
+from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import format_table
+from paasta_tools.utils import list_clusters
+from paasta_tools.utils import load_system_paasta_config
+from paasta_tools.utils import PaastaColors
+
+
+def add_subparser(
+    subparsers,
+) -> None:
+    list_deploy_queue_parser = subparsers.add_parser(
+        "list-deploy-queue",
+        help="Display the deploy queue for a PaaSTA cluster",
+        description="",  # TODO
+    )
+
+    list_deploy_queue_parser.add_argument(
+        "-c",
+        "--cluster",
+        dest="cluster",
+        help="The cluster for which to display the deploy queue",
+    ).completer = lazy_choices_completer(list_clusters)
+    list_deploy_queue_parser.add_argument(
+        "--json",
+        dest="json",
+        action="store_true",
+        default=False,
+        help="Output the raw API response JSON",
+    )
+    list_deploy_queue_parser.add_argument(
+        "-d",
+        "--soa-dir",
+        dest="soa_dir",
+        metavar="SOA_DIR",
+        default=DEFAULT_SOA_DIR,
+        help="define a different soa config directory",
+    )
+
+    list_deploy_queue_parser.set_defaults(command=list_deploy_queue)
+
+
+def list_deploy_queue(args) -> int:
+    cluster = args.cluster
+    all_clusters = list_clusters(soa_dir=args.soa_dir)
+    if cluster not in all_clusters:
+        print(
+            f"{cluster} does not appear to be a valid cluster. Run `paasta "
+            "list-clusters` to see available options."
+        )
+        return 1
+
+    system_paasta_config = load_system_paasta_config()
+    client = get_paasta_oapi_client(cluster, system_paasta_config, http_res=True)
+    if not client:
+        print("Cannot get a paasta API client")
+        return 1
+
     try:
-        process = Popen(shlex.split(command), stdout=PIPE, stderr=STDOUT, stdin=None)
-        process.name = command
-        # start the timer if we specified a timeout
-        if timeout:
-            proctimer = threading.Timer(timeout, _timeout, (process,))
-            proctimer.start()
-        for line in iter(process.stdout.readline, ""):
-            if stream:
-                paasta_print(line.rstrip("\n"))
-            output.append(line.rstrip("\n"))
-        # when finished, get the exit code
-        returncode = process.wait()
-    except OSError as e:
-        output.append(e.strerror.rstrip("\n"))
-        returncode = e.errno
-    except (KeyboardInterrupt, SystemExit):
-        # need to clean up the timing thread here
-        if timeout:
-            proctimer.cancel()
-        raise
+        deploy_queues = client.default.deploy_queue()
+    except client.api_error as exc:
+        print(PaastaColors.red(exc.reason))
+        return exc.status
+    except (client.connection_error, client.timeout_error) as exc:
+        print(PaastaColors.red(f"Could not connect to API: {exc.__class__.__name__}"))
+        return 1
+    except Exception as exc:
+        tb = sys.exc_info()[2]
+        print(PaastaColors.red(f"Exception when talking to the API: {exc}"))
+        print("".join(traceback.format_tb(tb)))
+        return 1
+
+    if args.json:
+        json.dump(deploy_queues.to_dict(), sys.stdout)
     else:
-        # Stop the timer
-        if timeout:
-            proctimer.cancel()
-    if returncode == -9:
-        output.append(f"Command '{command}' timed out (longer than {timeout}s)")
-    return returncode, "\n".join(output)
-
-
-def abort(message):
-    paasta_print(message)
-    sys.exit(1)
-
-
-def condquit(rc, message):
-    if rc != 0:
-        paasta_print(message)
-        sys.exit(rc)
-
-
-def docker_env_to_dict(environment_array):
-    environment = {}
-    for kv in environment_array:
-        k, v = kv.split("=", 1)
-        environment[k] = v
-    return environment
-
-
-def get_proxy_port(service_name, instance_name):
-    smartstack_yaml = "/nail/etc/services/%s/smartstack.yaml" % service_name
-    proxy_port = None
-    if os.path.exists(smartstack_yaml):
-        with open(smartstack_yaml, "r") as stream:
-            data = yaml.safe_load(stream)
-            if instance_name in data:
-                proxy_port = data[instance_name].get("proxy_port", None)
-    return proxy_port
-
-
-def get_last_killed(drained_apps, service, instance):
-    """look "back" in drained_apps, find at what time
-    the given (service, instance) was last killed"""
-    last_killed_t = -1000
-    for drained_app in reversed(drained_apps):
-        dt, dservice, dinstance = drained_app
-        if dservice == service and dinstance == instance:
-            last_killed_t = dt
-            break
-    return last_killed_t
-
-
-def has_all_paasta_env(environment):
-    for k in ("PAASTA_SERVICE", "PAASTA_INSTANCE", "MARATHON_PORT"):
-        if k not in environment:
-            return False
-
-    return True
-
-
-def main():
-    rc, output = cmd("sudo docker ps -q")
-    condquit(rc, "docker ps")
-    lines = output.split("\n")
-
-    if len(lines) == 0:
-        abort("no containers running")
-
-    running_container_ids = []
-
-    for line in lines:
-        if len(line) != 12:
-            abort("%s doesn't look like a container ID" % line)
-        running_container_ids.append(line.rstrip())
-
-    random.shuffle(running_container_ids)
-
-    drained_apps = []  # ( t_killed, service, instance )
-    smartstack_grace_sleep = 10
-    between_containers_grace_sleep = 10
-    min_kill_interval = 60  # minimum time to wait between same service.instance kills
-    hadown_expire_in_seconds = 120
-    t = 0
-
-    for container_id in running_container_ids:
-        rc, output = cmd("sudo docker inspect %s" % container_id)
-        condquit(rc, "docker inspect %s" % container_id)
-        docker_inspect_data = json.loads(output)
-        environment = docker_env_to_dict(docker_inspect_data[0]["Config"]["Env"])
-        if not has_all_paasta_env(environment):
-            paasta_print(
-                "# WARNING: %s is not a paasta container, skipping)" % (container_id)
-            )
-            continue
-        service = environment["PAASTA_SERVICE"]
-        instance = environment["PAASTA_INSTANCE"]
-        paasta_print(f"# {service}.{instance}")
-        marathon_port = int(environment["MARATHON_PORT"])
-        proxy_port = get_proxy_port(service, instance)
-        paasta_print(
-            f"# {container_id},{service},{instance},{proxy_port},{marathon_port}"
-        )
-        paasta_print(
-            "sudo hadown -P {} -e $((`date +'%s'`+{})) {}.{}".format(
-                marathon_port, hadown_expire_in_seconds, service, instance
-            )
+        formatted_deploy_queues = format_deploy_queues(deploy_queues, cluster)
+        print(formatted_deploy_queues)
+
+    return 0
+
+
+def format_deploy_queues(deploy_queues, cluster: str) -> str:
+    lines = [
+        f"Deploy Queue for Cluster {cluster}",
+        "  Available Service Instances:",
+    ]
+    available_instances_table = create_queue_entries_table(
+        deploy_queues.available_service_instances
+    )
+    lines.extend([f"    {line}" for line in available_instances_table])
+
+    lines.append("  Unavailable Service Instances:")
+    unavailable_instances_table = create_queue_entries_table(
+        deploy_queues.unavailable_service_instances
+    )
+    lines.extend([f"    {line}" for line in unavailable_instances_table])
+
+    return "\n".join(lines)
+
+
+def create_queue_entries_table(service_instances) -> List[str]:
+    if len(service_instances) == 0:
+        return [PaastaColors.grey("Empty")]
+
+    table_header = [
+        "Service Instance",
+        "Bounce by",
+        "Wait until",
+        "Enqueue time",
+        "Bounce Start Time",
+        "Processed Count",
+        "Failures",
+        "Watcher",
+    ]
+    rows = [table_header]
+    for service_instance in service_instances:
+        now = time.time()
+        bounce_by = format_timestamp(service_instance.bounce_by)
+        if service_instance.bounce_by < now:
+            bounce_by = PaastaColors.red(bounce_by)
+
+        failures = str(service_instance.failures)
+        if service_instance.failures > 10:
+            failures = PaastaColors.red(failures)
+
+        processed_count = str(service_instance.processed_count)
+        if service_instance.processed_count > 50:
+            processed_count = PaastaColors.red(processed_count)
+
+        rows.append(
+            [
+                f"{service_instance.service}.{service_instance.instance}",
+                bounce_by,
+                format_timestamp(service_instance.wait_until),
+                format_timestamp(service_instance.enqueue_time),
+                format_timestamp(service_instance.bounce_start_time),
+                processed_count,
+                failures,
+                service_instance.watcher,
+            ]
         )
-        paasta_print("sleep %s" % smartstack_grace_sleep)
-        t += smartstack_grace_sleep
-        paasta_print("sudo docker kill %s" % container_id)
-        paasta_print(f"sudo haup -P {marathon_port} {service}.{instance}")
-        last_killed_t = get_last_killed(drained_apps, service, instance)
-        drained_apps.append((t, service, instance))
-        # print "t:%s last_killed_t:%s" % (t, last_killed_t)
-        sleep_amount = between_containers_grace_sleep
-        if (t - last_killed_t) < min_kill_interval:
-            sleep_amount = (
-                min_kill_interval - (t - last_killed_t) + between_containers_grace_sleep
-            )
-        paasta_print("sleep %s" % sleep_amount)
-        t += sleep_amount
-        paasta_print()
+
+    return format_table(rows)
 
 
-if __name__ == "__main__":
-    main()
+def format_timestamp(timestamp: Union[int, float]) -> str:
+    datetime_obj = datetime.datetime.fromtimestamp(timestamp)
+    return datetime_obj.strftime("%Y-%m-%dT%H:%M:%S")
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/bounce_log_latency_parser.py` & `paasta-tools-1.0.0/paasta_tools/contrib/bounce_log_latency_parser.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,14 @@
-#!/usr/bin/env python3.6
+#!/usr/bin/env python3.8
 import itertools
 import json
 import sys
 from collections import defaultdict
 from datetime import datetime
 
-from paasta_tools.utils import paasta_print
-
 
 def get_datetime_from_ts(ts):
     tformat = "%Y-%m-%dT%H:%M:%S.%f"
     return datetime.strptime(ts, tformat)
 
 
 def get_deploy_durations_from_file(filename):
@@ -43,28 +41,28 @@
 
 
 def display_bounce_info(timedeltas):
     """
     timedeltas: iterable of timedelta objects
     """
     std = list(sorted(timedeltas))
-    paasta_print("Median time to bounce: {} seconds".format(std[len(std) / 2]))
-    paasta_print("10% time to bounce: {}".format(std[len(std) / 10]))
-    paasta_print("90% time to bounce: {}".format(std[len(std) * 9 / 10]))
+    print("Median time to bounce: {} seconds".format(std[len(std) / 2]))
+    print("10% time to bounce: {}".format(std[len(std) / 10]))
+    print("90% time to bounce: {}".format(std[len(std) * 9 / 10]))
 
 
 def main(filenames):
     for filename in filenames:
-        paasta_print(filename)
-        paasta_print("=========================")
+        print(filename)
+        print("=========================")
         timedeltas = get_deploy_durations_from_file(filename)
         for instance, tdlist in timedeltas.items():
             if timedeltas:
-                paasta_print("Instance: %s" % instance)
+                print("Instance: %s" % instance)
                 display_bounce_info(tdlist)
-        paasta_print("Overall:")
+        print("Overall:")
         display_bounce_info(itertools.chain.from_iterable(timedeltas.values()))
-        paasta_print("=========================")
+        print("=========================")
 
 
 if __name__ == "__main__":
     main(filenames=sys.argv[1:])
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/create_dynamodb_table.py` & `paasta-tools-1.0.0/paasta_tools/contrib/create_dynamodb_table.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/delete_old_marathon_deployments.py` & `paasta-tools-1.0.0/paasta_tools/delete_kubernetes_deployments.py`

 * *Files 23% similar despite different names*

```diff
@@ -8,101 +8,82 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+"""
+Usage: ./delete_paasta_contract_monitor.py <service.instance> [options]
+
+The following script is a setup on a cron job in k8s masters. This is responsible for deleting
+paasta-contract-monitor deployments and its services. By deleting the deployment itself,
+setup_kubernetes_job.py will be able to reschedule the deployment and its pods on different nodes.
+"""
 import argparse
-import datetime
 import logging
+import sys
 
-import dateutil.parser
-from dateutil import tz
-from pytimeparse import timeparse
-
-from paasta_tools import marathon_tools
-
+from paasta_tools.kubernetes_tools import delete_deployment
+from paasta_tools.kubernetes_tools import ensure_namespace
+from paasta_tools.kubernetes_tools import get_kubernetes_app_name
+from paasta_tools.kubernetes_tools import KubeClient
+from paasta_tools.utils import decompose_job_id
+from paasta_tools.utils import InvalidJobNameError
+from paasta_tools.utils import SPACER
 
 log = logging.getLogger(__name__)
 
 
-def parse_args():
-    parser = argparse.ArgumentParser()
+def parse_args(args=None) -> argparse.Namespace:
+    parser = argparse.ArgumentParser(description="Deletes list of deployments.")
     parser.add_argument(
-        "-a",
-        "--age",
-        dest="age",
-        type=timedelta_type,
-        default="1h",
-        help="Max age of a Marathon deployment before it is stopped."
-        "Any pytimeparse unit is supported",
+        "service_instance_list",
+        nargs="+",
+        help="The list of service instances to delete",
+        metavar=f"SERVICE{SPACER}INSTANCE",
     )
-    parser.add_argument(
-        "-n",
-        "--dry-run",
-        action="store_true",
-        help="Don't actually stop any Marathon deployments",
-    )
-    parser.add_argument("-v", "--verbose", action="store_true")
-    options = parser.parse_args()
-    return options
-
-
-def timedelta_type(value):
-    """Return the :class:`datetime.datetime.DateTime` for a time in the past.
-    :param value: a string containing a time format supported by :mod:`pytimeparse`
-    """
-    if value is None:
-        return None
-    return datetime_seconds_ago(timeparse.timeparse(value))
+    args = parser.parse_args(args=args)
+    return args
 
 
-def datetime_seconds_ago(seconds):
-    return now() - datetime.timedelta(seconds=seconds)
-
-
-def now():
-    return datetime.datetime.now(tz.tzutc())
-
-
-def delete_deployment_if_too_old(client, deployment, max_date, dry_run):
-    started_at = dateutil.parser.parse(deployment.version)
-    age = now() - started_at
-    if started_at < max_date:
-        if dry_run is True:
-            log.warning(
-                f"Would delete {deployment.id} for {deployment.affected_apps[0]} as it is {age} old"
-            )
-        else:
-            log.warning(
-                f"Deleting {deployment.id} for {deployment.affected_apps[0]} as it is {age} old"
-            )
-            client.delete_deployment(deployment_id=deployment.id, force=False)
-    else:
-        if dry_run is True:
-            log.warning(
-                f"NOT deleting {deployment.id} for {deployment.affected_apps[0]} as it is {age} old"
+def get_deployment_names_from_list(service_instance_list):
+    app_names = []
+    for service_instance in service_instance_list:
+        try:
+            service, instance, _, __ = decompose_job_id(service_instance)
+            app_name = get_kubernetes_app_name(service, instance)
+            app_names.append(app_name)
+        except InvalidJobNameError:
+            log.error(
+                f"Invalid service instance specified. Format is service{SPACER}instance."
             )
+            sys.exit(1)
+    return app_names
 
 
-def main():
-    args = parse_args()
-    if args.verbose:
-        logging.basicConfig(level=logging.DEBUG)
-    else:
-        logging.basicConfig(level=logging.WARNING)
-
-    clients = marathon_tools.get_list_of_marathon_clients()
-
-    for client in clients:
-        for deployment in client.list_deployments():
-            delete_deployment_if_too_old(
-                client=client,
-                deployment=deployment,
-                max_date=args.age,
-                dry_run=args.dry_run,
+def main(args=None) -> None:
+    args = parse_args(args=args)
+    service_instance_list = args.service_instance_list
+    deployment_names = get_deployment_names_from_list(service_instance_list)
+
+    log.debug(f"Deleting deployments: {deployment_names}")
+    kube_client = KubeClient()
+    ensure_namespace(kube_client=kube_client, namespace="paasta")
+
+    for deployment_name in deployment_names:
+        try:
+            log.debug(f"Deleting {deployment_name}")
+            delete_deployment(
+                kube_client=kube_client,
+                deployment_name=deployment_name,
+                namespace="paasta",
             )
+        except Exception as err:
+            log.error(f"Unable to delete {deployment_name}: {err}")
+            sys.exit(1)
+
+    sys.exit(0)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/utilization_check.py` & `paasta-tools-1.0.0/paasta_tools/contrib/utilization_check.py`

 * *Files 3% similar despite different names*

```diff
@@ -15,24 +15,23 @@
     calculate_resource_utilization_for_slaves,
 )
 from paasta_tools.metrics.metastatus_lib import filter_tasks_for_slaves
 from paasta_tools.metrics.metastatus_lib import get_all_tasks_from_state
 from paasta_tools.metrics.metastatus_lib import (
     resource_utillizations_from_resource_info,
 )
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 
 
 def main(hostnames: Sequence[str]) -> None:
     master = get_mesos_master()
     try:
         mesos_state = block(master.state)
     except MasterNotAvailableException as e:
-        paasta_print(PaastaColors.red("CRITICAL:  %s" % e.message))
+        print(PaastaColors.red("CRITICAL:  %s" % e.message))
         sys.exit(2)
     slaves = [
         slave
         for slave in mesos_state.get("slaves", [])
         if slave["hostname"] in hostnames
     ]
     tasks = get_all_tasks_from_state(mesos_state, include_orphans=True)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/shared_ip_check.py` & `paasta-tools-1.0.0/paasta_tools/contrib/shared_ip_check.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-#!/usr/bin/env python3.6
+#!/usr/bin/env python3.8
 import sys
 from collections import defaultdict
 
 import iptc
 
 from paasta_tools import iptables
 from paasta_tools.utils import get_docker_client
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/check_orphans.py` & `paasta-tools-1.0.0/paasta_tools/contrib/check_orphans.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,13 +1,16 @@
 #!/usr/bin/env python3
 import argparse
 import asyncio
+import functools
 import json
 import logging
 import os.path
+import re
+import socket
 import sys
 from collections import defaultdict
 from enum import Enum
 from typing import Any
 from typing import DefaultDict
 from typing import Dict
 from typing import List
@@ -28,38 +31,39 @@
 DEFAULT_NERVE_XINETD_PORT = 8735
 
 
 class ExitCode(Enum):
     OK = 0
     ORPHANS = 1
     COLLISIONS = 2
+    UNKNOWN = 3
 
 
 def get_zk_hosts(path: str) -> List[str]:
     with open(path) as f:
-        x = yaml.load(f)
+        x = yaml.safe_load(f)
     return [f"{host}:{port}" for host, port in x]
 
 
 SmartstackData = Dict[str, Dict[str, Any]]
 
 
-def get_zk_data(blacklisted_services: Set[str]) -> SmartstackData:
+def get_zk_data(ignored_services: Set[str]) -> SmartstackData:
     logger.info(f"using {DEFAULT_ZK_DISCOVERY_PATH} for zookeeper")
     zk_hosts = get_zk_hosts(DEFAULT_ZK_DISCOVERY_PATH)
 
     logger.debug(f"connecting to zk hosts {zk_hosts}")
     zk = KazooClient(hosts=zk_hosts)
     zk.start()
 
     logger.debug(f"pulling smartstack data from zookeeper")
     zk_data = {}
     services = zk.get_children(PREFIX)
     for service in services:
-        if service in blacklisted_services:
+        if service in ignored_services:
             continue
         service_instances = zk.get_children(os.path.join(PREFIX, service))
         instances_data = {}
         for instance in service_instances:
             try:
                 instance_node = zk.get(os.path.join(PREFIX, service, instance))
             except NoNodeError:
@@ -67,38 +71,74 @@
             instances_data[instance] = json.loads(instance_node[0])
             zk_data[service] = instances_data
 
     return zk_data
 
 
 class InstanceTuple(NamedTuple):
+    # paasta_host may be different from the service's host if running on k8s.
+    # We need the actual PaaSTA host because the k8s pod does not listen for
+    # xinetd connections.
+    paasta_host: str
     host: str
     port: int
     service: str
 
 
 def read_from_zk_data(registrations: SmartstackData) -> Set[InstanceTuple]:
     return {
-        InstanceTuple(instance_data["host"], instance_data["port"], service)
+        InstanceTuple(
+            host_to_ip(instance_data["name"], instance_data["host"]),
+            instance_data["host"],
+            instance_data["port"],
+            service,
+        )
         for service, instance in registrations.items()
         for instance_data in instance.values()
     }
 
 
+@functools.lru_cache()
+def host_to_ip(host: str, fallback: str) -> str:
+    """Try to resolve a host to an IP with a fallback.
+
+    Because DNS resolution is relatively slow and can't be easily performed
+    using asyncio, we cheat a little and use a regex for well-formed hostnames
+    to try to guess the IP without doing real resolution.
+
+    A fallback is needed because in some cases the nerve registration does not
+    match an actual hostname (e.g. "prod-db15" or "prod-splunk-master").
+    """
+    for match in (
+        re.match(r"^(\d+)-(\d+)-(\d+)-(\d+)-", host),
+        re.match(r"^ip-(\d+)-(\d+)-(\d+)-(\d+)", host),
+    ):
+        if match:
+            return ".".join(match.groups())
+    else:
+        try:
+            return socket.gethostbyname(host)
+        except socket.gaierror:
+            return fallback
+
+
 async def transfer_one_file(
     host: str, port: int = DEFAULT_NERVE_XINETD_PORT
 ) -> Tuple[str, Optional[str]]:
     logger.debug(f"getting file from {host}")
     try:
         reader, _ = await asyncio.wait_for(
-            asyncio.open_connection(host=host, port=port, limit=2 ** 32), timeout=1.0
+            asyncio.open_connection(host=host, port=port, limit=2**32), timeout=1.0
         )
         resp = await asyncio.wait_for(reader.read(), timeout=1.0)
-    except (asyncio.TimeoutError, ConnectionRefusedError):
-        logger.warning(f"error getting file from {host}")
+    except (asyncio.TimeoutError, ConnectionRefusedError) as ex:
+        # this is not ununusual because we sometimes advertise hosts from
+        # firewalled subnets where we can't make this connection to get
+        # the file. check y/ipam to see what the subnet means
+        logger.debug(f"error getting file from {host}: {ex!r}")
         return (host, None)
 
     return (host, resp.decode())
 
 
 async def gather_files(hosts: Set[str]) -> Dict[str, str]:
     logger.info("gathering files from {} hosts".format(len(hosts)))
@@ -109,20 +149,28 @@
             *tasks[idx : idx + CHUNK_SIZE], return_exceptions=True
         )
         responses.update(dict(resp))
     return responses
 
 
 def read_one_nerve_file(nerve_config: str) -> Set[InstanceTuple]:
-    services = json.loads(nerve_config)["services"]
+    nerve_config = json.loads(nerve_config)
     return {
         InstanceTuple(
-            service["host"], service["port"], service["zk_path"][len(PREFIX) :]
+            # The "instance_id" configured in nerve's config file is the same
+            # as the "name" attribute in a zookeeper registration (i.e. for
+            # PaaSTA hosts, it will be the hostname of the machine running
+            # nerve). To be able to easily compare the tuples using set
+            # operations, we resolve it to an IP in both places.
+            host_to_ip(nerve_config["instance_id"], service["host"]),
+            service["host"],
+            service["port"],
+            service["zk_path"][len(PREFIX) :],
         )
-        for service in services.values()
+        for service in nerve_config["services"].values()
         if service["zk_path"].startswith(PREFIX)
     }
 
 
 def read_nerve_files(
     nerve_configs: Dict[str, Optional[str]]
 ) -> Tuple[Set[InstanceTuple], Set[str]]:
@@ -133,18 +181,18 @@
             not_found_hosts.add(host)
         else:
             instance_set |= read_one_nerve_file(host_config)
     return instance_set, not_found_hosts
 
 
 def get_instance_data(
-    blacklisted_services: Set[str],
+    ignored_services: Set[str],
 ) -> Tuple[Set[InstanceTuple], Set[InstanceTuple]]:
     # Dump ZK
-    zk_data = get_zk_data(blacklisted_services)
+    zk_data = get_zk_data(ignored_services)
     zk_instance_data = read_from_zk_data(zk_data)
 
     hosts = {x[0] for x in zk_instance_data}
 
     # Dump Nerve configs from each host via xinetd
     results = asyncio.get_event_loop().run_until_complete(gather_files(hosts))
 
@@ -161,66 +209,97 @@
     )
     logger.info("nerve_instance_data len: {}".format(len(nerve_instance_data)))
 
     return zk_instance_data_filtered, nerve_instance_data
 
 
 def check_orphans(
-    zk_instance_data: Set[InstanceTuple], nerve_instance_data: Set[InstanceTuple]
+    zk_instance_data: Set[InstanceTuple],
+    nerve_instance_data: Set[InstanceTuple],
+    check_orphans: bool,
+    check_collisions: bool,
 ) -> ExitCode:
-    orphans = zk_instance_data - nerve_instance_data
 
-    # groupby host
-    orphans_by_host: DefaultDict[str, List[Tuple[int, str]]] = defaultdict(list)
-    for orphan in orphans:
-        orphans_by_host[orphan.host].append((orphan.port, orphan.service))
-
-    # collisions
-    instance_by_addr: DefaultDict[Tuple[str, int], Set[str]] = defaultdict(set)
-    for nerve_inst in nerve_instance_data:
-        instance_by_addr[(nerve_inst.host, nerve_inst.port)].add(nerve_inst.service)
-
-    collisions: List[str] = []
-    for zk_inst in zk_instance_data:
-        nerve_services = instance_by_addr[(zk_inst.host, zk_inst.port)]
-        if len(nerve_services) >= 1 and zk_inst.service not in nerve_services:
-            collisions.append(
-                f"[{zk_inst.host}:{zk_inst.port}] {zk_inst.service} collides with {nerve_services}"
+    if check_collisions:
+        # collisions
+        instance_by_addr: DefaultDict[Tuple[str, int], Set[str]] = defaultdict(set)
+        for nerve_inst in nerve_instance_data:
+            instance_by_addr[(nerve_inst.host, nerve_inst.port)].add(nerve_inst.service)
+        collisions: List[str] = []
+        for zk_inst in zk_instance_data:
+            nerve_services = instance_by_addr[(zk_inst.host, zk_inst.port)]
+            if len(nerve_services) >= 1 and zk_inst.service not in nerve_services:
+                collisions.append(
+                    f"[{zk_inst.host}:{zk_inst.port}] {zk_inst.service} collides with {nerve_services}"
+                )
+
+        if collisions:
+            logger.warning("Collisions found! Traffic is being misrouted!")
+            print("\n".join(collisions))
+            return ExitCode.COLLISIONS
+        else:
+            logger.info(
+                f"No collisions found out of {len(zk_instance_data)} service registrations seen."
             )
+    if check_orphans:
+        orphans = zk_instance_data - nerve_instance_data
 
-    if collisions:
-        logger.warning("Collisions found! Traffic is being misrouted!")
-        print("\n".join(collisions))
-        return ExitCode.COLLISIONS
-    elif orphans:
-        logger.warning("{} orphans found".format(len(orphans)))
-        print(dict(orphans_by_host))
-        return ExitCode.ORPHANS
-    else:
-        logger.info(
-            "No orphans found out of {} service registrations seen".format(
-                len(zk_instance_data)
+        # groupby host
+        orphans_by_host: DefaultDict[str, List[Tuple[int, str]]] = defaultdict(list)
+        for orphan in orphans:
+            orphans_by_host[orphan.host].append((orphan.port, orphan.service))
+
+        if orphans:
+            logger.warning("{} orphans found".format(len(orphans)))
+            print(dict(orphans_by_host))
+            return ExitCode.ORPHANS
+        else:
+            logger.info(
+                f"No orphans found out of {len(zk_instance_data)} service registrations seen."
             )
-        )
-        return ExitCode.OK
+
+    return ExitCode.OK
 
 
 def main() -> ExitCode:
     logging.basicConfig(level=logging.WARNING)
     parser = argparse.ArgumentParser()
     parser.add_argument(
-        "--blacklisted-services",
+        "--ignored-services",
+        # TODO(ckuehl|2020-08-27): Remove this deprecated option alias eventually.
+        "--blacklisted-services-DEPRECATED",
         default="",
         type=str,
         help="Comma separated list of services to ignore",
     )
+    parser.add_argument(
+        "--no-check-collisions",
+        default=False,
+        action="store_true",
+        help="Skip checking collisions",
+    )
+    parser.add_argument(
+        "--no-check-orphans",
+        default=False,
+        action="store_true",
+        help="Skip checking orphans",
+    )
     args = parser.parse_args()
 
+    if args.no_check_collisions and args.no_check_orphans:
+        logger.error("Must check at least one of orphans or collisions.")
+        return ExitCode.UNKNOWN
+
     zk_instance_data, nerve_instance_data = get_instance_data(
-        set(args.blacklisted_services.split(","))
+        set(args.ignored_services.split(","))
     )
 
-    return check_orphans(zk_instance_data, nerve_instance_data)
+    return check_orphans(
+        zk_instance_data,
+        nerve_instance_data,
+        check_orphans=not args.no_check_orphans,
+        check_collisions=not args.no_check_collisions,
+    )
 
 
 if __name__ == "__main__":
     sys.exit(main().value)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/kill_bad_containers.py` & `paasta-tools-1.0.0/paasta_tools/contrib/kill_bad_containers.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/is_pod_healthy_in_smartstack.py` & `paasta-tools-1.0.0/paasta_tools/contrib/is_pod_healthy_in_smartstack.py`

 * *Files 20% similar despite different names*

```diff
@@ -22,18 +22,29 @@
 synapse_port = system_paasta_config.get_synapse_port()
 synapse_host = "169.254.255.254"
 synapse_haproxy_url_format = system_paasta_config.get_synapse_haproxy_url_format()
 host_ip = os.environ["PAASTA_POD_IP"]
 port = sys.argv[1]
 services = sys.argv[2:]
 
+###############################################################
+#
+# This file is used in the hacheck sidecar, make sure to update `check_smartstack_up.sh`
+# when changing this file
+#
+###############################################################
+
 if are_services_up_on_ip_port(
     synapse_host=synapse_host,
     synapse_port=synapse_port,
     synapse_haproxy_url_format=synapse_haproxy_url_format,
     services=services,
     host_ip=host_ip,
     host_port=int(port),
 ):
     sys.exit(0)
 else:
+    print(
+        f"Could not find backend {host_ip}:{port} for service {services} "
+        f"on Synapse at {synapse_host}:{synapse_port}"
+    )
     sys.exit(1)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/contrib/paasta_update_soa_memcpu.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/push_to_registry.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,301 +1,275 @@
-#!/usr/bin/env python3
+#!/usr/bin/env python
+# Copyright 2015-2016 Yelp Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Contains methods used by the paasta client to upload a docker
+image to a registry.
+"""
 import argparse
-import contextlib
+import base64
+import binascii
 import json
 import os
-import subprocess
-import tempfile
-import time
+from typing import Optional
+from typing import Tuple
 
 import requests
-import ruamel.yaml as yaml
-from jira.client import JIRA
+from requests.exceptions import RequestException
+from requests.exceptions import SSLError
 
-
-def parse_args(argv):
-    parser = argparse.ArgumentParser(description="")
-    parser.add_argument(
+from paasta_tools.cli.utils import get_jenkins_build_output_url
+from paasta_tools.cli.utils import validate_full_git_sha
+from paasta_tools.cli.utils import validate_service_name
+from paasta_tools.generate_deployments_for_service import build_docker_image_name
+from paasta_tools.utils import _log
+from paasta_tools.utils import _log_audit
+from paasta_tools.utils import _run
+from paasta_tools.utils import build_docker_tag
+from paasta_tools.utils import build_image_identifier
+from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import get_service_docker_registry
+
+
+def add_subparser(subparsers: argparse._SubParsersAction) -> None:
+    list_parser = subparsers.add_parser(
+        "push-to-registry",
+        help="Uploads a docker image to a registry",
+        description=(
+            "'paasta push-to-registry' is a tool to upload a local docker image "
+            "to the configured PaaSTA docker registry with a predictable and "
+            "well-constructed image name. The image name must be predictable because "
+            "the other PaaSTA components are expecting a particular format for the docker "
+            "image name."
+        ),
+        epilog=(
+            "Note: Uploading to a docker registry often requires access to the local "
+            "docker socket as well as credentials to the remote registry"
+        ),
+    )
+    list_parser.add_argument(
         "-s",
-        "--splunk_creds",
-        help="Creds for Splunk API, user:pass",
-        dest="splunk_creds",
+        "--service",
+        help='Name of service for which you wish to upload a docker image. Leading "services-", '
+        "as included in a Jenkins job name, will be stripped.",
         required=True,
     )
-    parser.add_argument(
-        "-j",
-        "--jira_creds",
-        help="Creds for JIRA API, user:pass",
-        dest="jira_creds",
+    list_parser.add_argument(
+        "-c",
+        "--commit",
+        help="Git sha after which to name the remote image",
         required=True,
+        type=validate_full_git_sha,
     )
-    parser.add_argument(
-        "-n",
-        "--no-tick",
-        help="Do not create a JIRA ticket",
-        action="store_true",
-        dest="no_tick",
-        default=False,
+    list_parser.add_argument(
+        "--image-version",
+        type=str,
+        required=False,
+        default=None,
+        help=(
+            "Extra version metadata to use when naming the remote image."
+            " When set, both versioned and non-versioned Docker tags are"
+            " pushed to the registry. The image with a versioned tag is"
+            " expected to have been build locally."
+        ),
     )
-    parser.add_argument(
-        "-m",
-        "--manual",
-        help="Do not publish a review",
-        action="store_true",
-        dest="manual_rb",
-        default=False,
+    list_parser.add_argument(
+        "-d",
+        "--soa-dir",
+        dest="soa_dir",
+        metavar="SOA_DIR",
+        default=DEFAULT_SOA_DIR,
+        help="define a different soa config directory",
     )
-
-    parser.add_argument(
+    list_parser.add_argument(
         "-f",
-        "--file-splunk",
-        help="Splunk csv from which to pull data. Defaults to %(default)s",
-        dest="file_splunk",
-        default="paasta_overprovision_alerts_fired.csv",
+        "--force",
+        help=(
+            "Do not check if the image is already in the PaaSTA docker registry. "
+            "Push it anyway."
+        ),
+        action="store_true",
     )
-    return parser.parse_args(argv)
-
+    list_parser.set_defaults(command=paasta_push_to_registry)
 
-def tempdir():
-    return tempfile.TemporaryDirectory(prefix="repo", dir="/nail/tmp")
 
+def build_command(
+    upstream_job_name: str,
+    upstream_git_commit: str,
+    image_version: Optional[str] = None,
+) -> str:
+    # This is kinda dumb since we just cleaned the 'services-' off of the
+    # service so we could validate it, but the Docker image will have the full
+    # name with 'services-' so add it back.
+    tag = build_docker_tag(upstream_job_name, upstream_git_commit, image_version)
+    cmd = f"docker push {tag}"
+    return cmd
+
+
+def paasta_push_to_registry_impl(
+    args: argparse.Namespace, service: str, image_version: Optional[str]
+) -> int:
+    """Upload a docker image to a registry"""
+    image_identifier = build_image_identifier(args.commit, None, image_version)
+
+    if not args.force:
+        try:
+            if is_docker_image_already_in_registry(
+                service, args.soa_dir, args.commit, image_version
+            ):
+                print(
+                    "The docker image is already in the PaaSTA docker registry. "
+                    "I'm NOT overriding the existing image. "
+                    "Add --force to override the image in the registry if you are sure what you are doing."
+                )
+                return 0
+        except RequestException as e:
+            registry_uri = get_service_docker_registry(service, args.soa_dir)
+            print(
+                "Can not connect to the PaaSTA docker registry '%s' to verify if this image exists.\n"
+                "%s" % (registry_uri, str(e))
+            )
+            return 1
 
-@contextlib.contextmanager
-def cwd(path):
-    pwd = os.getcwd()
-    os.chdir(path)
-    try:
-        yield
-    finally:
-        os.chdir(pwd)
-
-
-@contextlib.contextmanager
-def in_tempdir():
-    with tempdir() as tmp:
-        with cwd(tmp):
-            yield
-
-
-def get_perf_data(creds, filename):
-    url = "https://splunk-api.yelpcorp.com/servicesNS/nobody/yelp_performance/search/jobs/export"
-    search = (
-        "| inputlookup {} |"
-        ' eval _time = search_time | where _time > relative_time(now(),"-7d")'
-    ).format(filename)
-    data = {"output_mode": "json", "search": search}
-    creds = creds.split(":")
-    resp = requests.post(url, data=data, auth=(creds[0], creds[1]))
-    resp_text = resp.text.split("\n")
-    resp_text = [x for x in resp_text if x]
-    resp_text = [json.loads(x) for x in resp_text]
-
-    services_to_update = []
-    for d in resp_text:
-        criteria = d["result"]["criteria"].split()
-        serv = {}
-        serv["service"] = criteria[0]
-        serv["cluster"] = criteria[1]
-        serv["instance"] = criteria[2]
-        serv["cpus"] = d["result"]["suggested_cpus"]
-        serv["owner"] = d["result"]["service_owner"]
-        serv["money"] = d["result"]["estimated_monthly_savings"]
-        serv["date"] = d["result"]["_time"].split(" ")[0]
-        serv["old_cpus"] = d["result"]["current_cpus"]
-        serv["project"] = d["result"]["project"]
-        services_to_update.append(serv)
-
-    return services_to_update
-
-
-def clone(branch_name):
-    remote = "git@sysgit.yelpcorp.com:yelpsoa-configs"
-    subprocess.check_call(("git", "clone", remote, "."))
-    subprocess.check_call(("git", "checkout", "-b", branch_name))
-
-
-def commit(filename, serv):
-    message = "Updating {} for {}provisioned cpu from {} to {} cpus".format(
-        filename, serv["state"], serv["old_cpus"], serv["cpus"]
+    cmd = build_command(service, args.commit, image_version)
+    loglines = []
+    returncode, output = _run(
+        cmd,
+        timeout=3600,
+        log=True,
+        component="build",
+        service=service,
+        loglevel="debug",
     )
-    subprocess.check_call(("git", "add", filename))
-    subprocess.check_call(("git", "commit", "-n", "-m", message))
-
-
-def get_reviewers_in_group(group_name):
-    """Using rbt's target-groups argument overrides our configured default review groups.
-    So we'll expand the group into usernames and pass those users in the group individually.
-    """
-    rightsizer_reviewers = json.loads(
-        subprocess.check_output(
-            (
-                "rbt",
-                "api-get",
-                "--server",
-                "https://reviewboard.yelpcorp.com",
-                f"groups/{group_name}/users/",
+    if returncode != 0:
+        loglines.append("ERROR: Failed to promote image for %s." % image_identifier)
+        output = get_jenkins_build_output_url()
+        if output:
+            loglines.append("See output: %s" % output)
+    else:
+        loglines.append(
+            "Successfully pushed image for %s to registry" % image_identifier
+        )
+        _log_audit(
+            action="push-to-registry",
+            action_details={"commit": args.commit},
+            service=service,
+        )
+    for logline in loglines:
+        _log(service=service, line=logline, component="build", level="event")
+    return returncode
+
+
+def paasta_push_to_registry(args: argparse.Namespace) -> int:
+    """Upload a docker images to a registry"""
+    service = args.service
+    if service and service.startswith("services-"):
+        service = service.split("services-", 1)[1]
+    validate_service_name(service, args.soa_dir)
+
+    returncode = 0
+    image_versions = [None]
+    if args.image_version:
+        # re-tag with legacy convention as well
+        returncode = retag_versioned_image(service, args.commit, args.image_version)
+        if returncode != 0:
+            _log(
+                service=service,
+                line=f"ERROR: failed re-tagging image. See output at {get_jenkins_build_output_url()}",
+                component="build",
+                level="event",
             )
-        ).decode("UTF-8")
-    )
-    return [user.get("username", "") for user in rightsizer_reviewers.get("users", {})]
+            return returncode
+        image_versions.append(args.image_version)
 
+    for image_version in image_versions:
+        returncode = paasta_push_to_registry_impl(args, service, image_version)
+        if returncode != 0:
+            break
 
-def get_reviewers(filename):
-    recent_authors = set()
-    authors = (
-        subprocess.check_output(("git", "log", "--format=%ae", "--", filename))
-        .decode("UTF-8")
-        .splitlines()
-    )
+    return returncode
 
-    authors = [x.split("@")[0] for x in authors]
-    for author in authors:
-        recent_authors.add(author)
-        if len(recent_authors) >= 3:
-            break
-    return recent_authors
 
+def retag_versioned_image(service: str, commit: str, image_version: str) -> int:
+    """Create new local tag without additional version information
 
-def review(filename, summary, description, manual_rb):
-    all_reviewers = get_reviewers(filename).union(get_reviewers_in_group("right-sizer"))
-    reviewers_arg = " ".join(all_reviewers)
-    if manual_rb:
-        subprocess.check_call(
-            (
-                "review-branch",
-                f"--summary={summary}",
-                f"--description={description}",
-                "--reviewers",
-                reviewers_arg,
-                "--server",
-                "https://reviewboard.yelpcorp.com",
-            )
-        )
-    else:
-        subprocess.check_call(
-            (
-                "review-branch",
-                f"--summary={summary}",
-                f"--description={description}",
-                "-p",
-                "--reviewers",
-                reviewers_arg,
-                "--server",
-                "https://reviewboard.yelpcorp.com",
-            )
-        )
+    :param str service: service name
+    :param str commit: commit sha
+    :param str image_version: additional image version metadata
+    :return: docker command return code
+    """
+    unversioned_tag = build_docker_tag(service, commit, None)
+    image_version_tag = build_docker_tag(service, commit, image_version)
+    returncode, _ = _run(
+        ["docker", "tag", image_version_tag, unversioned_tag],
+        timeout=30,
+        log=True,
+        component="build",
+        service=service,
+        loglevel="debug",
+    )
+    return returncode
 
 
-def edit_soa_configs(filename, instance, cpu):
-    with open(filename, "r") as fi:
-        yams = fi.read()
-        yams = yams.replace("cpus: .", "cpus: 0.")
-        data = yaml.round_trip_load(yams, preserve_quotes=True)
-
-    instdict = data[instance]
-    instdict["cpus"] = cpu
-    out = yaml.round_trip_dump(data, width=120)
-
-    with open(filename, "w") as fi:
-        fi.write(out)
-
-
-def create_jira_ticket(serv, creds, description):
-    creds = creds.split(":")
-    options = {"server": "https://jira.yelpcorp.com"}
-    jira_cli = JIRA(options=options, basic_auth=(creds[0], creds[1]))
-    jira_ticket = {}
-    # Sometimes a project has required fields we can't predict
+def read_docker_registry_creds(
+    registry_uri: str,
+) -> Tuple[Optional[str], Optional[str]]:
+    dockercfg_path = os.path.expanduser("~/.dockercfg")
     try:
-        jira_ticket = {
-            "project": {"key": serv["project"]},
-            "description": description,
-            "issuetype": {"name": "Improvement"},
-            "labels": ["perf-watching", "paasta-rightsizer"],
-            "summary": "{s}.{i} in {c} may be {o}provisioned".format(
-                s=serv["service"],
-                i=serv["instance"],
-                c=serv["cluster"],
-                o=serv["state"],
-            ),
-        }
-        tick = jira_cli.create_issue(fields=jira_ticket)
-    except Exception:
-        jira_ticket["project"] = {"key": "PEOBS"}
-        jira_ticket["labels"].append(serv["service"])
-        tick = jira_cli.create_issue(fields=jira_ticket)
-    return tick.key
-
-
-def _get_dashboard_qs_param(param, value):
-    # Some dashboards may ask for query string params like param=value, but not this provider.
-    return f"variables%5B%5D={param}%3D{param}:{value}"
-
-
-def main(argv=None):
-    args = parse_args(argv)
-    services_to_update = get_perf_data(args.splunk_creds, args.file_splunk)
-
-    for serv in services_to_update:
-        filename = "{}/{}.yaml".format(serv["service"], serv["cluster"])
-        cpus = float(serv["cpus"])
-        provisioned_state = "over"
-        if cpus > float(serv["old_cpus"]):
-            provisioned_state = "under"
-
-        serv["state"] = provisioned_state
-        ticket_desc = (
-            "This ticket and CR have been auto-generated to help keep PaaSTA right-sized."
-            "\nPEOBS will review this CR and give a shipit. Then an ops deputy from your team can merge"
-            " if these values look good for your service after review."
-            "\nOpen an issue with any concerns and someone from PEOBS will respond."
-            "\nWe suspect that {s}.{i} in {c} may have been {o}-provisioned"
-            " during the 1 week prior to {d}. It initially had {x} cpus, but based on the below dashboard,"
-            " we recommend {y} cpus."
-            "\n- Dashboard: https://y.yelpcorp.com/{o}provisioned?{cluster_param}&{service_param}&{instance_param}"
-            "\n- Service owner: {n}"
-            "\n- Estimated monthly excess cost: ${m}"
-            "\n\nFor more information and sizing examples for larger services:"
-            "\n- Runbook: https://y.yelpcorp.com/rb-provisioning-alert"
-            "\n- Alert owner: pe-observability@yelp.com"
-        ).format(
-            s=serv["service"],
-            c=serv["cluster"],
-            i=serv["instance"],
-            o=provisioned_state,
-            d=serv["date"],
-            n=serv["owner"],
-            m=serv["money"],
-            x=serv["old_cpus"],
-            y=serv["cpus"],
-            cluster_param=_get_dashboard_qs_param(
-                "paasta_cluster", serv["cluster"].replace("marathon-", "")
-            ),
-            service_param=_get_dashboard_qs_param("paasta_service", serv["service"]),
-            instance_param=_get_dashboard_qs_param("paasta_instance", serv["instance"]),
-        )
-        summary = f"Rightsizing {serv['service']}.{serv['instance']} in {serv['cluster']} to make it not have {provisioned_state}-provisioned cpu"  # noqa: E501
-        branch = ""
-        if args.no_tick:
-            branch = "rightsize-{}".format(int(time.time()))
-        else:
-            branch = create_jira_ticket(serv, args.jira_creds, ticket_desc)
-
-        with in_tempdir():
-            clone(branch)
-            edit_soa_configs(filename, serv["instance"], cpus)
-            try:
-                commit(filename, serv)
-                review(filename, summary, ticket_desc, args.manual_rb)
-            except Exception:
-                print(
-                    (
-                        "\nUnable to push changes to {f}. Check if {f} conforms to"
-                        "yelpsoa-configs yaml rules. No review created. To see the"
-                        "cpu suggestion for this service check {t}."
-                    ).format(f=filename, t=branch)
-                )
-                continue
+        with open(dockercfg_path) as f:
+            dockercfg = json.load(f)
+            auth = base64.b64decode(dockercfg[registry_uri]["auth"]).decode("utf-8")
+            first_colon = auth.find(":")
+            if first_colon != -1:
+                return (auth[:first_colon], auth[first_colon + 1 : -2])
+    except IOError:  # Can't open ~/.dockercfg
+        pass
+    except json.JSONDecodeError:  # JSON decoder error
+        pass
+    except binascii.Error:  # base64 decode error
+        pass
+    return (None, None)
+
+
+def is_docker_image_already_in_registry(service: str, soa_dir: str, sha: str, image_version: Optional[str] = None) -> bool:  # type: ignore
+    """Verifies that docker image exists in the paasta registry.
+
+    :param service: name of the service
+    :param sha: git sha
+    :returns: True, False or raises requests.exceptions.RequestException
+    """
+    registry_uri = get_service_docker_registry(service, soa_dir)
+    repository, tag = build_docker_image_name(service, sha, image_version).split(":", 1)
 
+    creds = read_docker_registry_creds(registry_uri)
+    uri = f"{registry_uri}/v2/{repository}/manifests/{tag}"
 
-if __name__ == "__main__":
-    main()
+    with requests.Session() as s:
+        try:
+            url = "https://" + uri
+            r = (
+                s.head(url, timeout=30)
+                if creds[0] is None
+                else s.head(url, auth=creds, timeout=30)
+            )
+        except SSLError:
+            # If no auth creds, fallback to trying http
+            if creds[0] is not None:
+                raise
+            url = "http://" + uri
+            r = s.head(url, timeout=30)
+
+        if r.status_code == 200:
+            return True
+        elif r.status_code == 404:
+            return False  # No Such Repository Error
+        r.raise_for_status()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cleanup_kubernetes_crd.py` & `paasta-tools-1.0.0/paasta_tools/cleanup_kubernetes_crd.py`

 * *Files 12% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 import sys
 
 import service_configuration_lib
 from kubernetes.client import V1DeleteOptions
 from kubernetes.client.rest import ApiException
 
 from paasta_tools.kubernetes_tools import KubeClient
+from paasta_tools.kubernetes_tools import paasta_prefixed
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import load_system_paasta_config
 
 log = logging.getLogger(__name__)
 
 
 def parse_args() -> argparse.Namespace:
@@ -89,24 +90,30 @@
 
 def cleanup_kube_crd(
     kube_client: KubeClient,
     cluster: str,
     soa_dir: str = DEFAULT_SOA_DIR,
     dry_run: bool = False,
 ) -> bool:
+    service_attr = paasta_prefixed("service")
     existing_crds = kube_client.apiextensions.list_custom_resource_definition(
-        label_selector="paasta.yelp.com/service"
+        label_selector=service_attr
     )
 
     success = True
     for crd in existing_crds.items:
-        service = crd.metadata.labels["paasta.yelp.com/service"]
+        service = crd.metadata.labels[service_attr]
         if not service:
-            log.error(
-                f"CRD {crd.metadata.name} has empty paasta.yelp.com/service label"
+            log.error(f"CRD {crd.metadata.name} has empty {service_attr} label")
+            continue
+
+        protected_attr = paasta_prefixed("protected")
+        if crd.metadata.labels.get(protected_attr) is not None:
+            log.info(
+                f"CRD {crd.metadata.name} has {protected_attr} label set - skipping."
             )
             continue
 
         crd_config = service_configuration_lib.read_extra_service_information(
             service, f"crd-{cluster}", soa_dir=soa_dir
         )
         if crd_config:
```

### Comparing `paasta-tools-0.92.1/paasta_tools/paasta_cluster_boost.py` & `paasta-tools-1.0.0/paasta_tools/paasta_cluster_boost.py`

 * *Files 10% similar despite different names*

```diff
@@ -14,15 +14,14 @@
 # limitations under the License.
 import argparse
 import logging
 import sys
 
 from paasta_tools.autoscaling import load_boost
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 
 log = logging.getLogger(__name__)
 
 
 def parse_args():
     """Parses the command line arguments passed to this script"""
     parser = argparse.ArgumentParser()
@@ -66,78 +65,60 @@
         dest="verbose",
         default=0,
         help="Print out more output.",
     )
     return parser.parse_args()
 
 
-def get_regions(pool: str) -> list:
-    """ Return the regions where we have slaves running for a given pool
-    """
-    system_paasta_config = load_system_paasta_config()
-    expected_slave_attributes = system_paasta_config.get_expected_slave_attributes()
-    if expected_slave_attributes is None:
-        return []
-
-    regions = []  # type: list
-    for slave in expected_slave_attributes:
-        slave_region = slave["datacenter"]
-        if slave["pool"] == pool:
-            if slave_region not in regions:
-                regions.append(slave_region)
-    return regions
-
-
 def paasta_cluster_boost(
     action: str, pool: str, boost: float, duration: int, override: bool
 ) -> bool:
-    """ Set, Get or clear a boost on a paasta cluster for a given pool in a given region
+    """Set, Get or clear a boost on a paasta cluster for a given pool in a given region
     :returns: None
     """
     system_config = load_system_paasta_config()
 
     if not system_config.get_cluster_boost_enabled():
-        paasta_print("ERROR: cluster_boost feature is not enabled.")
+        print("ERROR: cluster_boost feature is not enabled.")
         return False
 
-    regions = get_regions(pool)
-
+    regions = system_config.get_boost_regions()
     if len(regions) == 0:
-        paasta_print(f"ERROR: no slaves found in pool {pool}")
+        print(f"ERROR: no boost_regions configured in {system_config.directory}")
         return False
 
     for region in regions:
         zk_boost_path = load_boost.get_zk_cluster_boost_path(region=region, pool=pool)
         if action == "set":
             if not load_boost.set_boost_factor(
                 zk_boost_path=zk_boost_path,
                 region=region,
                 pool=pool,
                 factor=boost,
                 duration_minutes=duration,
                 override=override,
             ):
-                paasta_print(
+                print(
                     f"ERROR: Failed to set the boost for pool {pool}, region {region}."
                 )
                 return False
 
         elif action == "status":
             pass
 
         elif action == "clear":
             if not load_boost.clear_boost(zk_boost_path, region=region, pool=pool):
-                paasta_print("ERROR: Failed to clear the boost for pool {}, region {}.")
+                print("ERROR: Failed to clear the boost for pool {}, region {}.")
                 return False
 
         else:
             raise NotImplementedError("Action: '%s' is not implemented." % action)
             return False
 
-        paasta_print(
+        print(
             "Current boost value for path: {}: {}".format(
                 zk_boost_path, load_boost.get_boost_factor(zk_boost_path=zk_boost_path)
             )
         )
     return True
```

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/util.py` & `paasta-tools-1.0.0/paasta_tools/mesos/util.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/cfg.py` & `paasta-tools-1.0.0/paasta_tools/mesos/cfg.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/task.py` & `paasta-tools-1.0.0/paasta_tools/mesos/task.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/exceptions.py` & `paasta-tools-1.0.0/paasta_tools/mesos/exceptions.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/parallel.py` & `paasta-tools-1.0.0/paasta_tools/mesos/parallel.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/zookeeper.py` & `paasta-tools-1.0.0/paasta_tools/mesos/log.py`

 * *Files 24% similar despite different names*

```diff
@@ -9,29 +9,40 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import contextlib
+import functools
+import logging
+import sys
+import time
+
+debug = logging.debug
+
+
+def fatal(msg, code=1):
+    print(msg + "\n")
+    logging.error(msg)
+    sys.exit(code)
+
+
+def fn(f, *args, **kwargs):
+    logging.debug("{}: {} {}".format(repr(f), args, kwargs))
+    return f(*args, **kwargs)
+
+
+def duration(fn):
+    @functools.wraps(fn)
+    def timer(*args, **kwargs):
+        start = time.time()
+        try:
+            return fn(*args, **kwargs)
+        finally:
+            debug(
+                "duration: {}.{}: {:2.2f}s".format(
+                    fn.__module__, fn.__name__, time.time() - start
+                )
+            )
 
-import kazoo.client
-import kazoo.exceptions
-import kazoo.handlers.threading
-
-
-TIMEOUT = 1
-
-# Helper for testing
-client_class = kazoo.client.KazooClient
-
-
-@contextlib.contextmanager
-def client(*args, **kwargs):
-    zk = client_class(*args, **kwargs)
-    zk.start(timeout=TIMEOUT)
-    try:
-        yield zk
-    finally:
-        zk.stop()
-        zk.close()
+    return timer
```

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/framework.py` & `paasta-tools-1.0.0/paasta_tools/mesos/framework.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/master.py` & `paasta-tools-1.0.0/paasta_tools/mesos/master.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/slave.py` & `paasta-tools-1.0.0/paasta_tools/mesos/slave.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/mesos_file.py` & `paasta-tools-1.0.0/paasta_tools/mesos/mesos_file.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos/cluster.py` & `paasta-tools-1.0.0/paasta_tools/mesos/cluster.py`

 * *Files 5% similar despite different names*

```diff
@@ -13,29 +13,28 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import asyncio
 import itertools
 
 from . import exceptions
-from paasta_tools.utils import paasta_print
 
 
 async def get_files_for_tasks(task_list, file_list, max_workers):
     no_files_found = True
 
     async def process(task_fname):
         task, fname = task_fname
         try:
             fobj = await task.file(fname)
         except exceptions.SlaveDoesNotExist:
             if task is None:
-                paasta_print(f"(Unknown Task):{fname} (Slave no longer exists)")
+                print(f"(Unknown Task):{fname} (Slave no longer exists)")
             else:
-                paasta_print(f"{task['id']}:{task_fname} (Slave no longer exists)")
+                print(f"{task['id']}:{task_fname} (Slave no longer exists)")
             raise exceptions.SkipResult
 
         if await fobj.exists():
             return fobj
 
     elements = itertools.chain(
         *[[(task, fname) for fname in file_list] for task in task_list]
```

### Comparing `paasta-tools-0.92.1/paasta_tools/paasta_execute_docker_command.py` & `paasta-tools-1.0.0/paasta_tools/paasta_execute_docker_command.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,15 +28,15 @@
 import argparse
 import signal
 import sys
 from contextlib import contextmanager
 
 from paasta_tools.mesos_tools import get_container_id_for_mesos_id
 from paasta_tools.utils import get_docker_client
-from paasta_tools.utils import paasta_print
+from paasta_tools.utils import is_using_unprivileged_containers
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         description="Executes given command in Docker container for given Mesos task ID"
     )
     parser.add_argument("-i", "--mesos-id", required=True, help="Mesos task ID")
@@ -65,15 +65,19 @@
         yield
     finally:
         signal.alarm(0)
 
 
 def execute_in_container(docker_client, container_id, cmd, timeout):
     container_info = docker_client.inspect_container(container_id)
-    if container_info["ExecIDs"] and len(container_info["ExecIDs"]) > 0:
+    if (
+        container_info["ExecIDs"]
+        and len(container_info["ExecIDs"]) > 0
+        and not is_using_unprivileged_containers()
+    ):
         for possible_exec_id in container_info["ExecIDs"]:
             exec_info = docker_client.exec_inspect(possible_exec_id)["ProcessConfig"]
             if exec_info["entrypoint"] == "/bin/sh" and exec_info["arguments"] == [
                 "-c",
                 cmd,
             ]:
                 exec_id = possible_exec_id
@@ -85,37 +89,35 @@
     return (output, return_code)
 
 
 def main():
     args = parse_args()
 
     if not args.mesos_id:
-        paasta_print(
+        print(
             "The Mesos task id you supplied seems to be an empty string! Please provide a valid task id."
         )
         sys.exit(2)
 
     docker_client = get_docker_client()
 
     container_id = get_container_id_for_mesos_id(docker_client, args.mesos_id)
 
     if container_id:
         try:
             with time_limit(args.timeout):
                 output, return_code = execute_in_container(
                     docker_client, container_id, args.cmd, args.timeout
                 )
-            paasta_print(output)
+            print(output)
         except TimeoutException:
-            paasta_print("Command timed out!")
+            print("Command timed out!")
             return_code = 1
         finally:
             sys.exit(return_code)
     else:
-        paasta_print(
-            "Could not find container with MESOS_TASK_ID '%s'." % args.mesos_id
-        )
+        print("Could not find container with MESOS_TASK_ID '%s'." % args.mesos_id)
         sys.exit(1)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/service.py` & `paasta-tools-1.0.0/paasta_tools/api/views/service.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/instance.py` & `paasta-tools-1.0.0/paasta_tools/instance/kubernetes.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,1096 +1,1355 @@
-#!/usr/bin/env python
-# Copyright 2015-2016 Yelp Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-"""
-PaaSTA service instance status/start/stop etc.
-"""
 import asyncio
-import datetime
-import logging
-import re
-import traceback
+from collections import defaultdict
+from enum import Enum
 from typing import Any
+from typing import DefaultDict
 from typing import Dict
+from typing import Iterable
 from typing import List
 from typing import Mapping
 from typing import MutableMapping
 from typing import Optional
-from typing import overload
 from typing import Sequence
+from typing import Set
 from typing import Tuple
 from typing import Union
 
 import a_sync
-import isodate
+import pytz
+from kubernetes.client import V1Container
+from kubernetes.client import V1ControllerRevision
 from kubernetes.client import V1Pod
+from kubernetes.client import V1Probe
 from kubernetes.client import V1ReplicaSet
 from kubernetes.client.rest import ApiException
-from marathon import MarathonClient
-from marathon.models.app import MarathonApp
-from marathon.models.app import MarathonTask
-from pyramid.response import Response
-from pyramid.view import view_config
-from requests.exceptions import ReadTimeout
+from mypy_extensions import TypedDict
 
-import paasta_tools.mesos.exceptions as mesos_exceptions
 from paasta_tools import cassandracluster_tools
+from paasta_tools import eks_tools
+from paasta_tools import envoy_tools
 from paasta_tools import flink_tools
 from paasta_tools import kafkacluster_tools
 from paasta_tools import kubernetes_tools
-from paasta_tools import marathon_tools
-from paasta_tools import paasta_remote_run
-from paasta_tools import tron_tools
-from paasta_tools.api import settings
-from paasta_tools.api.views.exception import ApiFailure
-from paasta_tools.autoscaling.autoscaling_service_lib import get_autoscaling_info
-from paasta_tools.cli.cmds.status import get_actual_deployments
+from paasta_tools import monkrelaycluster_tools
+from paasta_tools import nrtsearchservice_tools
+from paasta_tools import smartstack_tools
+from paasta_tools import vitesscluster_tools
 from paasta_tools.cli.utils import LONG_RUNNING_INSTANCE_TYPE_HANDLERS
-from paasta_tools.kubernetes_tools import get_tail_lines_for_kubernetes_pod
-from paasta_tools.kubernetes_tools import KubeClient
+from paasta_tools.instance.hpa_metrics_parser import HPAMetricsDict
+from paasta_tools.instance.hpa_metrics_parser import HPAMetricsParser
+from paasta_tools.kubernetes_tools import get_pod_event_messages
+from paasta_tools.kubernetes_tools import get_tail_lines_for_kubernetes_container
 from paasta_tools.kubernetes_tools import KubernetesDeploymentConfig
+from paasta_tools.kubernetes_tools import paasta_prefixed
+from paasta_tools.long_running_service_tools import (
+    get_expected_instance_count_for_namespace,
+)
 from paasta_tools.long_running_service_tools import LongRunningServiceConfig
 from paasta_tools.long_running_service_tools import ServiceNamespaceConfig
-from paasta_tools.marathon_tools import get_short_task_id
-from paasta_tools.mesos.task import Task
-from paasta_tools.mesos_tools import (
-    get_cached_list_of_not_running_tasks_from_frameworks,
-)
-from paasta_tools.mesos_tools import get_cached_list_of_running_tasks_from_frameworks
-from paasta_tools.mesos_tools import get_cpu_shares
-from paasta_tools.mesos_tools import get_first_status_timestamp
-from paasta_tools.mesos_tools import get_mesos_slaves_grouped_by_attribute
-from paasta_tools.mesos_tools import get_short_hostname_from_task
-from paasta_tools.mesos_tools import get_slaves
-from paasta_tools.mesos_tools import get_tail_lines_for_mesos_task
-from paasta_tools.mesos_tools import get_task
-from paasta_tools.mesos_tools import get_tasks_from_app_id
-from paasta_tools.mesos_tools import results_or_unknown
-from paasta_tools.mesos_tools import select_tasks_by_id
-from paasta_tools.mesos_tools import TaskNotFound
-from paasta_tools.smartstack_tools import backend_is_up
-from paasta_tools.smartstack_tools import get_backends
-from paasta_tools.smartstack_tools import HaproxyBackend
-from paasta_tools.smartstack_tools import KubeSmartstackReplicationChecker
+from paasta_tools.smartstack_tools import KubeSmartstackEnvoyReplicationChecker
 from paasta_tools.smartstack_tools import match_backends_and_pods
-from paasta_tools.smartstack_tools import match_backends_and_tasks
 from paasta_tools.utils import calculate_tail_lines
-from paasta_tools.utils import INSTANCE_TYPES_K8S
-from paasta_tools.utils import INSTANCE_TYPES_WITH_SET_STATE
-from paasta_tools.utils import NoConfigurationForServiceError
-from paasta_tools.utils import NoDockerImageError
-from paasta_tools.utils import TimeoutError
-from paasta_tools.utils import validate_service_instance
 
-log = logging.getLogger(__name__)
 
+INSTANCE_TYPES_CR = {
+    "flink",
+    "flinkeks",
+    "cassandracluster",
+    "kafkacluster",
+    "vitesscluster",
+}
+INSTANCE_TYPES_K8S = {
+    "cassandracluster",
+    "eks",
+    "kubernetes",
+}
+INSTANCE_TYPES = INSTANCE_TYPES_K8S.union(INSTANCE_TYPES_CR)
 
-def tron_instance_status(
-    instance_status: Mapping[str, Any], service: str, instance: str, verbose: int
-) -> Mapping[str, Any]:
-    status: Dict[str, Any] = {}
-    client = tron_tools.get_tron_client()
-    short_job, action = instance.split(".")
-    job = f"{service}.{short_job}"
-    job_content = client.get_job_content(job=job)
+INSTANCE_TYPES_WITH_SET_STATE = {"flink", "flinkeks"}
+INSTANCE_TYPE_CR_ID = dict(
+    flink=flink_tools.cr_id,
+    flinkeks=flink_tools.cr_id,
+    cassandracluster=cassandracluster_tools.cr_id,
+    kafkacluster=kafkacluster_tools.cr_id,
+    vitesscluster=vitesscluster_tools.cr_id,
+    nrtsearchservice=nrtsearchservice_tools.cr_id,
+    nrtsearchserviceeks=nrtsearchservice_tools.cr_id,
+    monkrelaycluster=monkrelaycluster_tools.cr_id,
+)
 
-    try:
-        latest_run_id = client.get_latest_job_run_id(job_content=job_content)
-        if latest_run_id is None:
-            action_run = {"state": "Hasn't run yet (no job run id found)"}
-        else:
-            action_run = client.get_action_run(
-                job=job, action=action, run_id=latest_run_id
-            )
-    except Exception as e:
-        action_run = {"state": f"Failed to get latest run info: {e}"}
 
-    # job info
-    status["job_name"] = short_job
-    status["job_status"] = job_content["status"]
-    status["job_schedule"] = "{} {}".format(
-        job_content["scheduler"]["type"], job_content["scheduler"]["value"]
-    )
-    status["job_url"] = (
-        tron_tools.get_tron_dashboard_for_cluster(settings.cluster) + f"#job/{job}"
-    )
-
-    if action:
-        status["action_name"] = action
-    if action_run.get("state"):
-        status["action_state"] = action_run["state"]
-    if action_run.get("start_time"):
-        status["action_start_time"] = action_run["start_time"]
-    if action_run.get("raw_command"):
-        status["action_raw_command"] = action_run["raw_command"]
-    if action_run.get("stdout"):
-        status["action_stdout"] = "\n".join(action_run["stdout"])
-    if action_run.get("stderr"):
-        status["action_stderr"] = "\n".join(action_run["stderr"])
-    if action_run.get("command"):
-        status["action_command"] = action_run["command"]
+class ServiceMesh(Enum):
+    SMARTSTACK = "smartstack"
+    ENVOY = "envoy"
 
-    return status
 
+class KubernetesAutoscalingStatusDict(TypedDict):
+    min_instances: int
+    max_instances: int
+    metrics: List
+    desired_replicas: int
+    last_scale_time: str
 
-def adhoc_instance_status(
-    instance_status: Mapping[str, Any], service: str, instance: str, verbose: int
-) -> List[Dict[str, Any]]:
-    status = []
-    filtered = paasta_remote_run.remote_run_filter_frameworks(service, instance)
-    filtered.sort(key=lambda x: x.name)
-    for f in filtered:
-        launch_time, run_id = re.match(
-            r"paasta-remote [^\s]+ (\w+) (\w+)", f.name
-        ).groups()
-        status.append(
-            {"launch_time": launch_time, "run_id": run_id, "framework_id": f.id}
-        )
-    return status
+
+class KubernetesVersionDict(TypedDict, total=False):
+    name: str
+    type: str
+    replicas: int
+    ready_replicas: int
+    create_timestamp: int
+    git_sha: str
+    image_version: Optional[str]
+    config_sha: str
+    pods: Sequence[Mapping[str, Any]]
+    namespace: str
+
+
+def cr_id(service: str, instance: str, instance_type: str) -> Mapping[str, str]:
+    cr_id_fn = INSTANCE_TYPE_CR_ID.get(instance_type)
+    if not cr_id_fn:
+        raise RuntimeError(f"Unknown instance type {instance_type}")
+    return cr_id_fn(service, instance)
+
+
+def can_handle(instance_type: str) -> bool:
+    return instance_type in INSTANCE_TYPES
 
 
-def kubernetes_cr_status(cr_id: dict, verbose: int) -> Optional[Mapping[str, Any]]:
-    client = settings.kubernetes_client
-    if client is not None:
-        return kubernetes_tools.get_cr_status(kube_client=client, cr_id=cr_id)
-    return None
-
-
-def kubernetes_cr_metadata(cr_id: dict, verbose: int) -> Optional[Mapping[str, Any]]:
-    client = settings.kubernetes_client
-    if client is not None:
-        return kubernetes_tools.get_cr_metadata(kube_client=client, cr_id=cr_id)
-    return None
+def can_set_state(instance_type: str) -> bool:
+    return instance_type in INSTANCE_TYPES_WITH_SET_STATE
 
 
-def kubernetes_instance_status(
-    instance_status: Mapping[str, Any],
+def set_cr_desired_state(
+    kube_client: kubernetes_tools.KubeClient,
     service: str,
     instance: str,
-    verbose: int,
-    include_smartstack: bool,
     instance_type: str,
-) -> Mapping[str, Any]:
-    kstatus: Dict[str, Any] = {}
-    config_loader = LONG_RUNNING_INSTANCE_TYPE_HANDLERS[instance_type].loader
-    job_config = config_loader(
-        service=service,
-        instance=instance,
-        cluster=settings.cluster,
-        soa_dir=settings.soa_dir,
-        load_deployments=True,
-    )
-    client = settings.kubernetes_client
-    if client is not None:
-        # bouncing status can be inferred from app_count, ref get_bouncing_status
-        pod_list = kubernetes_tools.pods_for_service_instance(
-            service=job_config.service,
-            instance=job_config.instance,
-            kube_client=client,
-            namespace=job_config.get_kubernetes_namespace(),
+    desired_state: str,
+) -> None:
+    try:
+        kubernetes_tools.set_cr_desired_state(
+            kube_client=kube_client,
+            cr_id=cr_id(service, instance, instance_type),
+            desired_state=desired_state,
         )
-        replicaset_list = kubernetes_tools.replicasets_for_service_instance(
-            service=job_config.service,
-            instance=job_config.instance,
-            kube_client=client,
-            namespace=job_config.get_kubernetes_namespace(),
+    except ApiException as e:
+        error_message = (
+            f"Error while setting state {desired_state} of "
+            f"{service}.{instance}: {e}"
         )
-        active_shas = kubernetes_tools.get_active_shas_for_service(pod_list)
-        kstatus["app_count"] = max(
-            len(active_shas["config_sha"]), len(active_shas["git_sha"])
-        )
-        kstatus["desired_state"] = job_config.get_desired_state()
-        kstatus["bounce_method"] = job_config.get_bounce_method()
-        kubernetes_job_status(
-            kstatus=kstatus,
-            client=client,
-            namespace=job_config.get_kubernetes_namespace(),
-            job_config=job_config,
-            verbose=verbose,
-            pod_list=pod_list,
-            replicaset_list=replicaset_list,
+        raise RuntimeError(error_message)
+
+
+async def autoscaling_status(
+    kube_client: kubernetes_tools.KubeClient,
+    job_config: LongRunningServiceConfig,
+    namespace: str,
+) -> KubernetesAutoscalingStatusDict:
+    hpa = await kubernetes_tools.get_hpa(
+        kube_client,
+        name=job_config.get_sanitised_deployment_name(),
+        namespace=namespace,
+    )
+    if hpa is None:
+        return KubernetesAutoscalingStatusDict(
+            min_instances=-1,
+            max_instances=-1,
+            metrics=[],
+            desired_replicas=-1,
+            last_scale_time="unknown (could not find HPA object)",
         )
 
-        evicted_count = 0
-        for pod in pod_list:
-            if pod.status.reason == "Evicted":
-                evicted_count += 1
-        kstatus["evicted_count"] = evicted_count
-
-        if include_smartstack:
-            service_namespace_config = kubernetes_tools.load_service_namespace_config(
-                service=job_config.get_service_name_smartstack(),
-                namespace=job_config.get_nerve_namespace(),
-                soa_dir=settings.soa_dir,
-            )
-            if "proxy_port" in service_namespace_config:
-                kstatus["smartstack"] = kubernetes_smartstack_status(
-                    service=job_config.get_service_name_smartstack(),
-                    instance=job_config.get_nerve_namespace(),
-                    job_config=job_config,
-                    service_namespace_config=service_namespace_config,
-                    pods=pod_list,
-                    should_return_individual_backends=verbose > 0,
-                )
-    return kstatus
+    # Parse metrics sources, based on
+    # https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/V2beta2ExternalMetricSource.md#v2beta2externalmetricsource
+    parser = HPAMetricsParser(hpa)
 
+    # https://github.com/python/mypy/issues/7217
+    metrics_by_name: DefaultDict[str, HPAMetricsDict] = defaultdict(
+        lambda: HPAMetricsDict()
+    )
 
-@a_sync.to_blocking
-async def kubernetes_job_status(
+    if hpa.spec.metrics is not None:
+        for metric_spec in hpa.spec.metrics:
+            parsed = parser.parse_target(metric_spec)
+            metrics_by_name[parsed["name"]].update(parsed)
+
+    if hpa.status.current_metrics is not None:
+        for metric_spec in hpa.status.current_metrics:
+            parsed = parser.parse_current(metric_spec)
+            if parsed is not None:
+                metrics_by_name[parsed["name"]].update(parsed)
+
+    metric_stats = list(metrics_by_name.values())
+
+    last_scale_time = (
+        hpa.status.last_scale_time.replace(tzinfo=pytz.UTC).isoformat()
+        if getattr(hpa.status, "last_scale_time")
+        else "N/A"
+    )
+
+    return KubernetesAutoscalingStatusDict(
+        min_instances=hpa.spec.min_replicas,
+        max_instances=hpa.spec.max_replicas,
+        metrics=metric_stats,
+        desired_replicas=hpa.status.desired_replicas,
+        last_scale_time=last_scale_time,
+    )
+
+
+async def pod_info(
+    pod: V1Pod,
+    client: kubernetes_tools.KubeClient,
+    num_tail_lines: int,
+) -> Dict[str, Any]:
+    container_statuses = pod.status.container_statuses or []
+    try:
+        pod_event_messages = await get_pod_event_messages(client, pod)
+    except asyncio.TimeoutError:
+        pod_event_messages = [{"error": "Could not fetch events for pod"}]
+    containers = [
+        dict(
+            name=container.name,
+            tail_lines=await get_tail_lines_for_kubernetes_container(
+                client,
+                pod,
+                container,
+                num_tail_lines,
+            ),
+        )
+        for container in container_statuses
+    ]
+    return {
+        "name": pod.metadata.name,
+        "host": kubernetes_tools.get_pod_hostname(client, pod),
+        "deployed_timestamp": pod.metadata.creation_timestamp.timestamp(),
+        "phase": pod.status.phase,
+        "ready": kubernetes_tools.is_pod_ready(pod),
+        "containers": containers,
+        "reason": pod.status.reason,
+        "message": pod.status.message,
+        "events": pod_event_messages,
+        "git_sha": pod.metadata.labels.get("paasta.yelp.com/git_sha"),
+        "config_sha": pod.metadata.labels.get("paasta.yelp.com/config_sha"),
+    }
+
+
+# TODO: Cleanup
+# Only used in old kubernetes_status
+async def job_status(
     kstatus: MutableMapping[str, Any],
     client: kubernetes_tools.KubeClient,
     job_config: LongRunningServiceConfig,
     pod_list: Sequence[V1Pod],
     replicaset_list: Sequence[V1ReplicaSet],
     verbose: int,
     namespace: str,
 ) -> None:
     app_id = job_config.get_sanitised_deployment_name()
     kstatus["app_id"] = app_id
     kstatus["pods"] = []
     kstatus["replicasets"] = []
+
     if verbose > 0:
         num_tail_lines = calculate_tail_lines(verbose)
+        kstatus["pods"] = await asyncio.gather(
+            *[pod_info(pod, client, num_tail_lines) for pod in pod_list]
+        )
 
-        for pod in pod_list:
-            if num_tail_lines > 0:
-                tail_lines = await get_tail_lines_for_kubernetes_pod(
-                    client, pod, num_tail_lines
-                )
-            else:
-                tail_lines = {}
-
-            kstatus["pods"].append(
-                {
-                    "name": pod.metadata.name,
-                    "host": pod.spec.node_name,
-                    "deployed_timestamp": pod.metadata.creation_timestamp.timestamp(),
-                    "phase": pod.status.phase,
-                    "tail_lines": tail_lines,
-                    "reason": pod.status.reason,
-                    "message": pod.status.message,
-                }
-            )
-        for replicaset in replicaset_list:
-            try:
-                ready_replicas = replicaset.status.ready_replicas
-                if ready_replicas is None:
-                    ready_replicas = 0
-            except AttributeError:
-                ready_replicas = 0
-
-            kstatus["replicasets"].append(
-                {
-                    "name": replicaset.metadata.name,
-                    "replicas": replicaset.spec.replicas,
-                    "ready_replicas": ready_replicas,
-                    "create_timestamp": replicaset.metadata.creation_timestamp.timestamp(),
-                }
-            )
+    for replicaset in replicaset_list:
+        kstatus["replicasets"].append(
+            {
+                "name": replicaset.metadata.name,
+                "replicas": replicaset.spec.replicas,
+                "ready_replicas": ready_replicas_from_replicaset(replicaset),
+                "create_timestamp": replicaset.metadata.creation_timestamp.timestamp(),
+                "git_sha": replicaset.metadata.labels.get("paasta.yelp.com/git_sha"),
+                "config_sha": replicaset.metadata.labels.get(
+                    "paasta.yelp.com/config_sha"
+                ),
+            }
+        )
 
     kstatus["expected_instance_count"] = job_config.get_instances()
 
     app = kubernetes_tools.get_kubernetes_app_by_name(
         name=app_id, kube_client=client, namespace=namespace
     )
-    deploy_status = kubernetes_tools.get_kubernetes_app_deploy_status(
-        app=app, desired_instances=job_config.get_instances()
+    desired_instances = (
+        job_config.get_instances() if job_config.get_desired_state() != "stop" else 0
+    )
+    deploy_status, message = kubernetes_tools.get_kubernetes_app_deploy_status(
+        app=app,
+        desired_instances=desired_instances,
     )
     kstatus["deploy_status"] = kubernetes_tools.KubernetesDeployStatus.tostring(
         deploy_status
     )
+    kstatus["deploy_status_message"] = message
     kstatus["running_instance_count"] = (
         app.status.ready_replicas if app.status.ready_replicas else 0
     )
     kstatus["create_timestamp"] = app.metadata.creation_timestamp.timestamp()
     kstatus["namespace"] = app.metadata.namespace
 
 
-def marathon_instance_status(
-    instance_status: Mapping[str, Any],
+async def get_backends_from_mesh_status(
+    mesh_status_task: "asyncio.Future[Dict[str, Any]]",
+) -> Set[str]:
+    status = await mesh_status_task
+    if status.get("locations"):
+        backends = {be["address"] for be in status["locations"][0].get("backends", [])}
+    else:
+        backends = set()
+
+    return backends
+
+
+async def mesh_status(
     service: str,
+    service_mesh: ServiceMesh,
     instance: str,
-    verbose: int,
-    include_smartstack: bool,
-    include_mesos: bool,
+    job_config: LongRunningServiceConfig,
+    service_namespace_config: ServiceNamespaceConfig,
+    pods_task: "asyncio.Future[V1Pod]",
+    settings: Any,
+    should_return_individual_backends: bool = False,
 ) -> Mapping[str, Any]:
-    mstatus: Dict[str, Any] = {}
+    registration = job_config.get_registrations()[0]
+    instance_pool = job_config.get_pool()
 
-    job_config = marathon_tools.load_marathon_service_config(
-        service, instance, settings.cluster, soa_dir=settings.soa_dir
-    )
-    marathon_apps_with_clients = marathon_tools.get_marathon_apps_with_clients(
-        clients=settings.marathon_clients.get_all_clients_for_service(job_config),
-        embed_tasks=True,
-        service_name=service,
+    async_get_nodes = a_sync.to_async(kubernetes_tools.get_all_nodes)
+    nodes = await async_get_nodes(settings.kubernetes_client)
+
+    replication_checker = KubeSmartstackEnvoyReplicationChecker(
+        nodes=nodes,
+        system_paasta_config=settings.system_paasta_config,
     )
-    matching_apps_with_clients = marathon_tools.get_matching_apps_with_clients(
-        service, instance, marathon_apps_with_clients
+    node_hostname_by_location = replication_checker.get_allowed_locations_and_hosts(
+        job_config
     )
 
-    mstatus.update(
-        marathon_job_status(
-            service, instance, job_config, matching_apps_with_clients, verbose
-        )
+    expected_smartstack_count = get_expected_instance_count_for_namespace(
+        service=service,
+        namespace=job_config.get_nerve_namespace(),
+        cluster=settings.cluster,
+        instance_type_class=KubernetesDeploymentConfig,
     )
+    expected_count_per_location = int(
+        expected_smartstack_count / len(node_hostname_by_location)
+    )
+    mesh_status: MutableMapping[str, Any] = {
+        "registration": registration,
+        "expected_backends_per_location": expected_count_per_location,
+        "locations": [],
+    }
 
-    if include_smartstack:
-        service_namespace_config = marathon_tools.load_service_namespace_config(
-            service=service,
-            namespace=job_config.get_nerve_namespace(),
-            soa_dir=settings.soa_dir,
-        )
-        if "proxy_port" in service_namespace_config:
-            tasks = [
-                task for app, _ in matching_apps_with_clients for task in app.tasks
-            ]
-
-            mstatus["smartstack"] = marathon_smartstack_status(
-                service,
-                instance,
-                job_config,
-                service_namespace_config,
-                tasks,
-                should_return_individual_backends=verbose > 0,
+    pods = await pods_task
+    for location, hosts in node_hostname_by_location.items():
+        host = replication_checker.get_hostname_in_pool(hosts, instance_pool)
+        if service_mesh == ServiceMesh.SMARTSTACK:
+            mesh_status["locations"].append(
+                _build_smartstack_location_dict(
+                    synapse_host=host,
+                    synapse_port=settings.system_paasta_config.get_synapse_port(),
+                    synapse_haproxy_url_format=settings.system_paasta_config.get_synapse_haproxy_url_format(),
+                    registration=registration,
+                    pods=pods,
+                    location=location,
+                    should_return_individual_backends=should_return_individual_backends,
+                )
             )
-
-    if include_mesos:
-        mstatus["mesos"] = marathon_mesos_status(service, instance, verbose)
-
-    return mstatus
+        elif service_mesh == ServiceMesh.ENVOY:
+            mesh_status["locations"].append(
+                _build_envoy_location_dict(
+                    envoy_host=host,
+                    envoy_admin_port=settings.system_paasta_config.get_envoy_admin_port(),
+                    envoy_admin_endpoint_format=settings.system_paasta_config.get_envoy_admin_endpoint_format(),
+                    registration=registration,
+                    pods=pods,
+                    location=location,
+                    should_return_individual_backends=should_return_individual_backends,
+                )
+            )
+    return mesh_status
 
 
-def marathon_job_status(
-    service: str,
-    instance: str,
-    job_config: marathon_tools.MarathonServiceConfig,
-    marathon_apps_with_clients: List[Tuple[MarathonApp, MarathonClient]],
-    verbose: int,
+def _build_envoy_location_dict(
+    envoy_host: str,
+    envoy_admin_port: int,
+    envoy_admin_endpoint_format: str,
+    registration: str,
+    pods: Iterable[V1Pod],
+    location: str,
+    should_return_individual_backends: bool,
 ) -> MutableMapping[str, Any]:
-    job_status_fields: MutableMapping[str, Any] = {
-        "app_statuses": [],
-        "app_count": len(marathon_apps_with_clients),
-        "desired_state": job_config.get_desired_state(),
-        "bounce_method": job_config.get_bounce_method(),
-        "expected_instance_count": job_config.get_instances(),
+    backends = envoy_tools.get_backends(
+        registration,
+        envoy_host=envoy_host,
+        envoy_admin_port=envoy_admin_port,
+        envoy_admin_endpoint_format=envoy_admin_endpoint_format,
+    )
+    sorted_envoy_backends = sorted(
+        [
+            backend[0]
+            for _, service_backends in backends.items()
+            for backend in service_backends
+        ],
+        key=lambda backend: backend["eds_health_status"],
+    )
+    casper_proxied_backends = {
+        (backend["address"], backend["port_value"])
+        for _, service_backends in backends.items()
+        for backend, is_casper_proxied_backend in service_backends
+        if is_casper_proxied_backend
     }
 
-    try:
-        desired_app_id = job_config.format_marathon_app_dict()["id"]
-    except NoDockerImageError:
-        error_msg = "Docker image is not in deployments.json."
-        job_status_fields["error_message"] = error_msg
-        return job_status_fields
-
-    job_status_fields["desired_app_id"] = desired_app_id
-
-    deploy_status_for_desired_app = None
-    dashboard_links = get_marathon_dashboard_links(
-        settings.marathon_clients, settings.system_paasta_config
-    )
-    tasks_running = 0
-    for app, marathon_client in marathon_apps_with_clients:
-        deploy_status = marathon_tools.get_marathon_app_deploy_status(
-            marathon_client, app
-        )
-
-        app_status = marathon_app_status(
-            app,
-            marathon_client,
-            dashboard_links.get(marathon_client) if dashboard_links else None,
-            deploy_status,
-            list_tasks=verbose > 0,
-        )
-        job_status_fields["app_statuses"].append(app_status)
-
-        if app.id.lstrip("/") == desired_app_id.lstrip("/"):
-            deploy_status_for_desired_app = marathon_tools.MarathonDeployStatus.tostring(
-                deploy_status
-            )
-        tasks_running += app.tasks_running
-
-    job_status_fields["deploy_status"] = (
-        deploy_status_for_desired_app or "Waiting for bounce"
+    matched_envoy_backends_and_pods = envoy_tools.match_backends_and_pods(
+        sorted_envoy_backends,
+        pods,
     )
-    job_status_fields["running_instance_count"] = tasks_running
-
-    if verbose > 0:
-        autoscaling_info = get_autoscaling_info(marathon_apps_with_clients, job_config)
-        if autoscaling_info is not None:
-            autoscaling_info_dict = autoscaling_info._asdict()
 
-            for field in ("current_utilization", "target_instances"):
-                if autoscaling_info_dict[field] is None:
-                    del autoscaling_info_dict[field]
-
-            job_status_fields["autoscaling_info"] = autoscaling_info_dict
-
-    return job_status_fields
+    return envoy_tools.build_envoy_location_dict(
+        location,
+        matched_envoy_backends_and_pods,
+        should_return_individual_backends,
+        casper_proxied_backends,
+    )
 
 
-def marathon_app_status(
-    app: MarathonApp,
-    marathon_client: MarathonClient,
-    dashboard_link: Optional[str],
-    deploy_status: int,
-    list_tasks: bool = False,
+def _build_smartstack_location_dict(
+    synapse_host: str,
+    synapse_port: int,
+    synapse_haproxy_url_format: str,
+    registration: str,
+    pods: Iterable[V1Pod],
+    location: str,
+    should_return_individual_backends: bool,
 ) -> MutableMapping[str, Any]:
-    app_status = {
-        "tasks_running": app.tasks_running,
-        "tasks_healthy": app.tasks_healthy,
-        "tasks_staged": app.tasks_staged,
-        "tasks_total": app.instances,
-        "create_timestamp": isodate.parse_datetime(app.version).timestamp(),
-        "deploy_status": marathon_tools.MarathonDeployStatus.tostring(deploy_status),
-    }
-
-    app_queue = marathon_tools.get_app_queue(marathon_client, app.id)
-    if deploy_status == marathon_tools.MarathonDeployStatus.Delayed:
-        _, backoff_seconds = marathon_tools.get_app_queue_status_from_queue(app_queue)
-        app_status["backoff_seconds"] = backoff_seconds
-
-    unused_offers_summary = marathon_tools.summarize_unused_offers(app_queue)
-    if unused_offers_summary is not None:
-        app_status["unused_offers"] = unused_offers_summary
-
-    if dashboard_link:
-        app_status["dashboard_url"] = "{}/ui/#/apps/%2F{}".format(
-            dashboard_link.rstrip("/"), app.id.lstrip("/")
-        )
-
-    if list_tasks is True:
-        app_status["tasks"] = []
-        for task in app.tasks:
-            app_status["tasks"].append(build_marathon_task_dict(task))
-
-    return app_status
-
-
-def build_marathon_task_dict(marathon_task: MarathonTask) -> MutableMapping[str, Any]:
-    task_dict = {
-        "id": get_short_task_id(marathon_task.id),
-        "host": marathon_task.host.split(".")[0],
-        "port": marathon_task.ports[0],
-        "deployed_timestamp": marathon_task.staged_at.timestamp(),
-    }
-
-    if marathon_task.health_check_results:
-        task_dict["is_healthy"] = marathon_tools.is_task_healthy(marathon_task)
+    sorted_backends = sorted(
+        smartstack_tools.get_backends(
+            registration,
+            synapse_host=synapse_host,
+            synapse_port=synapse_port,
+            synapse_haproxy_url_format=synapse_haproxy_url_format,
+        ),
+        key=lambda backend: backend["status"],
+        reverse=True,  # put 'UP' backends above 'MAINT' backends
+    )
 
-    return task_dict
+    matched_backends_and_pods = match_backends_and_pods(sorted_backends, pods)
+    location_dict = smartstack_tools.build_smartstack_location_dict(
+        location, matched_backends_and_pods, should_return_individual_backends
+    )
+    return location_dict
 
 
-def marathon_smartstack_status(
+def cr_status(
     service: str,
     instance: str,
-    job_config: marathon_tools.MarathonServiceConfig,
-    service_namespace_config: ServiceNamespaceConfig,
-    tasks: Sequence[MarathonTask],
-    should_return_individual_backends: bool = False,
+    verbose: int,
+    instance_type: str,
+    kube_client: Any,
 ) -> Mapping[str, Any]:
-    registration = job_config.get_registrations()[0]
-    discover_location_type = service_namespace_config.get_discover()
+    status: MutableMapping[str, Any] = {}
+    cr = (
+        kubernetes_tools.get_cr(
+            kube_client=kube_client, cr_id=cr_id(service, instance, instance_type)
+        )
+        or {}
+    )
+    crstatus = cr.get("status")
+    metadata = cr.get("metadata")
+    if crstatus is not None:
+        status["status"] = crstatus
+    if metadata is not None:
+        status["metadata"] = metadata
+    return status
 
-    grouped_slaves = get_mesos_slaves_grouped_by_attribute(
-        slaves=get_slaves(), attribute=discover_location_type
-    )
 
-    # rebuild the dict, replacing the slave object with just their hostname
-    slave_hostname_by_location = {
-        attribute_value: [slave["hostname"] for slave in slaves]
-        for attribute_value, slaves in grouped_slaves.items()
-    }
+def filter_actually_running_replicasets(
+    replicaset_list: Sequence[V1ReplicaSet],
+) -> List[V1ReplicaSet]:
+    return [
+        rs
+        for rs in replicaset_list
+        if not (rs.spec.replicas == 0 and ready_replicas_from_replicaset(rs) == 0)
+    ]
 
-    expected_smartstack_count = marathon_tools.get_expected_instance_count_for_namespace(
-        service, instance, settings.cluster
+
+def bounce_status(
+    service: str, instance: str, settings: Any, is_eks: bool = False
+) -> Dict[str, Any]:
+    status: Dict[str, Any] = {}
+    # this should be the only place where it matters that we use eks_tools.
+    # apart from loading config files, we should be using kubernetes_tools
+    # everywhere.
+    job_config: Union[KubernetesDeploymentConfig, eks_tools.EksDeploymentConfig]
+    if is_eks:
+        job_config = eks_tools.load_eks_service_config(
+            service=service,
+            instance=instance,
+            cluster=settings.cluster,
+            soa_dir=settings.soa_dir,
+            load_deployments=True,
+        )
+    else:
+        job_config = kubernetes_tools.load_kubernetes_service_config(
+            service=service,
+            instance=instance,
+            cluster=settings.cluster,
+            soa_dir=settings.soa_dir,
+            load_deployments=True,
+        )
+    expected_instance_count = job_config.get_instances()
+    status["expected_instance_count"] = expected_instance_count
+    desired_state = job_config.get_desired_state()
+    status["desired_state"] = desired_state
+
+    kube_client = settings.kubernetes_client
+    if kube_client is None:
+        raise RuntimeError("Could not load Kubernetes client!")
+
+    app = kubernetes_tools.get_kubernetes_app_by_name(
+        name=job_config.get_sanitised_deployment_name(),
+        kube_client=kube_client,
+        namespace=job_config.get_kubernetes_namespace(),
     )
-    expected_count_per_location = int(
-        expected_smartstack_count / len(slave_hostname_by_location)
+    status["running_instance_count"] = (
+        app.status.ready_replicas if app.status.ready_replicas else 0
     )
-    smartstack_status: MutableMapping[str, Any] = {
-        "registration": registration,
-        "expected_backends_per_location": expected_count_per_location,
-        "locations": [],
-    }
 
-    for location, hosts in slave_hostname_by_location.items():
-        synapse_host = hosts[0]
-        sorted_backends = sorted(
-            get_backends(
-                registration,
-                synapse_host=synapse_host,
-                synapse_port=settings.system_paasta_config.get_synapse_port(),
-                synapse_haproxy_url_format=settings.system_paasta_config.get_synapse_haproxy_url_format(),
-            ),
-            key=lambda backend: backend["status"],
-            reverse=True,  # put 'UP' backends above 'MAINT' backends
+    deploy_status, message = kubernetes_tools.get_kubernetes_app_deploy_status(
+        app=app,
+        desired_instances=(expected_instance_count if desired_state != "stop" else 0),
+    )
+    status["deploy_status"] = kubernetes_tools.KubernetesDeployStatus.tostring(
+        deploy_status
+    )
+
+    if job_config.get_persistent_volumes():
+        version_objects = a_sync.block(
+            kubernetes_tools.controller_revisions_for_service_instance,
+            service=job_config.service,
+            instance=job_config.instance,
+            kube_client=kube_client,
+            namespace=job_config.get_kubernetes_namespace(),
         )
-        matched_backends_and_tasks = match_backends_and_tasks(sorted_backends, tasks)
-        location_dict = build_smartstack_location_dict(
-            location, matched_backends_and_tasks, should_return_individual_backends
+    else:
+        replicasets = a_sync.block(
+            kubernetes_tools.replicasets_for_service_instance,
+            service=job_config.service,
+            instance=job_config.instance,
+            kube_client=kube_client,
+            namespace=job_config.get_kubernetes_namespace(),
         )
-        smartstack_status["locations"].append(location_dict)
+        version_objects = filter_actually_running_replicasets(replicasets)
 
-    return smartstack_status
+    active_versions = kubernetes_tools.get_active_versions_for_service(
+        [app, *version_objects],
+    )
+    status["active_shas"] = [
+        (deployment_version.sha, config_sha)
+        for deployment_version, config_sha in active_versions
+    ]
+    status["active_versions"] = [
+        (deployment_version.sha, deployment_version.image_version, config_sha)
+        for deployment_version, config_sha in active_versions
+    ]
+    status["app_count"] = len(active_versions)
+    return status
 
 
-def kubernetes_smartstack_status(
+async def get_pods_for_service_instance_multiple_namespaces(
     service: str,
     instance: str,
+    kube_client: kubernetes_tools.KubeClient,
+    namespaces: Iterable[str],
+) -> Sequence[V1Pod]:
+    ret: List[V1Pod] = []
+
+    for coro in asyncio.as_completed(
+        [
+            kubernetes_tools.pods_for_service_instance(
+                service=service,
+                instance=instance,
+                kube_client=kube_client,
+                namespace=namespace,
+            )
+            for namespace in namespaces
+        ]
+    ):
+        ret.extend(await coro)
+
+    return ret
+
+
+def find_all_relevant_namespaces(
+    service: str,
+    instance: str,
+    kube_client: kubernetes_tools.KubeClient,
     job_config: LongRunningServiceConfig,
-    service_namespace_config: ServiceNamespaceConfig,
-    pods: Sequence[V1Pod],
-    should_return_individual_backends: bool = False,
-) -> Mapping[str, Any]:
+) -> Set[str]:
+    return {job_config.get_kubernetes_namespace()} | {
+        deployment.namespace
+        for deployment in kubernetes_tools.list_deployments_in_managed_namespaces(
+            kube_client=kube_client,
+            label_selector=f"{paasta_prefixed('service')}={service},{paasta_prefixed('instance')}={instance}",
+        )
+    }
 
-    registration = job_config.get_registrations()[0]
-    instance_pool = job_config.get_pool()
 
-    smartstack_replication_checker = KubeSmartstackReplicationChecker(
-        nodes=kubernetes_tools.get_all_nodes(settings.kubernetes_client),
-        system_paasta_config=settings.system_paasta_config,
+@a_sync.to_blocking
+async def kubernetes_status_v2(
+    service: str,
+    instance: str,
+    verbose: int,
+    include_envoy: bool,
+    instance_type: str,
+    settings: Any,
+) -> Dict[str, Any]:
+    status: Dict[str, Any] = {}
+    config_loader = LONG_RUNNING_INSTANCE_TYPE_HANDLERS[instance_type].loader
+    job_config = config_loader(
+        service=service,
+        instance=instance,
+        cluster=settings.cluster,
+        soa_dir=settings.soa_dir,
+        load_deployments=True,
     )
-    node_hostname_by_location = smartstack_replication_checker.get_allowed_locations_and_hosts(
-        job_config
+    kube_client = settings.kubernetes_client
+    if kube_client is None:
+        return status
+
+    relevant_namespaces = await a_sync.to_async(find_all_relevant_namespaces)(
+        service, instance, kube_client, job_config
+    )
+
+    tasks: List["asyncio.Future[Dict[str, Any]]"] = []
+
+    if (
+        verbose > 1
+        and job_config.is_autoscaling_enabled()
+        and job_config.get_autoscaling_params().get("decision_policy", "") != "bespoke"  # type: ignore
+    ):
+        autoscaling_task = asyncio.create_task(
+            autoscaling_status(
+                kube_client, job_config, job_config.get_kubernetes_namespace()
+            )
+        )
+        tasks.append(autoscaling_task)
+    else:
+        autoscaling_task = None
+
+    pods_task = asyncio.create_task(
+        get_pods_for_service_instance_multiple_namespaces(
+            service=service,
+            instance=instance,
+            kube_client=kube_client,
+            namespaces=relevant_namespaces,
+        )
     )
+    tasks.append(pods_task)
 
-    expected_smartstack_count = marathon_tools.get_expected_instance_count_for_namespace(
+    service_namespace_config = kubernetes_tools.load_service_namespace_config(
         service=service,
-        namespace=instance,
-        cluster=settings.cluster,
-        instance_type_class=KubernetesDeploymentConfig,
-    )
-    expected_count_per_location = int(
-        expected_smartstack_count / len(node_hostname_by_location)
+        namespace=job_config.get_nerve_namespace(),
+        soa_dir=settings.soa_dir,
     )
-    smartstack_status: MutableMapping[str, Any] = {
-        "registration": registration,
-        "expected_backends_per_location": expected_count_per_location,
-        "locations": [],
-    }
+    if "proxy_port" in service_namespace_config:
+        mesh_status_task = asyncio.create_task(
+            mesh_status(
+                service=service,
+                service_mesh=ServiceMesh.ENVOY,
+                instance=job_config.get_nerve_namespace(),
+                job_config=job_config,
+                service_namespace_config=service_namespace_config,
+                pods_task=pods_task,
+                should_return_individual_backends=True,
+                settings=settings,
+            )
+        )
+        backends_task = asyncio.create_task(
+            get_backends_from_mesh_status(mesh_status_task)
+        )
+        tasks.extend([mesh_status_task, backends_task])
+    else:
+        mesh_status_task = None
+        backends_task = None
 
-    for location, hosts in node_hostname_by_location.items():
-        synapse_host = smartstack_replication_checker.get_first_host_in_pool(
-            hosts, instance_pool
+    if job_config.get_persistent_volumes():
+        pod_status_by_sha_and_readiness_task = asyncio.create_task(
+            get_pod_status_tasks_by_sha_and_readiness(
+                pods_task,
+                backends_task,
+                kube_client,
+                verbose,
+            )
         )
-        sorted_backends = sorted(
-            get_backends(
-                registration,
-                synapse_host=synapse_host,
-                synapse_port=settings.system_paasta_config.get_synapse_port(),
-                synapse_haproxy_url_format=settings.system_paasta_config.get_synapse_haproxy_url_format(),
-            ),
-            key=lambda backend: backend["status"],
-            reverse=True,  # put 'UP' backends above 'MAINT' backends
+        versions_task = asyncio.create_task(
+            get_versions_for_controller_revisions(
+                kube_client=kube_client,
+                service=service,
+                instance=instance,
+                namespaces=relevant_namespaces,
+                pod_status_by_sha_and_readiness_task=pod_status_by_sha_and_readiness_task,
+            )
         )
+        tasks.extend([pod_status_by_sha_and_readiness_task, versions_task])
+    else:
+        pod_status_by_replicaset_task = asyncio.create_task(
+            get_pod_status_tasks_by_replicaset(
+                pods_task,
+                backends_task,
+                kube_client,
+                verbose,
+            )
+        )
+        versions_task = asyncio.create_task(
+            get_versions_for_replicasets(
+                kube_client=kube_client,
+                service=service,
+                instance=instance,
+                namespaces=relevant_namespaces,
+                pod_status_by_replicaset_task=pod_status_by_replicaset_task,
+            )
+        )
+        tasks.extend([pod_status_by_replicaset_task, versions_task])
+
+    await asyncio.gather(*tasks, return_exceptions=True)
 
-        matched_backends_and_pods = match_backends_and_pods(sorted_backends, pods)
-        location_dict = build_smartstack_location_dict(
-            location, matched_backends_and_pods, should_return_individual_backends
+    desired_state = job_config.get_desired_state()
+    status["app_name"] = job_config.get_sanitised_deployment_name()
+    status["desired_state"] = desired_state
+    status["desired_instances"] = (
+        job_config.get_instances() if desired_state != "stop" else 0
+    )
+    status["bounce_method"] = job_config.get_bounce_method()
+
+    try:
+        pods_task.result()  # just verifies we have a valid result
+        # These tasks also depend on pods_task, so we cannot populate them without pods
+        status["versions"] = versions_task.result()
+        if mesh_status_task is not None:
+            status["envoy"] = mesh_status_task.result()
+    except asyncio.TimeoutError:
+        status["versions"] = []
+        status["error_message"] = (
+            "Could not fetch instance data. "
+            "This is usually a temporary problem.  Please try again or contact #compute-infra for help if you continue to see this message\n"
         )
-        smartstack_status["locations"].append(location_dict)
 
-    return smartstack_status
+    if autoscaling_task is not None:
+        try:
+            status["autoscaling_status"] = autoscaling_task.result()
+        except Exception as e:
+            if "error_message" not in status:
+                status["error_message"] = (
+                    f"Unknown error occurred while fetching autoscaling status. "
+                    f"Please contact #compute-infra for help: {e}"
+                )
+            else:
+                status[
+                    "error_message"
+                ] += f"Unknown error occurred while fetching autoscaling status: {e}"
+    return status
 
 
-@overload
-def build_smartstack_location_dict(
-    location: str,
-    matched_backends_and_tasks: List[
-        Tuple[Optional[HaproxyBackend], Optional[MarathonTask]]
-    ],
-    should_return_individual_backends: bool = False,
-) -> MutableMapping[str, Any]:
-    ...
+async def get_pod_status_tasks_by_replicaset(
+    pods_task: "asyncio.Future[V1Pod]",
+    backends_task: "asyncio.Future[Dict[str, Any]]",
+    client: kubernetes_tools.KubeClient,
+    verbose: int,
+) -> Dict[str, List["asyncio.Future[Dict[str, Any]]"]]:
+    num_tail_lines = calculate_tail_lines(verbose)
+    pods = await pods_task
+    tasks_by_replicaset: DefaultDict[
+        str, List["asyncio.Future[Dict[str, Any]]"]
+    ] = defaultdict(list)
+    for pod in pods:
+        for owner_reference in pod.metadata.owner_references:
+            if owner_reference.kind == "ReplicaSet":
+                pod_status_task = asyncio.create_task(
+                    get_pod_status(pod, backends_task, client, num_tail_lines)
+                )
+                tasks_by_replicaset[owner_reference.name].append(pod_status_task)
 
+    return tasks_by_replicaset
 
-@overload
-def build_smartstack_location_dict(
-    location: str,
-    matched_backends_and_tasks: List[Tuple[Optional[HaproxyBackend], Optional[V1Pod]]],
-    should_return_individual_backends: bool = False,
-) -> MutableMapping[str, Any]:
-    ...
 
+async def get_versions_for_replicasets(
+    kube_client: kubernetes_tools.KubeClient,
+    service: str,
+    instance: str,
+    namespaces: Iterable[str],
+    pod_status_by_replicaset_task: "asyncio.Future[Mapping[str, Sequence[asyncio.Future[Dict[str, Any]]]]]",
+) -> List[KubernetesVersionDict]:
+
+    replicaset_list: List[V1ReplicaSet] = []
+    for coro in asyncio.as_completed(
+        [
+            kubernetes_tools.replicasets_for_service_instance(
+                service=service,
+                instance=instance,
+                kube_client=kube_client,
+                namespace=namespace,
+            )
+            for namespace in namespaces
+        ]
+    ):
+        replicaset_list.extend(await coro)
+
+    # For the purpose of active_versions/app_count, don't count replicasets that
+    # are at 0/0.
+    actually_running_replicasets = filter_actually_running_replicasets(replicaset_list)
+
+    pod_status_by_replicaset = await pod_status_by_replicaset_task
+    versions = await asyncio.gather(
+        *[
+            get_replicaset_status(
+                replicaset,
+                kube_client,
+                pod_status_by_replicaset.get(replicaset.metadata.name),
+            )
+            for replicaset in actually_running_replicasets
+        ]
+    )
+    return versions
 
-def build_smartstack_location_dict(
-    location: str,
-    matched_backends_and_tasks=List[
-        Tuple[Optional[HaproxyBackend], Optional[Union[MarathonTask, V1Pod]]]
-    ],
-    should_return_individual_backends: bool = False,
-):
-    running_backends_count = 0
-    backends = []
-    for backend, task in matched_backends_and_tasks:
-        if backend is None:
-            continue
-        if backend_is_up(backend):
-            running_backends_count += 1
-        if should_return_individual_backends:
-            backends.append(build_smartstack_backend_dict(backend, task))
 
+async def get_replicaset_status(
+    replicaset: V1ReplicaSet,
+    client: kubernetes_tools.KubeClient,
+    pod_status_tasks: Sequence["asyncio.Future[Dict[str, Any]]"],
+) -> KubernetesVersionDict:
     return {
-        "name": location,
-        "running_backends_count": running_backends_count,
-        "backends": backends,
+        "name": replicaset.metadata.name,
+        "type": "ReplicaSet",
+        "replicas": replicaset.spec.replicas,
+        "ready_replicas": ready_replicas_from_replicaset(replicaset),
+        "create_timestamp": replicaset.metadata.creation_timestamp.timestamp(),
+        "git_sha": replicaset.metadata.labels.get("paasta.yelp.com/git_sha"),
+        "image_version": replicaset.metadata.labels.get(
+            "paasta.yelp.com/image_version", None
+        ),
+        "config_sha": replicaset.metadata.labels.get("paasta.yelp.com/config_sha"),
+        "pods": await asyncio.gather(*pod_status_tasks) if pod_status_tasks else [],
+        "namespace": replicaset.metadata.namespace,
     }
 
 
-@overload
-def build_smartstack_backend_dict(
-    smartstack_backend: HaproxyBackend, task: Optional[MarathonTask]
-) -> MutableMapping[str, Any]:
-    ...
-
+async def get_pod_status(
+    pod: V1Pod,
+    backends_task: "asyncio.Future[Dict[str, Any]]",
+    client: Any,
+    num_tail_lines: int,
+) -> Dict[str, Any]:
+    events_task = asyncio.create_task(
+        get_pod_event_messages(client, pod, max_age_in_seconds=900)
+    )
+    containers_task = asyncio.create_task(
+        get_pod_containers(pod, client, num_tail_lines)
+    )
+
+    await asyncio.gather(events_task, containers_task, return_exceptions=True)
+
+    reason = pod.status.reason
+    message = pod.status.message
+    scheduled = kubernetes_tools.is_pod_scheduled(pod)
+    ready = kubernetes_tools.is_pod_ready(pod)
+    delete_timestamp = (
+        pod.metadata.deletion_timestamp.timestamp()
+        if pod.metadata.deletion_timestamp
+        else None
+    )
 
-@overload
-def build_smartstack_backend_dict(
-    smartstack_backend: HaproxyBackend, task: V1Pod
-) -> MutableMapping[str, Any]:
-    ...
+    try:
+        # Filter events to only last 15m
+        pod_event_messages = events_task.result()
+    except asyncio.TimeoutError:
+        pod_event_messages = [{"error": "Could not retrieve events. Please try again."}]
+
+    if not scheduled and reason != "Evicted":
+        sched_condition = kubernetes_tools.get_pod_condition(pod, "PodScheduled")
+        # If the condition is not yet available (e.g. pod not fully created yet), defer to Status messages
+        if sched_condition:
+            reason = sched_condition.reason
+            message = sched_condition.message
+
+    mesh_ready = None
+    if backends_task is not None:
+        # TODO: Remove this once k8s readiness reflects mesh readiness, PAASTA-17266
+        mesh_ready = pod.status.pod_ip in (await backends_task)
 
+    return {
+        "name": pod.metadata.name,
+        "ip": pod.status.pod_ip,
+        "host": pod.status.host_ip,
+        "phase": pod.status.phase,
+        "reason": reason,
+        "message": message,
+        "scheduled": scheduled,
+        "ready": ready,
+        "mesh_ready": mesh_ready,
+        "containers": containers_task.result(),
+        "create_timestamp": pod.metadata.creation_timestamp.timestamp(),
+        "delete_timestamp": delete_timestamp,
+        "events": pod_event_messages,
+    }
 
-def build_smartstack_backend_dict(
-    smartstack_backend: HaproxyBackend, task: Union[V1Pod, Optional[MarathonTask]]
-) -> MutableMapping[str, Any]:
-    svname = smartstack_backend["svname"]
-    if isinstance(task, V1Pod):
-        node_hostname = svname.split("_")[0]
-        pod_ip = svname.split("_")[1].split(":")[0]
-        hostname = f"{node_hostname}:{pod_ip}"
-    else:
-        hostname = svname.split("_")[0]
-    port = svname.split("_")[-1].split(":")[-1]
 
-    smartstack_backend_dict = {
-        "hostname": hostname,
-        "port": int(port),
-        "status": smartstack_backend["status"],
-        "check_status": smartstack_backend["check_status"],
-        "check_code": smartstack_backend["check_code"],
-        "last_change": int(smartstack_backend["lastchg"]),
-        "has_associated_task": task is not None,
-    }
+def get_container_healthcheck(pod_ip: str, probe: V1Probe) -> Dict[str, Any]:
+    if getattr(probe, "http_get", None):
+        return {
+            "http_url": f"http://{pod_ip}:{probe.http_get.port}{probe.http_get.path}"
+        }
+    if getattr(probe, "tcp_socket", None):
+        return {"tcp_port": f"{probe.tcp_socket.port}"}
+    if getattr(probe, "_exec", None):
+        return {"cmd": f"{' '.join(probe._exec.command)}"}
+    return {}
 
-    check_duration = smartstack_backend["check_duration"]
-    if check_duration:
-        smartstack_backend_dict["check_duration"] = int(check_duration)
 
-    return smartstack_backend_dict
+async def get_pod_containers(
+    pod: V1Pod, client: Any, num_tail_lines: int
+) -> List[Dict[str, Any]]:
+    containers = []
+    statuses = pod.status.container_statuses or []
+    container_specs = pod.spec.containers
+    for cs in statuses:
+        specs: List[V1Container] = [c for c in container_specs if c.name == cs.name]
+        healthcheck_grace_period = 0
+        healthcheck = None
+        if specs:
+            # There should be only one matching spec
+            spec = specs[0]
+            if spec.liveness_probe:
+                healthcheck_grace_period = (
+                    spec.liveness_probe.initial_delay_seconds or 0
+                )
+                healthcheck = get_container_healthcheck(
+                    pod.status.pod_ip, spec.liveness_probe
+                )
 
+        state_dict = cs.state.to_dict()
+        state = None
+        reason = None
+        message = None
+        start_timestamp = None
+        for state_name, this_state in state_dict.items():
+            # Each container has only populated state at a time
+            if this_state:
+                state = state_name
+                if "reason" in this_state:
+                    reason = this_state["reason"]
+                if "message" in this_state:
+                    message = this_state["message"]
+                if this_state.get("started_at"):
+                    start_timestamp = this_state["started_at"].timestamp()
+
+        last_state_dict = cs.last_state.to_dict()
+        last_state = None
+        last_reason = None
+        last_message = None
+        last_duration = None
+        last_timestamp = None
+        for state_name, this_state in last_state_dict.items():
+            if this_state:
+                last_state = state_name
+                if "reason" in this_state:
+                    last_reason = this_state["reason"]
+                if "message" in this_state:
+                    last_message = this_state["message"]
+                if this_state.get("started_at"):
+                    if this_state.get("finished_at"):
+                        last_duration = (
+                            this_state["finished_at"] - this_state["started_at"]
+                        ).total_seconds()
 
-@a_sync.to_blocking
-async def marathon_mesos_status(
-    service: str, instance: str, verbose: int
-) -> MutableMapping[str, Any]:
-    mesos_status: MutableMapping[str, Any] = {}
+                    last_timestamp = this_state["started_at"].timestamp()
 
-    job_id = marathon_tools.format_job_id(service, instance)
-    job_id_filter_string = f"{job_id}{marathon_tools.MESOS_TASK_SPACER}"
+        async def get_tail_lines() -> MutableMapping[str, Any]:
+            try:
+                return await get_tail_lines_for_kubernetes_container(
+                    client,
+                    pod,
+                    cs,
+                    num_tail_lines,
+                    previous=False,
+                )
+            except asyncio.TimeoutError:
+                return {"error_message": f"Could not fetch logs for {cs.name}"}
 
-    try:
-        running_and_active_tasks = select_tasks_by_id(
-            await get_cached_list_of_running_tasks_from_frameworks(),
-            job_id=job_id_filter_string,
+        # get previous log lines as well if this container restarted recently
+        async def get_previous_tail_lines() -> MutableMapping[str, Any]:
+            if state == "running" and kubernetes_tools.recent_container_restart(
+                cs.restart_count, last_state, last_timestamp
+            ):
+                try:
+                    return await get_tail_lines_for_kubernetes_container(
+                        client,
+                        pod,
+                        cs,
+                        num_tail_lines,
+                        previous=True,
+                    )
+                except asyncio.TimeoutError:
+                    return {
+                        "error_message": f"Could not fetch previous logs for {cs.name}"
+                    }
+            return None
+
+        tail_lines, previous_tail_lines = await asyncio.gather(
+            asyncio.ensure_future(get_tail_lines()),
+            asyncio.ensure_future(get_previous_tail_lines()),
+        )
+
+        containers.append(
+            {
+                "name": cs.name,
+                "restart_count": cs.restart_count,
+                "state": state,
+                "reason": reason,
+                "message": message,
+                "last_state": last_state,
+                "last_reason": last_reason,
+                "last_message": last_message,
+                "last_duration": last_duration,
+                "last_timestamp": last_timestamp,
+                "previous_tail_lines": previous_tail_lines,
+                "timestamp": start_timestamp,
+                "healthcheck_grace_period": healthcheck_grace_period,
+                "healthcheck_cmd": healthcheck,
+                "tail_lines": tail_lines,
+            }
+        )
+    return containers
+
+
+async def get_pod_status_tasks_by_sha_and_readiness(
+    pods_task: "asyncio.Future[V1Pod]",
+    backends_task: "asyncio.Future[Dict[str, Any]]",
+    client: kubernetes_tools.KubeClient,
+    verbose: int,
+) -> DefaultDict[
+    Tuple[str, str], DefaultDict[bool, List["asyncio.Future[Dict[str, Any]]"]]
+]:
+    num_tail_lines = calculate_tail_lines(verbose)
+    tasks_by_sha_and_readiness: DefaultDict[
+        Tuple[str, str], DefaultDict[bool, List["asyncio.Future[Dict[str, Any]]"]]
+    ] = defaultdict(lambda: defaultdict(list))
+    for pod in await pods_task:
+        git_sha = pod.metadata.labels["paasta.yelp.com/git_sha"]
+        config_sha = pod.metadata.labels["paasta.yelp.com/config_sha"]
+        is_ready = kubernetes_tools.is_pod_ready(pod)
+        pod_status_task = asyncio.create_task(
+            get_pod_status(pod, backends_task, client, num_tail_lines)
+        )
+        tasks_by_sha_and_readiness[(git_sha, config_sha)][is_ready].append(
+            pod_status_task
         )
-    except (ReadTimeout, asyncio.TimeoutError):
-        return {"error_message": "Talking to Mesos timed out. It may be overloaded."}
 
-    mesos_status["running_task_count"] = len(running_and_active_tasks)
+    return tasks_by_sha_and_readiness
 
-    if verbose > 0:
-        num_tail_lines = calculate_tail_lines(verbose)
-        running_task_dict_futures = []
 
-        for task in running_and_active_tasks:
-            running_task_dict_futures.append(
-                asyncio.ensure_future(get_mesos_running_task_dict(task, num_tail_lines))
+async def get_versions_for_controller_revisions(
+    kube_client: kubernetes_tools.KubeClient,
+    service: str,
+    instance: str,
+    namespaces: Iterable[str],
+    pod_status_by_sha_and_readiness_task: "asyncio.Future[Mapping[Tuple[str, str], Mapping[bool, Sequence[asyncio.Future[Mapping[str, Any]]]]]]",
+) -> List[KubernetesVersionDict]:
+    controller_revision_list: List[V1ControllerRevision] = []
+
+    for coro in asyncio.as_completed(
+        [
+            kubernetes_tools.controller_revisions_for_service_instance(
+                service=service,
+                instance=instance,
+                kube_client=kube_client,
+                namespace=namespace,
             )
+            for namespace in namespaces
+        ]
+    ):
+        controller_revision_list.extend(await coro)
 
-        non_running_tasks = select_tasks_by_id(
-            await get_cached_list_of_not_running_tasks_from_frameworks(),
-            job_id=job_id_filter_string,
-        )
-        non_running_tasks.sort(key=lambda task: get_first_status_timestamp(task) or 0)
-        non_running_tasks = list(reversed(non_running_tasks[-10:]))
-        non_running_task_dict_futures = []
-        for task in non_running_tasks:
-            non_running_task_dict_futures.append(
-                asyncio.ensure_future(
-                    get_mesos_non_running_task_dict(task, num_tail_lines)
-                )
+    cr_by_shas: Dict[Tuple[str, str], V1ControllerRevision] = {}
+    for cr in controller_revision_list:
+        git_sha = cr.metadata.labels["paasta.yelp.com/git_sha"]
+        config_sha = cr.metadata.labels["paasta.yelp.com/config_sha"]
+        cr_by_shas[(git_sha, config_sha)] = cr
+
+    pod_status_by_sha_and_readiness = await pod_status_by_sha_and_readiness_task
+    versions = await asyncio.gather(
+        *[
+            get_version_for_controller_revision(
+                cr,
+                kube_client,
+                pod_status_by_sha_and_readiness[(git_sha, config_sha)],
             )
-
-        all_task_dict_futures = (
-            running_task_dict_futures + non_running_task_dict_futures
-        )
-        if len(all_task_dict_futures):
-            await asyncio.wait(all_task_dict_futures)
-
-        mesos_status["running_tasks"] = [
-            task_future.result() for task_future in running_task_dict_futures
-        ]
-        mesos_status["non_running_tasks"] = [
-            task_future.result() for task_future in non_running_task_dict_futures
+            for (git_sha, config_sha), cr in cr_by_shas.items()
         ]
+    )
 
-    return mesos_status
+    return versions
 
 
-async def get_mesos_running_task_dict(
-    task: Task, num_tail_lines: int
-) -> MutableMapping[str, Any]:
-    short_hostname_future = asyncio.ensure_future(
-        results_or_unknown(get_short_hostname_from_task(task))
-    )
-    mem_limit_future = asyncio.ensure_future(_task_result_or_error(task.mem_limit()))
-    rss_future = asyncio.ensure_future(_task_result_or_error(task.rss()))
-    cpu_shares_future = asyncio.ensure_future(
-        _task_result_or_error(get_cpu_shares(task))
-    )
-    cpu_time_future = asyncio.ensure_future(_task_result_or_error(task.cpu_time()))
-
-    futures = [
-        short_hostname_future,
-        mem_limit_future,
-        rss_future,
-        cpu_shares_future,
-        cpu_time_future,
+async def get_version_for_controller_revision(
+    cr: V1ControllerRevision,
+    client: Any,
+    pod_status_tasks_by_readiness: Mapping[
+        bool, Sequence["asyncio.Future[Mapping[str, Any]]"]
+    ],
+) -> KubernetesVersionDict:
+    all_pod_status_tasks = [
+        task for tasks in pod_status_tasks_by_readiness.values() for task in tasks
     ]
-    if num_tail_lines > 0:
-        tail_lines_future = asyncio.ensure_future(
-            get_tail_lines_for_mesos_task(task, get_short_task_id, num_tail_lines)
-        )
-        futures.append(tail_lines_future)
-    else:
-        tail_lines_future = None
-
-    await asyncio.wait(futures)
-
-    task_dict = {
-        "id": get_short_task_id(task["id"]),
-        "hostname": short_hostname_future.result(),
-        "mem_limit": mem_limit_future.result(),
-        "rss": rss_future.result(),
-        "cpu_shares": cpu_shares_future.result(),
-        "cpu_used_seconds": cpu_time_future.result(),
-        "tail_lines": tail_lines_future.result() if tail_lines_future else {},
+    await asyncio.gather(*all_pod_status_tasks)
+    return {
+        "name": cr.metadata.name,
+        "type": "ControllerRevision",
+        "replicas": len(all_pod_status_tasks),
+        "ready_replicas": len(pod_status_tasks_by_readiness[True]),
+        "create_timestamp": cr.metadata.creation_timestamp.timestamp(),
+        "git_sha": cr.metadata.labels.get("paasta.yelp.com/git_sha"),
+        "image_version": cr.metadata.labels.get("paasta.yelp.com/image_version", None),
+        "config_sha": cr.metadata.labels.get("paasta.yelp.com/config_sha"),
+        "pods": [task.result() for task in all_pod_status_tasks],
+        "namespace": cr.metadata.namespace,
     }
 
-    task_start_time = get_first_status_timestamp(task)
-    if task_start_time is not None:
-        task_dict["deployed_timestamp"] = task_start_time
-        current_time = int(datetime.datetime.now().strftime("%s"))
-        task_dict["duration_seconds"] = current_time - round(task_start_time)
-
-    return task_dict
-
-
-async def _task_result_or_error(future):
-    try:
-        return {"value": await future}
-    except (AttributeError, mesos_exceptions.SlaveDoesNotExist):
-        return {"error_message": "None"}
-    except TimeoutError:
-        return {"error_message": "Timed Out"}
-    except Exception:
-        return {"error_message": "Unknown"}
 
+# TODO: Cleanup old kubernetes status
+@a_sync.to_blocking
+async def kubernetes_status(
+    service: str,
+    instance: str,
+    verbose: int,
+    include_envoy: bool,
+    instance_type: str,
+    settings: Any,
+) -> Mapping[str, Any]:
+    kstatus: Dict[str, Any] = {}
+    config_loader = LONG_RUNNING_INSTANCE_TYPE_HANDLERS[instance_type].loader
+    job_config = config_loader(
+        service=service,
+        instance=instance,
+        cluster=settings.cluster,
+        soa_dir=settings.soa_dir,
+        load_deployments=True,
+    )
+    kube_client = settings.kubernetes_client
+    if kube_client is None:
+        return kstatus
 
-async def get_mesos_non_running_task_dict(
-    task: Task, num_tail_lines: int
-) -> MutableMapping[str, Any]:
-    if num_tail_lines > 0:
-        tail_lines = await get_tail_lines_for_mesos_task(
-            task, get_short_task_id, num_tail_lines
+    app = kubernetes_tools.get_kubernetes_app_by_name(
+        name=job_config.get_sanitised_deployment_name(),
+        kube_client=kube_client,
+        namespace=job_config.get_kubernetes_namespace(),
+    )
+    # bouncing status can be inferred from app_count, ref get_bouncing_status
+
+    # this task is necessary for mesh_status, but most other use cases want
+    # just the list of pods
+    pods_task = asyncio.create_task(
+        kubernetes_tools.pods_for_service_instance(
+            service=job_config.service,
+            instance=job_config.instance,
+            kube_client=kube_client,
+            namespace=job_config.get_kubernetes_namespace(),
         )
-    else:
-        tail_lines = {}
-
-    task_dict = {
-        "id": get_short_task_id(task["id"]),
-        "hostname": await results_or_unknown(get_short_hostname_from_task(task)),
-        "state": task["state"],
-        "tail_lines": tail_lines,
-    }
-
-    task_start_time = get_first_status_timestamp(task)
-    if task_start_time is not None:
-        task_dict["deployed_timestamp"] = task_start_time
-
-    return task_dict
-
+    )
+    pod_list = await pods_task
+    replicaset_list = await kubernetes_tools.replicasets_for_service_instance(
+        service=job_config.service,
+        instance=job_config.instance,
+        kube_client=kube_client,
+        namespace=job_config.get_kubernetes_namespace(),
+    )
+    # For the purpose of active_versions/app_count, don't count replicasets that are at 0/0.
+    actually_running_replicasets = filter_actually_running_replicasets(replicaset_list)
+    active_versions = kubernetes_tools.get_active_versions_for_service(
+        [app, *pod_list, *actually_running_replicasets]
+    )
+    kstatus["app_count"] = len(active_versions)
+    kstatus["desired_state"] = job_config.get_desired_state()
+    kstatus["bounce_method"] = job_config.get_bounce_method()
+    kstatus["active_shas"] = [
+        (deployment_version.sha, config_sha)
+        for deployment_version, config_sha in active_versions
+    ]
+    kstatus["active_versions"] = [
+        (deployment_version.sha, deployment_version.image_version, config_sha)
+        for deployment_version, config_sha in active_versions
+    ]
 
-INSTANCE_TYPE_CR_ID = dict(
-    flink=flink_tools.cr_id,
-    cassandracluster=cassandracluster_tools.cr_id,
-    kafkacluster=kafkacluster_tools.cr_id,
-)
+    await job_status(
+        kstatus=kstatus,
+        client=kube_client,
+        namespace=job_config.get_kubernetes_namespace(),
+        job_config=job_config,
+        verbose=verbose,
+        pod_list=pod_list,
+        replicaset_list=replicaset_list,
+    )
 
+    if (
+        job_config.is_autoscaling_enabled() is True
+        and job_config.get_autoscaling_params().get("decision_policy", "") != "bespoke"  # type: ignore
+    ):
+        try:
+            kstatus["autoscaling_status"] = await autoscaling_status(
+                kube_client, job_config, job_config.get_kubernetes_namespace()
+            )
+        except Exception as e:
+            kstatus[
+                "error_message"
+            ] = f"Unknown error occurred while fetching autoscaling status. Please contact #compute-infra for help: {e}"
+
+    evicted_count = 0
+    for pod in pod_list:
+        if pod.status.reason == "Evicted":
+            evicted_count += 1
+    kstatus["evicted_count"] = evicted_count
 
-def cr_id_fn_for_instance_type(instance_type: str):
-    if instance_type not in INSTANCE_TYPE_CR_ID:
-        raise ApiFailure(f"Error looking up cr_id function for {instance_type}", 500)
+    if include_envoy:
+        service_namespace_config = kubernetes_tools.load_service_namespace_config(
+            service=service,
+            namespace=job_config.get_nerve_namespace(),
+            soa_dir=settings.soa_dir,
+        )
+        if "proxy_port" in service_namespace_config:
+            kstatus["envoy"] = await mesh_status(
+                service=service,
+                service_mesh=ServiceMesh.ENVOY,
+                instance=job_config.get_nerve_namespace(),
+                job_config=job_config,
+                service_namespace_config=service_namespace_config,
+                pods_task=pods_task,
+                should_return_individual_backends=verbose > 0,
+                settings=settings,
+            )
+    return kstatus
 
-    return INSTANCE_TYPE_CR_ID[instance_type]
 
+def instance_status(
+    service: str,
+    instance: str,
+    verbose: int,
+    include_envoy: bool,
+    use_new: bool,
+    instance_type: str,
+    settings: Any,
+) -> Mapping[str, Any]:
+    status = {}
 
-@view_config(
-    route_name="service.instance.status", request_method="GET", renderer="json"
-)
-def instance_status(request):
-    service = request.swagger_data.get("service")
-    instance = request.swagger_data.get("instance")
-    verbose = request.swagger_data.get("verbose") or 0
-    include_smartstack = request.swagger_data.get("include_smartstack")
-    if include_smartstack is None:
-        include_smartstack = True
-    include_mesos = request.swagger_data.get("include_mesos")
-    if include_mesos is None:
-        include_mesos = True
-
-    instance_status: Dict[str, Any] = {}
-    instance_status["service"] = service
-    instance_status["instance"] = instance
-    try:
-        instance_type = validate_service_instance(
-            service, instance, settings.cluster, settings.soa_dir
+    if not can_handle(instance_type):
+        raise RuntimeError(
+            f"Unknown instance type: {instance_type!r}, "
+            f"can handle: {INSTANCE_TYPES}"
         )
-    except NoConfigurationForServiceError:
-        error_message = (
-            "Deployment key %s not found.  Try to execute the corresponding pipeline if it's a fresh instance"
-            % ".".join([settings.cluster, instance])
-        )
-        raise ApiFailure(error_message, 404)
-    except Exception:
-        error_message = traceback.format_exc()
-        raise ApiFailure(error_message, 500)
 
-    if instance_type != "tron":
-        try:
-            actual_deployments = get_actual_deployments(service, settings.soa_dir)
-        except Exception:
-            error_message = traceback.format_exc()
-            raise ApiFailure(error_message, 500)
-
-        version = get_deployment_version(actual_deployments, settings.cluster, instance)
-        # exit if the deployment key is not found
-        if not version:
-            error_message = (
-                "Deployment key %s not found.  Try to execute the corresponding pipeline if it's a fresh instance"
-                % ".".join([settings.cluster, instance])
-            )
-            raise ApiFailure(error_message, 404)
-
-        instance_status["git_sha"] = version
-    else:
-        instance_status["git_sha"] = ""
+    if instance_type in INSTANCE_TYPES_CR:
+        status[instance_type] = cr_status(
+            service=service,
+            instance=instance,
+            instance_type=instance_type,
+            verbose=verbose,
+            kube_client=settings.kubernetes_client,
+        )
 
-    try:
-        if instance_type == "marathon":
-            instance_status["marathon"] = marathon_instance_status(
-                instance_status,
-                service,
-                instance,
-                verbose,
-                include_smartstack=include_smartstack,
-                include_mesos=include_mesos,
-            )
-        elif instance_type == "adhoc":
-            instance_status["adhoc"] = adhoc_instance_status(
-                instance_status, service, instance, verbose
-            )
-        elif instance_type == "kubernetes":
-            instance_status["kubernetes"] = kubernetes_instance_status(
-                instance_status,
-                service,
-                instance,
-                verbose,
-                include_smartstack=include_smartstack,
+    if instance_type in INSTANCE_TYPES_K8S:
+        if use_new:
+            status["kubernetes_v2"] = kubernetes_status_v2(
+                service=service,
+                instance=instance,
                 instance_type=instance_type,
+                verbose=verbose,
+                include_envoy=include_envoy,
+                settings=settings,
             )
-        elif instance_type == "tron":
-            instance_status["tron"] = tron_instance_status(
-                instance_status, service, instance, verbose
-            )
-        elif instance_type in INSTANCE_TYPES_K8S:
-            cr_id_fn = cr_id_fn_for_instance_type(instance_type)
-            cr_id = cr_id_fn(service, instance)
-            status = kubernetes_cr_status(cr_id, verbose)
-            metadata = kubernetes_cr_metadata(cr_id, verbose)
-            instance_status[instance_type] = {}
-            if status is not None:
-                instance_status[instance_type]["status"] = status
-            if metadata is not None:
-                instance_status[instance_type]["metadata"] = metadata
         else:
-            error_message = (
-                f"Unknown instance_type {instance_type} of {service}.{instance}"
-            )
-            raise ApiFailure(error_message, 404)
-        if instance_type == "cassandracluster":
-            instance_status["kubernetes"] = kubernetes_instance_status(
-                instance_status,
-                service,
-                instance,
-                verbose,
-                include_smartstack=include_smartstack,
+            status["kubernetes"] = kubernetes_status(
+                service=service,
+                instance=instance,
                 instance_type=instance_type,
+                verbose=verbose,
+                include_envoy=include_envoy,
+                settings=settings,
             )
-    except Exception:
-        error_message = traceback.format_exc()
-        raise ApiFailure(error_message, 500)
-
-    return instance_status
 
-
-@view_config(
-    route_name="service.instance.set_state", request_method="POST", renderer="json"
-)
-def instance_set_state(request,) -> None:
-    service = request.swagger_data.get("service")
-    instance = request.swagger_data.get("instance")
-    desired_state = request.swagger_data.get("desired_state")
-
-    try:
-        instance_type = validate_service_instance(
-            service, instance, settings.cluster, settings.soa_dir
-        )
-    except NoConfigurationForServiceError:
-        error_message = "deployment key %s not found" % ".".join(
-            [settings.cluster, instance]
-        )
-        raise ApiFailure(error_message, 404)
-    except Exception:
-        error_message = traceback.format_exc()
-        raise ApiFailure(error_message, 500)
-
-    if instance_type in INSTANCE_TYPES_WITH_SET_STATE:
-        try:
-            cr_id_fn = cr_id_fn_for_instance_type(instance_type)
-            kube_client = KubeClient()
-            kubernetes_tools.set_cr_desired_state(
-                kube_client=kube_client,
-                cr_id=cr_id_fn(service=service, instance=instance),
-                desired_state=desired_state,
-            )
-        except ApiException as e:
-            error_message = (
-                f"Error while setting state {desired_state} of "
-                f"{service}.{instance}: {e}"
-            )
-            raise ApiFailure(error_message, 500)
-    else:
-        error_message = (
-            f"instance_type {instance_type} of {service}.{instance} doesn't "
-            f"support set_state, must be in INSTANCE_TYPES_WITH_SET_STATE, "
-            f"currently: {INSTANCE_TYPES_WITH_SET_STATE}"
-        )
-        raise ApiFailure(error_message, 404)
+    return status
 
 
-@view_config(
-    route_name="service.instance.tasks.task", request_method="GET", renderer="json"
-)
-def instance_task(request):
-    status = instance_status(request)
-    task_id = request.swagger_data.get("task_id", None)
-    verbose = request.swagger_data.get("verbose", False)
+def ready_replicas_from_replicaset(replicaset: V1ReplicaSet) -> int:
     try:
-        mstatus = status["marathon"]
-    except KeyError:
-        raise ApiFailure("Only marathon tasks supported", 400)
-    try:
-        task = a_sync.block(get_task, task_id, app_id=mstatus["app_id"])
-    except TaskNotFound:
-        raise ApiFailure(f"Task with id {task_id} not found", 404)
-    except Exception:
-        error_message = traceback.format_exc()
-        raise ApiFailure(error_message, 500)
-    if verbose:
-        task = add_slave_info(task)
-        task = add_executor_info(task)
-    return task._Task__items
-
-
-@view_config(route_name="service.instance.tasks", request_method="GET", renderer="json")
-def instance_tasks(request):
-    status = instance_status(request)
-    slave_hostname = request.swagger_data.get("slave_hostname", None)
-    verbose = request.swagger_data.get("verbose", False)
-    try:
-        mstatus = status["marathon"]
-    except KeyError:
-        raise ApiFailure("Only marathon tasks supported", 400)
-    tasks = a_sync.block(
-        get_tasks_from_app_id, mstatus["desired_app_id"], slave_hostname=slave_hostname
-    )
-    if verbose:
-        tasks = [add_executor_info(task) for task in tasks]
-        tasks = [add_slave_info(task) for task in tasks]
-    return [task._Task__items for task in tasks]
-
-
-@view_config(route_name="service.instance.delay", request_method="GET", renderer="json")
-def instance_delay(request):
-    service = request.swagger_data.get("service")
-    instance = request.swagger_data.get("instance")
-    job_config = marathon_tools.load_marathon_service_config(
-        service, instance, settings.cluster, soa_dir=settings.soa_dir
-    )
-    client = settings.marathon_clients.get_current_client_for_service(job_config)
-    app_id = job_config.format_marathon_app_dict()["id"]
-    app_queue = marathon_tools.get_app_queue(client, app_id)
-    unused_offers_summary = marathon_tools.summarize_unused_offers(app_queue)
+        ready_replicas = replicaset.status.ready_replicas
+        if ready_replicas is None:
+            ready_replicas = 0
+    except AttributeError:
+        ready_replicas = 0
 
-    if len(unused_offers_summary) != 0:
-        return unused_offers_summary
-    else:
-        response = Response()
-        response.status_int = 204
-        return response
+    return ready_replicas
 
 
-def add_executor_info(task):
-    task._Task__items["executor"] = a_sync.block(task.executor).copy()
-    task._Task__items["executor"].pop("tasks", None)
-    task._Task__items["executor"].pop("completed_tasks", None)
-    task._Task__items["executor"].pop("queued_tasks", None)
-    return task
-
+@a_sync.to_blocking
+async def kubernetes_mesh_status(
+    service: str,
+    instance: str,
+    instance_type: str,
+    settings: Any,
+    include_envoy: bool = True,
+) -> Mapping[str, Any]:
 
-def add_slave_info(task):
-    task._Task__items["slave"] = a_sync.block(task.slave)._MesosSlave__items.copy()
-    return task
+    if not include_envoy:
+        raise RuntimeError("No mesh types specified when requesting mesh status")
+    if instance_type not in LONG_RUNNING_INSTANCE_TYPE_HANDLERS:
+        raise RuntimeError(
+            f"Getting mesh status for {instance_type} instances is not supported"
+        )
 
+    config_loader = LONG_RUNNING_INSTANCE_TYPE_HANDLERS[instance_type].loader
+    job_config = config_loader(
+        service=service,
+        instance=instance,
+        cluster=settings.cluster,
+        soa_dir=settings.soa_dir,
+        load_deployments=True,
+    )
+    service_namespace_config = kubernetes_tools.load_service_namespace_config(
+        service=service,
+        namespace=job_config.get_nerve_namespace(),
+        soa_dir=settings.soa_dir,
+    )
+    if "proxy_port" not in service_namespace_config:
+        raise RuntimeError(
+            f"Instance '{service}.{instance}' is not configured for the mesh"
+        )
 
-def get_marathon_dashboard_links(marathon_clients, system_paasta_config):
-    """Return a dict of marathon clients and their corresponding dashboard URLs"""
-    cluster = system_paasta_config.get_cluster()
-    try:
-        links = (
-            system_paasta_config.get_dashboard_links().get(cluster).get("Marathon RO")
+    kube_client = settings.kubernetes_client
+    pods_task = asyncio.create_task(
+        kubernetes_tools.pods_for_service_instance(
+            service=job_config.service,
+            instance=job_config.instance,
+            kube_client=kube_client,
+            namespace=job_config.get_kubernetes_namespace(),
         )
-    except KeyError:
-        pass
-    if isinstance(links, list) and len(links) >= len(marathon_clients.current):
-        return {client: url for client, url in zip(marathon_clients.current, links)}
-    return None
+    )
 
+    kmesh: Dict[str, Any] = {}
+    mesh_status_kwargs = dict(
+        service=service,
+        instance=job_config.get_nerve_namespace(),
+        job_config=job_config,
+        service_namespace_config=service_namespace_config,
+        pods_task=pods_task,
+        should_return_individual_backends=True,
+        settings=settings,
+    )
+    if include_envoy:
+        kmesh["envoy"] = await mesh_status(
+            service_mesh=ServiceMesh.ENVOY,
+            **mesh_status_kwargs,
+        )
 
-def get_deployment_version(
-    actual_deployments: Mapping[str, str], cluster: str, instance: str
-) -> Optional[str]:
-    key = ".".join((cluster, instance))
-    return actual_deployments[key][:8] if key in actual_deployments else None
+    return kmesh
```

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/resources.py` & `paasta-tools-1.0.0/paasta_tools/api/views/resources.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/exception.py` & `paasta-tools-1.0.0/paasta_tools/api/views/exception.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/version.py` & `paasta-tools-1.0.0/paasta_tools/api/views/version.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/__init__.py` & `paasta-tools-1.0.0/paasta_tools/api/views/__init__.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/pause_autoscaler.py` & `paasta-tools-1.0.0/paasta_tools/api/views/pause_autoscaler.py`

 * *Files 0% similar despite different names*

```diff
@@ -49,15 +49,14 @@
     expiry_time = current_time + minutes * 60
     with ZookeeperPool() as zk:
         try:
             zk.ensure_path(ZK_PAUSE_AUTOSCALE_PATH)
             zk.set(ZK_PAUSE_AUTOSCALE_PATH, str(expiry_time).encode("utf-8"))
         except Exception as e:
             raise ApiFailure(e, 500)
-
     return
 
 
 @view_config(
     route_name="service_autoscaler.pause.delete",
     request_method="DELETE",
     renderer="json",
```

### Comparing `paasta-tools-0.92.1/paasta_tools/api/views/metastatus.py` & `paasta-tools-1.0.0/paasta_tools/api/views/metastatus.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/api/api_docs/swagger.json` & `paasta-tools-1.0.0/paasta_tools/api/api_docs/swagger.json`

 * *Files 22% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.9279526031049392%*

 * *Differences: {"'definitions'": "{'InstanceStatus': {'properties': {'flink': {'properties': {'status': {'$ref': "*

 * *                  "'#/definitions/InstanceStatusFlinkStatus'}, 'metadata': {'$ref': "*

 * *                  "'#/definitions/InstanceStatusFlinkMetadata'}}}, 'kubernetes_v2': "*

 * *                  "OrderedDict([('$ref', '#/definitions/InstanceStatusKubernetesV2'), "*

 * *                  "('description', 'Kubernetes instance status')]), 'flinkeks': "*

 * *                  "OrderedDict([('type', 'object'), ('properties', Ord […]*

```diff
@@ -18,66 +18,482 @@
                 "run_id": {
                     "description": "id of the single run",
                     "type": "string"
                 }
             },
             "type": "object"
         },
+        "AutoscalerCountMsg": {
+            "description": "Specification for autoscaler count endpoints.",
+            "properties": {
+                "calculated_instances": {
+                    "type": "integer"
+                },
+                "desired_instances": {
+                    "type": "integer"
+                },
+                "status": {
+                    "type": "string"
+                }
+            },
+            "type": "object"
+        },
+        "DeployQueue": {
+            "description": "Current state of the deployd queue",
+            "properties": {
+                "available_service_instances": {
+                    "description": "Service instances that are ready to be acted on by deployd",
+                    "items": {
+                        "$ref": "#/definitions/DeployQueueServiceInstance"
+                    },
+                    "type": "array"
+                },
+                "unavailable_service_instances": {
+                    "description": "Service instances that deployd is waiting to act on",
+                    "items": {
+                        "$ref": "#/definitions/DeployQueueServiceInstance"
+                    },
+                    "type": "array"
+                }
+            },
+            "type": "object"
+        },
+        "DeployQueueServiceInstance": {
+            "description": "An instance of a service in the deploy queue",
+            "properties": {
+                "bounce_by": {
+                    "description": "Desired timestamp by which the service instance should be bounced",
+                    "format": "float",
+                    "type": "number"
+                },
+                "bounce_start_time": {
+                    "description": "Timestamp at which service instance was first added to the queue",
+                    "format": "float",
+                    "type": "number"
+                },
+                "enqueue_time": {
+                    "description": "Timestamp at which the service instance was last added to the queue",
+                    "format": "float",
+                    "type": "number"
+                },
+                "failures": {
+                    "description": "Number of times deployment actions have failed on this service instance",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "instance": {
+                    "description": "Name of the instance of the service",
+                    "type": "string"
+                },
+                "processed_count": {
+                    "description": "Number of times any deployment action has been taken on this service instance",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "service": {
+                    "description": "Name of the service",
+                    "type": "string"
+                },
+                "wait_until": {
+                    "description": "Timestamp before which no action should be taken on this service instance",
+                    "format": "float",
+                    "type": "number"
+                },
+                "watcher": {
+                    "description": "The process that enqueued the task",
+                    "type": "string"
+                }
+            },
+            "type": "object"
+        },
+        "EnvoyBackend": {
+            "properties": {
+                "address": {
+                    "description": "Address of the host on which the backend is running",
+                    "type": "string"
+                },
+                "eds_health_status": {
+                    "description": "Status of the backend in Envoy as reported by the EDS",
+                    "type": "string"
+                },
+                "has_associated_task": {
+                    "description": "Whether this backend has an associated task running",
+                    "type": "boolean"
+                },
+                "hostname": {
+                    "description": "Name of the host on which the backend is running",
+                    "type": "string"
+                },
+                "port_value": {
+                    "description": "Port number on which the backend responds",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "weight": {
+                    "description": "The weight of this backend in the cluster",
+                    "format": "int32",
+                    "type": "integer"
+                }
+            },
+            "type": "object"
+        },
+        "EnvoyLocation": {
+            "properties": {
+                "backends": {
+                    "description": "Envoy backends running in this location",
+                    "items": {
+                        "$ref": "#/definitions/EnvoyBackend"
+                    },
+                    "type": "array"
+                },
+                "is_proxied_through_casper": {
+                    "description": "Whether this backend is proxied through Casper",
+                    "type": "boolean"
+                },
+                "name": {
+                    "description": "Name of the location",
+                    "type": "string"
+                },
+                "running_backends_count": {
+                    "description": "Number of running backends for the service in this location",
+                    "format": "int32",
+                    "type": "integer"
+                }
+            },
+            "type": "object"
+        },
+        "EnvoyStatus": {
+            "properties": {
+                "expected_backends_per_location": {
+                    "description": "Number of backends expected to be present in each location",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "locations": {
+                    "description": "Locations the service is deployed",
+                    "items": {
+                        "$ref": "#/definitions/EnvoyLocation"
+                    },
+                    "type": "array"
+                },
+                "registration": {
+                    "description": "Registration name of the service in Smartstack",
+                    "type": "string"
+                }
+            },
+            "type": "object"
+        },
+        "FlinkClusterOverview": {
+            "properties": {
+                "jobs-cancelled": {
+                    "description": "Number of flink jobs in cancelled state",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "jobs-failed": {
+                    "description": "Number of flink jobs in failed state",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "jobs-finished": {
+                    "description": "Number of flink jobs in finished state",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "jobs-running": {
+                    "description": "Number of flink jobs in running state",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "slots-available": {
+                    "description": "Available task slots",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "slots-total": {
+                    "description": "Total task slots in the cluster",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "taskmanagers": {
+                    "description": "Number of taskmanagers in the cluster",
+                    "format": "int32",
+                    "type": "integer"
+                }
+            },
+            "type": "object"
+        },
+        "FlinkConfig": {
+            "properties": {
+                "flink-revision": {
+                    "type": "string"
+                },
+                "flink-version": {
+                    "type": "string"
+                }
+            },
+            "type": "object"
+        },
+        "FlinkJob": {
+            "properties": {
+                "id": {
+                    "description": "ID of the flink job",
+                    "type": "string"
+                },
+                "status": {
+                    "enum": [
+                        "INITIALIZING",
+                        "CREATED",
+                        "RUNNING",
+                        "FAILING",
+                        "FAILED",
+                        "CANCELLING",
+                        "CANCELED",
+                        "FINISHED",
+                        "RESTARTING",
+                        "SUSPENDED",
+                        "RECONCILING"
+                    ],
+                    "type": "string"
+                }
+            },
+            "type": "object"
+        },
+        "FlinkJobDetails": {
+            "properties": {
+                "jid": {
+                    "description": "ID of the flink job",
+                    "type": "string"
+                },
+                "name": {
+                    "description": "name of the flink job",
+                    "type": "string"
+                },
+                "start-time": {
+                    "description": "timestamp of job start time",
+                    "format": "float",
+                    "type": "number"
+                },
+                "state": {
+                    "description": "state of the flink job",
+                    "enum": [
+                        "INITIALIZING",
+                        "CREATED",
+                        "RUNNING",
+                        "FAILING",
+                        "FAILED",
+                        "CANCELLING",
+                        "CANCELED",
+                        "FINISHED",
+                        "RESTARTING",
+                        "SUSPENDED",
+                        "RECONCILING"
+                    ],
+                    "type": "string"
+                }
+            },
+            "type": "object"
+        },
+        "FlinkJobs": {
+            "properties": {
+                "jobs": {
+                    "description": "Flink jobs",
+                    "items": {
+                        "$ref": "#/definitions/FlinkJob"
+                    },
+                    "type": "array"
+                }
+            },
+            "type": "object"
+        },
         "FloatAndError": {
             "properties": {
                 "error_message": {
                     "type": "string"
                 },
                 "value": {
                     "format": "float",
                     "type": "number"
                 }
             },
             "type": "object"
         },
+        "HPAMetric": {
+            "properties": {
+                "current_value": {
+                    "description": "setpoint/target_value as specified in yelpsoa_configs",
+                    "type": "string"
+                },
+                "name": {
+                    "description": "name of the metric",
+                    "type": "string"
+                },
+                "target_value": {
+                    "description": "setpoint/target_value as specified in yelpsoa_configs",
+                    "type": "string"
+                }
+            },
+            "type": "object"
+        },
+        "InstanceBounceStatus": {
+            "properties": {
+                "active_shas": {
+                    "description": "List of git/config SHAs running.",
+                    "items": {
+                        "items": {
+                            "type": "string",
+                            "x-nullable": true
+                        },
+                        "type": "array"
+                    },
+                    "type": "array"
+                },
+                "active_versions": {
+                    "description": "List of git SHA/image_version/config SHAs running.",
+                    "items": {
+                        "items": {
+                            "type": "string",
+                            "x-nullable": true
+                        },
+                        "type": "array"
+                    },
+                    "type": "array"
+                },
+                "app_count": {
+                    "description": "The number of different running versions of the same service (0 for stopped, 1 for running and 1+ for bouncing)",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "deploy_status": {
+                    "description": "Deploy status of a Kubernetes service",
+                    "enum": [
+                        "Running",
+                        "Deploying",
+                        "Stopped",
+                        "Delayed",
+                        "Waiting",
+                        "NotRunning"
+                    ],
+                    "type": "string"
+                },
+                "desired_state": {
+                    "description": "Desired state of a service, for Kubernetes",
+                    "enum": [
+                        "start",
+                        "stop"
+                    ],
+                    "type": "string"
+                },
+                "expected_instance_count": {
+                    "description": "The number of desired instances of the service",
+                    "format": "int32",
+                    "type": "integer"
+                },
+                "running_instance_count": {
+                    "description": "The number of actual running instances of the service",
+                    "format": "int32",
+                    "type": "integer"
+                }
+            },
+            "type": "object"
+        },
         "InstanceDelay": {
             "type": "object"
         },
-        "InstanceMetadataFlink": {
-            "description": "Flink instance metadata",
+        "InstanceMeshStatus": {
+            "properties": {
+                "envoy": {
+                    "$ref": "#/definitions/EnvoyStatus",
+                    "description": "Status of the service in Envoy"
+                },
+                "instance": {
+                    "description": "Instance name",
+                    "type": "string"
+                },
+                "service": {
+                    "description": "Service name",
+                    "type": "string"
+                },
+                "smartstack": {
+                    "$ref": "#/definitions/SmartstackStatus",
+                    "description": "Status of the service in smartstack"
+                }
+            },
+            "type": "object"
+        },
+        "InstanceMetadataKafkaCluster": {
+            "description": "Kafka instance metadata",
             "type": "object"
         },
         "InstanceStatus": {
             "properties": {
                 "adhoc": {
                     "$ref": "#/definitions/InstanceStatusAdhoc",
                     "description": "Adhoc instance status"
                 },
+                "cassandracluster": {
+                    "description": "Nullable CassandraCluster instance status",
+                    "properties": {
+                        "status": {
+                            "$ref": "#/definitions/InstanceStatusCassandraCluster"
+                        }
+                    },
+                    "type": "object"
+                },
                 "flink": {
                     "description": "Nullable Flink instance status and metadata",
                     "properties": {
                         "metadata": {
-                            "$ref": "#/definitions/InstanceMetadataFlink"
+                            "$ref": "#/definitions/InstanceStatusFlinkMetadata"
                         },
                         "status": {
-                            "$ref": "#/definitions/InstanceStatusFlink"
+                            "$ref": "#/definitions/InstanceStatusFlinkStatus"
+                        }
+                    },
+                    "type": "object"
+                },
+                "flinkeks": {
+                    "description": "Nullable Flink instance status and metadata",
+                    "properties": {
+                        "metadata": {
+                            "$ref": "#/definitions/InstanceStatusFlinkMetadata"
+                        },
+                        "status": {
+                            "$ref": "#/definitions/InstanceStatusFlinkStatus"
                         }
                     },
                     "type": "object"
                 },
                 "git_sha": {
                     "description": "Git sha of a service",
                     "type": "string"
                 },
                 "instance": {
                     "description": "Instance name",
                     "type": "string"
                 },
+                "kafkacluster": {
+                    "description": "Nullable KafkaCluster instance status and metadata",
+                    "properties": {
+                        "metadata": {
+                            "$ref": "#/definitions/InstanceMetadataKafkaCluster"
+                        },
+                        "status": {
+                            "$ref": "#/definitions/InstanceStatusKafkaCluster"
+                        }
+                    },
+                    "type": "object"
+                },
                 "kubernetes": {
                     "$ref": "#/definitions/InstanceStatusKubernetes",
                     "description": "Kubernetes instance status"
                 },
-                "marathon": {
-                    "$ref": "#/definitions/InstanceStatusMarathon",
-                    "description": "Marathon instance status"
+                "kubernetes_v2": {
+                    "$ref": "#/definitions/InstanceStatusKubernetesV2",
+                    "description": "Kubernetes instance status"
                 },
                 "service": {
                     "description": "Service name",
                     "type": "string"
                 },
                 "tron": {
                     "$ref": "#/definitions/InstanceStatusTron",
@@ -89,29 +505,96 @@
         "InstanceStatusAdhoc": {
             "description": "List of runs associated with job",
             "items": {
                 "$ref": "#/definitions/AdhocLaunchHistory"
             },
             "type": "array"
         },
-        "InstanceStatusFlink": {
+        "InstanceStatusCassandraCluster": {
+            "description": "Cassandra instance status",
+            "type": "object"
+        },
+        "InstanceStatusFlinkMetadata": {
+            "description": "Flink instance metadata",
+            "type": "object"
+        },
+        "InstanceStatusFlinkStatus": {
             "description": "Flink instance status",
             "type": "object"
         },
+        "InstanceStatusKafkaCluster": {
+            "description": "Kafka instance status",
+            "type": "object"
+        },
         "InstanceStatusKubernetes": {
             "properties": {
+                "active_shas": {
+                    "description": "List of git/config SHAs running.",
+                    "items": {
+                        "items": {
+                            "type": "string",
+                            "x-nullable": true
+                        },
+                        "maxItems": 2,
+                        "minItems": 2,
+                        "type": "array"
+                    },
+                    "type": "array"
+                },
+                "active_versions": {
+                    "description": "List of git SHA/image_version/config SHAs running.",
+                    "items": {
+                        "items": {
+                            "type": "string",
+                            "x-nullable": true
+                        },
+                        "maxItems": 3,
+                        "minItems": 3,
+                        "type": "array"
+                    },
+                    "type": "array"
+                },
                 "app_count": {
                     "description": "The number of different running versions of the same service (0 for stopped, 1 for running and 1+ for bouncing)",
                     "format": "int32",
                     "type": "integer"
                 },
                 "app_id": {
                     "description": "ID of the desired version of a service instance",
                     "type": "string"
                 },
+                "autoscaling_status": {
+                    "description": "HPA associated to this app",
+                    "properties": {
+                        "desired_replicas": {
+                            "description": "desired number of _instances as calculated by HPA",
+                            "type": "integer"
+                        },
+                        "last_scale_time": {
+                            "description": "timestamp of last autoscale",
+                            "type": "string"
+                        },
+                        "max_instances": {
+                            "description": "min_instances as specified in yelpsoa_configs",
+                            "type": "integer"
+                        },
+                        "metrics": {
+                            "description": "Current metrics",
+                            "items": {
+                                "$ref": "#/definitions/HPAMetric"
+                            },
+                            "type": "array"
+                        },
+                        "min_instances": {
+                            "description": "min_instances as specified in yelpsoa_configs",
+                            "type": "integer"
+                        }
+                    },
+                    "type": "object"
+                },
                 "backoff_seconds": {
                     "description": "backoff in seconds before launching the next task",
                     "format": "int32",
                     "type": "integer"
                 },
                 "bounce_method": {
                     "description": "Method to transit between new and old versions of a service",
@@ -136,22 +619,30 @@
                         "Stopped",
                         "Delayed",
                         "Waiting",
                         "NotRunning"
                     ],
                     "type": "string"
                 },
+                "deploy_status_message": {
+                    "description": "Reason for the deploy status",
+                    "type": "string"
+                },
                 "desired_state": {
                     "description": "Desired state of a service, for Kubernetes",
                     "enum": [
                         "start",
                         "stop"
                     ],
                     "type": "string"
                 },
+                "envoy": {
+                    "$ref": "#/definitions/EnvoyStatus",
+                    "description": "Status of the service in Envoy"
+                },
                 "error_message": {
                     "description": "Error message when a kubernetes object (Deployment/Statefulset) cannot be found",
                     "type": "string"
                 },
                 "expected_instance_count": {
                     "description": "The number of desired instances of the service",
                     "format": "int32",
@@ -188,100 +679,41 @@
             "required": [
                 "desired_state",
                 "app_count",
                 "bounce_method"
             ],
             "type": "object"
         },
-        "InstanceStatusMarathon": {
+        "InstanceStatusKubernetesV2": {
             "properties": {
-                "app_count": {
-                    "description": "The number of different running versions of the same service (0 for stopped, 1 for running and 1+ for bouncing)",
-                    "format": "int32",
-                    "type": "integer"
-                },
-                "app_statuses": {
-                    "description": "Statuses of each app of the service",
-                    "items": {
-                        "$ref": "#/definitions/MarathonAppStatus"
-                    },
-                    "type": "array"
-                },
-                "autoscaling_info": {
-                    "$ref": "#/definitions/MarathonAutoscalingInfo",
-                    "description": "Autoscaling information for the service"
+                "app_name": {
+                    "description": "Name of Kubernetes Deployment or Statefulset for instance",
+                    "type": "string"
                 },
-                "backoff_seconds": {
-                    "description": "backoff in seconds before launching the next task",
+                "desired_instances": {
+                    "description": "Number of instances desired for this app",
                     "format": "int32",
                     "type": "integer"
                 },
-                "bounce_method": {
-                    "description": "Method to transit between new and old versions of a service",
-                    "enum": [
-                        "brutal",
-                        "upthendown",
-                        "downthenup",
-                        "crossover"
-                    ],
-                    "type": "string"
-                },
-                "deploy_status": {
-                    "description": "Deploy status of a marathon service",
-                    "enum": [
-                        "Running",
-                        "Deploying",
-                        "Stopped",
-                        "Delayed",
-                        "Waiting",
-                        "Waiting for bounce",
-                        "NotRunning"
-                    ],
-                    "type": "string"
-                },
-                "desired_app_id": {
-                    "description": "ID of the desired version of a service instance",
-                    "type": "string"
-                },
                 "desired_state": {
-                    "description": "Desired state of a service, for Marathon",
-                    "enum": [
-                        "start",
-                        "stop"
-                    ],
+                    "description": "Desired state of the app (start or stop)",
                     "type": "string"
                 },
                 "error_message": {
-                    "description": "Error message when a marathon job ID cannot be found",
+                    "description": "Error message if we cannot assemble status for the instance",
                     "type": "string"
                 },
-                "expected_instance_count": {
-                    "description": "The number of desired instances of the service",
-                    "format": "int32",
-                    "type": "integer"
-                },
-                "mesos": {
-                    "$ref": "#/definitions/MarathonMesosStatus",
-                    "description": "Status of the service in Mesos"
-                },
-                "running_instance_count": {
-                    "description": "The number of actual running instances of the service",
-                    "format": "int32",
-                    "type": "integer"
-                },
-                "smartstack": {
-                    "$ref": "#/definitions/SmartstackStatus",
-                    "description": "Status of the service in smartstack"
+                "versions": {
+                    "description": "Individual versions of the instance",
+                    "items": {
+                        "$ref": "#/definitions/KubernetesVersion"
+                    },
+                    "type": "array"
                 }
             },
-            "required": [
-                "desired_state",
-                "app_count",
-                "bounce_method"
-            ],
             "type": "object"
         },
         "InstanceStatusTron": {
             "properties": {
                 "action_command": {
                     "description": "The command of the action",
                     "type": "string"
@@ -351,296 +783,267 @@
                 "value": {
                     "format": "int32",
                     "type": "integer"
                 }
             },
             "type": "object"
         },
-        "KubernetesPod": {
+        "KubernetesContainer": {
             "properties": {
-                "deployed_timestamp": {
-                    "description": "Time at which the pod was deployed",
-                    "format": "float",
-                    "type": "number"
-                },
                 "name": {
-                    "description": "name of the pod in Kubernetes",
-                    "type": "string"
-                },
-                "phase": {
-                    "description": "The status of the pod",
+                    "description": "Name of the container",
                     "type": "string"
                 },
                 "tail_lines": {
                     "$ref": "#/definitions/TaskTailLines",
-                    "description": "Stdout and stderr tail of the task"
+                    "description": "Stdout and stderr tail of the container"
                 }
             },
             "type": "object"
         },
-        "KubernetesReplicaSet": {
+        "KubernetesContainerV2": {
             "properties": {
-                "create_timestamp": {
-                    "description": "Time at which the replicaset was created",
+                "last_duration": {
+                    "description": "Duration in seconds of previous state",
                     "format": "float",
-                    "type": "number"
+                    "type": "number",
+                    "x-nullable": true
+                },
+                "last_message": {
+                    "description": "Details about the previous state of the container",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "last_reason": {
+                    "description": "Short description of the previous state of the container",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "last_state": {
+                    "description": "State of the container",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "max_healthcheck_period": {
+                    "description": "Time in seconds for maximum healthcheck failure",
+                    "format": "float",
+                    "type": "number",
+                    "x-nullable": true
+                },
+                "message": {
+                    "description": "Details about the state of the container",
+                    "type": "string",
+                    "x-nullable": true
                 },
                 "name": {
-                    "description": "name of the replicaset in Kubernetes",
+                    "description": "Name of the container",
                     "type": "string"
                 },
-                "ready_replicas": {
-                    "description": "number of ready replicas for the replicaset",
-                    "format": "int32",
-                    "type": "integer"
+                "reason": {
+                    "description": "Short description of the state of the container",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "restart_count": {
+                    "description": "Number of restarts since container creation",
+                    "type": "integer",
+                    "x-nullable": true
                 },
-                "replicas": {
-                    "description": "number of desired replicas for the replicaset",
-                    "format": "int32",
-                    "type": "integer"
+                "state": {
+                    "description": "State of the container",
+                    "type": "string"
+                },
+                "timestamp": {
+                    "description": "Unix timestamp at which state transitioned",
+                    "format": "float",
+                    "type": "number",
+                    "x-nullable": true
                 }
             },
             "type": "object"
         },
-        "MarathonAppStatus": {
+        "KubernetesPod": {
             "properties": {
-                "backoff_seconds": {
-                    "description": "Backoff in seconds before launching next task",
-                    "format": "int32",
-                    "type": "integer"
+                "containers": {
+                    "items": {
+                        "$ref": "#/definitions/KubernetesContainer",
+                        "description": "List of containers in the pod"
+                    },
+                    "type": "array"
                 },
-                "create_timestamp": {
-                    "description": "Unix timestamp when this app was created",
+                "deployed_timestamp": {
+                    "description": "Time at which the pod was deployed",
                     "format": "float",
                     "type": "number"
                 },
-                "dashboard_url": {
-                    "description": "Marathon dashboard URL for this app",
-                    "type": "string"
-                },
-                "deploy_status": {
-                    "description": "Deploy status of this app",
+                "host": {
+                    "description": "name of the pod's host",
                     "type": "string"
                 },
-                "tasks": {
-                    "description": "Tasks associated to this app",
-                    "items": {
-                        "$ref": "#/definitions/MarathonTask"
-                    },
-                    "type": "array"
+                "message": {
+                    "description": "long message explaining the pod's state",
+                    "type": "string",
+                    "x-nullable": true
                 },
-                "tasks_healthy": {
-                    "description": "Number of healthy tasks for this app",
-                    "format": "int32",
-                    "type": "integer"
-                },
-                "tasks_running": {
-                    "description": "Number running tasks for this app",
-                    "format": "int32",
-                    "type": "integer"
+                "name": {
+                    "description": "name of the pod in Kubernetes",
+                    "type": "string"
                 },
-                "tasks_staged": {
-                    "description": "Number of staged tasks for this app",
-                    "format": "int32",
-                    "type": "integer"
+                "phase": {
+                    "description": "The status of the pod",
+                    "type": "string"
                 },
-                "tasks_total": {
-                    "description": "Total number of tasks for this app",
-                    "format": "int32",
-                    "type": "integer"
+                "ready": {
+                    "description": "Whether or not the pod is ready (i.e. all containers up)",
+                    "type": "boolean"
                 },
-                "unused_offer_reason_counts": {
-                    "description": "Mapping of reason offer was refused to the number of times that type of refusal was seen",
-                    "type": "object"
+                "reason": {
+                    "description": "short message explaining the pod's state",
+                    "type": "string",
+                    "x-nullable": true
                 }
             },
             "type": "object"
         },
-        "MarathonAutoscalingInfo": {
+        "KubernetesPodV2": {
             "properties": {
-                "current_instances": {
-                    "description": "The number of instances of the service currently running",
-                    "format": "int32",
-                    "type": "integer"
+                "containers": {
+                    "items": {
+                        "$ref": "#/definitions/KubernetesContainerV2",
+                        "description": "List of containers in the pod"
+                    },
+                    "type": "array"
                 },
-                "current_utilization": {
-                    "description": "The current utilization of the instances' allocated resources",
+                "create_timestamp": {
+                    "description": "Unix timestamp at which pod was created",
                     "format": "float",
                     "type": "number"
                 },
-                "max_instances": {
-                    "description": "The maximum number of instances that the autoscaler will scale to",
-                    "format": "int32",
-                    "type": "integer"
+                "delete_timestamp": {
+                    "description": "Unix timestamp at which pod should be deleted",
+                    "format": "float",
+                    "type": "number",
+                    "x-nullable": true
                 },
-                "min_instances": {
-                    "description": "The minimum number of instances that the autoscaler will scale to",
-                    "format": "int32",
-                    "type": "integer"
+                "host": {
+                    "description": "Host IP this pod was scheduled on",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "ip": {
+                    "description": "Pod IP",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "message": {
+                    "description": "short message with details about the pod's state",
+                    "type": "string",
+                    "x-nullable": true
                 },
-                "target_instances": {
-                    "description": "The autoscaler's current target number of instances of this service to run",
-                    "format": "int32",
-                    "type": "integer"
-                }
-            },
-            "type": "object"
-        },
-        "MarathonDashboard": {
-            "additionalProperties": {
-                "$ref": "#/definitions/MarathonDashboardCluster"
-            },
-            "description": "A list of Marathon service, instance, and shard url for one or more clusters",
-            "type": "object"
-        },
-        "MarathonDashboardCluster": {
-            "description": "List of all the MarathonDashboardItems for a cluster",
-            "items": {
-                "$ref": "#/definitions/MarathonDashboardItem"
-            },
-            "type": "array"
-        },
-        "MarathonDashboardItem": {
-            "description": "Marathon service, instance, and shard url",
-            "properties": {
-                "instance": {
-                    "description": "Instance name",
+                "name": {
+                    "description": "Pod name",
                     "type": "string"
                 },
-                "service": {
-                    "description": "Service name",
+                "phase": {
+                    "description": "The lifecycle phase of the pod",
                     "type": "string"
                 },
-                "shard_url": {
-                    "description": "Marathon Shard URL",
-                    "type": "string"
+                "ready": {
+                    "description": "Whether or not the pod is ready",
+                    "type": "boolean"
+                },
+                "reason": {
+                    "description": "brief description of the pod's state",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "scheduled": {
+                    "description": "Whether or not the pod is scheduled",
+                    "type": "boolean"
                 }
             },
             "type": "object"
         },
-        "MarathonMesosNonrunningTask": {
+        "KubernetesReplicaSet": {
             "properties": {
-                "deployed_timestamp": {
-                    "description": "The unix timestamp at which the task was deployed",
+                "config_sha": {
+                    "description": "Hash of the configuration of this replicaset",
+                    "type": "string",
+                    "x-nullable": true
+                },
+                "create_timestamp": {
+                    "description": "Time at which the replicaset was created",
                     "format": "float",
                     "type": "number"
                 },
-                "hostname": {
-                    "description": "Name of the Mesos agent on which this task is running",
-                    "type": "string"
+                "git_sha": {
+                    "description": "Service git SHA that this replicaset runs",
+                    "type": "string",
+                    "x-nullable": true
                 },
-                "id": {
-                    "description": "The ID of the task in Mesos",
+                "name": {
+                    "description": "name of the replicaset in Kubernetes",
                     "type": "string"
                 },
-                "state": {
-                    "description": "The current state of the task",
-                    "type": "string"
+                "ready_replicas": {
+                    "description": "number of ready replicas for the replicaset",
+                    "format": "int32",
+                    "type": "integer"
                 },
-                "tail_lines": {
-                    "$ref": "#/definitions/TaskTailLines",
-                    "description": "Stdout and stderr tail of the task"
+                "replicas": {
+                    "description": "number of desired replicas for the replicaset",
+                    "format": "int32",
+                    "type": "integer"
                 }
             },
             "type": "object"
         },
-        "MarathonMesosRunningTask": {
+        "KubernetesVersion": {
             "properties": {
-                "cpu_shares": {
-                    "$ref": "#/definitions/FloatAndError",
-                    "description": "The portion of a CPU that the task can use"
-                },
-                "cpu_used_seconds": {
-                    "$ref": "#/definitions/FloatAndError",
-                    "description": "The number of seconds of CPU time the task has used"
+                "config_sha": {
+                    "description": "SHA of configuration for this instance",
+                    "type": "string"
                 },
-                "deployed_timestamp": {
-                    "description": "The unix timestamp at which the task was deployed",
+                "create_timestamp": {
+                    "description": "Unix timestamp when version was created",
                     "format": "float",
                     "type": "number"
                 },
-                "duration_seconds": {
-                    "description": "The duration over which the task has been running in seconds",
-                    "format": "int32",
-                    "type": "integer"
-                },
-                "hostname": {
-                    "description": "Name of the Mesos agent on which this task is running",
+                "git_sha": {
+                    "description": "Git SHA of service code for this version of the instance",
                     "type": "string"
                 },
-                "id": {
-                    "description": "The ID of the task in Mesos",
+                "name": {
+                    "description": "Name of the version (only valid for ReplicaSets)",
                     "type": "string"
                 },
-                "mem_limit": {
-                    "$ref": "#/definitions/IntegerAndError",
-                    "description": "The maximum amount of memory the task is allowed to use"
-                },
-                "rss": {
-                    "$ref": "#/definitions/IntegerAndError",
-                    "description": "The tasks's resident set size"
-                },
-                "tail_lines": {
-                    "$ref": "#/definitions/TaskTailLines",
-                    "description": "Stdout and stderr tail of the task"
-                }
-            },
-            "type": "object"
-        },
-        "MarathonMesosStatus": {
-            "properties": {
-                "error_message": {
-                    "description": "Error message when Mesos tasks cannot be queried",
+                "namespace": {
+                    "description": "Kubernetes namespace this version is in",
                     "type": "string"
                 },
-                "non_running_tasks": {
-                    "description": "Non-running tasks associated to this service",
+                "pods": {
+                    "description": "Pods associated to this version",
                     "items": {
-                        "$ref": "#/definitions/MarathonMesosNonrunningTask"
+                        "$ref": "#/definitions/KubernetesPodV2"
                     },
                     "type": "array"
                 },
-                "running_task_count": {
-                    "description": "The number of running Mesos tasks associated to this service",
+                "ready_replicas": {
+                    "description": "Number of replicas currently ready",
                     "format": "int32",
                     "type": "integer"
                 },
-                "running_tasks": {
-                    "description": "Currently running tasks associated to this service",
-                    "items": {
-                        "$ref": "#/definitions/MarathonMesosRunningTask"
-                    },
-                    "type": "array"
-                }
-            },
-            "type": "object"
-        },
-        "MarathonTask": {
-            "properties": {
-                "deployed_timestamp": {
-                    "description": "Time at which the task was deployed",
-                    "format": "float",
-                    "type": "number"
-                },
-                "host": {
-                    "description": "Name of the host on which the task is running",
-                    "type": "string"
-                },
-                "id": {
-                    "description": "ID of the task in Mesos",
-                    "type": "string"
-                },
-                "is_healthy": {
-                    "description": "Whether Marathon thinks the task is healthy",
-                    "type": "boolean"
-                },
-                "port": {
-                    "description": "Port on which the task is listening",
+                "replicas": {
+                    "description": "Desired number of replicas for this version",
                     "format": "int32",
                     "type": "integer"
+                },
+                "type": {
+                    "description": "Type of version (ReplicaSet or ControllerRevision)",
+                    "type": "string"
                 }
             },
             "type": "object"
         },
         "MetaStatus": {
             "description": "Result of `paasta metastatus` command",
             "properties": {
@@ -799,28 +1202,200 @@
         }
     },
     "info": {
         "title": "Paasta API",
         "version": "1.0.0"
     },
     "paths": {
-        "/marathon_dashboard": {
+        "/deploy_queue": {
+            "get": {
+                "operationId": "deploy_queue",
+                "responses": {
+                    "200": {
+                        "description": "Contents of deploy queue",
+                        "schema": {
+                            "$ref": "#/definitions/DeployQueue"
+                        }
+                    }
+                },
+                "summary": "Get deploy queue contents"
+            }
+        },
+        "/flink/{service}/{instance}/config": {
             "get": {
-                "operationId": "marathon_dashboard",
+                "operationId": "get_flink_cluster_config",
+                "parameters": [
+                    {
+                        "description": "Service name",
+                        "in": "path",
+                        "name": "service",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "description": "Instance name",
+                        "in": "path",
+                        "name": "instance",
+                        "required": true,
+                        "type": "string"
+                    }
+                ],
                 "responses": {
                     "200": {
-                        "description": "List of service instances and information on their Marathon shard",
+                        "description": "Get config of a flink cluster",
                         "schema": {
-                            "$ref": "#/definitions/MarathonDashboard"
+                            "$ref": "#/definitions/FlinkConfig"
                         }
+                    },
+                    "404": {
+                        "description": "Flink cluster not found"
+                    },
+                    "500": {
+                        "description": "Instance failure"
+                    },
+                    "599": {
+                        "description": "Temporary issue fetching instance"
                     }
                 },
-                "summary": "Get marathon service instances and their shards",
+                "summary": "Get config of a flink cluster",
                 "tags": [
-                    "marathon_dashboard"
+                    "service"
+                ]
+            }
+        },
+        "/flink/{service}/{instance}/jobs": {
+            "get": {
+                "operationId": "list_flink_cluster_jobs",
+                "parameters": [
+                    {
+                        "description": "Service name",
+                        "in": "path",
+                        "name": "service",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "description": "Instance name",
+                        "in": "path",
+                        "name": "instance",
+                        "required": true,
+                        "type": "string"
+                    }
+                ],
+                "responses": {
+                    "200": {
+                        "description": "Get list of flink jobs in a flink cluster",
+                        "schema": {
+                            "$ref": "#/definitions/FlinkJobs"
+                        }
+                    },
+                    "400": {
+                        "description": "Flink cluster not found"
+                    },
+                    "500": {
+                        "description": "Instance failure"
+                    },
+                    "599": {
+                        "description": "Temporary issue fetching instance"
+                    }
+                },
+                "summary": "Get list of flink jobs in a flink cluster",
+                "tags": [
+                    "service"
+                ]
+            }
+        },
+        "/flink/{service}/{instance}/jobs/{job_id}": {
+            "get": {
+                "operationId": "get_flink_cluster_job_details",
+                "parameters": [
+                    {
+                        "description": "Service name",
+                        "in": "path",
+                        "name": "service",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "description": "Instance name",
+                        "in": "path",
+                        "name": "instance",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "description": "Job id",
+                        "in": "path",
+                        "name": "job_id",
+                        "required": true,
+                        "type": "string"
+                    }
+                ],
+                "responses": {
+                    "200": {
+                        "description": "Get details of a flink job in a flink cluster",
+                        "schema": {
+                            "$ref": "#/definitions/FlinkJobDetails"
+                        }
+                    },
+                    "400": {
+                        "description": "Flink cluster not found"
+                    },
+                    "500": {
+                        "description": "Instance failure"
+                    },
+                    "599": {
+                        "description": "Temporary issue fetching instance"
+                    }
+                },
+                "summary": "Get details of a flink job in a flink cluster",
+                "tags": [
+                    "service"
+                ]
+            }
+        },
+        "/flink/{service}/{instance}/overview": {
+            "get": {
+                "operationId": "get_flink_cluster_overview",
+                "parameters": [
+                    {
+                        "description": "Service name",
+                        "in": "path",
+                        "name": "service",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "description": "Instance name",
+                        "in": "path",
+                        "name": "instance",
+                        "required": true,
+                        "type": "string"
+                    }
+                ],
+                "responses": {
+                    "200": {
+                        "description": "Get overview of a flink cluster",
+                        "schema": {
+                            "$ref": "#/definitions/FlinkClusterOverview"
+                        }
+                    },
+                    "404": {
+                        "description": "Flink cluster not found"
+                    },
+                    "500": {
+                        "description": "Instance failure"
+                    },
+                    "599": {
+                        "description": "Temporary issue fetching instance"
+                    }
+                },
+                "summary": "Get overview of a flink cluster",
+                "tags": [
+                    "service"
                 ]
             }
         },
         "/metastatus": {
             "get": {
                 "operationId": "metastatus",
                 "parameters": [
@@ -956,14 +1531,15 @@
                 "responses": {
                     "200": {
                         "description": "Services and their instances on the current cluster",
                         "schema": {
                             "properties": {
                                 "services": {
                                     "items": {
+                                        "items": {},
                                         "type": "array"
                                     },
                                     "type": "array"
                                 }
                             },
                             "type": "object"
                         }
@@ -1028,20 +1604,15 @@
                         "type": "string"
                     }
                 ],
                 "responses": {
                     "200": {
                         "description": "Get desired instance count for a service instance",
                         "schema": {
-                            "properties": {
-                                "desired_instances": {
-                                    "type": "integer"
-                                }
-                            },
-                            "type": "object"
+                            "$ref": "#/definitions/AutoscalerCountMsg"
                         }
                     },
                     "404": {
                         "description": "Deployment key not found"
                     },
                     "500": {
                         "description": "Instance failure"
@@ -1070,39 +1641,23 @@
                         "type": "string"
                     },
                     {
                         "in": "body",
                         "name": "json_body",
                         "required": true,
                         "schema": {
-                            "properties": {
-                                "calculated_instances": {
-                                    "type": "integer"
-                                },
-                                "desired_instances": {
-                                    "type": "integer"
-                                }
-                            },
-                            "type": "object"
+                            "$ref": "#/definitions/AutoscalerCountMsg"
                         }
                     }
                 ],
                 "responses": {
                     "202": {
                         "description": "Set desired instance count for a service instance",
                         "schema": {
-                            "properties": {
-                                "desired_instances": {
-                                    "type": "integer"
-                                },
-                                "status": {
-                                    "type": "string"
-                                }
-                            },
-                            "type": "object"
+                            "$ref": "#/definitions/AutoscalerCountMsg"
                         }
                     },
                     "404": {
                         "description": "Deployment key not found"
                     },
                     "500": {
                         "description": "Instance failure"
@@ -1110,14 +1665,56 @@
                 },
                 "summary": "Get status of service_name.instance_name",
                 "tags": [
                     "autoscaler"
                 ]
             }
         },
+        "/services/{service}/{instance}/bounce_status": {
+            "get": {
+                "operationId": "bounce_status_instance",
+                "parameters": [
+                    {
+                        "description": "Service name",
+                        "in": "path",
+                        "name": "service",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "description": "Instance name",
+                        "in": "path",
+                        "name": "instance",
+                        "required": true,
+                        "type": "string"
+                    }
+                ],
+                "responses": {
+                    "200": {
+                        "description": "Bounce status of an instance",
+                        "schema": {
+                            "$ref": "#/definitions/InstanceBounceStatus"
+                        }
+                    },
+                    "204": {
+                        "description": "Instance is not bounceable and therefore has no bounce status"
+                    },
+                    "404": {
+                        "description": "Deployment key not found"
+                    },
+                    "500": {
+                        "description": "Instance failure"
+                    }
+                },
+                "summary": "Get bounce status of service_name.instance_name",
+                "tags": [
+                    "service"
+                ]
+            }
+        },
         "/services/{service}/{instance}/delay": {
             "get": {
                 "operationId": "delay_instance",
                 "parameters": [
                     {
                         "description": "Service name",
                         "in": "path",
@@ -1152,14 +1749,72 @@
                 },
                 "summary": "Get the possible reasons for a deployment delay for a marathon service.instance",
                 "tags": [
                     "service"
                 ]
             }
         },
+        "/services/{service}/{instance}/mesh_status": {
+            "get": {
+                "operationId": "mesh_instance",
+                "parameters": [
+                    {
+                        "description": "Service name",
+                        "in": "path",
+                        "name": "service",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "description": "Instance name",
+                        "in": "path",
+                        "name": "instance",
+                        "required": true,
+                        "type": "string"
+                    },
+                    {
+                        "default": true,
+                        "description": "Include Smartstack information",
+                        "in": "query",
+                        "name": "include_smartstack",
+                        "required": false,
+                        "type": "boolean"
+                    },
+                    {
+                        "default": true,
+                        "description": "Include Envoy information",
+                        "in": "query",
+                        "name": "include_envoy",
+                        "required": false,
+                        "type": "boolean"
+                    }
+                ],
+                "responses": {
+                    "200": {
+                        "description": "Mesh status of an instance",
+                        "schema": {
+                            "$ref": "#/definitions/InstanceMeshStatus"
+                        }
+                    },
+                    "404": {
+                        "description": "Deployment key not found"
+                    },
+                    "405": {
+                        "description": "Instance type not supported"
+                    },
+                    "500": {
+                        "description": "Instance failure"
+                    }
+                },
+                "summary": "Get mesh status for service_name.instance_name",
+                "tags": [
+                    "service"
+                ]
+            }
+        },
         "/services/{service}/{instance}/state/{desired_state}": {
             "post": {
                 "operationId": "instance_set_state",
                 "parameters": [
                     {
                         "description": "Service name",
                         "in": "path",
@@ -1229,19 +1884,33 @@
                         "description": "Include Smartstack information",
                         "in": "query",
                         "name": "include_smartstack",
                         "required": false,
                         "type": "boolean"
                     },
                     {
+                        "description": "Include Envoy information",
+                        "in": "query",
+                        "name": "include_envoy",
+                        "required": false,
+                        "type": "boolean"
+                    },
+                    {
                         "description": "Include Mesos information",
                         "in": "query",
                         "name": "include_mesos",
                         "required": false,
                         "type": "boolean"
+                    },
+                    {
+                        "description": "Use new version of paasta status for services",
+                        "in": "query",
+                        "name": "new",
+                        "required": false,
+                        "type": "boolean"
                     }
                 ],
                 "responses": {
                     "200": {
                         "description": "Detailed status of an instance",
                         "schema": {
                             "$ref": "#/definitions/InstanceStatus"
@@ -1392,12 +2061,12 @@
     ],
     "schemes": [
         "http"
     ],
     "swagger": "2.0",
     "tags": [
         {
-            "description": "information about a paasta service",
+            "description": "Information about a paasta service.",
             "name": "service"
         }
     ]
 }
```

### Comparing `paasta-tools-0.92.1/paasta_tools/api/api.py` & `paasta-tools-1.0.0/paasta_tools/api/api.py`

 * *Files 14% similar despite different names*

```diff
@@ -12,30 +12,38 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
 Responds to paasta service and instance requests.
 """
 import argparse
+import contextlib
 import logging
 import os
 import sys
 
 import manhole
 import requests_cache
 import service_configuration_lib
+import yaml
 from pyramid.config import Configurator
 from wsgicors import CORS
 
 import paasta_tools.api
 from paasta_tools import kubernetes_tools
-from paasta_tools import marathon_tools
 from paasta_tools.api import settings
+from paasta_tools.api.tweens import profiling
+from paasta_tools.api.tweens import request_logger
 from paasta_tools.utils import load_system_paasta_config
 
+try:
+    import clog
+except ImportError:
+    clog = None
+
 
 log = logging.getLogger(__name__)
 
 
 def parse_paasta_api_args():
     parser = argparse.ArgumentParser(description="Runs a PaaSTA API server")
     parser.add_argument(
@@ -55,21 +63,34 @@
     )
     parser.add_argument(
         "-c",
         "--cluster",
         dest="cluster",
         help="specify a cluster. If no empty, the cluster from /etc/paasta is used",
     )
+    parser.add_argument(
+        "--max-request-seconds",
+        default=120,
+        dest="max_request_seconds",
+        help="Maximum seconds allowed for a worker to process a request",
+    )
+    parser.add_argument(
+        "-w",
+        "--workers",
+        default=4,
+        help="Number of gunicorn workers to run",
+    )
     args = parser.parse_args()
     return args
 
 
 def make_app(global_config=None):
     paasta_api_path = os.path.dirname(paasta_tools.api.__file__)
     setup_paasta_api()
+    setup_clog()
 
     config = Configurator(
         settings={
             "service_name": "paasta-api",
             "pyramid_swagger.schema_directory": os.path.join(
                 paasta_api_path, "api_docs"
             ),
@@ -79,19 +100,45 @@
                 "/(swagger.json)\\b",
             ],
             "pyramid_swagger.swagger_versions": ["2.0"],
         }
     )
 
     config.include("pyramid_swagger")
+    config.include(request_logger)
+
+    config.add_route(
+        "flink.service.instance.jobs", "/v1/flink/{service}/{instance}/jobs"
+    )
+
+    config.add_route(
+        "flink.service.instance.job_details",
+        "/v1/flink/{service}/{instance}/jobs/{job_id}",
+    )
+
+    config.add_route(
+        "flink.service.instance.overview", "/v1/flink/{service}/{instance}/overview"
+    )
+    config.add_route(
+        "flink.service.instance.config", "/v1/flink/{service}/{instance}/config"
+    )
+    config.include(profiling)
+
     config.add_route("resources.utilization", "/v1/resources/utilization")
     config.add_route(
         "service.instance.status", "/v1/services/{service}/{instance}/status"
     )
     config.add_route(
+        "service.instance.mesh_status", "/v1/services/{service}/{instance}/mesh_status"
+    )
+    config.add_route(
+        "service.instance.bounce_status",
+        "/v1/services/{service}/{instance}/bounce_status",
+    )
+    config.add_route(
         "service.instance.set_state",
         "/v1/services/{service}/{instance}/state/{desired_state}",
     )
     config.add_route(
         "service.instance.delay", "/v1/services/{service}/{instance}/delay"
     )
     config.add_route(
@@ -125,18 +172,16 @@
     )
     config.add_route(
         "service_autoscaler.pause.get",
         "/v1/service_autoscaler/pause",
         request_method="GET",
     )
     config.add_route("version", "/v1/version")
-    config.add_route(
-        "marathon_dashboard", "/v1/marathon_dashboard", request_method="GET"
-    )
     config.add_route("metastatus", "/v1/metastatus")
+    config.add_route("deploy_queue.list", "/v1/deploy_queue")
     config.scan()
     return CORS(
         config.make_wsgi_app(), headers="*", methods="*", maxage="180", origin="*"
     )
 
 
 _app = None
@@ -166,25 +211,14 @@
 
     settings.system_paasta_config = load_system_paasta_config()
     if os.environ.get("PAASTA_API_CLUSTER"):
         settings.cluster = os.environ.get("PAASTA_API_CLUSTER")
     else:
         settings.cluster = settings.system_paasta_config.get_cluster()
 
-    settings.marathon_clients = marathon_tools.get_marathon_clients(
-        marathon_tools.get_marathon_servers(settings.system_paasta_config)
-    )
-
-    settings.marathon_servers = marathon_tools.get_marathon_servers(
-        system_paasta_config=settings.system_paasta_config
-    )
-    settings.marathon_clients = marathon_tools.get_marathon_clients(
-        marathon_servers=settings.marathon_servers, cached=False
-    )
-
     try:
         settings.kubernetes_client = kubernetes_tools.KubeClient()
     except FileNotFoundError:
         log.info("Kubernetes not found")
         settings.kubernetes_client = None
     except Exception:
         log.exception("Error while initializing KubeClient")
@@ -192,32 +226,67 @@
 
     # Set up transparent cache for http API calls. With expire_after, responses
     # are removed only when the same request is made. Expired storage is not a
     # concern here. Thus remove_expired_responses is not needed.
     requests_cache.install_cache("paasta-api", backend="memory", expire_after=5)
 
 
+def setup_clog(config_file="/nail/srv/configs/clog.yaml"):
+    if clog:
+        if os.path.exists(config_file):
+            with open(config_file) as fp:
+                clog_config = yaml.safe_load(fp)
+        else:
+            # these are barebones basic configs from /nail/srv/configs/clog.yaml
+            clog_config = {
+                "scribe_host": "169.254.255.254",
+                "scribe_port": 1463,
+                "monk_disable": False,
+                "scribe_disable": False,
+            }
+        clog.config.configure_from_dict(clog_config)
+
+
 def main(argv=None):
     args = parse_paasta_api_args()
 
     if args.debug:
         os.environ["PAASTA_API_DEBUG"] = "1"
 
     if args.soa_dir:
         os.environ["PAASTA_API_SOA_DIR"] = args.soa_dir
 
     if args.cluster:
         os.environ["PAASTA_API_CLUSTER"] = args.cluster
 
-    os.execlp(
-        os.path.join(sys.exec_prefix, "bin", "gunicorn"),
+    gunicorn_args = [
         "gunicorn",
         "-w",
-        "4",
+        str(args.workers),
         "--bind",
         f":{args.port}",
+        "--timeout",
+        str(args.max_request_seconds),
+        "--graceful-timeout",
+        str(args.max_request_seconds),
         "paasta_tools.api.api:application",
-    )
+    ]
+
+    if argv:
+        with redirect_argv(gunicorn_args):
+            from gunicorn.app import wsgiapp
+
+            wsgiapp.run()
+    else:
+        os.execlp(os.path.join(sys.exec_prefix, "bin", "gunicorn"), *gunicorn_args)
+
+
+@contextlib.contextmanager
+def redirect_argv(args):
+    sys._argv = sys.argv[:]
+    sys.argv = args
+    yield
+    sys.argv = sys._argv
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/api/__init__.py` & `paasta-tools-1.0.0/paasta_tools/cli/__init__.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/api/settings.py` & `paasta-tools-1.0.0/paasta_tools/api/settings.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,21 +13,21 @@
 # limitations under the License.
 """
 Settings of the paasta-api server.
 """
 import os
 from typing import Optional
 
+from paasta_tools import utils
 from paasta_tools.kubernetes_tools import KubeClient
-from paasta_tools.marathon_tools import MarathonClients
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import SystemPaastaConfig
 
 soa_dir: str = os.environ.get("PAASTA_API_SOA_DIR", DEFAULT_SOA_DIR)
 
 # The following `type: ignore` mypy hints are there because variables below de
 # juro have `Optional[T]` type, but de facto are always initialized to a value
 # of the corresponding type after the application is started.
 cluster: str = None  # type: ignore
-marathon_clients: MarathonClients = None  # type: ignore
+hostname: str = utils.get_hostname()
 kubernetes_client: Optional[KubeClient] = None
 system_paasta_config: Optional[SystemPaastaConfig]
```

### Comparing `paasta-tools-0.92.1/paasta_tools/get_mesos_leader.py` & `paasta-tools-1.0.0/paasta_tools/get_mesos_leader.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,16 +14,15 @@
 # limitations under the License.
 """
 Usage: ./get_mesos_leader.py
 
 Displays the hostname of the current mesos-master leader.
 """
 from paasta_tools.mesos_tools import get_mesos_leader
-from paasta_tools.utils import paasta_print
 
 
 def main():
-    paasta_print(get_mesos_leader())
+    print(get_mesos_leader())
 
 
 if __name__ == "__main__":
     exit(main())
```

### Comparing `paasta-tools-0.92.1/paasta_tools/utils.py` & `paasta-tools-1.0.0/paasta_tools/utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 import os
 import pwd
 import queue
 import re
 import shlex
 import signal
 import socket
+import ssl
 import sys
 import tempfile
 import threading
 import time
 import warnings
 from collections import OrderedDict
 from enum import Enum
@@ -52,52 +53,56 @@
 from typing import ContextManager
 from typing import Dict
 from typing import FrozenSet
 from typing import IO
 from typing import Iterable
 from typing import Iterator
 from typing import List
+from typing import Literal
 from typing import Mapping
+from typing import NamedTuple
 from typing import Optional
 from typing import Sequence
 from typing import Set
 from typing import Tuple
 from typing import Type
 from typing import TypeVar
 from typing import Union
 
 import choice
 import dateutil.tz
+import ldap3
 import requests_cache
 import service_configuration_lib
-import yaml
 from docker import Client
 from docker.utils import kwargs_from_env
 from kazoo.client import KazooClient
 from mypy_extensions import TypedDict
+from service_configuration_lib import read_extra_service_information
 from service_configuration_lib import read_service_configuration
 
 import paasta_tools.cli.fsm
 
 
 # DO NOT CHANGE SPACER, UNLESS YOU'RE PREPARED TO CHANGE ALL INSTANCES
 # OF IT IN OTHER LIBRARIES (i.e. service_configuration_lib).
 # It's used to compose a job's full ID from its name and instance
 SPACER = "."
 INFRA_ZK_PATH = "/nail/etc/zookeeper_discovery/infrastructure/"
 PATH_TO_SYSTEM_PAASTA_CONFIG_DIR = os.environ.get(
     "PAASTA_SYSTEM_CONFIG_DIR", "/etc/paasta/"
 )
 DEFAULT_SOA_DIR = service_configuration_lib.DEFAULT_SOA_DIR
+DEFAULT_VAULT_TOKEN_FILE = "/root/.vault_token"
+AUTO_SOACONFIG_SUBDIR = "autotuned_defaults"
 DEFAULT_DOCKERCFG_LOCATION = "file:///root/.dockercfg"
 DEPLOY_PIPELINE_NON_DEPLOY_STEPS = (
     "itest",
     "itest-and-push-to-registry",
     "security-check",
-    "performance-check",
     "push-to-registry",
 )
 # Default values for _log
 ANY_CLUSTER = "N/A"
 ANY_INSTANCE = "N/A"
 DEFAULT_LOGLEVEL = "event"
 no_escape = re.compile(r"\x1B\[[0-9;]*[mK]")
@@ -110,33 +115,76 @@
 DEFAULT_SYNAPSE_HAPROXY_URL_FORMAT = (
     "http://{host:s}:{port:d}/;csv;norefresh;scope={scope:s}"
 )
 
 DEFAULT_CPU_PERIOD = 100000
 DEFAULT_CPU_BURST_ADD = 1
 
+DEFAULT_SOA_CONFIGS_GIT_URL = "sysgit.yelpcorp.com"
+
 log = logging.getLogger(__name__)
 log.addHandler(logging.NullHandler())
 
 INSTANCE_TYPES = (
-    "marathon",
     "paasta_native",
     "adhoc",
     "kubernetes",
+    "eks",
     "tron",
     "flink",
+    "flinkeks",
     "cassandracluster",
     "kafkacluster",
+    "vitesscluster",
+    "monkrelays",
+    "nrtsearchservice",
+    "nrtsearchserviceeks",
 )
-INSTANCE_TYPES_K8S = {"flink", "cassandracluster", "kafkacluster"}
-INSTANCE_TYPES_WITH_SET_STATE = {"flink"}
+
+PAASTA_K8S_INSTANCE_TYPES = {
+    "kubernetes",
+    "eks",
+}
+
+INSTANCE_TYPE_TO_K8S_NAMESPACE = {
+    "marathon": "paasta",
+    "adhoc": "paasta",
+    "tron": "tron",
+    "flink": "paasta-flinks",
+    "flinkeks": "paasta-flinks",
+    "cassandracluster": "paasta-cassandraclusters",
+    "kafkacluster": "paasta-kafkaclusters",
+    "vitesscluster": "paasta-vitessclusters",
+    "nrtsearchservice": "paasta-nrtsearchservices",
+    "nrtsearchserviceeks": "paasta-nrtsearchservices",
+}
+
+SHARED_SECRETS_K8S_NAMESPACES = {"paasta-spark", "paasta-cassandraclusters"}
+
+CAPS_DROP = [
+    "SETPCAP",
+    "MKNOD",
+    "AUDIT_WRITE",
+    "CHOWN",
+    "NET_RAW",
+    "DAC_OVERRIDE",
+    "FOWNER",
+    "FSETID",
+    "KILL",
+    "SETGID",
+    "SETUID",
+    "NET_BIND_SERVICE",
+    "SYS_CHROOT",
+    "SETFCAP",
+]
 
 
 class RollbackTypes(Enum):
     AUTOMATIC_SLO_ROLLBACK = "automatic_slo_rollback"
+    AUTOMATIC_METRIC_ROLLBACK = "automatic_metric_rollback"
     USER_INITIATED_ROLLBACK = "user_initiated_rollback"
 
 
 class TimeCacheEntry(TypedDict):
     data: Any
     fetch_time: float
 
@@ -193,15 +241,15 @@
 # here to represent that. The getter functions will convert to the safe versions above.
 UnsafeDeployBlacklist = Optional[Sequence[Sequence[str]]]
 UnsafeDeployWhitelist = Optional[Sequence[Union[str, Sequence[str]]]]
 
 
 Constraint = Sequence[str]
 
-# e.g. ['GROUP_BY', 'habitat', 2]. Marathon doesn't like that so we'll convert to Constraint later.
+# e.g. ['GROUP_BY', 'habitat', 2]. Tron doesn't like that so we'll convert to Constraint later.
 UnstringifiedConstraint = Sequence[Union[str, int, float]]
 
 SecurityConfigDict = Dict  # Todo: define me.
 
 
 class VolumeWithMode(TypedDict):
     mode: str
@@ -221,62 +269,122 @@
 
 class PersistentVolume(VolumeWithMode):
     size: int
     container_path: str
     storage_class_name: str
 
 
+class SecretVolumeItem(TypedDict, total=False):
+    key: str
+    path: str
+    mode: Union[str, int]
+
+
+class SecretVolume(TypedDict, total=False):
+    secret_name: str
+    container_path: str
+    default_mode: Union[str, int]
+    items: List[SecretVolumeItem]
+
+
+class ProjectedSAVolume(TypedDict, total=False):
+    container_path: str
+    audience: str
+    expiration_seconds: int
+
+
+class TronSecretVolume(SecretVolume, total=False):
+    secret_volume_name: str
+
+
+class MonitoringDict(TypedDict, total=False):
+    alert_after: Union[str, float]
+    check_every: str
+    check_oom_events: bool
+    component: str
+    description: str
+    notification_email: Union[str, bool]
+    page: bool
+    priority: str
+    project: str
+    realert_every: float
+    runbook: str
+    slack_channels: Union[str, List[str]]
+    tags: List[str]
+    team: str
+    ticket: bool
+    tip: str
+
+
 class InstanceConfigDict(TypedDict, total=False):
     deploy_group: str
     mem: float
     cpus: float
     disk: float
     cmd: str
+    namespace: str
     args: List[str]
     cfs_period_us: float
     cpu_burst_add: float
     cap_add: List
     env: Dict[str, str]
-    monitoring: Dict[str, str]
+    monitoring: MonitoringDict
     deploy_blacklist: UnsafeDeployBlacklist
     deploy_whitelist: UnsafeDeployWhitelist
     pool: str
     persistent_volumes: List[PersistentVolume]
     role: str
     extra_volumes: List[DockerVolume]
     aws_ebs_volumes: List[AwsEbsVolume]
+    secret_volumes: List[SecretVolume]
+    projected_sa_volumes: List[ProjectedSAVolume]
     security: SecurityConfigDict
     dependencies_reference: str
     dependencies: Dict[str, Dict]
     constraints: List[UnstringifiedConstraint]
     extra_constraints: List[UnstringifiedConstraint]
     net: str
     extra_docker_args: Dict[str, str]
     gpus: int
     branch: str
+    iam_role: str
+    iam_role_provider: str
+    service: str
 
 
 class BranchDictV1(TypedDict, total=False):
     docker_image: str
     desired_state: str
     force_bounce: Optional[str]
 
 
 class BranchDictV2(TypedDict):
     git_sha: str
     docker_image: str
+    image_version: Optional[str]
     desired_state: str
     force_bounce: Optional[str]
 
 
 class DockerParameter(TypedDict):
     key: str
     value: str
 
 
+KubeContainerResourceRequest = TypedDict(
+    "KubeContainerResourceRequest",
+    {
+        "cpu": float,
+        "memory": str,
+        "ephemeral-storage": str,
+    },
+    total=False,
+)
+
+
 def safe_deploy_blacklist(input: UnsafeDeployBlacklist) -> DeployBlacklist:
     return [(t, l) for t, l in input]
 
 
 def safe_deploy_whitelist(input: UnsafeDeployWhitelist) -> DeployWhitelist:
     try:
         location_type, allowed_values = input
@@ -336,26 +444,37 @@
             "instance": self.instance,
             "service": self.service,
         }
 
     def get_cluster(self) -> str:
         return self.cluster
 
+    def get_namespace(self) -> str:
+        """Get namespace from config, default to the value from INSTANCE_TYPE_TO_K8S_NAMESPACE for this instance type, 'paasta' if that isn't defined."""
+        return self.config_dict.get(
+            "namespace",
+            INSTANCE_TYPE_TO_K8S_NAMESPACE.get(self.get_instance_type(), "paasta"),
+        )
+
     def get_instance(self) -> str:
         return self.instance
 
     def get_service(self) -> str:
         return self.service
 
     @property
     def job_id(self) -> str:
         return self._job_id
 
-    def get_docker_registry(self) -> str:
-        return get_service_docker_registry(self.service, self.soa_dir)
+    def get_docker_registry(
+        self, system_paasta_config: Optional["SystemPaastaConfig"] = None
+    ) -> str:
+        return get_service_docker_registry(
+            self.service, self.soa_dir, system_config=system_paasta_config
+        )
 
     def get_branch(self) -> str:
         return get_paasta_branch(
             cluster=self.get_cluster(), instance=self.get_instance()
         )
 
     def get_deploy_group(self) -> str:
@@ -363,18 +482,18 @@
 
     def get_team(self) -> str:
         return self.config_dict.get("monitoring", {}).get("team", None)
 
     def get_mem(self) -> float:
         """Gets the memory required from the service's configuration.
 
-        Defaults to 1024 (1G) if no value specified in the config.
+        Defaults to 4096 (4G) if no value specified in the config.
 
-        :returns: The amount of memory specified by the config, 1024 if not specified"""
-        mem = self.config_dict.get("mem", 1024)
+        :returns: The amount of memory specified by the config, 4096 if not specified"""
+        mem = self.config_dict.get("mem", 4096)
         return mem
 
     def get_mem_swap(self) -> str:
         """Gets the memory-swap value. This value is passed to the docker
         container to ensure that the total memory limit (memory + swap) is the
         same value as the 'mem' key in soa-configs. Note - this value *has* to
         be >= to the mem key, so we always round up to the closest MB and add
@@ -383,18 +502,18 @@
         mem = self.get_mem()
         mem_swap = int(math.ceil(mem + 64))
         return "%sm" % mem_swap
 
     def get_cpus(self) -> float:
         """Gets the number of cpus required from the service's configuration.
 
-        Defaults to .25 (1/4 of a cpu) if no value specified in the config.
+        Defaults to 1 cpu if no value specified in the config.
 
-        :returns: The number of cpus specified in the config, .25 if not specified"""
-        cpus = self.config_dict.get("cpus", 0.25)
+        :returns: The number of cpus specified in the config, 1 if not specified"""
+        cpus = self.config_dict.get("cpus", 1)
         return cpus
 
     def get_cpu_burst_add(self) -> float:
         """Returns the number of additional cpus a container is allowed to use.
         Defaults to DEFAULT_CPU_BURST_ADD"""
         return self.config_dict.get("cpu_burst_add", DEFAULT_CPU_BURST_ADD)
 
@@ -429,48 +548,47 @@
             yield {"key": "cap-add", "value": f"{value}"}
 
     def get_cap_drop(self) -> Iterable[DockerParameter]:
         """Generates --cap-drop options to be passed to docker by default, which
         makes them not able to perform special privilege escalation stuff
         https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities
         """
-        caps = [
-            "SETPCAP",
-            "MKNOD",
-            "AUDIT_WRITE",
-            "CHOWN",
-            "NET_RAW",
-            "DAC_OVERRIDE",
-            "FOWNER",
-            "FSETID",
-            "KILL",
-            "SETGID",
-            "SETUID",
-            "NET_BIND_SERVICE",
-            "SYS_CHROOT",
-            "SETFCAP",
-        ]
-        for cap in caps:
+        for cap in CAPS_DROP:
             yield {"key": "cap-drop", "value": cap}
 
+    def get_cap_args(self) -> Iterable[DockerParameter]:
+        """Generate all --cap-add/--cap-drop parameters, ensuring not to have overlapping settings"""
+        cap_adds = list(self.get_cap_add())
+        if cap_adds and is_using_unprivileged_containers():
+            log.warning(
+                "Unprivileged containerizer detected, adding capabilities will not work properly"
+            )
+        yield from cap_adds
+        added_caps = [cap["value"] for cap in cap_adds]
+        for cap in self.get_cap_drop():
+            if cap["value"] not in added_caps:
+                yield cap
+
     def format_docker_parameters(
-        self, with_labels: bool = True
+        self,
+        with_labels: bool = True,
+        system_paasta_config: Optional["SystemPaastaConfig"] = None,
     ) -> List[DockerParameter]:
         """Formats extra flags for running docker.  Will be added in the format
         `["--%s=%s" % (e['key'], e['value']) for e in list]` to the `docker run` command
         Note: values must be strings
 
         :param with_labels: Whether to build docker parameters with or without labels
         :returns: A list of parameters to be added to docker run"""
         parameters: List[DockerParameter] = [
             {"key": "memory-swap", "value": self.get_mem_swap()},
             {"key": "cpu-period", "value": "%s" % int(self.get_cpu_period())},
             {"key": "cpu-quota", "value": "%s" % int(self.get_cpu_quota())},
         ]
-        if self.use_docker_disk_quota():
+        if self.use_docker_disk_quota(system_paasta_config=system_paasta_config):
             parameters.append(
                 {
                     "key": "storage-opt",
                     "value": f"size={int(self.get_disk() * 1024 * 1024)}",
                 }
             )
         if with_labels:
@@ -480,21 +598,24 @@
                     {"key": "label", "value": "paasta_instance=%s" % self.instance},
                 ]
             )
         extra_docker_args = self.get_extra_docker_args()
         if extra_docker_args:
             for key, value in extra_docker_args.items():
                 parameters.extend([{"key": key, "value": value}])
-        parameters.extend(self.get_cap_add())
         parameters.extend(self.get_docker_init())
-        parameters.extend(self.get_cap_drop())
+        parameters.extend(self.get_cap_args())
         return parameters
 
-    def use_docker_disk_quota(self) -> bool:
-        return load_system_paasta_config().get_enforce_disk_quota()
+    def use_docker_disk_quota(
+        self, system_paasta_config: Optional["SystemPaastaConfig"] = None
+    ) -> bool:
+        if system_paasta_config is None:
+            system_paasta_config = load_system_paasta_config()
+        return system_paasta_config.get_enforce_disk_quota()
 
     def get_docker_init(self) -> Iterable[DockerParameter]:
         return [{"key": "init", "value": "true"}]
 
     def get_disk(self, default: float = 1024) -> float:
         """Gets the amount of disk space in MiB required from the service's configuration.
 
@@ -532,15 +653,17 @@
 
         :returns: A string specified in the config, None if not specified"""
         return self.config_dict.get("cmd", None)
 
     def get_instance_type(self) -> Optional[str]:
         return getattr(self, "config_filename_prefix", None)
 
-    def get_env_dictionary(self) -> Dict[str, str]:
+    def get_env_dictionary(
+        self, system_paasta_config: Optional["SystemPaastaConfig"] = None
+    ) -> Dict[str, str]:
         """A dictionary of key/value pairs that represent environment variables
         to be injected to the container environment"""
         env = {
             "PAASTA_SERVICE": self.service,
             "PAASTA_INSTANCE": self.instance,
             "PAASTA_CLUSTER": self.cluster,
             "PAASTA_DEPLOY_GROUP": self.get_deploy_group(),
@@ -548,32 +671,39 @@
             "PAASTA_RESOURCE_CPUS": str(self.get_cpus()),
             "PAASTA_RESOURCE_MEM": str(self.get_mem()),
             "PAASTA_RESOURCE_DISK": str(self.get_disk()),
         }
         if self.get_gpus() is not None:
             env["PAASTA_RESOURCE_GPUS"] = str(self.get_gpus())
         try:
-            env["PAASTA_GIT_SHA"] = get_git_sha_from_dockerurl(self.get_docker_url())
+            env["PAASTA_GIT_SHA"] = get_git_sha_from_dockerurl(
+                self.get_docker_url(system_paasta_config=system_paasta_config)
+            )
         except Exception:
             pass
+        image_version = self.get_image_version()
+        if image_version is not None:
+            env["PAASTA_IMAGE_VERSION"] = image_version
         team = self.get_team()
         if team:
             env["PAASTA_MONITORING_TEAM"] = team
         instance_type = self.get_instance_type()
         if instance_type:
             env["PAASTA_INSTANCE_TYPE"] = instance_type
         user_env = self.config_dict.get("env", {})
         env.update(user_env)
         return {str(k): str(v) for (k, v) in env.items()}
 
-    def get_env(self) -> Dict[str, str]:
+    def get_env(
+        self, system_paasta_config: Optional["SystemPaastaConfig"] = None
+    ) -> Dict[str, str]:
         """Basic get_env that simply returns the basic env, other classes
         might need to override this getter for more implementation-specific
         env getting"""
-        return self.get_env_dictionary()
+        return self.get_env_dictionary(system_paasta_config=system_paasta_config)
 
     def get_args(self) -> Optional[List[str]]:
         """Get the docker args specified in the service's configuration.
 
         If not specified in the config and if cmd is not specified, defaults to an empty array.
         If not specified in the config but cmd is specified, defaults to null.
         If specified in the config and if cmd is also specified, throws an exception. Only one may be specified.
@@ -590,15 +720,15 @@
                 return args
             else:
                 # TODO validation stuff like this should be moved into a check_*
                 raise InvalidInstanceConfig(
                     "Instance configuration can specify cmd or args, but not both."
                 )
 
-    def get_monitoring(self) -> Dict[str, Any]:
+    def get_monitoring(self) -> MonitoringDict:
         """Get monitoring overrides defined for the given instance"""
         return self.config_dict.get("monitoring", {})
 
     def get_deploy_constraints(
         self,
         blacklist: DeployBlacklist,
         whitelist: DeployWhitelist,
@@ -631,19 +761,31 @@
         """Get the docker image name (with tag) for a given service branch from
         a generated deployments.json file."""
         if self.branch_dict is not None:
             return self.branch_dict["docker_image"]
         else:
             return ""
 
-    def get_docker_url(self) -> str:
+    def get_image_version(self) -> Optional[str]:
+        """Get additional information identifying the Docker image from a
+        generated deployments.json file."""
+        if self.branch_dict is not None and "image_version" in self.branch_dict:
+            return self.branch_dict["image_version"]
+        else:
+            return None
+
+    def get_docker_url(
+        self, system_paasta_config: Optional["SystemPaastaConfig"] = None
+    ) -> str:
         """Compose the docker url.
         :returns: '<registry_uri>/<docker_image>'
         """
-        registry_uri = self.get_docker_registry()
+        registry_uri = self.get_docker_registry(
+            system_paasta_config=system_paasta_config
+        )
         docker_image = self.get_docker_image()
         if not docker_image:
             raise NoDockerImageError(
                 "Docker url not available because there is no docker_image"
             )
         docker_url = f"{registry_uri}/{docker_image}"
         return docker_url
@@ -700,24 +842,30 @@
 
     def check_security(self) -> Tuple[bool, str]:
         security = self.config_dict.get("security")
         if security is None:
             return True, ""
 
         outbound_firewall = security.get("outbound_firewall")
+
         if outbound_firewall is None:
             return True, ""
 
-        if outbound_firewall not in ("block", "monitor"):
+        if outbound_firewall is not None and outbound_firewall not in (
+            "block",
+            "monitor",
+        ):
             return (
                 False,
                 'Unrecognized outbound_firewall value "%s"' % outbound_firewall,
             )
 
-        unknown_keys = set(security.keys()) - {"outbound_firewall"}
+        unknown_keys = set(security.keys()) - {
+            "outbound_firewall",
+        }
         if unknown_keys:
             return (
                 False,
                 'Unrecognized items in security dict of service config: "%s"'
                 % ",".join(unknown_keys),
             )
 
@@ -760,22 +908,24 @@
             return (
                 False,
                 'Your service config specifies "%s", an unsupported parameter.' % param,
             )
 
     def validate(
         self,
-        params: List[str] = [
-            "cpus",
-            "mem",
-            "security",
-            "dependencies_reference",
-            "deploy_group",
-        ],
+        params: Optional[List[str]] = None,
     ) -> List[str]:
+        if params is None:
+            params = [
+                "cpus",
+                "mem",
+                "security",
+                "dependencies_reference",
+                "deploy_group",
+            ]
         error_msgs = []
         for param in params:
             check_passed, check_msg = self.check(param)
             if not check_passed:
                 error_msgs.append(check_msg)
         return error_msgs
 
@@ -784,31 +934,42 @@
         if deploy_group is not None:
             pipeline_deploy_groups = get_pipeline_deploy_groups(
                 service=self.service, soa_dir=self.soa_dir
             )
             if deploy_group not in pipeline_deploy_groups:
                 return (
                     False,
-                    f"{self.service}.{self.instance} uses deploy_group {deploy_group}, but it is not deploy.yaml",
+                    f"{self.service}.{self.instance} uses deploy_group {deploy_group}, but {deploy_group} is not deployed to in deploy.yaml",
                 )  # noqa: E501
         return True, ""
 
     def get_extra_volumes(self) -> List[DockerVolume]:
         """Extra volumes are a specially formatted list of dictionaries that should
         be bind mounted in a container The format of the dictionaries should
         conform to the `Mesos container volumes spec
         <https://mesosphere.github.io/marathon/docs/native-docker.html>`_"""
         return self.config_dict.get("extra_volumes", [])
 
     def get_aws_ebs_volumes(self) -> List[AwsEbsVolume]:
         return self.config_dict.get("aws_ebs_volumes", [])
 
+    def get_secret_volumes(self) -> List[SecretVolume]:
+        return self.config_dict.get("secret_volumes", [])
+
+    def get_projected_sa_volumes(self) -> List[ProjectedSAVolume]:
+        return self.config_dict.get("projected_sa_volumes", [])
+
+    def get_iam_role(self) -> str:
+        return self.config_dict.get("iam_role", "")
+
+    def get_iam_role_provider(self) -> str:
+        return self.config_dict.get("iam_role_provider", "aws")
+
     def get_role(self) -> Optional[str]:
-        """Which mesos role of nodes this job should run on.
-        """
+        """Which mesos role of nodes this job should run on."""
         return self.config_dict.get("role")
 
     def get_pool(self) -> str:
         """Which pool of nodes this job should run on. This can be used to mitigate noisy neighbors, by putting
         particularly noisy or noise-sensitive jobs into different pools.
 
         This is implemented with an attribute "pool" on each mesos slave and by adding a constraint or node selector.
@@ -832,19 +993,15 @@
         """
         :returns: the docker networking mode the container should be started with.
         """
         return self.config_dict.get("net", "bridge")
 
     def get_volumes(self, system_volumes: Sequence[DockerVolume]) -> List[DockerVolume]:
         volumes = list(system_volumes) + list(self.get_extra_volumes())
-        deduped = {
-            v["containerPath"].rstrip("/") + v["hostPath"].rstrip("/"): v
-            for v in volumes
-        }.values()
-        return sort_dicts(deduped)
+        return _reorder_docker_volumes(volumes)
 
     def get_persistent_volumes(self) -> Sequence[PersistentVolume]:
         return self.config_dict.get("persistent_volumes", [])
 
     def get_dependencies_reference(self) -> Optional[str]:
         """Get the reference to an entry in dependencies.yaml
 
@@ -999,14 +1156,18 @@
     @staticmethod
     def color_text(color: str, text: str) -> str:
         """Return text that can be printed color.
 
         :param color: ANSI color code
         :param text: a string
         :return: a string with ANSI color encoding"""
+
+        if os.getenv("NO_COLOR", "0") == "1":
+            return text
+
         # any time text returns to default, we want to insert our color.
         replaced = text.replace(PaastaColors.DEFAULT, PaastaColors.DEFAULT + color)
         # then wrap the beginning and end in our color/default.
         return color + replaced + PaastaColors.DEFAULT
 
     @staticmethod
     def cyan(text: str) -> str:
@@ -1029,76 +1190,84 @@
         return PaastaColors.color_text(PaastaColors.GREY, text)
 
     @staticmethod
     def default(text: str) -> str:
         return PaastaColors.color_text(PaastaColors.DEFAULT, text)
 
 
-LOG_COMPONENTS = OrderedDict(
+LOG_COMPONENTS: Mapping[str, Mapping[str, Any]] = OrderedDict(
     [
         (
             "build",
             {
                 "color": PaastaColors.blue,
-                "help": "Jenkins build jobs output, like the itest, promotion, security checks, etc.",
+                "help": (
+                    "Logs for pre-deployment steps, such as itests, "
+                    "image building, and security checks."
+                ),
                 "source_env": "devc",
             },
         ),
         (
             "deploy",
             {
                 "color": PaastaColors.cyan,
-                "help": "Output from the paasta deploy code. (setup_marathon_job, bounces, etc)",
+                "help": (
+                    "Logs for deployment steps and actions, such as "
+                    "bouncing, start/stop/restart, and instance cleanup."
+                ),
                 "additional_source_envs": ["devc"],
             },
         ),
         (
             "monitoring",
             {
                 "color": PaastaColors.green,
                 "help": "Logs from Sensu checks for the service",
             },
         ),
         (
-            "marathon",
-            {
-                "color": PaastaColors.magenta,
-                "help": "Logs from Marathon for the service",
-            },
-        ),
-        (
             "app_output",
             {
                 "color": compose(PaastaColors.yellow, PaastaColors.bold),
-                "help": "Stderr and stdout of the actual process spawned by Mesos. "
-                "Convenience alias for both the stdout and stderr components",
+                "help": (
+                    "Stderr and stdout from a service's running processes. "
+                    "Alias for both the stdout and stderr components."
+                ),
             },
         ),
         (
             "stdout",
             {
                 "color": PaastaColors.yellow,
-                "help": "Stdout from the process spawned by Mesos.",
+                "help": "Stdout from a service's running processes.",
             },
         ),
         (
             "stderr",
             {
                 "color": PaastaColors.yellow,
-                "help": "Stderr from the process spawned by Mesos.",
+                "help": "Stderr from a service's running processes.",
             },
         ),
         (
             "security",
             {
                 "color": PaastaColors.red,
                 "help": "Logs from security-related services such as firewall monitoring",
             },
         ),
         ("oom", {"color": PaastaColors.red, "help": "Kernel OOM events."}),
+        (
+            "task_lifecycle",
+            {
+                "color": PaastaColors.bold,
+                "help": "Logs that tell you about task startup, failures, healthchecks, etc.",
+            },
+        ),
         # I'm leaving these planned components here since they provide some hints
         # about where we want to go. See PAASTA-78.
         #
         # But I'm commenting them out so they don't delude users into believing we
         # can expose logs that we cannot actually expose. See PAASTA-927.
         #
         # ('app_request', {
@@ -1136,25 +1305,32 @@
         raise NoSuchLogComponent
 
 
 def get_git_url(service: str, soa_dir: str = DEFAULT_SOA_DIR) -> str:
     """Get the git url for a service. Assumes that the service's
     repo matches its name, and that it lives in services- i.e.
     if this is called with the string 'test', the returned
-    url will be git@git.yelpcorp.com:services/test.
+    url will be git@github.yelpcorp.com:services/test.
 
     :param service: The service name to get a URL for
     :returns: A git url to the service's repository"""
     general_config = service_configuration_lib.read_service_configuration(
         service, soa_dir=soa_dir
     )
-    default_location = "git@git.yelpcorp.com:services/%s" % service
+    # TODO: PAASTA-16927: get this from system config `.git_config`
+    default_location = format_git_url(
+        "git", "github.yelpcorp.com", f"services/{service}"
+    )
     return general_config.get("git_url", default_location)
 
 
+def format_git_url(git_user: str, git_server: str, repo_name: str) -> str:
+    return f"{git_user}@{git_server}:{repo_name}"
+
+
 def get_service_docker_registry(
     service: str,
     soa_dir: str = DEFAULT_SOA_DIR,
     system_config: Optional["SystemPaastaConfig"] = None,
 ) -> str:
     if service is None:
         raise NotImplementedError('"None" is not a valid service')
@@ -1310,14 +1486,15 @@
     timestamp: str = None,
 ) -> str:
     """Accepts a string 'line'.
 
     Returns an appropriately-formatted dictionary which can be serialized to
     JSON for logging and which contains 'line'.
     """
+
     validate_log_component(component)
     if not timestamp:
         timestamp = _now()
     line = remove_ansi_escape_sequences(line.strip())
     message = json.dumps(
         {
             "timestamp": timestamp,
@@ -1384,45 +1561,34 @@
 try:
     import clog
 
     # Somehow clog turns on DeprecationWarnings, so we need to disable them
     # again after importing it.
     warnings.filterwarnings("ignore", category=DeprecationWarning)
 
-    @register_log_writer("scribe")
-    class ScribeLogWriter(LogWriter):
-        def __init__(
-            self,
-            scribe_host: str = "169.254.255.254",
-            scribe_port: int = 1463,
-            scribe_disable: bool = False,
-            **kwargs: Any,
-        ) -> None:
-            clog.config.configure(
-                scribe_host=scribe_host,
-                scribe_port=scribe_port,
-                scribe_disable=scribe_disable,
-            )
+    class CLogWriter(LogWriter):
+        def __init__(self, **kwargs: Any):
+            clog.config.configure(**kwargs)
 
         def log(
             self,
             service: str,
             line: str,
             component: str,
             level: str = DEFAULT_LOGLEVEL,
             cluster: str = ANY_CLUSTER,
             instance: str = ANY_INSTANCE,
         ) -> None:
             """This expects someone (currently the paasta cli main()) to have already
             configured the log object. We'll just write things to it.
             """
             if level == "event":
-                paasta_print(f"[service {service}] {line}", file=sys.stdout)
+                print(f"[service {service}] {line}", file=sys.stdout)
             elif level == "debug":
-                paasta_print(f"[service {service}] {line}", file=sys.stderr)
+                print(f"[service {service}] {line}", file=sys.stderr)
             else:
                 raise NoSuchLogLevel
             log_name = get_log_name_for_service(service)
             formatted_line = format_log_line(
                 level, cluster, service, instance, component, line
             )
             clog.log_line(log_name, formatted_line)
@@ -1445,14 +1611,43 @@
                 action_details=action_details,
                 service=service,
                 cluster=cluster,
                 instance=instance,
             )
             clog.log_line(log_name, formatted_line)
 
+    @register_log_writer("monk")
+    class MonkLogWriter(CLogWriter):
+        def __init__(
+            self,
+            monk_host: str = "169.254.255.254",
+            monk_port: int = 1473,
+            monk_disable: bool = False,
+            **kwargs: Any,
+        ) -> None:
+            super().__init__(
+                monk_host=monk_host,
+                monk_port=monk_port,
+                monk_disable=monk_disable,
+            )
+
+    @register_log_writer("scribe")
+    class ScribeLogWriter(CLogWriter):
+        def __init__(
+            self,
+            scribe_host: str = "169.254.255.254",
+            scribe_port: int = 1463,
+            scribe_disable: bool = False,
+            **kwargs: Any,
+        ) -> None:
+            super().__init__(
+                scribe_host=scribe_host,
+                scribe_port=scribe_port,
+                scribe_disable=scribe_disable,
+            )
 
 except ImportError:
     warnings.warn("clog is unavailable")
 
 
 @register_log_writer("null")
 class NullLogWriter(LogWriter):
@@ -1534,15 +1729,15 @@
         # http://article.gmane.org/gmane.linux.kernel/43445
 
         try:
             with io.FileIO(path, mode=self.mode, closefd=True) as f:
                 with self.maybe_flock(f):
                     f.write(message.encode("UTF-8"))
         except IOError as e:
-            paasta_print(
+            print(
                 "Could not log to {}: {}: {} -- would have logged: {}".format(
                     path, type(e).__name__, str(e), message
                 ),
                 file=sys.stderr,
             )
 
     def log(
@@ -1595,15 +1790,15 @@
         yield
     finally:
         fcntl.flock(fd.fileno(), fcntl.LOCK_UN)
 
 
 @contextlib.contextmanager
 def timed_flock(fd: _AnyIO, seconds: int = 1) -> Iterator[None]:
-    """ Attempt to grab an exclusive flock with a timeout. Uses Timeout, so will
+    """Attempt to grab an exclusive flock with a timeout. Uses Timeout, so will
     raise a TimeoutError if `seconds` elapses before the flock can be obtained
     """
     # We don't want to wrap the user code in the timeout, just the flock grab
     flock_context = flock(fd)
     with Timeout(seconds=seconds):
         flock_context.__enter__()
     try:
@@ -1665,31 +1860,26 @@
     target_utilization: float
     drain_timeout: int
 
 
 PoolToResourcePoolSettingsDict = Dict[str, ResourcePoolSettings]
 
 
-class MarathonConfigDict(TypedDict, total=False):
-    user: str
-    password: str
-    url: List[str]
-
-
 class LocalRunConfig(TypedDict, total=False):
     default_cluster: str
 
 
 class RemoteRunConfig(TypedDict, total=False):
     default_role: str
 
 
 class SparkRunConfig(TypedDict, total=False):
     default_cluster: str
     default_pool: str
+    default_spark_driver_iam_role: str
 
 
 class PaastaNativeConfig(TypedDict, total=False):
     principal: str
     secret: str
 
 
@@ -1704,82 +1894,147 @@
 class KubeCustomResourceDict(TypedDict, total=False):
     version: str
     file_prefix: str
     kube_kind: KubeKindDict
     group: str
 
 
+class KubeStateMetricsCollectorConfigDict(TypedDict, total=False):
+    unaggregated_metrics: List[str]
+    summed_metric_to_group_keys: Dict[str, List[str]]
+    label_metric_to_label_key: Dict[str, List[str]]
+    label_renames: Dict[str, str]
+
+
+class TopologySpreadConstraintDict(TypedDict, total=False):
+    topology_key: str
+    when_unsatisfiable: Literal["ScheduleAnyway", "DoNotSchedule"]
+    max_skew: int
+
+
 class SystemPaastaConfigDict(TypedDict, total=False):
+    allowed_pools: Dict[str, List[str]]
+    api_client_timeout: int
     api_endpoints: Dict[str, str]
+    api_profiling_config: Dict
     auth_certificate_ttl: str
+    auto_config_instance_types_enabled: Dict[str, bool]
+    auto_config_instance_type_aliases: Dict[str, str]
     auto_hostname_unique_size: int
-    cluster: str
+    boost_regions: List[str]
     cluster_autoscaler_max_decrease: float
     cluster_autoscaler_max_increase: float
     cluster_autoscaling_draining_enabled: bool
     cluster_autoscaling_resources: IdToClusterAutoscalingResourcesDict
     cluster_boost_enabled: bool
     cluster_fqdn_format: str
     clusters: Sequence[str]
+    cluster: str
+    cr_owners: Dict[str, str]
     dashboard_links: Dict[str, Dict[str, str]]
+    datastore_credentials_vault_env_overrides: Dict[str, str]
+    default_push_groups: List
+    default_should_use_uwsgi_exporter: bool
     deploy_blacklist: UnsafeDeployBlacklist
-    deploy_whitelist: UnsafeDeployWhitelist
     deployd_big_bounce_deadline: float
     deployd_log_level: str
     deployd_maintenance_polling_frequency: int
+    deployd_max_service_instance_failures: int
     deployd_metrics_provider: str
     deployd_number_workers: int
     deployd_startup_bounce_deadline: float
     deployd_startup_oracle_enabled: bool
-    deployd_worker_failure_backoff_factor: int
     deployd_use_zk_queue: bool
+    deployd_worker_failure_backoff_factor: int
+    deploy_whitelist: UnsafeDeployWhitelist
     disabled_watchers: List
-    docker_registry: str
     dockercfg_location: str
+    docker_registry: str
     enable_client_cert_auth: bool
     enable_nerve_readiness_check: bool
+    enable_envoy_readiness_check: bool
     enforce_disk_quota: bool
+    envoy_admin_domain_name: str
+    envoy_admin_endpoint_format: str
+    envoy_nerve_readiness_check_script: List[str]
+    envoy_readiness_check_script: List[str]
     expected_slave_attributes: ExpectedSlaveAttributes
     filter_bogus_mesos_cputime_enabled: bool
     fsm_template: str
+    git_config: Dict
     hacheck_sidecar_image_url: str
+    hacheck_sidecar_volumes: List[DockerVolume]
+    kubernetes_add_registration_labels: bool
     kubernetes_custom_resources: List[KubeCustomResourceDict]
     kubernetes_use_hacheck_sidecar: bool
+    ldap_host: str
+    ldap_reader_password: str
+    ldap_reader_username: str
+    ldap_search_base: str
+    ldap_search_ou: str
     local_run_config: LocalRunConfig
     log_reader: LogReaderConfig
     log_writer: LogWriterConfig
     maintenance_resource_reservation_enabled: bool
-    marathon_servers: List[MarathonConfigDict]
+    mark_for_deployment_max_polling_threads: int
+    mark_for_deployment_default_polling_interval: float
+    mark_for_deployment_default_diagnosis_interval: float
+    mark_for_deployment_default_default_time_before_first_diagnosis: float
+    mark_for_deployment_should_ping_for_unhealthy_pods: bool
     mesos_config: Dict
     metrics_provider: str
     monitoring_config: Dict
-    nerve_readiness_check_script: str
+    nerve_readiness_check_script: List[str]
+    nerve_register_k8s_terminating: bool
     paasta_native: PaastaNativeConfig
+    paasta_status_version: str
+    pdb_max_unavailable: Union[str, int]
     pki_backend: str
-    previous_marathon_servers: List[MarathonConfigDict]
+    pod_defaults: Dict[str, Any]
+    pool_node_affinities: Dict[str, Dict[str, List[str]]]
+    topology_spread_constraints: List[TopologySpreadConstraintDict]
+    readiness_check_prefix_template: List[str]
     register_k8s_pods: bool
-    register_marathon_services: bool
     register_native_services: bool
     remote_run_config: RemoteRunConfig
     resource_pool_settings: PoolToResourcePoolSettingsDict
     secret_provider: str
     security_check_command: str
     sensu_host: str
     sensu_port: int
+    service_discovery_providers: Dict[str, Any]
     slack: Dict[str, str]
     spark_run_config: SparkRunConfig
+    supported_storage_classes: Sequence[str]
     synapse_haproxy_url_format: str
     synapse_host: str
     synapse_port: int
     taskproc: Dict
     tron: Dict
+    gunicorn_exporter_sidecar_image_url: str
     vault_cluster_map: Dict
     vault_environment: str
     volumes: List[DockerVolume]
     zookeeper: str
+    tron_k8s_cluster_overrides: Dict[str, str]
+    skip_cpu_override_validation: List[str]
+    spark_k8s_role: str
+    cluster_aliases: Dict[str, str]
+    hacheck_match_initial_delay: bool
+    spark_ui_port: int
+    spark_driver_port: int
+    spark_blockmanager_port: int
+    skip_cpu_burst_validation: List[str]
+    tron_default_pool_override: str
+    spark_kubeconfig: str
+    kube_clusters: Dict
+    spark_use_eks_default: bool
+    sidecar_requirements_config: Dict[str, KubeContainerResourceRequest]
+    eks_cluster_aliases: Dict[str, str]
+    secret_sync_delay_seconds: float
 
 
 def load_system_paasta_config(
     path: str = PATH_TO_SYSTEM_PAASTA_CONFIG_DIR,
 ) -> "SystemPaastaConfig":
     """
     Reads Paasta configs in specified directory in lexicographical order and deep merges
@@ -1832,14 +2087,30 @@
         with open(filename) as f:
             config = deep_merge_dictionaries(
                 json.load(f), config, allow_duplicate_keys=False
             )
     return SystemPaastaConfig(config, path)
 
 
+class PoolsNotConfiguredError(Exception):
+    pass
+
+
+def validate_pool(
+    cluster: str, pool: str, system_paasta_config: "SystemPaastaConfig"
+) -> bool:
+    if pool:
+        valid_pools = system_paasta_config.get_pools_for_cluster(cluster)
+        if not valid_pools:
+            raise PoolsNotConfiguredError
+        # at this point, we can be sure that `valid_pools` is populated
+        return pool in valid_pools
+    return True
+
+
 class SystemPaastaConfig:
     def __init__(self, config: SystemPaastaConfigDict, directory: str) -> None:
         self.directory = directory
         self.config_dict = config
 
     def __eq__(self, other: Any) -> bool:
         if isinstance(other, SystemPaastaConfig):
@@ -1848,14 +2119,32 @@
                 and self.config_dict == other.config_dict
             )
         return False
 
     def __repr__(self) -> str:
         return f"SystemPaastaConfig({self.config_dict!r}, {self.directory!r})"
 
+    def get_secret_sync_delay_seconds(self) -> float:
+        return self.config_dict.get("secret_sync_delay_seconds", 0)
+
+    def get_spark_use_eks_default(self) -> bool:
+        return self.config_dict.get("spark_use_eks_default", False)
+
+    def get_sidecar_requirements_config(
+        self,
+    ) -> Dict[str, KubeContainerResourceRequest]:
+        return self.config_dict.get("sidecar_requirements_config", {})
+
+    def get_tron_default_pool_override(self) -> str:
+        """Get the default pool override variable defined in this host's cluster config file.
+
+        :returns: The default_pool_override specified in the paasta configuration
+        """
+        return self.config_dict.get("tron_default_pool_override", "default")
+
     def get_zk_hosts(self) -> str:
         """Get the zk_hosts defined in this hosts's cluster config file.
         Strips off the zk:// prefix, if it exists, for use with Kazoo.
 
         :returns: The zk_hosts specified in the paasta configuration
         """
         try:
@@ -1880,14 +2169,28 @@
             return self.config_dict["docker_registry"]
         except KeyError:
             raise PaastaNotConfiguredError(
                 "Could not find docker registry in configuration directory: %s"
                 % self.directory
             )
 
+    def get_hacheck_sidecar_volumes(self) -> List[DockerVolume]:
+        """Get the hacheck sidecar volumes defined in this host's hacheck_sidecar_volumes config file.
+
+        :returns: The list of volumes specified in the paasta configuration
+        """
+        try:
+            volumes = self.config_dict["hacheck_sidecar_volumes"]
+        except KeyError:
+            raise PaastaNotConfiguredError(
+                "Could not find hacheck_sidecar_volumes in configuration directory: %s"
+                % self.directory
+            )
+        return _reorder_docker_volumes(list(volumes))
+
     def get_volumes(self) -> Sequence[DockerVolume]:
         """Get the volumes defined in this host's volumes config file.
 
         :returns: The list of volumes specified in the paasta configuration
         """
         try:
             return self.config_dict["volumes"]
@@ -1907,33 +2210,85 @@
             raise PaastaNotConfiguredError(
                 "Could not find cluster in configuration directory: %s" % self.directory
             )
 
     def get_dashboard_links(self) -> Mapping[str, Mapping[str, str]]:
         return self.config_dict["dashboard_links"]
 
+    def get_cr_owners(self) -> Dict[str, str]:
+        return self.config_dict["cr_owners"]
+
     def get_auto_hostname_unique_size(self) -> int:
         """
         We automatically add a ["hostname", "UNIQUE"] constraint to "small" services running in production clusters.
         If there are less than or equal to this number of instances, we consider it small.
         We fail safe and return -1 to avoid adding the ['hostname', 'UNIQUE'] constraint if this value is not defined
 
         :returns: The integer size of a small service
         """
         return self.config_dict.get("auto_hostname_unique_size", -1)
 
+    def get_auto_config_instance_types_enabled(self) -> Dict[str, bool]:
+        return self.config_dict.get("auto_config_instance_types_enabled", {})
+
+    def get_auto_config_instance_type_aliases(self) -> Dict[str, str]:
+        """
+        Allow re-using another instance type's autotuned data. This is useful when an instance can be trivially moved around
+        type-wise as it allows us to avoid data races/issues with the autotuned recommendations generator/updater.
+        """
+        return self.config_dict.get("auto_config_instance_type_aliases", {})
+
+    def get_api_client_timeout(self) -> int:
+        """
+        We've seen the Paasta API get hung up sometimes and the client not realizing this will sit idle forever.
+        This will be used to specify the default timeout
+        """
+        return self.config_dict.get("api_client_timeout", 120)
+
     def get_api_endpoints(self) -> Mapping[str, str]:
         return self.config_dict["api_endpoints"]
 
     def get_enable_client_cert_auth(self) -> bool:
         """
         If enabled present a client certificate from ~/.paasta/pki/<cluster>.crt and ~/.paasta/pki/<cluster>.key
         """
         return self.config_dict.get("enable_client_cert_auth", True)
 
+    def get_enable_nerve_readiness_check(self) -> bool:
+        """
+        If enabled perform readiness checks on nerve
+        """
+        return self.config_dict.get("enable_nerve_readiness_check", True)
+
+    def get_enable_envoy_readiness_check(self) -> bool:
+        """
+        If enabled perform readiness checks on envoy
+        """
+        return self.config_dict.get("enable_envoy_readiness_check", False)
+
+    def get_nerve_readiness_check_script(self) -> List[str]:
+        return self.config_dict.get(
+            "nerve_readiness_check_script", ["/check_smartstack_up.sh"]
+        )
+
+    def get_envoy_readiness_check_script(self) -> List[str]:
+        return self.config_dict.get(
+            "envoy_readiness_check_script",
+            ["/check_proxy_up.sh", "--enable-envoy", "--envoy-check-mode", "eds-dir"],
+        )
+
+    def get_envoy_nerve_readiness_check_script(self) -> List[str]:
+        return self.config_dict.get(
+            "envoy_nerve_readiness_check_script",
+            ["/check_proxy_up.sh", "--enable-smartstack", "--enable-envoy"],
+        )
+
+    def get_nerve_register_k8s_terminating(self) -> bool:
+        return self.config_dict.get("nerve_register_k8s_terminating", True)
+
     def get_enforce_disk_quota(self) -> bool:
         """
         If enabled, add `--storage-opt size=SIZE` arg to `docker run` calls,
         enforcing the disk quota as a result.
 
         Please note that this should be enabled only for a suported environment
         (which at the moment is only `overlay2` driver backed by `XFS`
@@ -1945,20 +2300,14 @@
     def get_auth_certificate_ttl(self) -> str:
         """
         How long to request for ttl on auth certificates. Note that this maybe limited
         by policy in Vault
         """
         return self.config_dict.get("auth_certificate_ttl", "11h")
 
-    def get_pki_backend(self) -> str:
-        """
-        The Vault pki backend to use for issueing certificates
-        """
-        return self.config_dict.get("pki_backend", "paastaca")
-
     def get_fsm_template(self) -> str:
         fsm_path = os.path.dirname(paasta_tools.cli.fsm.__file__)
         template_path = os.path.join(fsm_path, "template")
         return self.config_dict.get("fsm_template", template_path)
 
     def get_log_writer(self) -> LogWriterConfig:
         """Get the log_writer configuration out of global paasta config
@@ -2017,14 +2366,22 @@
         startup. Generally this is desirable behavior. If you are performing a bounce
         of *all* services you will want to disable this.
 
         :returns: A boolean
         """
         return self.config_dict.get("deployd_startup_oracle_enabled", True)
 
+    def get_deployd_max_service_instance_failures(self) -> int:
+        """Determines how many times a service instance entry in deployd's queue
+        can fail before it will be removed from the queue.
+
+        :returns: An integer
+        """
+        return self.config_dict.get("deployd_max_service_instance_failures", 20)
+
     def get_sensu_host(self) -> str:
         """Get the host that we should send sensu events to.
 
         :returns: the sensu_host string, or localhost if not specified.
         """
         return self.config_dict.get("sensu_host", "localhost")
 
@@ -2059,45 +2416,41 @@
         arguments, host and port. Defaults to "http://{host:s}:{port:d}/;csv;norefresh".
 
         :returns: A format string for constructing the URL of haproxy-synapse's status page."""
         return self.config_dict.get(
             "synapse_haproxy_url_format", DEFAULT_SYNAPSE_HAPROXY_URL_FORMAT
         )
 
+    def get_service_discovery_providers(self) -> Dict[str, Any]:
+        return self.config_dict.get("service_discovery_providers", {})
+
     def get_cluster_autoscaling_resources(self) -> IdToClusterAutoscalingResourcesDict:
         return self.config_dict.get("cluster_autoscaling_resources", {})
 
     def get_cluster_autoscaling_draining_enabled(self) -> bool:
-        """ Enable mesos maintenance mode and trigger draining of instances before the
+        """Enable mesos maintenance mode and trigger draining of instances before the
         autoscaler terminates the instance.
 
         :returns A bool"""
         return self.config_dict.get("cluster_autoscaling_draining_enabled", True)
 
     def get_cluster_autoscaler_max_increase(self) -> float:
-        """ Set the maximum increase that the cluster autoscaler can make in each run
+        """Set the maximum increase that the cluster autoscaler can make in each run
 
         :returns A float"""
         return self.config_dict.get("cluster_autoscaler_max_increase", 0.2)
 
     def get_cluster_autoscaler_max_decrease(self) -> float:
-        """ Set the maximum decrease that the cluster autoscaler can make in each run
+        """Set the maximum decrease that the cluster autoscaler can make in each run
 
         :returns A float"""
         return self.config_dict.get("cluster_autoscaler_max_decrease", 0.1)
 
-    def get_maintenance_resource_reservation_enabled(self) -> bool:
-        """ Enable un/reserving of resources when we un/drain a host in mesos maintenance
-        *and* after tasks are killed in setup_marathon_job etc.
-
-        :returns A bool"""
-        return self.config_dict.get("maintenance_resource_reservation_enabled", True)
-
     def get_cluster_boost_enabled(self) -> bool:
-        """ Enable the cluster boost. Note that the boost only applies to the CPUs.
+        """Enable the cluster boost. Note that the boost only applies to the CPUs.
         If the boost is toggled on here but not configured, it will be transparent.
 
         :returns A bool: True means cluster boost is enabled."""
         return self.config_dict.get("cluster_boost_enabled", False)
 
     def get_resource_pool_settings(self) -> PoolToResourcePoolSettingsDict:
         return self.config_dict.get("resource_pool_settings", {})
@@ -2105,19 +2458,19 @@
     def get_cluster_fqdn_format(self) -> str:
         """Get a format string that constructs a DNS name pointing at the paasta masters in a cluster. This format
         string gets one parameter: cluster. Defaults to 'paasta-{cluster:s}.yelp'.
 
         :returns: A format string for constructing the FQDN of the masters in a given cluster."""
         return self.config_dict.get("cluster_fqdn_format", "paasta-{cluster:s}.yelp")
 
-    def get_marathon_servers(self) -> List[MarathonConfigDict]:
-        return self.config_dict.get("marathon_servers", [])
+    def get_paasta_status_version(self) -> str:
+        """Get paasta status version string (new | old). Defaults to 'old'.
 
-    def get_previous_marathon_servers(self) -> List[MarathonConfigDict]:
-        return self.config_dict.get("previous_marathon_servers", [])
+        :returns: A string with the version desired version of paasta status."""
+        return self.config_dict.get("paasta_status_version", "old")
 
     def get_local_run_config(self) -> LocalRunConfig:
         """Get the local-run config
 
         :returns: The local-run job config dictionary"""
         return self.config_dict.get("local_run_config", {})
 
@@ -2214,77 +2567,263 @@
         return self.config_dict.get("deployd_log_level", "INFO")
 
     def get_deployd_use_zk_queue(self) -> bool:
         return self.config_dict.get("deployd_use_zk_queue", True)
 
     def get_hacheck_sidecar_image_url(self) -> str:
         """Get the docker image URL for the hacheck sidecar container"""
-        return self.config_dict.get(
-            "hacheck_sidecar_image_url",
-            "docker-paasta.yelpcorp.com:443/hacheck-k8s-sidecar",
-        )
+        return self.config_dict.get("hacheck_sidecar_image_url")
 
     def get_register_k8s_pods(self) -> bool:
         """Enable registration of k8s services in nerve"""
         return self.config_dict.get("register_k8s_pods", False)
 
+    def get_kubernetes_add_registration_labels(self) -> bool:
+        return self.config_dict.get("kubernetes_add_registration_labels", False)
+
     def get_kubernetes_custom_resources(self) -> Sequence[KubeCustomResourceDict]:
-        """List of custom resources that should be synced by setup_kubernetes_cr """
+        """List of custom resources that should be synced by setup_kubernetes_cr"""
         return self.config_dict.get("kubernetes_custom_resources", [])
 
     def get_kubernetes_use_hacheck_sidecar(self) -> bool:
         return self.config_dict.get("kubernetes_use_hacheck_sidecar", True)
 
-    def get_register_marathon_services(self) -> bool:
-        """Enable registration of marathon services in nerve"""
-        return self.config_dict.get("register_marathon_services", True)
-
     def get_register_native_services(self) -> bool:
         """Enable registration of native paasta services in nerve"""
         return self.config_dict.get("register_native_services", False)
 
-    def get_nerve_readiness_check_script(self) -> str:
-        """Script to check service is up in smartstack"""
-        return self.config_dict.get(
-            "nerve_readiness_check_script", "/check_smartstack_up.sh"
-        )
-
     def get_taskproc(self) -> Dict:
         return self.config_dict.get("taskproc", {})
 
     def get_disabled_watchers(self) -> List:
         return self.config_dict.get("disabled_watchers", [])
 
+    def get_pool_node_affinities(self) -> Dict[str, Dict[str, List[str]]]:
+        """Node selectors that will be applied to all Pods in a pool"""
+        return self.config_dict.get("pool_node_affinities", {})
+
+    def get_topology_spread_constraints(self) -> List[TopologySpreadConstraintDict]:
+        """List of TopologySpreadConstraints that will be applied to all Pods in the cluster"""
+        return self.config_dict.get("topology_spread_constraints", [])
+
+    def get_datastore_credentials_vault_overrides(self) -> Dict[str, str]:
+        """In order to use different Vault shards, vault-tools allows you to override
+        environment variables (CA, token file, and URL). DB credentials are stored in
+        a different shard to minimize the impact on the core Vault shard (which has
+        size restrictions derived from Zookeeper limitations)."""
+        return self.config_dict.get("datastore_credentials_vault_env_overrides", {})
+
     def get_vault_environment(self) -> Optional[str]:
-        """ Get the environment name for the vault cluster
+        """Get the environment name for the vault cluster
         This must match the environment keys in the secret json files
         used by all services in this cluster"""
         return self.config_dict.get("vault_environment")
 
     def get_vault_cluster_config(self) -> dict:
-        """ Get a map from paasta_cluster to vault ecosystem. We need
+        """Get a map from paasta_cluster to vault ecosystem. We need
         this because not every ecosystem will have its own vault cluster"""
         return self.config_dict.get("vault_cluster_map", {})
 
     def get_secret_provider_name(self) -> str:
-        """ Get the name for the configured secret_provider, used to
+        """Get the name for the configured secret_provider, used to
         decrypt secrets"""
         return self.config_dict.get("secret_provider", "paasta_tools.secret_providers")
 
     def get_slack_token(self) -> str:
-        """ Get a slack token for slack notifications. Returns None if there is
-        none available """
+        """Get a slack token for slack notifications. Returns None if there is
+        none available"""
         return self.config_dict.get("slack", {}).get("token", None)
 
     def get_tron_config(self) -> dict:
         return self.config_dict.get("tron", {})
 
     def get_clusters(self) -> Sequence[str]:
         return self.config_dict.get("clusters", [])
 
+    def get_supported_storage_classes(self) -> Sequence[str]:
+        return self.config_dict.get("supported_storage_classes", [])
+
+    def get_envoy_admin_endpoint_format(self) -> str:
+        """Get the format string for Envoy's admin interface."""
+        return self.config_dict.get(
+            "envoy_admin_endpoint_format", "http://{host:s}:{port:d}/{endpoint:s}"
+        )
+
+    def get_envoy_admin_port(self) -> int:
+        """Get the port that Envoy's admin interface is listening on
+        from /etc/services."""
+        return socket.getservbyname(
+            self.config_dict.get("envoy_admin_domain_name", "envoy-admin")
+        )
+
+    def get_pdb_max_unavailable(self) -> Union[str, int]:
+        return self.config_dict.get("pdb_max_unavailable", 0)
+
+    def get_boost_regions(self) -> List[str]:
+        return self.config_dict.get("boost_regions", [])
+
+    def get_pod_defaults(self) -> Dict[str, Any]:
+        return self.config_dict.get("pod_defaults", {})
+
+    def get_ldap_search_base(self) -> str:
+        return self.config_dict.get("ldap_search_base", None)
+
+    def get_ldap_search_ou(self) -> str:
+        return self.config_dict.get("ldap_search_ou", None)
+
+    def get_ldap_host(self) -> str:
+        return self.config_dict.get("ldap_host", None)
+
+    def get_ldap_reader_username(self) -> str:
+        return self.config_dict.get("ldap_reader_username", None)
+
+    def get_ldap_reader_password(self) -> str:
+        return self.config_dict.get("ldap_reader_password", None)
+
+    def get_default_push_groups(self) -> List:
+        return self.config_dict.get("default_push_groups", None)
+
+    def get_git_config(self) -> Dict:
+        """Gets git configuration. Includes repo names and their git servers.
+
+        :returns: the git config dict
+        """
+        return self.config_dict.get(
+            "git_config",
+            {
+                "git_user": "git",
+                "repos": {
+                    "yelpsoa-configs": {
+                        "repo_name": "yelpsoa-configs",
+                        "git_server": DEFAULT_SOA_CONFIGS_GIT_URL,
+                        "deploy_server": DEFAULT_SOA_CONFIGS_GIT_URL,
+                    },
+                },
+            },
+        )
+
+    def get_git_repo_config(self, repo_name: str) -> Dict:
+        """Gets the git configuration for a specific repo.
+
+        :returns: the git config dict for a specific repo.
+        """
+        return self.get_git_config().get("repos", {}).get(repo_name, {})
+
+    def default_should_use_uwsgi_exporter(self) -> bool:
+        return self.config_dict.get("default_should_use_uwsgi_exporter", False)
+
+    def get_gunicorn_exporter_sidecar_image_url(self) -> str:
+        """Get the docker image URL for the gunicorn_exporter sidecar container"""
+        return self.config_dict.get(
+            "gunicorn_exporter_sidecar_image_url",
+            "docker-paasta.yelpcorp.com:443/gunicorn_exporter-k8s-sidecar:v0.24.0-yelp0",
+        )
+
+    def get_mark_for_deployment_max_polling_threads(self) -> int:
+        return self.config_dict.get("mark_for_deployment_max_polling_threads", 4)
+
+    def get_mark_for_deployment_default_polling_interval(self) -> float:
+        return self.config_dict.get("mark_for_deployment_default_polling_interval", 60)
+
+    def get_mark_for_deployment_default_diagnosis_interval(self) -> float:
+        return self.config_dict.get(
+            "mark_for_deployment_default_diagnosis_interval", 60
+        )
+
+    def get_mark_for_deployment_default_time_before_first_diagnosis(self) -> float:
+        return self.config_dict.get(
+            "mark_for_deployment_default_default_time_before_first_diagnosis", 300
+        )
+
+    def get_mark_for_deployment_should_ping_for_unhealthy_pods(self) -> bool:
+        return self.config_dict.get(
+            "mark_for_deployment_should_ping_for_unhealthy_pods", True
+        )
+
+    def get_spark_k8s_role(self) -> str:
+        return self.config_dict.get("spark_k8s_role", "spark")
+
+    def get_spark_driver_port(self) -> int:
+        # default value is an arbitrary value
+        return self.config_dict.get("spark_driver_port", 33001)
+
+    def get_spark_blockmanager_port(self) -> int:
+        # default value is an arbitrary value
+        return self.config_dict.get("spark_blockmanager_port", 33002)
+
+    def get_api_profiling_config(self) -> Dict:
+        return self.config_dict.get(
+            "api_profiling_config",
+            {"cprofile_sampling_enabled": False},
+        )
+
+    def get_skip_cpu_override_validation_services(self) -> List[str]:
+        return self.config_dict.get("skip_cpu_override_validation", [])
+
+    def get_skip_cpu_burst_validation_services(self) -> List[str]:
+        return self.config_dict.get("skip_cpu_burst_validation", [])
+
+    def get_cluster_aliases(self) -> Dict[str, str]:
+        return self.config_dict.get("cluster_aliases", {})
+
+    def get_eks_cluster_aliases(self) -> Dict[str, str]:
+        return self.config_dict.get("eks_cluster_aliases", {})
+
+    def get_cluster_pools(self) -> Dict[str, List[str]]:
+        return self.config_dict.get("allowed_pools", {})
+
+    def get_spark_driver_iam_role(self) -> str:
+        return self.get_spark_run_config().get("default_spark_driver_iam_role", "")
+
+    def get_spark_executor_iam_role(self) -> str:
+        # use the same IAM role as the Spark driver
+        return self.get_spark_run_config().get("default_spark_driver_iam_role", "")
+
+    def get_pools_for_cluster(self, cluster: str) -> List[str]:
+        return self.get_cluster_pools().get(cluster, [])
+
+    def get_hacheck_match_initial_delay(self) -> bool:
+        return self.config_dict.get("hacheck_match_initial_delay", False)
+
+    def get_readiness_check_prefix_template(self) -> List[str]:
+        """A prefix that will be added to the beginning of the readiness check command. Meant for e.g. `flock` and
+        `timeout`."""
+        # We use flock+timeout here to work around issues discovered in PAASTA-17673:
+        # In k8s 1.18, probe timeout wasn't respected at all.
+        # When we upgraded to k8s 1.20, the timeout started being partially respected - k8s would stop waiting for a
+        # response, but wouldn't kill the command within the container (with the dockershim CRI).
+        # Flock prevents multiple readiness probes from running at once, using lots of CPU.
+        # The generous timeout allows for a slow readiness probe, but ensures that a truly-stuck readiness probe command
+        # will eventually be killed so another process can retry.
+        # Once we move off dockershim, we'll likely need to increase the readiness probe timeout, but we can then remove
+        # this wrapper.
+        return self.config_dict.get(
+            "readiness_check_prefix_template",
+            ["flock", "-n", "/readiness_check_lock", "timeout", "120"],
+        )
+
+    def get_tron_k8s_cluster_overrides(self) -> Dict[str, str]:
+        """
+        Return a mapping of a tron cluster -> compute cluster. Returns an empty dict if there are no overrides set.
+
+        This exists as we have certain Tron masters that are named differently from the compute cluster that should
+        actually be used (e.g., we might have tron-XYZ-test-prod, but instead of scheduling on XYZ-test-prod, we'd
+        like to schedule jobs on test-prod).
+
+        To control this, we have an optional config item that we'll puppet onto Tron masters that need this type of
+        tron master -> compute cluster override which this function will read.
+        """
+        return self.config_dict.get("tron_k8s_cluster_overrides", {})
+
+    def get_spark_kubeconfig(self) -> str:
+        return self.config_dict.get("spark_kubeconfig", "/etc/kubernetes/spark.conf")
+
+    def get_kube_clusters(self) -> Dict:
+        return self.config_dict.get("kube_clusters", {})
+
 
 def _run(
     command: Union[str, List[str]],
     env: Mapping[str, str] = os.environ,
     timeout: float = None,
     log: bool = False,
     stream: bool = False,
@@ -2302,15 +2841,15 @@
         to pass at least a :service: and a :component: parameter. Optionally you
         can pass :cluster:, :instance: and :loglevel: parameters for logging.
     We wanted to use plumbum instead of rolling our own thing with
     subprocess.Popen but were blocked by
     https://github.com/tomerfiliba/plumbum/issues/162 and our local BASH_FUNC
     magic.
     """
-    output = []
+    output: List[str] = []
     if log:
         service = kwargs["service"]
         component = kwargs["component"]
         cluster = kwargs.get("cluster", ANY_CLUSTER)
         instance = kwargs.get("instance", ANY_INSTANCE)
         loglevel = kwargs.get("loglevel", DEFAULT_LOGLEVEL)
     try:
@@ -2332,22 +2871,19 @@
             signal.signal(signal.SIGINT, signal_handler)
             signal.signal(signal.SIGTERM, signal_handler)
 
         # start the timer if we specified a timeout
         if timeout:
             proctimer = threading.Timer(timeout, _timeout, [process])
             proctimer.start()
+
+        outfn: Any = print if stream else output.append
         for linebytes in iter(process.stdout.readline, b""):
             line = linebytes.decode("utf-8", errors="replace").rstrip("\n")
-            linebytes = linebytes.strip(b"\n")
-            # additional indentation is for the paasta status command only
-            if stream:
-                paasta_print(linebytes)
-            else:
-                output.append(line)
+            outfn(line)
 
             if log:
                 _log(
                     service=service,
                     line=line,
                     component=component,
                     level=loglevel,
@@ -2474,41 +3010,49 @@
 
 
 def build_docker_image_name(service: str) -> str:
     """docker-paasta.yelpcorp.com:443 is the URL for the Registry where PaaSTA
     will look for your images.
 
     :returns: a sanitized-for-Jenkins (s,/,-,g) version of the
-    service's path in git. E.g. For git.yelpcorp.com:services/foo the
+    service's path in git. E.g. For github.yelpcorp.com:services/foo the
     docker image name is docker_registry/services-foo.
     """
     docker_registry_url = get_service_docker_registry(service)
     name = f"{docker_registry_url}/services-{service}"
     return name
 
 
-def build_docker_tag(service: str, upstream_git_commit: str) -> str:
+def build_docker_tag(
+    service: str, upstream_git_commit: str, image_version: Optional[str] = None
+) -> str:
     """Builds the DOCKER_TAG string
 
     upstream_git_commit is the SHA that we're building. Usually this is the
     tip of origin/master.
     """
     tag = "{}:paasta-{}".format(build_docker_image_name(service), upstream_git_commit)
+    if image_version is not None:
+        tag += f"-{image_version}"
     return tag
 
 
-def check_docker_image(service: str, tag: str) -> bool:
+def check_docker_image(
+    service: str,
+    commit: str,
+    image_version: Optional[str] = None,
+) -> bool:
     """Checks whether the given image for :service: with :tag: exists.
 
     :raises: ValueError if more than one docker image with :tag: found.
     :returns: True if there is exactly one matching image found.
     """
     docker_client = get_docker_client()
     image_name = build_docker_image_name(service)
-    docker_tag = build_docker_tag(service, tag)
+    docker_tag = build_docker_tag(service, commit, image_version)
     images = docker_client.images(name=image_name)
     # image['RepoTags'] may be None
     # Fixed upstream but only in docker-py 2.
     # https://github.com/docker/docker-py/issues/1401
     result = [image for image in images if docker_tag in (image["RepoTags"] or [])]
     if len(result) > 1:
         raise ValueError(
@@ -2543,14 +3087,39 @@
 def get_hostname() -> str:
     """Returns the fully-qualified domain name of the server this code is
     running on.
     """
     return socket.getfqdn()
 
 
+def get_files_of_type_in_dir(
+    file_type: str,
+    service: str = None,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> List[str]:
+    """Recursively search path if type of file exists.
+
+    :param file_type: a string of a type of a file (kubernetes, slo, etc.)
+    :param service: a string of a service
+    :param soa_dir: a string of a path to a soa_configs directory
+    :return: a list
+    """
+    # TODO: Only use INSTANCE_TYPES as input by making file_type Literal
+    service = "**" if service is None else service
+    soa_dir = DEFAULT_SOA_DIR if soa_dir is None else soa_dir
+    file_type += "-*.yaml"
+    return [
+        file_path
+        for file_path in glob.glob(
+            os.path.join(soa_dir, service, file_type),
+            recursive=True,
+        )
+    ]
+
+
 def get_soa_cluster_deploy_files(
     service: str = None, soa_dir: str = DEFAULT_SOA_DIR, instance_type: str = None
 ) -> Iterator[Tuple[str, str]]:
     if service is None:
         service = "*"
     service_path = os.path.join(soa_dir, service)
 
@@ -2576,15 +3145,15 @@
 
 def list_clusters(
     service: str = None, soa_dir: str = DEFAULT_SOA_DIR, instance_type: str = None
 ) -> List[str]:
     """Returns a sorted list of clusters a service is configured to deploy to,
     or all clusters if ``service`` is not specified.
 
-    Includes every cluster that has a ``marathon-*.yaml`` or ``tron-*.yaml`` file associated with it.
+    Includes every cluster that has a ``kubernetes-*.yaml`` or ``tron-*.yaml`` file associated with it.
 
     :param service: The service name. If unspecified, clusters running any service will be included.
     :returns: A sorted list of cluster names
     """
     clusters = set()
     for cluster, _ in get_soa_cluster_deploy_files(
         service=service, soa_dir=soa_dir, instance_type=instance_type
@@ -2613,133 +3182,117 @@
                 service, cluster, instance_type, soa_dir=soa_dir
             )
         for service_instance in si_list:
             instances.add(service_instance[1])
     return instances
 
 
-def get_tron_instance_list_from_yaml(
-    service: str, cluster: str, soa_dir: str
-) -> Collection[Tuple[str, str]]:
-    instance_list = []
-    try:
-        tron_config_content = load_tron_yaml(
-            service=service, cluster=cluster, soa_dir=soa_dir
-        )
-    except NoConfigurationForServiceError:
-        return []
-    jobs = extract_jobs_from_tron_yaml(config=tron_config_content)
-    for job_name, job in jobs.items():
-        action_names = get_action_names_from_job(job=job)
-        for name in action_names:
-            instance = f"{job_name}.{name}"
-            instance_list.append((service, instance))
-    return instance_list
-
-
-def get_action_names_from_job(job: dict) -> Collection[str]:
-    # Warning: This duplicates some logic from TronActionConfig, but can't be imported here
-    # dute to circular imports
-    actions = job.get("actions", {})
-    if isinstance(actions, dict):
-        return list(actions.keys())
-    elif actions is None:
-        return []
-    else:
-        raise TypeError("Tron actions must be a dictionary")
-
-
-def load_tron_yaml(service: str, cluster: str, soa_dir: str) -> Dict[str, Any]:
-    config = service_configuration_lib.read_extra_service_information(
-        service_name=service, extra_info=f"tron-{cluster}", soa_dir=soa_dir
-    )
-    if not config:
-        raise NoConfigurationForServiceError(
-            "No Tron configuration found for service %s" % service
-        )
-    return config
-
-
-def extract_jobs_from_tron_yaml(config: Dict) -> Dict[str, Any]:
+def filter_templates_from_config(config: Dict) -> Dict[str, Any]:
     config = {
         key: value for key, value in config.items() if not key.startswith("_")
     }  # filter templates
     return config or {}
 
 
-def get_instance_list_from_yaml(
-    service: str, conf_file: str, soa_dir: str
+def read_service_instance_names(
+    service: str, instance_type: str, cluster: str, soa_dir: str
 ) -> Collection[Tuple[str, str]]:
     instance_list = []
-    instances = service_configuration_lib.read_extra_service_information(
-        service, conf_file, soa_dir=soa_dir
+    conf_file = f"{instance_type}-{cluster}"
+    config = service_configuration_lib.read_extra_service_information(
+        service,
+        conf_file,
+        soa_dir=soa_dir,
+        deepcopy=False,
     )
-    for instance in instances:
-        if instance.startswith("_"):
-            log.debug(
-                f"Ignoring {service}.{instance} as instance name begins with '_'."
-            )
-        else:
+    config = filter_templates_from_config(config)
+    if instance_type == "tron":
+        for job_name, job in config.items():
+            action_names = list(job.get("actions", {}).keys())
+            for name in action_names:
+                instance = f"{job_name}.{name}"
+                instance_list.append((service, instance))
+    else:
+        for instance in config:
             instance_list.append((service, instance))
     return instance_list
 
 
+def get_production_deploy_group(service: str, soa_dir: str = DEFAULT_SOA_DIR) -> str:
+    service_configuration = read_service_configuration(service, soa_dir)
+    return service_configuration.get("deploy", {}).get("production_deploy_group", None)
+
+
 def get_pipeline_config(service: str, soa_dir: str = DEFAULT_SOA_DIR) -> List[Dict]:
     service_configuration = read_service_configuration(service, soa_dir)
     return service_configuration.get("deploy", {}).get("pipeline", [])
 
 
+def is_secrets_for_teams_enabled(service: str, soa_dir: str = DEFAULT_SOA_DIR) -> bool:
+    service_yaml_contents = read_extra_service_information(service, "service", soa_dir)
+    return service_yaml_contents.get("secrets_for_owner_team", False)
+
+
+def get_pipeline_deploy_group_configs(
+    service: str, soa_dir: str = DEFAULT_SOA_DIR
+) -> List[Dict]:
+    pipeline_steps = []
+    for step in get_pipeline_config(service, soa_dir):
+        # added support for parallel steps in a deploy.yaml
+        # parallel steps would break previous functionality as steps arent
+        # expected to be nested in a parallel block
+        if step.get("parallel"):
+            for parallel_step in step.get("parallel"):
+                if parallel_step.get("step"):
+                    pipeline_steps.append(parallel_step)
+        else:
+            pipeline_steps.append(step)
+    return [step for step in pipeline_steps if is_deploy_step(step["step"])]
+
+
 def get_pipeline_deploy_groups(
     service: str, soa_dir: str = DEFAULT_SOA_DIR
 ) -> List[str]:
-    pipeline_steps = [step["step"] for step in get_pipeline_config(service, soa_dir)]
-    return [step for step in pipeline_steps if is_deploy_step(step)]
+    deploy_group_configs = get_pipeline_deploy_group_configs(service, soa_dir)
+    return [step["step"] for step in deploy_group_configs]
 
 
 def get_service_instance_list_no_cache(
     service: str,
     cluster: Optional[str] = None,
     instance_type: str = None,
     soa_dir: str = DEFAULT_SOA_DIR,
 ) -> List[Tuple[str, str]]:
     """Enumerate the instances defined for a service as a list of tuples.
 
     :param service: The service name
     :param cluster: The cluster to read the configuration for
-    :param instance_type: The type of instances to examine: 'marathon', 'tron', or None (default) for both
+    :param instance_type: The type of instances to examine: 'kubernetes', 'tron', or None (default) for both
     :param soa_dir: The SOA config directory to read from
     :returns: A list of tuples of (name, instance) for each instance defined for the service name
     """
 
     instance_types: Tuple[str, ...]
     if not cluster:
         cluster = load_system_paasta_config().get_cluster()
     if instance_type in INSTANCE_TYPES:
         instance_types = (instance_type,)
     else:
         instance_types = INSTANCE_TYPES
 
     instance_list: List[Tuple[str, str]] = []
     for srv_instance_type in instance_types:
-        conf_file = f"{srv_instance_type}-{cluster}"
-        log.debug(
-            f"Enumerating all instances for config file: {soa_dir}/*/{conf_file}.yaml"
-        )
-        if srv_instance_type == "tron":
-            instance_list.extend(
-                get_tron_instance_list_from_yaml(
-                    service=service, cluster=cluster, soa_dir=soa_dir
-                )
-            )
-        else:
-            instance_list.extend(
-                get_instance_list_from_yaml(
-                    service=service, conf_file=conf_file, soa_dir=soa_dir
-                )
+        instance_list.extend(
+            read_service_instance_names(
+                service=service,
+                instance_type=srv_instance_type,
+                cluster=cluster,
+                soa_dir=soa_dir,
             )
+        )
     log.debug("Enumerated the following instances: %s", instance_list)
     return instance_list
 
 
 @time_cache(ttl=5)
 def get_service_instance_list(
     service: str,
@@ -2747,58 +3300,136 @@
     instance_type: str = None,
     soa_dir: str = DEFAULT_SOA_DIR,
 ) -> List[Tuple[str, str]]:
     """Enumerate the instances defined for a service as a list of tuples.
 
     :param service: The service name
     :param cluster: The cluster to read the configuration for
-    :param instance_type: The type of instances to examine: 'marathon', 'tron', or None (default) for both
+    :param instance_type: The type of instances to examine: 'kubernetes', 'tron', or None (default) for both
     :param soa_dir: The SOA config directory to read from
     :returns: A list of tuples of (name, instance) for each instance defined for the service name
     """
     return get_service_instance_list_no_cache(
         service=service, cluster=cluster, instance_type=instance_type, soa_dir=soa_dir
     )
 
 
 def get_services_for_cluster(
     cluster: str = None, instance_type: str = None, soa_dir: str = DEFAULT_SOA_DIR
 ) -> List[Tuple[str, str]]:
     """Retrieve all services and instances defined to run in a cluster.
 
     :param cluster: The cluster to read the configuration for
-    :param instance_type: The type of instances to examine: 'marathon', 'tron', or None (default) for both
+    :param instance_type: The type of instances to examine: 'kubernetes', 'tron', or None (default) for both
     :param soa_dir: The SOA config directory to read from
     :returns: A list of tuples of (service, instance)
     """
 
     if not cluster:
         cluster = load_system_paasta_config().get_cluster()
     rootdir = os.path.abspath(soa_dir)
     log.debug(
         "Retrieving all service instance names from %s for cluster %s", rootdir, cluster
     )
     instance_list: List[Tuple[str, str]] = []
     for srv_dir in os.listdir(rootdir):
-        service_instance_list = get_service_instance_list(
-            srv_dir, cluster, instance_type, soa_dir
+        instance_list.extend(
+            get_service_instance_list(srv_dir, cluster, instance_type, soa_dir)
         )
-        for service_instance in service_instance_list:
-            service, instance = service_instance
-            if instance.startswith("_"):
-                log.debug(
-                    f"Ignoring {service}.{instance} as instance name begins with '_'."
-                )
-            else:
-                instance_list.append(service_instance)
     return instance_list
 
 
-def parse_yaml_file(yaml_file: str) -> Any:
-    return yaml.safe_load(open(yaml_file))
+def load_service_instance_configs(
+    service: str,
+    instance_type: str,
+    cluster: str,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> Dict[str, InstanceConfigDict]:
+    conf_file = f"{instance_type}-{cluster}"
+    user_configs = service_configuration_lib.read_extra_service_information(
+        service,
+        conf_file,
+        soa_dir=soa_dir,
+        deepcopy=False,
+    )
+    user_configs = filter_templates_from_config(user_configs)
+    auto_configs = load_service_instance_auto_configs(
+        service, instance_type, cluster, soa_dir
+    )
+    merged = {}
+    for instance_name, user_config in user_configs.items():
+        auto_config = auto_configs.get(instance_name, {})
+        merged[instance_name] = deep_merge_dictionaries(
+            overrides=user_config,
+            defaults=auto_config,
+        )
+    return merged
+
+
+def load_service_instance_config(
+    service: str,
+    instance: str,
+    instance_type: str,
+    cluster: str,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> InstanceConfigDict:
+    if instance.startswith("_"):
+        raise InvalidJobNameError(
+            f"Unable to load {instance_type} config for {service}.{instance} as instance name starts with '_'"
+        )
+    conf_file = f"{instance_type}-{cluster}"
+
+    # We pass deepcopy=False here and then do our own deepcopy of the subset of the data we actually care about. Without
+    # this optimization, any code that calls load_service_instance_config for every instance in a yaml file is ~O(n^2).
+    user_config = copy.deepcopy(
+        service_configuration_lib.read_extra_service_information(
+            service, conf_file, soa_dir=soa_dir, deepcopy=False
+        ).get(instance)
+    )
+    if user_config is None:
+        raise NoConfigurationForServiceError(
+            f"{instance} not found in config file {soa_dir}/{service}/{conf_file}.yaml."
+        )
+
+    auto_config = load_service_instance_auto_configs(
+        service, instance_type, cluster, soa_dir
+    ).get(instance, {})
+    return deep_merge_dictionaries(
+        overrides=user_config,
+        defaults=auto_config,
+    )
+
+
+def load_service_instance_auto_configs(
+    service: str,
+    instance_type: str,
+    cluster: str,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> Dict[str, Dict[str, Any]]:
+    enabled_types = load_system_paasta_config().get_auto_config_instance_types_enabled()
+    # this looks a little funky: but what we're generally trying to do here is ensure that
+    # certain types of instances can be moved between instance types without having to worry
+    # about any sort of data races (or data weirdness) in autotune.
+    # instead, what we do is map certain instance types to whatever we've picked as the "canonical"
+    # instance type in autotune and always merge from there.
+    realized_type = (
+        load_system_paasta_config()
+        .get_auto_config_instance_type_aliases()
+        .get(instance_type, instance_type)
+    )
+    conf_file = f"{realized_type}-{cluster}"
+    if enabled_types.get(realized_type):
+        return service_configuration_lib.read_extra_service_information(
+            service,
+            f"{AUTO_SOACONFIG_SUBDIR}/{conf_file}",
+            soa_dir=soa_dir,
+            deepcopy=False,
+        )
+    else:
+        return {}
 
 
 def get_docker_host() -> str:
     return os.environ.get("DOCKER_HOST", "unix://var/run/docker.sock")
 
 
 def get_docker_client() -> Client:
@@ -2840,43 +3471,44 @@
     def __exit__(self, type: Any, value: Any, traceback: Any) -> None:
         signal.alarm(0)
         signal.signal(signal.SIGALRM, self.old_handler)
 
 
 def print_with_indent(line: str, indent: int = 2) -> None:
     """Print a line with a given indent level"""
-    paasta_print(" " * indent + line)
+    print(" " * indent + line)
 
 
 class NoDeploymentsAvailable(Exception):
     pass
 
 
-def load_deployments_json(
-    service: str, soa_dir: str = DEFAULT_SOA_DIR
-) -> "DeploymentsJsonV1":
-    deployment_file = os.path.join(soa_dir, service, "deployments.json")
-    if os.path.isfile(deployment_file):
-        with open(deployment_file) as f:
-            return DeploymentsJsonV1(json.load(f)["v1"])
-    else:
-        e = f"{deployment_file} was not found. 'generate_deployments_for_service --service {service}' must be run first"
-        raise NoDeploymentsAvailable(e)
+class DeploymentVersion(NamedTuple):
+    sha: str
+    image_version: Optional[str]
 
+    def __repr__(self) -> str:
+        # Represented as commit if no image_version, standard tuple repr otherwise
+        return (
+            f"DeploymentVersion(sha={self.sha}, image_version={self.image_version})"
+            if self.image_version
+            else self.sha
+        )
 
-def load_v2_deployments_json(
-    service: str, soa_dir: str = DEFAULT_SOA_DIR
-) -> "DeploymentsJsonV2":
-    deployment_file = os.path.join(soa_dir, service, "deployments.json")
-    if os.path.isfile(deployment_file):
-        with open(deployment_file) as f:
-            return DeploymentsJsonV2(service=service, config_dict=json.load(f)["v2"])
-    else:
-        e = f"{deployment_file} was not found. 'generate_deployments_for_service --service {service}' must be run first"
-        raise NoDeploymentsAvailable(e)
+    def short_sha_repr(self, sha_len: int = 8) -> str:
+        # Same as __repr__ but allows us to print the shortned commit sha.
+        short_sha = self.sha[:sha_len]
+        return (
+            f"DeploymentVersion(sha={short_sha}, image_version={self.image_version})"
+            if self.image_version
+            else short_sha
+        )
+
+    def json(self) -> str:
+        return json.dumps(self._asdict())
 
 
 DeploymentsJsonV1Dict = Dict[str, BranchDictV1]
 
 DeployGroup = str
 BranchName = str
 
@@ -2885,14 +3517,15 @@
     force_bounce: Optional[str]
     desired_state: str
 
 
 class _DeploymentsJsonV2DeploymentsDict(TypedDict):
     docker_image: str
     git_sha: str
+    image_version: Optional[str]
 
 
 class DeploymentsJsonV2Dict(TypedDict):
     deployments: Dict[DeployGroup, _DeploymentsJsonV2DeploymentsDict]
     controls: Dict[BranchName, _DeploymentsJsonV2ControlsDict]
 
 
@@ -2924,35 +3557,69 @@
     def get_branch_dict(
         self, service: str, branch: str, deploy_group: str
     ) -> BranchDictV2:
         full_branch = f"{service}:{branch}"
         branch_dict: BranchDictV2 = {
             "docker_image": self.get_docker_image_for_deploy_group(deploy_group),
             "git_sha": self.get_git_sha_for_deploy_group(deploy_group),
+            "image_version": self.get_image_version_for_deploy_group(deploy_group),
             "desired_state": self.get_desired_state_for_branch(full_branch),
             "force_bounce": self.get_force_bounce_for_branch(full_branch),
         }
         return branch_dict
 
     def get_deploy_groups(self) -> Collection[str]:
         return self.config_dict["deployments"].keys()
 
     def get_docker_image_for_deploy_group(self, deploy_group: str) -> str:
         try:
-            return self.config_dict["deployments"][deploy_group]["docker_image"]
+            deploy_group_config = self.config_dict["deployments"][deploy_group]
         except KeyError:
             e = f"{self.service} not deployed to {deploy_group}. Has mark-for-deployment been run?"
             raise NoDeploymentsAvailable(e)
+        try:
+            return deploy_group_config["docker_image"]
+        except KeyError:
+            e = f"The configuration for service {self.service} in deploy group {deploy_group} does not contain 'docker_image' metadata."
+            raise KeyError(e)
 
     def get_git_sha_for_deploy_group(self, deploy_group: str) -> str:
         try:
-            return self.config_dict["deployments"][deploy_group]["git_sha"]
+            deploy_group_config = self.config_dict["deployments"][deploy_group]
+        except KeyError:
+            e = f"{self.service} not deployed to {deploy_group}. Has mark-for-deployment been run?"
+            raise NoDeploymentsAvailable(e)
+        try:
+            return deploy_group_config["git_sha"]
+        except KeyError:
+            e = f"The configuration for service {self.service} in deploy group {deploy_group} does not contain 'git_sha' metadata."
+            raise KeyError(e)
+
+    def get_image_version_for_deploy_group(self, deploy_group: str) -> Optional[str]:
+        try:
+            deploy_group_config = self.config_dict["deployments"][deploy_group]
         except KeyError:
             e = f"{self.service} not deployed to {deploy_group}. Has mark-for-deployment been run?"
             raise NoDeploymentsAvailable(e)
+        try:
+            # TODO: Once these changes have propagated image_version should
+            # always be present in the deployments.json file, so remove the
+            # .get() call.
+            return deploy_group_config.get("image_version", None)
+        except KeyError:
+            e = f"The configuration for service {self.service} in deploy group {deploy_group} does not contain 'image_version' metadata."
+            raise KeyError(e)
+
+    def get_deployment_version_for_deploy_group(
+        self, deploy_group: str
+    ) -> DeploymentVersion:
+        return DeploymentVersion(
+            sha=self.get_git_sha_for_deploy_group(deploy_group),
+            image_version=self.get_image_version_for_deploy_group(deploy_group),
+        )
 
     def get_desired_state_for_branch(self, control_branch: str) -> str:
         try:
             return self.config_dict["controls"][control_branch].get(
                 "desired_state", "start"
             )
         except KeyError:
@@ -2965,49 +3632,128 @@
                 "force_bounce", None
             )
         except KeyError:
             e = f"{self.service} not configured for {control_branch}. Has mark-for-deployment been run?"
             raise NoDeploymentsAvailable(e)
 
 
+def load_deployments_json(service: str, soa_dir: str = DEFAULT_SOA_DIR) -> Any:
+    deployment_file = os.path.join(soa_dir, service, "deployments.json")
+    if os.path.isfile(deployment_file):
+        with open(deployment_file) as f:
+            config_dict = json.load(f)
+            return (
+                DeploymentsJsonV1(config_dict["v1"])
+                if "v1" in config_dict
+                else DeploymentsJsonV2(service=service, config_dict=config_dict["v2"])
+            )
+    else:
+        e = f"{deployment_file} was not found. 'generate_deployments_for_service --service {service}' must be run first"
+        raise NoDeploymentsAvailable(e)
+
+
+def load_v2_deployments_json(
+    service: str, soa_dir: str = DEFAULT_SOA_DIR
+) -> DeploymentsJsonV2:
+    deployment_file = os.path.join(soa_dir, service, "deployments.json")
+    if os.path.isfile(deployment_file):
+        with open(deployment_file) as f:
+            return DeploymentsJsonV2(service=service, config_dict=json.load(f)["v2"])
+    else:
+        e = f"{deployment_file} was not found. 'generate_deployments_for_service --service {service}' must be run first"
+        raise NoDeploymentsAvailable(e)
+
+
 def get_paasta_branch(cluster: str, instance: str) -> str:
     return SPACER.join((cluster, instance))
 
 
 def parse_timestamp(tstamp: str) -> datetime.datetime:
     return datetime.datetime.strptime(tstamp, "%Y%m%dT%H%M%S")
 
 
 def format_timestamp(dt: datetime.datetime = None) -> str:
     if dt is None:
         dt = datetime.datetime.utcnow()
     return dt.strftime("%Y%m%dT%H%M%S")
 
 
-def get_paasta_tag_from_deploy_group(identifier: str, desired_state: str) -> str:
+def get_paasta_tag_from_deploy_group(
+    identifier: str, desired_state: str, image_version: Optional[str] = None
+) -> str:
     timestamp = format_timestamp(datetime.datetime.utcnow())
-    return f"paasta-{identifier}-{timestamp}-{desired_state}"
+    if image_version:
+        return f"paasta-{identifier}+{image_version}-{timestamp}-{desired_state}"
+    else:
+        return f"paasta-{identifier}-{timestamp}-{desired_state}"
 
 
 def get_paasta_tag(cluster: str, instance: str, desired_state: str) -> str:
     timestamp = format_timestamp(datetime.datetime.utcnow())
     return f"paasta-{cluster}.{instance}-{timestamp}-{desired_state}"
 
 
 def format_tag(tag: str) -> str:
     return "refs/tags/%s" % tag
 
 
+def get_latest_deployment_tag(
+    refs: Dict[str, str], deploy_group: str
+) -> Tuple[str, str, Optional[str]]:
+    """Gets the latest deployment tag and sha for the specified deploy_group
+
+    :param refs: A dictionary mapping git refs to shas
+    :param deploy_group: The deployment group to return a deploy tag for
+
+    :returns: A tuple of the form (ref, sha, image_version) where ref is the
+              actual deployment tag (with the most recent timestamp), sha is
+              the sha it points at and image_version provides additional
+              version information about the image
+    """
+    most_recent_dtime = None
+    most_recent_ref = None
+    most_recent_sha = None
+    most_recent_image_version = None
+    pattern = re.compile(
+        r"^refs/tags/paasta-%s(?:\+(?P<image_version>.*)){0,1}-(?P<dtime>\d{8}T\d{6})-deploy$"
+        % deploy_group
+    )
+
+    for ref_name, sha in refs.items():
+        match = pattern.match(ref_name)
+        if match:
+            gd = match.groupdict()
+            dtime = gd["dtime"]
+            if most_recent_dtime is None or dtime > most_recent_dtime:
+                most_recent_dtime = dtime
+                most_recent_ref = ref_name
+                most_recent_sha = sha
+                most_recent_image_version = gd["image_version"]
+    return most_recent_ref, most_recent_sha, most_recent_image_version
+
+
+def build_image_identifier(
+    git_sha: str, sha_len: Optional[int] = None, image_version: Optional[str] = None
+) -> str:
+    image = git_sha
+    if sha_len is not None:
+        image = image[:sha_len]
+    if image_version is not None:
+        image += f"-{image_version}"
+
+    return image
+
+
 class NoDockerImageError(Exception):
     pass
 
 
 def get_config_hash(config: Any, force_bounce: str = None) -> str:
     """Create an MD5 hash of the configuration dictionary to be sent to
-    Marathon. Or anything really, so long as str(config) works. Returns
+    Kubernetes. Or anything really, so long as str(config) works. Returns
     the first 8 characters so things are not really long.
 
     :param config: The configuration to hash
     :param force_bounce: a timestamp (in the form of a string) that is appended before hashing
                          that can be used to force a hash change
     :returns: A MD5 hash of str(config)
     """
@@ -3015,26 +3761,61 @@
     hasher.update(
         json.dumps(config, sort_keys=True).encode("UTF-8")
         + (force_bounce or "").encode("UTF-8")
     )
     return "config%s" % hasher.hexdigest()[:8]
 
 
-def get_git_sha_from_dockerurl(docker_url: str) -> str:
-    parts = docker_url.split("/")
-    parts = parts[-1].split("-")
-    return parts[-1][:8]
+def get_git_sha_from_dockerurl(docker_url: str, long: bool = False) -> str:
+    """We encode the sha of the code that built a docker image *in* the docker
+    url. This function takes that url as input and outputs the sha.
+    """
+    if ":paasta-" in docker_url:
+        deployment_version = get_deployment_version_from_dockerurl(docker_url)
+        git_sha = deployment_version.sha if deployment_version else ""
+    # Fall back to the old behavior if the docker_url does not follow the
+    # expected pattern
+    else:
+        parts = docker_url.split("/")
+        parts = parts[-1].split("-")
+        git_sha = parts[-1]
+
+    return git_sha if long else git_sha[:8]
+
+
+def get_image_version_from_dockerurl(docker_url: str) -> Optional[str]:
+    """We can optionally encode additional metadata about the docker image *in*
+    the docker url. This function takes that url as input and outputs the sha.
+    """
+    deployment_version = get_deployment_version_from_dockerurl(docker_url)
+    return deployment_version.image_version if deployment_version else None
+
+
+def get_deployment_version_from_dockerurl(docker_url: str) -> DeploymentVersion:
+    regex_match = re.match(
+        r".*:paasta-(?P<git_sha>[A-Za-z0-9]+)(-(?P<image_version>.+))?", docker_url
+    )
+
+    return (
+        DeploymentVersion(
+            sha=regex_match.group("git_sha"),
+            image_version=regex_match.group("image_version"),
+        )
+        if regex_match is not None
+        else None
+    )
 
 
 def get_code_sha_from_dockerurl(docker_url: str) -> str:
-    """We encode the sha of the code that built a docker image *in* the docker
-    url. This function takes that url as input and outputs the partial sha
+    """code_sha is hash extracted from docker url prefixed with "git", short
+    hash is used because it's embedded in mesos task names and there's length
+    limit.
     """
     try:
-        git_sha = get_git_sha_from_dockerurl(docker_url)
+        git_sha = get_git_sha_from_dockerurl(docker_url, long=False)
         return "git%s" % git_sha
     except Exception:
         return "gitUNKNOWN"
 
 
 def is_under_replicated(
     num_available: int, expected_count: int, crit_threshold: int
@@ -3056,48 +3837,44 @@
     else:
         return (False, ratio)
 
 
 def deploy_blacklist_to_constraints(
     deploy_blacklist: DeployBlacklist,
 ) -> List[Constraint]:
-    """Converts a blacklist of locations into marathon appropriate constraints.
-
-    https://mesosphere.github.io/marathon/docs/constraints.html#unlike-operator
+    """Converts a blacklist of locations into tron appropriate constraints.
 
     :param blacklist: List of lists of locations to blacklist
     :returns: List of lists of constraints
     """
     constraints: List[Constraint] = []
     for blacklisted_location in deploy_blacklist:
         constraints.append([blacklisted_location[0], "UNLIKE", blacklisted_location[1]])
 
     return constraints
 
 
 def deploy_whitelist_to_constraints(
     deploy_whitelist: DeployWhitelist,
 ) -> List[Constraint]:
-    """Converts a whitelist of locations into marathon appropriate constraints
-
-    https://mesosphere.github.io/marathon/docs/constraints.html#like-operator
+    """Converts a whitelist of locations into tron appropriate constraints
 
     :param deploy_whitelist: List of lists of locations to whitelist
     :returns: List of lists of constraints
     """
     if deploy_whitelist is not None:
         (region_type, regions) = deploy_whitelist
         regionstr = "|".join(regions)
 
         return [[region_type, "LIKE", regionstr]]
     return []
 
 
 def terminal_len(text: str) -> int:
-    """Return the number of characters that text will take up on a terminal. """
+    """Return the number of characters that text will take up on a terminal."""
     return len(remove_ansi_escape_sequences(text))
 
 
 def format_table(
     rows: Iterable[Union[str, Sequence[str]]], min_spacing: int = 2
 ) -> List[str]:
     """Formats a table for use on the command line.
@@ -3245,41 +4022,43 @@
     Returns the average value of an iterable
     """
     return sum(iterable) / len(iterable)
 
 
 def prompt_pick_one(sequence: Collection[str], choosing: str) -> str:
     if not sys.stdin.isatty():
-        paasta_print(
+        print(
             "No {choosing} specified and no TTY present to ask."
             "Please specify a {choosing} using the cli.".format(choosing=choosing),
             file=sys.stderr,
         )
         sys.exit(1)
 
     if not sequence:
-        paasta_print(
+        print(
             f"PaaSTA needs to pick a {choosing} but none were found.", file=sys.stderr
         )
         sys.exit(1)
 
     global_actions = [str("quit")]
     choices = [(item, item) for item in sequence]
 
     if len(choices) == 1:
         return choices[0][0]
 
     chooser = choice.Menu(choices=choices, global_actions=global_actions)
-    chooser.title = 'Please pick a {choosing} from the choices below (or "quit" to quit):'.format(
-        choosing=str(choosing)
+    chooser.title = (
+        'Please pick a {choosing} from the choices below (or "quit" to quit):'.format(
+            choosing=str(choosing)
+        )
     )
     try:
         result = chooser.ask()
     except (KeyboardInterrupt, EOFError):
-        paasta_print("")
+        print("")
         sys.exit(1)
 
     if isinstance(result, tuple) and result[1] == str("quit"):
         sys.exit(1)
     else:
         return result
 
@@ -3289,48 +4068,14 @@
         return obj
     elif isinstance(obj, str):
         return obj.encode("UTF-8")
     else:
         return str(obj).encode("UTF-8")
 
 
-TLS = threading.local()
-
-
-@contextlib.contextmanager
-def set_paasta_print_file(file: Any) -> Iterator[None]:
-    TLS.paasta_print_file = file
-    yield
-    TLS.paasta_print_file = None
-
-
-def paasta_print(*args: Any, **kwargs: Any) -> None:
-    f = kwargs.pop("file", sys.stdout) or sys.stdout
-    f = getattr(TLS, "paasta_print_file", f) or f
-    buf = getattr(f, "buffer", None)
-    # Here we're assuming that the file object works with strings and its
-    # `buffer` works with bytes. So, if the file object doesn't have `buffer`,
-    # we output via the file object itself using strings.
-    obj_to_arg: Callable[[Any], Any]
-    if buf is not None:
-        f = buf
-        obj_to_arg = to_bytes
-    else:
-
-        def obj_to_arg(o: Any) -> str:
-            return to_bytes(o).decode("UTF-8", errors="ignore")
-
-    end = obj_to_arg(kwargs.pop("end", "\n"))
-    sep = obj_to_arg(kwargs.pop("sep", " "))
-    assert not kwargs, kwargs
-    to_print = sep.join(obj_to_arg(x) for x in args) + end
-    f.write(to_print)
-    f.flush()
-
-
 _TimeoutFuncRetType = TypeVar("_TimeoutFuncRetType")
 
 
 def timeout(
     seconds: int = 10,
     error_message: str = os.strerror(errno.ETIME),
     use_signals: bool = True,
@@ -3435,13 +4180,79 @@
 
 
 def load_all_configs(
     cluster: str, file_prefix: str, soa_dir: str
 ) -> Mapping[str, Mapping[str, Any]]:
     config_dicts = {}
     for service in os.listdir(soa_dir):
-        config_dicts[
-            service
-        ] = service_configuration_lib.read_extra_service_information(
-            service, f"{file_prefix}-{cluster}", soa_dir=soa_dir
+        config_dicts[service] = load_service_instance_configs(
+            service, file_prefix, cluster, soa_dir
         )
     return config_dicts
+
+
+def ldap_user_search(
+    cn: str,
+    search_base: str,
+    search_ou: str,
+    ldap_host: str,
+    username: str,
+    password: str,
+) -> Set[str]:
+    """Connects to LDAP and raises a subclass of LDAPOperationResult when it fails"""
+    tls_config = ldap3.Tls(
+        validate=ssl.CERT_REQUIRED, ca_certs_file="/etc/ssl/certs/ca-certificates.crt"
+    )
+    server = ldap3.Server(ldap_host, use_ssl=True, tls=tls_config)
+    conn = ldap3.Connection(
+        server, user=username, password=password, raise_exceptions=True
+    )
+    conn.bind()
+
+    search_filter = f"(&(memberOf=CN={cn},{search_ou})(!(userAccountControl=514)))"
+    entries = conn.extend.standard.paged_search(
+        search_base=search_base,
+        search_scope=ldap3.SUBTREE,
+        search_filter=search_filter,
+        attributes=["sAMAccountName"],
+        paged_size=1000,
+        time_limit=10,
+    )
+    return {entry["attributes"]["sAMAccountName"] for entry in entries}
+
+
+def _reorder_docker_volumes(volumes: List[DockerVolume]) -> List[DockerVolume]:
+    deduped = {
+        v["containerPath"].rstrip("/") + v["hostPath"].rstrip("/"): v for v in volumes
+    }.values()
+    return sort_dicts(deduped)
+
+
+def get_k8s_url_for_cluster(cluster: str) -> Optional[str]:
+    """
+    Annoyingly, there's two layers of aliases: one to figure out what
+    k8s server url to use (this one) and another to figure out what
+    soaconfigs filename to use ;_;
+
+    This exists so that we can map something like `--cluster pnw-devc`
+    into spark-pnw-devc's k8s apiserver url without needing to update
+    any soaconfigs/alter folk's muscle memory.
+
+    Ideally we can get rid of this entirely once spark-run reads soaconfigs
+    in a manner more closely aligned to what we do with other paasta workloads
+    (i.e., have it automatically determine where to run based on soaconfigs
+    filenames - and not rely on explicit config)
+    """
+    realized_cluster = (
+        load_system_paasta_config().get_eks_cluster_aliases().get(cluster, cluster)
+    )
+    return (
+        load_system_paasta_config()
+        .get_kube_clusters()
+        .get(realized_cluster, {})
+        .get("server")
+    )
+
+
+@lru_cache(maxsize=1)
+def is_using_unprivileged_containers() -> bool:
+    return "podman" in os.getenv("DOCKER_HOST", "")
```

### Comparing `paasta-tools-0.92.1/paasta_tools/tron_tools.py` & `paasta-tools-1.0.0/paasta_tools/long_running_service_tools.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,729 +1,669 @@
-# Copyright 2015-2018 Yelp Inc.
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-import datetime
-import difflib
-import glob
-import json
+import copy
 import logging
 import os
-import pkgutil
-import re
-import subprocess
-from string import Formatter
+import socket
+from typing import Dict
 from typing import List
+from typing import Mapping
+from typing import Optional
+from typing import Sequence
 from typing import Tuple
+from typing import Type
+
+import service_configuration_lib
 
-import yaml
-from service_configuration_lib import pick_random_port
-from service_configuration_lib import read_yaml_file
-from service_configuration_lib.spark_config import get_mesos_spark_env
-from service_configuration_lib.spark_config import stringify_spark_env
-
-try:
-    from yaml.cyaml import CSafeDumper as Dumper
-except ImportError:  # pragma: no cover (no libyaml-dev / pypy)
-    Dumper = yaml.SafeDumper  # type: ignore
-
-from paasta_tools.spark_tools import get_default_event_log_dir
-from paasta_tools.spark_tools import load_mesos_secret_for_spark
-from paasta_tools.mesos_tools import find_mesos_leader
-from paasta_tools.tron.client import TronClient
-from paasta_tools.tron import tron_command_context
+from paasta_tools.autoscaling.utils import AutoscalingParamsDict
+from paasta_tools.autoscaling.utils import MetricsProviderDict
+from paasta_tools.paasta_service_config_loader import PaastaServiceConfigLoader
+from paasta_tools.utils import BranchDictV2
+from paasta_tools.utils import compose_job_id
+from paasta_tools.utils import decompose_job_id
+from paasta_tools.utils import deep_merge_dictionaries
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import DeployBlacklist
+from paasta_tools.utils import DeployWhitelist
 from paasta_tools.utils import InstanceConfig
+from paasta_tools.utils import InstanceConfigDict
 from paasta_tools.utils import InvalidInstanceConfig
+from paasta_tools.utils import InvalidJobNameError
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import load_v2_deployments_json
-from paasta_tools.utils import NoConfigurationForServiceError
-from paasta_tools.utils import NoDeploymentsAvailable
-from paasta_tools.utils import paasta_print
-from paasta_tools.utils import load_tron_yaml
-from paasta_tools.utils import extract_jobs_from_tron_yaml
-
-from paasta_tools import monitoring_tools
-from paasta_tools.monitoring_tools import list_teams
-from typing import Optional
-from typing import Dict
-from typing import Any
+from paasta_tools.utils import SystemPaastaConfig
 
 log = logging.getLogger(__name__)
-logging.getLogger("tron").setLevel(logging.WARNING)
-
-MASTER_NAMESPACE = "MASTER"
-SPACER = "."
-VALID_MONITORING_KEYS = set(
-    json.loads(
-        pkgutil.get_data("paasta_tools.cli", "schemas/tron_schema.json").decode()
-    )["definitions"]["job"]["properties"]["monitoring"]["properties"].keys()
-)
-MESOS_EXECUTOR_NAMES = ("paasta", "spark")
-
-
-class TronNotConfigured(Exception):
-    pass
-
-
-class InvalidTronConfig(Exception):
-    pass
-
-
-class TronConfig(dict):
-    """System-level configuration for Tron."""
-
-    def __init__(self, config):
-        super().__init__(config)
-
-    def get_cluster_name(self):
-        """:returns The name of the Tron cluster"""
-        try:
-            return self["cluster_name"]
-        except KeyError:
-            raise TronNotConfigured(
-                "Could not find name of Tron cluster in system Tron config"
-            )
+logging.getLogger("long_running_service_tools").setLevel(logging.WARNING)
 
-    def get_url(self):
-        """:returns The URL for the Tron master's API"""
-        try:
-            return self["url"]
-        except KeyError:
-            raise TronNotConfigured(
-                "Could not find URL of Tron master in system Tron config"
-            )
-
-
-def get_tronfig_folder(cluster, soa_dir):
-    return os.path.join(soa_dir, "tron", cluster)
-
-
-def load_tron_config():
-    return TronConfig(load_system_paasta_config().get_tron_config())
-
-
-def get_tron_client():
-    return TronClient(load_tron_config().get_url())
-
-
-def compose_instance(job, action):
-    return f"{job}{SPACER}{action}"
+ZK_PAUSE_AUTOSCALE_PATH = "/autoscaling/paused"
+DEFAULT_CONTAINER_PORT = 8888
 
+DEFAULT_AUTOSCALING_SETPOINT = 0.8
+DEFAULT_DESIRED_ACTIVE_REQUESTS_PER_REPLICA = 1
+DEFAULT_ACTIVE_REQUESTS_AUTOSCALING_MOVING_AVERAGE_WINDOW = 1800
+DEFAULT_UWSGI_AUTOSCALING_MOVING_AVERAGE_WINDOW = 1800
+DEFAULT_PISCINA_AUTOSCALING_MOVING_AVERAGE_WINDOW = 1800
+DEFAULT_GUNICORN_AUTOSCALING_MOVING_AVERAGE_WINDOW = 1800
+
+METRICS_PROVIDER_CPU = "cpu"
+METRICS_PROVIDER_UWSGI = "uwsgi"
+METRICS_PROVIDER_GUNICORN = "gunicorn"
+METRICS_PROVIDER_PISCINA = "piscina"
+METRICS_PROVIDER_ACTIVE_REQUESTS = "active-requests"
+METRICS_PROVIDER_PROMQL = "arbitrary_promql"
+
+ALL_METRICS_PROVIDERS = [
+    METRICS_PROVIDER_CPU,
+    METRICS_PROVIDER_UWSGI,
+    METRICS_PROVIDER_GUNICORN,
+    METRICS_PROVIDER_PISCINA,
+    METRICS_PROVIDER_ACTIVE_REQUESTS,
+    METRICS_PROVIDER_PROMQL,
+]
+
+
+class LongRunningServiceConfigDict(InstanceConfigDict, total=False):
+    autoscaling: AutoscalingParamsDict
+    drain_method: str
+    fs_group: int
+    container_port: int
+    drain_method_params: Dict
+    healthcheck_cmd: str
+    healthcheck_grace_period_seconds: float
+    healthcheck_interval_seconds: float
+    healthcheck_max_consecutive_failures: int
+    healthcheck_mode: str
+    healthcheck_timeout_seconds: float
+    healthcheck_uri: str
+    instances: int
+    max_instances: int
+    min_instances: int
+    nerve_ns: str
+    network_mode: str
+    registrations: List[str]
+    replication_threshold: int
+    bounce_start_deadline: float
+    bounce_margin_factor: float
+    should_ping_for_unhealthy_pods: bool
+    weight: int
+
+
+class ServiceNamespaceConfig(dict):
+    def get_healthcheck_mode(self) -> str:
+        """Get the healthcheck mode for the service. In most cases, this will match the mode
+        of the service, but we do provide the opportunity for users to specify both. Default to the mode
+        if no healthcheck_mode is specified.
+        """
+        healthcheck_mode = self.get("healthcheck_mode", None)
+        if not healthcheck_mode:
+            return self.get_mode()
+        else:
+            return healthcheck_mode
 
-def decompose_instance(instance):
-    """Get (job_name, action_name) from an instance."""
-    decomposed = instance.split(SPACER)
-    if len(decomposed) != 2:
-        raise InvalidInstanceConfig("Invalid instance name: %s" % instance)
-    return (decomposed[0], decomposed[1])
-
-
-class StringFormatter(Formatter):
-    def __init__(self, context=None):
-        Formatter.__init__(self)
-        self.context = context
-
-    def get_value(self, key, args, kwds):
-        if isinstance(key, str):
-            try:
-                return kwds[key]
-            except KeyError:
-                return self.context[key]
+    def get_mode(self) -> str:
+        """Get the mode that the service runs in and check that we support it.
+        If the mode is not specified, we check whether the service uses smartstack
+        in order to determine the appropriate default value. If proxy_port is specified
+        in the config, the service uses smartstack, and we can thus safely assume its mode is http.
+        If the mode is not defined and the service does not use smartstack, we set the mode to None.
+        """
+        mode = self.get("mode", None)
+        if mode is None:
+            if not self.is_in_smartstack():
+                return None
             else:
-                return Formatter.get_value(key, args, kwds)
+                return "http"
+        elif mode in ["http", "tcp", "https"]:
+            return mode
+        else:
+            raise InvalidSmartstackMode("Unknown mode: %s" % mode)
 
+    def get_healthcheck_uri(self) -> str:
+        return self.get("healthcheck_uri", "/status")
 
-def parse_time_variables(command: str, parse_time: datetime.datetime = None) -> str:
-    """Parses an input string and uses the Tron-style dateparsing
-    to replace time variables. Currently supports only the date/time
-    variables listed in the tron documentation:
-    http://tron.readthedocs.io/en/latest/command_context.html#built-in-cc
-
-    :param input_string: input string to be parsed
-    :param parse_time: Reference Datetime object to parse the date and time strings, defaults to now.
-    :returns: A string with the date and time variables replaced
-    """
-    if parse_time is None:
-        parse_time = datetime.datetime.now()
-    # We build up a tron context object that has the right
-    # methods to parse tron-style time syntax
-    job_context = tron_command_context.JobRunContext(
-        tron_command_context.CommandContext()
-    )
-    # The tron context object needs the run_time attribute set so it knows
-    # how to interpret the date strings
-    job_context.job_run.run_time = parse_time
-    return StringFormatter(job_context).format(command)
+    def get_discover(self) -> str:
+        return self.get("discover", "region")
+
+    def is_in_smartstack(self) -> bool:
+        return "proxy_port" in self
 
 
-class TronActionConfig(InstanceConfig):
-    config_filename_prefix = "tron"
+class LongRunningServiceConfig(InstanceConfig):
+    config_dict: LongRunningServiceConfigDict
 
     def __init__(
         self,
-        service,
-        instance,
-        cluster,
-        config_dict,
-        branch_dict,
-        soa_dir=DEFAULT_SOA_DIR,
-    ):
+        service: str,
+        cluster: str,
+        instance: str,
+        config_dict: LongRunningServiceConfigDict,
+        branch_dict: Optional[BranchDictV2],
+        soa_dir: str = DEFAULT_SOA_DIR,
+    ) -> None:
         super().__init__(
             cluster=cluster,
             instance=instance,
             service=service,
             config_dict=config_dict,
             branch_dict=branch_dict,
             soa_dir=soa_dir,
         )
-        self.job, self.action = decompose_instance(instance)
-
-    def get_job_name(self):
-        return self.job
 
-    def get_action_name(self):
-        return self.action
-
-    def get_deploy_group(self) -> Optional[str]:
-        return self.config_dict.get("deploy_group", None)
-
-    def get_cmd(self):
-        command = self.config_dict.get("command")
-        if self.get_executor() == "spark":
-            # Spark expects to be able to write to MESOS_SANDBOX if it is set
-            # but the default value (/mnt/mesos/sandbox) doesn't get mounted in
-            # our Docker containers, so we unset it here.  (Un-setting is fine,
-            # since Spark will just write to /tmp instead).
-            command = "unset MESOS_DIRECTORY MESOS_SANDBOX; " + command
-        return command
-
-    def get_env(self):
-        env = super().get_env()
-        spark_env = {}
-        if self.get_executor() == "spark":
-            spark_env = get_mesos_spark_env(
-                spark_app_name="tron_spark_{self.get_service()}_{self.get_instance()}",
-                spark_ui_port=pick_random_port(
-                    f"{self.get_service()}{self.get_instance()}".encode()
-                ),
-                mesos_leader=find_mesos_leader(self.get_cluster()),
-                mesos_secret=load_mesos_secret_for_spark(),
-                paasta_cluster=self.get_cluster(),
-                paasta_pool=self.get_pool(),
-                paasta_service=self.get_service(),
-                paasta_instance=self.get_instance(),
-                docker_img=self.get_docker_url(),
-                volumes=format_volumes(
-                    self.get_volumes(load_system_paasta_config().get_volumes())
-                ),
-                user_spark_opts=self.config_dict.get("spark_args"),
-                event_log_dir=get_default_event_log_dir(
-                    service=self.get_service(),
-                    aws_credentials_yaml=self.config_dict.get("aws_credentials"),
-                ),
-            )
-            env["SPARK_OPTS"] = stringify_spark_env(spark_env)
+    def get_bounce_method(self) -> str:
+        raise NotImplementedError
 
+    def get_namespace(self) -> str:
+        """Get namespace from config"""
+        raise NotImplementedError
+
+    def get_kubernetes_namespace(self) -> str:
+        """
+        Only needed on kubernetes LongRunningServiceConfig
+        """
+        raise NotImplementedError
+
+    def get_sanitised_deployment_name(self) -> str:
+        """
+        Only needed on kubernetes LongRunningServiceConfig
+        """
+        raise NotImplementedError
+
+    def get_service_name_smartstack(self) -> str:
+        """
+        This is just the service name here
+        For cassandra we have to override this to support apollo
+        """
+        return self.get_service()
+
+    def get_env(
+        self, system_paasta_config: Optional[SystemPaastaConfig] = None
+    ) -> Dict[str, str]:
+        env = super().get_env(system_paasta_config=system_paasta_config)
+        env["PAASTA_PORT"] = str(self.get_container_port())
         return env
 
-    def get_cpu_burst_add(self) -> float:
-        """ For Tron jobs, we don't let them burst by default, because they
-        don't represent "real-time" workloads, and should not impact
-        neighbors """
-        return self.config_dict.get("cpu_burst_add", 0)
-
-    def get_executor(self):
-        return self.config_dict.get("executor", None)
-
-    def get_healthcheck_mode(self, _) -> None:
-        return None
-
-    def get_node(self):
-        return self.config_dict.get("node")
-
-    def get_retries(self):
-        return self.config_dict.get("retries")
-
-    def get_retries_delay(self):
-        return self.config_dict.get("retries_delay")
-
-    def get_requires(self):
-        return self.config_dict.get("requires")
-
-    def get_expected_runtime(self):
-        return self.config_dict.get("expected_runtime")
-
-    def get_triggered_by(self):
-        return self.config_dict.get("triggered_by", None)
-
-    def get_trigger_downstreams(self):
-        return self.config_dict.get("trigger_downstreams", None)
-
-    def get_on_upstream_rerun(self):
-        return self.config_dict.get("on_upstream_rerun", None)
+    def get_container_port(self) -> int:
+        return self.config_dict.get("container_port", DEFAULT_CONTAINER_PORT)
 
-    def get_trigger_timeout(self):
-        return self.config_dict.get("trigger_timeout", None)
+    def get_drain_method(self, service_namespace_config: ServiceNamespaceConfig) -> str:
+        """Get the drain method specified in the service's configuration.
 
-    def get_calculated_constraints(self):
-        """Combine all configured Mesos constraints."""
-        constraints = self.get_constraints()
-        if constraints is not None:
-            return constraints
-        else:
-            constraints = self.get_extra_constraints()
-            constraints.extend(
-                self.get_deploy_constraints(
-                    blacklist=self.get_deploy_blacklist(),
-                    whitelist=self.get_deploy_whitelist(),
-                    # Don't have configs for the paasta cluster
-                    system_deploy_blacklist=[],
-                    system_deploy_whitelist=None,
-                )
+        :param service_config: The service instance's configuration dictionary
+        :returns: The drain method specified in the config, or 'noop' if not specified"""
+        default = "noop"
+        # Default to hacheck draining if the service is in smartstack
+        if service_namespace_config.is_in_smartstack():
+            default = "hacheck"
+        return self.config_dict.get("drain_method", default)
+
+    def get_drain_method_params(
+        self, service_namespace_config: ServiceNamespaceConfig
+    ) -> Dict:
+        """Get the drain method parameters specified in the service's configuration.
+
+        :param service_config: The service instance's configuration dictionary
+        :returns: The drain_method_params dictionary specified in the config, or {} if not specified"""
+        default: Dict = {}
+        if service_namespace_config.is_in_smartstack():
+            default = {"delay": 60}
+        return self.config_dict.get("drain_method_params", default)
+
+    # FIXME(jlynch|2016-08-02, PAASTA-4964): DEPRECATE nerve_ns and remove it
+    def get_nerve_namespace(self) -> str:
+        return decompose_job_id(self.get_registrations()[0])[1]
+
+    def get_registrations(self) -> List[str]:
+        for registration in self.get_invalid_registrations():
+            log.error(
+                "Provided registration {} for service "
+                "{} is invalid".format(registration, self.service)
             )
-            constraints.extend(self.get_pool_constraints())
-            return constraints
 
-    def get_nerve_namespace(self) -> None:
-        return None
+        registrations = self.config_dict.get("registrations", [])
 
-    def validate(self):
-        error_msgs = []
-        error_msgs.extend(super().validate())
-        # Tron is a little special, because it can *not* have a deploy group
-        # But only if an action is running via ssh and not via paasta
-        if (
-            self.get_deploy_group() is None
-            and self.get_executor() in MESOS_EXECUTOR_NAMES
-        ):
-            error_msgs.append(
-                f"{self.get_job_name()}.{self.get_action_name()} must have a deploy_group set"
+        # Backwards compatibility with nerve_ns
+        # FIXME(jlynch|2016-08-02, PAASTA-4964): DEPRECATE nerve_ns and remove it
+        if not registrations and "nerve_ns" in self.config_dict:
+            registrations.append(
+                compose_job_id(self.service, self.config_dict["nerve_ns"])
             )
-        return error_msgs
 
+        return registrations or [compose_job_id(self.service, self.instance)]
 
-class TronJobConfig:
-    """Represents a job in Tron, consisting of action(s) and job-level configuration values."""
-
-    def __init__(
-        self,
-        name: str,
-        config_dict: Dict[str, Any],
-        cluster: str,
-        service: Optional[str] = None,
-        load_deployments: bool = True,
-        soa_dir: str = DEFAULT_SOA_DIR,
-    ) -> None:
-        self.name = name
-        self.config_dict = config_dict
-        self.cluster = cluster
-        self.service = service
-        self.load_deployments = load_deployments
-        self.soa_dir = soa_dir
-
-    def get_name(self):
-        return self.name
-
-    def get_node(self):
-        return self.config_dict.get("node")
-
-    def get_schedule(self):
-        return self.config_dict.get("schedule")
-
-    def get_monitoring(self):
-        srv_monitoring = dict(
-            monitoring_tools.read_monitoring_config(self.service, soa_dir=self.soa_dir)
+    def get_invalid_registrations(self) -> List[str]:
+        registrations = self.config_dict.get("registrations", [])
+        invalid_registrations: List[str] = []
+        for registration in registrations:
+            try:
+                decompose_job_id(registration)
+            except InvalidJobNameError:
+                invalid_registrations.append(registration)
+        return invalid_registrations
+
+    def get_replication_crit_percentage(self) -> int:
+        return self.config_dict.get("replication_threshold", 50)
+
+    def get_fs_group(self) -> Optional[int]:
+        return self.config_dict.get("fs_group")
+
+    def get_healthcheck_uri(
+        self, service_namespace_config: ServiceNamespaceConfig
+    ) -> str:
+        return self.config_dict.get(
+            "healthcheck_uri", service_namespace_config.get_healthcheck_uri()
         )
-        tron_monitoring = self.config_dict.get("monitoring", {})
-        srv_monitoring.update(tron_monitoring)
-        # filter out non-tron monitoring keys
-        srv_monitoring = {
-            k: v for k, v in srv_monitoring.items() if k in VALID_MONITORING_KEYS
-        }
-        return srv_monitoring
-
-    def get_queueing(self):
-        return self.config_dict.get("queueing")
-
-    def get_run_limit(self):
-        return self.config_dict.get("run_limit")
-
-    def get_all_nodes(self):
-        return self.config_dict.get("all_nodes")
-
-    def get_enabled(self):
-        return self.config_dict.get("enabled")
-
-    def get_allow_overlap(self):
-        return self.config_dict.get("allow_overlap")
-
-    def get_max_runtime(self):
-        return self.config_dict.get("max_runtime")
-
-    def get_time_zone(self):
-        return self.config_dict.get("time_zone")
 
-    def get_service(self) -> Optional[str]:
-        return self.service or self.config_dict.get("service")
-
-    def get_deploy_group(self) -> Optional[str]:
-        return self.config_dict.get("deploy_group", None)
+    def get_healthcheck_cmd(self) -> str:
+        cmd = self.config_dict.get("healthcheck_cmd", None)
+        if cmd is None:
+            raise InvalidInstanceConfig(
+                "healthcheck mode 'cmd' requires a healthcheck_cmd to run"
+            )
+        else:
+            return cmd
 
-    def get_cluster(self):
-        return self.cluster
+    def get_healthcheck_grace_period_seconds(self) -> float:
+        """
+        How long before kubernetes will start sending healthcheck and liveness probes.
+        """
+        return self.config_dict.get("healthcheck_grace_period_seconds", 60)
+
+    def get_healthcheck_interval_seconds(self) -> float:
+        return self.config_dict.get("healthcheck_interval_seconds", 10)
+
+    def get_healthcheck_timeout_seconds(self) -> float:
+        return self.config_dict.get("healthcheck_timeout_seconds", 10)
+
+    def get_healthcheck_max_consecutive_failures(self) -> int:
+        return self.config_dict.get("healthcheck_max_consecutive_failures", 30)
+
+    def get_healthcheck_mode(
+        self, service_namespace_config: ServiceNamespaceConfig
+    ) -> str:
+        mode = self.config_dict.get("healthcheck_mode", None)
+        if mode is None:
+            mode = service_namespace_config.get_healthcheck_mode()
+        elif mode not in ["http", "https", "tcp", "cmd", None]:
+            raise InvalidHealthcheckMode("Unknown mode: %s" % mode)
+        return mode
+
+    def get_bounce_start_deadline(self) -> float:
+        return self.config_dict.get("bounce_start_deadline", 0)
+
+    def get_autoscaled_instances(self) -> int:
+        raise NotImplementedError()
+
+    def get_instances(self, with_limit: bool = True) -> int:
+        """Gets the number of instances for a service, ignoring whether the user has requested
+        the service to be started or stopped"""
+        if self.is_autoscaling_enabled():
+            autoscaled_instances = self.get_autoscaled_instances()
+            if autoscaled_instances is None:
+                return self.get_max_instances()
+            else:
+                limited_instances = (
+                    self.limit_instance_count(autoscaled_instances)
+                    if with_limit
+                    else autoscaled_instances
+                )
+                return limited_instances
+        else:
+            instances = self.config_dict.get("instances", 1)
+            log.debug("Autoscaling not enabled, returning %d instances" % instances)
+            return instances
+
+    def get_min_instances(self) -> int:
+        return self.config_dict.get("min_instances", 1)
+
+    def is_autoscaling_enabled(self) -> bool:
+        return self.get_max_instances() is not None
+
+    def get_max_instances(self) -> Optional[int]:
+        return self.config_dict.get("max_instances", None)
+
+    def get_desired_instances(self) -> int:
+        """Get the number of instances specified in zookeeper or the service's configuration.
+        If the number of instances in zookeeper is less than min_instances, returns min_instances.
+        If the number of instances in zookeeper is greater than max_instances, returns max_instances.
+
+        Defaults to 0 if not specified in the config.
+
+        :returns: The number of instances specified in the config, 0 if not
+                  specified or if desired_state is not 'start'.
+        """
+        if self.get_desired_state() == "start":
+            return self.get_instances()
+        else:
+            log.debug("Instance is set to stop. Returning '0' instances")
+            return 0
 
-    def get_expected_runtime(self):
-        return self.config_dict.get("expected_runtime")
+    def limit_instance_count(self, instances: int) -> int:
+        """
+        Returns param instances if it is between min_instances and max_instances.
+        Returns max_instances if instances > max_instances
+        Returns min_instances if instances < min_instances
+        """
+        return max(self.get_min_instances(), min(self.get_max_instances(), instances))
+
+    def get_autoscaling_params(self) -> AutoscalingParamsDict:
+        default_provider_params: MetricsProviderDict = {
+            "type": METRICS_PROVIDER_CPU,
+            "decision_policy": "proportional",
+            "setpoint": DEFAULT_AUTOSCALING_SETPOINT,
+        }
 
-    def _get_action_config(self, action_name, action_dict):
-        action_service = action_dict.setdefault("service", self.get_service())
-        action_deploy_group = action_dict.setdefault(
-            "deploy_group", self.get_deploy_group()
+        params = copy.deepcopy(
+            self.config_dict.get("autoscaling", AutoscalingParamsDict({}))
         )
-        if action_service and action_deploy_group and self.load_deployments:
-            try:
-                deployments_json = load_v2_deployments_json(
-                    service=action_service, soa_dir=self.soa_dir
-                )
-                branch_dict = {
-                    "docker_image": deployments_json.get_docker_image_for_deploy_group(
-                        action_deploy_group
-                    ),
-                    "git_sha": deployments_json.get_git_sha_for_deploy_group(
-                        action_deploy_group
-                    ),
-                    # TODO: add Tron instances when generating deployments json
-                    "desired_state": "start",
-                    "force_bounce": None,
-                }
-            except NoDeploymentsAvailable:
-                log.warning(
-                    f'Docker image unavailable for {action_service}.{self.get_name()}.{action_dict.get("name")}'
-                    " is it deployed yet?"
-                )
-                branch_dict = None
+        if "metrics_providers" not in params or len(params["metrics_providers"]) == 0:
+            params["metrics_providers"] = [default_provider_params]
         else:
-            branch_dict = None
-        action_dict["monitoring"] = self.get_monitoring()
+            params["metrics_providers"] = [
+                deep_merge_dictionaries(
+                    overrides=provider,
+                    defaults=default_provider_params,
+                )
+                for provider in params["metrics_providers"]
+            ]
+        return params
+
+    def get_autoscaling_metrics_provider(
+        self, provider_type: str
+    ) -> Optional[MetricsProviderDict]:
+        autoscaling_params = self.get_autoscaling_params()
+        # We only allow one metric provider of each type, so we can bail early if we find a match
+        for provider in autoscaling_params["metrics_providers"]:
+            if provider["type"] == provider_type:
+                return provider
+        return None
 
-        return TronActionConfig(
-            service=action_service,
-            instance=compose_instance(self.get_name(), action_name),
-            cluster=self.get_cluster(),
-            config_dict=action_dict,
-            branch_dict=branch_dict,
-            soa_dir=self.soa_dir,
+    def should_use_metrics_provider(self, provider_type: str) -> bool:
+        return (
+            self.is_autoscaling_enabled()
+            and self.get_autoscaling_metrics_provider(provider_type) is not None
         )
 
-    def get_actions(self):
-        actions = self.config_dict.get("actions")
-        return [
-            self._get_action_config(name, action_dict)
-            for name, action_dict in actions.items()
-        ]
-
-    def get_cleanup_action(self):
-        action_dict = self.config_dict.get("cleanup_action")
-        if not action_dict:
-            return None
-
-        # TODO: we should keep this trickery outside paasta repo
-        return self._get_action_config("cleanup", action_dict)
-
-    def check_monitoring(self) -> Tuple[bool, str]:
-        monitoring = self.get_monitoring()
-        valid_teams = list_teams()
-        if monitoring is not None:
-            team_name = monitoring.get("team", None)
-            if team_name is None:
-                return False, "Team name is required for monitoring"
-            elif team_name not in valid_teams:
-                suggest_teams = difflib.get_close_matches(
-                    word=team_name, possibilities=valid_teams
-                )
-                return (
-                    False,
-                    f"Invalid team name: {team_name}. Do you mean one of these: {suggest_teams}",
-                )
-        return True, ""
-
-    def check_actions(self) -> Tuple[bool, List[str]]:
-        actions = self.get_actions()
-        cleanup_action = self.get_cleanup_action()
-        if cleanup_action:
-            actions.append(cleanup_action)
-
-        checks_passed = True
-        msgs: List[str] = []
-        for action in actions:
-            action_msgs = action.validate()
-            if action_msgs:
-                checks_passed = False
-                msgs.extend(action_msgs)
-        return checks_passed, msgs
-
-    def validate(self) -> List[str]:
-        _, error_msgs = self.check_actions()
-        checks = ["check_monitoring"]
-        for check in checks:
-            check_passed, check_msg = getattr(self, check)()
-            if not check_passed:
-                error_msgs.append(check_msg)
-        return error_msgs
-
-    def __eq__(self, other):
-        if isinstance(other, type(self)):
-            return self.config_dict == other.config_dict
-        return False
+    def validate(
+        self,
+        params: Optional[List[str]] = None,
+    ) -> List[str]:
+        error_messages = super().validate(params=params)
+        invalid_registrations = self.get_invalid_registrations()
+        if invalid_registrations:
+            service_instance = compose_job_id(self.service, self.instance)
+            registrations_str = ", ".join(invalid_registrations)
+            error_messages.append(
+                f"Service registrations must be of the form service.registration. "
+                f"The following registrations for {service_instance} are "
+                f"invalid: {registrations_str}"
+            )
+        return error_messages
 
+    def get_bounce_margin_factor(self) -> float:
+        return self.config_dict.get("bounce_margin_factor", 1.0)
 
-def format_volumes(paasta_volume_list):
-    return [
-        {
-            "container_path": v["containerPath"],
-            "host_path": v["hostPath"],
-            "mode": v["mode"],
-        }
-        for v in paasta_volume_list
-    ]
+    def get_should_ping_for_unhealthy_pods(self, default: bool) -> bool:
+        return self.config_dict.get("should_ping_for_unhealthy_pods", default)
 
+    def get_weight(self) -> int:
+        return self.config_dict.get("weight", 10)
 
-def format_master_config(master_config, default_volumes, dockercfg_location):
-    mesos_options = master_config.get("mesos_options", {})
-    mesos_options.update(
-        {
-            "default_volumes": format_volumes(default_volumes),
-            "dockercfg_location": dockercfg_location,
-        }
-    )
-    master_config["mesos_options"] = mesos_options
-    return master_config
 
+class InvalidHealthcheckMode(Exception):
+    pass
 
-def format_tron_action_dict(action_config):
-    """Generate a dict of tronfig for an action, from the TronActionConfig.
 
-    :param job_config: TronActionConfig
+def get_healthcheck_for_instance(
+    service: str,
+    instance: str,
+    service_manifest: LongRunningServiceConfig,
+    random_port: int,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> Tuple[Optional[str], Optional[str]]:
     """
-    executor = action_config.get_executor()
-    result = {
-        "command": action_config.get_cmd(),
-        "executor": executor,
-        "requires": action_config.get_requires(),
-        "node": action_config.get_node(),
-        "retries": action_config.get_retries(),
-        "retries_delay": action_config.get_retries_delay(),
-        "expected_runtime": action_config.get_expected_runtime(),
-        "trigger_downstreams": action_config.get_trigger_downstreams(),
-        "triggered_by": action_config.get_triggered_by(),
-        "on_upstream_rerun": action_config.get_on_upstream_rerun(),
-        "trigger_timeout": action_config.get_trigger_timeout(),
-    }
-    if executor in MESOS_EXECUTOR_NAMES:
-        result["executor"] = "mesos"
-        result["cpus"] = action_config.get_cpus()
-        result["mem"] = action_config.get_mem()
-        result["disk"] = action_config.get_disk()
-        result["env"] = action_config.get_env()
-        result["extra_volumes"] = format_volumes(action_config.get_extra_volumes())
-        result["docker_parameters"] = [
-            {"key": param["key"], "value": param["value"]}
-            for param in action_config.format_docker_parameters()
-        ]
-        constraint_labels = ["attribute", "operator", "value"]
-        result["constraints"] = [
-            dict(zip(constraint_labels, constraint))
-            for constraint in action_config.get_calculated_constraints()
-        ]
-
-        # If deployments were not loaded
-        if not action_config.get_docker_image():
-            result["docker_image"] = ""
-        else:
-            result["docker_image"] = action_config.get_docker_url()
-
-    # Only pass non-None values, so Tron will use defaults for others
-    return {key: val for key, val in result.items() if val is not None}
+    Returns healthcheck for a given service instance in the form of a tuple (mode, healthcheck_command)
+    or (None, None) if no healthcheck
+    """
+    namespace = service_manifest.get_nerve_namespace()
+    smartstack_config = load_service_namespace_config(
+        service=service, namespace=namespace, soa_dir=soa_dir
+    )
+    mode = service_manifest.get_healthcheck_mode(smartstack_config)
+    hostname = socket.getfqdn()
 
+    if mode == "http" or mode == "https":
+        path = service_manifest.get_healthcheck_uri(smartstack_config)
+        healthcheck_command = "%s://%s:%d%s" % (mode, hostname, random_port, path)
+    elif mode == "tcp":
+        healthcheck_command = "%s://%s:%d" % (mode, hostname, random_port)
+    elif mode == "cmd":
+        healthcheck_command = service_manifest.get_healthcheck_cmd()
+    else:
+        mode = None
+        healthcheck_command = None
+    return (mode, healthcheck_command)
+
+
+def load_service_namespace_config(
+    service: str, namespace: str, soa_dir: str = DEFAULT_SOA_DIR
+) -> ServiceNamespaceConfig:
+    """Attempt to read the configuration for a service's namespace in a more strict fashion.
+
+    Retrieves the following keys:
+
+    - proxy_port: the proxy port defined for the given namespace
+    - healthcheck_mode: the mode for the healthcheck (http or tcp)
+    - healthcheck_port: An alternate port to use for health checking
+    - healthcheck_uri: URI target for healthchecking
+    - healthcheck_timeout_s: healthcheck timeout in seconds
+    - healthcheck_body_expect: an expected string in healthcheck response body
+    - updown_timeout_s: updown_service timeout in seconds
+    - timeout_connect_ms: proxy frontend timeout in milliseconds
+    - timeout_server_ms: proxy server backend timeout in milliseconds
+    - retries: the number of retries on a proxy backend
+    - mode: the mode the service is run in (http or tcp)
+    - routes: a list of tuples of (source, destination)
+    - discover: the scope at which to discover services e.g. 'habitat'
+    - advertise: a list of scopes to advertise services at e.g. ['habitat', 'region']
+    - extra_advertise: a list of tuples of (source, destination)
+      e.g. [('region:dc6-prod', 'region:useast1-prod')]
+    - extra_healthcheck_headers: a dict of HTTP headers that must
+      be supplied when health checking. E.g. { 'Host': 'example.com' }
+    - lb_policy: Envoy load balancer policies. E.g. "ROUND_ROBIN"
+
+    :param service: The service name
+    :param namespace: The namespace to read
+    :param soa_dir: The SOA config directory to read from
+    :returns: A dict of the above keys, if they were defined
+    """
 
-def format_tron_job_dict(job_config):
-    """Generate a dict of tronfig for a job, from the TronJobConfig.
+    smartstack_config = service_configuration_lib.read_extra_service_information(
+        service_name=service,
+        extra_info="smartstack",
+        soa_dir=soa_dir,
+        deepcopy=False,
+    )
 
-    :param job_config: TronJobConfig
-    """
-    action_dict = {
-        action_config.get_action_name(): format_tron_action_dict(action_config)
-        for action_config in job_config.get_actions()
-    }
+    namespace_config_from_file = smartstack_config.get(namespace, {})
 
-    result = {
-        "node": job_config.get_node(),
-        "schedule": job_config.get_schedule(),
-        "actions": action_dict,
-        "monitoring": job_config.get_monitoring(),
-        "queueing": job_config.get_queueing(),
-        "run_limit": job_config.get_run_limit(),
-        "all_nodes": job_config.get_all_nodes(),
-        "enabled": job_config.get_enabled(),
-        "allow_overlap": job_config.get_allow_overlap(),
-        "max_runtime": job_config.get_max_runtime(),
-        "time_zone": job_config.get_time_zone(),
-        "expected_runtime": job_config.get_expected_runtime(),
+    service_namespace_config = ServiceNamespaceConfig()
+    # We can't really use .get, as we don't want the key to be in the returned
+    # dict at all if it doesn't exist in the config file.
+    # We also can't just copy the whole dict, as we only care about some keys
+    # and there's other things that appear in the smartstack section in
+    # several cases.
+    key_whitelist = {
+        "healthcheck_mode",
+        "healthcheck_uri",
+        "healthcheck_port",
+        "healthcheck_timeout_s",
+        "healthcheck_body_expect",
+        "updown_timeout_s",
+        "proxy_port",
+        "timeout_connect_ms",
+        "timeout_server_ms",
+        "retries",
+        "mode",
+        "discover",
+        "advertise",
+        "extra_healthcheck_headers",
+        "lb_policy",
     }
-    cleanup_config = job_config.get_cleanup_action()
-    if cleanup_config:
-        cleanup_action = format_tron_action_dict(cleanup_config)
-        result["cleanup_action"] = cleanup_action
 
-    # Only pass non-None values, so Tron will use defaults for others
-    return {key: val for key, val in result.items() if val is not None}
+    for key, value in namespace_config_from_file.items():
+        if key in key_whitelist:
+            service_namespace_config[key] = value
+
+    # Other code in paasta_tools checks 'mode' after the config file
+    # is loaded, so this ensures that it is set to the appropriate default
+    # if not otherwise specified, even if appropriate default is None.
+    service_namespace_config["mode"] = service_namespace_config.get_mode()
+
+    if "routes" in namespace_config_from_file:
+        service_namespace_config["routes"] = [
+            (route["source"], dest)
+            for route in namespace_config_from_file["routes"]
+            for dest in route["destinations"]
+        ]
 
+    if "extra_advertise" in namespace_config_from_file:
+        service_namespace_config["extra_advertise"] = [
+            (src, dst)
+            for src in namespace_config_from_file["extra_advertise"]
+            for dst in namespace_config_from_file["extra_advertise"][src]
+        ]
 
-def load_tron_instance_config(
-    service: str,
-    instance: str,
-    cluster: str,
-    load_deployments: bool = True,
-    soa_dir: str = DEFAULT_SOA_DIR,
-) -> TronActionConfig:
-    jobs = load_tron_service_config(
-        service=service,
-        cluster=cluster,
-        load_deployments=load_deployments,
-        soa_dir=soa_dir,
-    )
-    requested_job, requested_action = instance.split(".")
-    for job in jobs:
-        if job.get_name() == requested_job:
-            for action in job.get_actions():
-                if action.get_action_name() == requested_action:
-                    return action
-    raise NoConfigurationForServiceError(
-        f"No tron configuration found for {service} {instance}"
-    )
+    return service_namespace_config
 
 
-def load_tron_service_config(
-    service, cluster, load_deployments=True, soa_dir=DEFAULT_SOA_DIR
-):
-    """Load all configured jobs for a service, and any additional config values."""
-    config = load_tron_yaml(service=service, cluster=cluster, soa_dir=soa_dir)
-    jobs = extract_jobs_from_tron_yaml(config)
-    job_configs = [
-        TronJobConfig(
-            name=name,
-            service=service,
-            cluster=cluster,
-            config_dict=job,
-            load_deployments=load_deployments,
-            soa_dir=soa_dir,
-        )
-        for name, job in jobs.items()
-    ]
-    return job_configs
+class InvalidSmartstackMode(Exception):
+    pass
 
 
-def create_complete_master_config(cluster, soa_dir=DEFAULT_SOA_DIR):
-    system_paasta_config = load_system_paasta_config()
-    tronfig_folder = get_tronfig_folder(soa_dir=soa_dir, cluster=cluster)
-    config = read_yaml_file(os.path.join(tronfig_folder, f"MASTER.yaml"))
-    master_config = format_master_config(
-        config,
-        system_paasta_config.get_volumes(),
-        system_paasta_config.get_dockercfg_location(),
+def get_proxy_port_for_instance(
+    service_config: LongRunningServiceConfig,
+) -> Optional[int]:
+    """Get the proxy_port defined in the first namespace configuration for a
+    service instance.
+
+    This means that the namespace first has to be loaded from the service instance's
+    configuration, and then the proxy_port has to loaded from the smartstack configuration
+    for that namespace.
+
+    :param service_config: The instance of the services LongRunningServiceConfig
+    :returns: The proxy_port for the service instance, or None if not defined"""
+    registration = service_config.get_registrations()[0]
+    service, namespace, _, __ = decompose_job_id(registration)
+    nerve_dict = load_service_namespace_config(
+        service=service, namespace=namespace, soa_dir=service_config.soa_dir
     )
-    return yaml.dump(master_config, Dumper=Dumper, default_flow_style=False)
+    return nerve_dict.get("proxy_port")
 
 
-def create_complete_config(service, cluster, soa_dir=DEFAULT_SOA_DIR):
-    """Generate a namespace configuration file for Tron, for a service."""
-    job_configs = load_tron_service_config(
-        service=service, cluster=cluster, load_deployments=True, soa_dir=soa_dir
-    )
-    preproccessed_config = {}
-    preproccessed_config["jobs"] = {
-        job_config.get_name(): format_tron_job_dict(job_config)
-        for job_config in job_configs
-    }
-    return yaml.dump(preproccessed_config, Dumper=Dumper, default_flow_style=False)
+def host_passes_blacklist(
+    host_attributes: Mapping[str, str], blacklist: DeployBlacklist
+) -> bool:
+    """
+    :param host: A single host attributes dict
+    :param blacklist: A list of lists like [["location_type", "location"], ["foo", "bar"]]
+    :returns: boolean, True if the host gets passed the blacklist
+    """
+    try:
+        for location_type, location in blacklist:
+            if host_attributes.get(location_type) == location:
+                return False
+    except ValueError as e:
+        log.error(f"Errors processing the following blacklist: {blacklist}")
+        log.error("I will assume the host does not pass\nError was: %s" % e)
+        return False
+    return True
 
 
-def validate_complete_config(
-    service: str, cluster: str, soa_dir: str = DEFAULT_SOA_DIR
-) -> List[str]:
-    job_configs = load_tron_service_config(
-        service=service, cluster=cluster, load_deployments=False, soa_dir=soa_dir
-    )
+def host_passes_whitelist(
+    host_attributes: Mapping[str, str], whitelist: DeployWhitelist
+) -> bool:
+    """
+    :param host: A single host attributes dict.
+    :param whitelist: A 2 item list like ["location_type", ["location1", 'location2']]
+    :returns: boolean, True if the host gets past the whitelist
+    """
+    # No whitelist, so disable whitelisting behavior.
+    if whitelist is None or len(whitelist) == 0:
+        return True
+    try:
+        (location_type, locations) = whitelist
+        if host_attributes.get(location_type) in locations:
+            return True
+    except ValueError as e:
+        log.error(f"Errors processing the following whitelist: {whitelist}")
+        log.error("I will assume the host does not pass\nError was: %s" % e)
+        return False
+    return False
 
-    # PaaSTA-specific validation
-    for job_config in job_configs:
-        check_msgs = job_config.validate()
-        if check_msgs:
-            return check_msgs
 
-    master_config_path = os.path.join(
-        os.path.abspath(soa_dir), "tron", cluster, MASTER_NAMESPACE + ".yaml"
+def get_all_namespaces(
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> Sequence[Tuple[str, ServiceNamespaceConfig]]:
+    """Get all the smartstack namespaces across all services.
+    This is mostly so synapse can get everything it needs in one call.
+
+    :param soa_dir: The SOA config directory to read from
+    :returns: A list of tuples of the form (service.namespace, namespace_config)"""
+    rootdir = os.path.abspath(soa_dir)
+    namespace_list: List[Tuple[str, ServiceNamespaceConfig]] = []
+    for srv_dir in os.listdir(rootdir):
+        namespace_list.extend(get_all_namespaces_for_service(srv_dir, soa_dir))
+    return namespace_list
+
+
+def get_all_namespaces_for_service(
+    service: str, soa_dir: str = DEFAULT_SOA_DIR, full_name: bool = True
+) -> Sequence[Tuple[str, ServiceNamespaceConfig]]:
+    """Get all the smartstack namespaces listed for a given service name.
+
+    :param service: The service name
+    :param soa_dir: The SOA config directory to read from
+    :param full_name: A boolean indicating if the service name should be prepended to the namespace in the
+                      returned tuples as described below (Default: True)
+    :returns: A list of tuples of the form (service<SPACER>namespace, namespace_config) if full_name is true,
+              otherwise of the form (namespace, namespace_config)
+    """
+    service_config = service_configuration_lib.read_service_configuration(
+        service, soa_dir
     )
+    smartstack = service_config.get("smartstack", {})
+    namespace_list = []
+    for namespace in smartstack:
+        if full_name:
+            name = compose_job_id(service, namespace)
+        else:
+            name = namespace
+        namespace_list.append((name, smartstack[namespace]))
+    return namespace_list
 
-    preproccessed_config = {}
-    # Use Tronfig on generated config from PaaSTA to validate the rest
-    preproccessed_config["jobs"] = {
-        job_config.get_name(): format_tron_job_dict(job_config)
-        for job_config in job_configs
-    }
 
-    complete_config = yaml.dump(preproccessed_config, Dumper=Dumper)
+def get_expected_instance_count_for_namespace(
+    service: str,
+    namespace: str,
+    instance_type_class: Type[LongRunningServiceConfig],
+    cluster: str = None,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> int:
+    """Get the number of expected instances for a namespace, based on the number
+    of instances set to run on that namespace as specified in service configuration files.
+
+    :param service: The service's name
+    :param namespace: The namespace for that service to check
+    instance_type_class: The type of the instance, options are e.g. KubernetesDeploymentConfig,
+    :param soa_dir: The SOA configuration directory to read from
+    :returns: An integer value of the # of expected instances for the namespace"""
+    total_expected = 0
+    if not cluster:
+        cluster = load_system_paasta_config().get_cluster()
 
-    proc = subprocess.run(
-        ["tronfig", "-", "-V", "-n", service, "-m", master_config_path],
-        input=complete_config,
-        stdout=subprocess.PIPE,
-        stderr=subprocess.PIPE,
-        encoding="utf-8",
+    pscl = PaastaServiceConfigLoader(
+        service=service, soa_dir=soa_dir, load_deployments=False
     )
-
-    if proc.returncode != 0:
-        process_errors = proc.stderr.strip()
-        if process_errors:  # Error running tronfig
-            paasta_print(proc.stderr)
-        return [proc.stdout.strip()]
-
-    return []
-
-
-def get_tron_namespaces(cluster, soa_dir):
-    tron_config_file = f"tron-{cluster}.yaml"
-    config_dirs = [
-        _dir[0]
-        for _dir in os.walk(os.path.abspath(soa_dir))
-        if tron_config_file in _dir[2]
-    ]
-    namespaces = [os.path.split(config_dir)[1] for config_dir in config_dirs]
-    return namespaces
-
-
-def list_tron_clusters(service: str, soa_dir: str = DEFAULT_SOA_DIR) -> List[str]:
-    """Returns the Tron clusters a service is configured to deploy to."""
-    search_re = r"/tron-([0-9a-z-_]*)\.yaml$"
-    service_dir = os.path.join(soa_dir, service)
-    clusters = []
-    for filename in glob.glob(f"{service_dir}/*.yaml"):
-        cluster_re_match = re.search(search_re, filename)
-        if cluster_re_match is not None:
-            clusters.append(cluster_re_match.group(1))
-    return clusters
-
-
-def get_tron_dashboard_for_cluster(cluster: str):
-    dashboards = load_system_paasta_config().get_dashboard_links()[cluster]
-    if "Tron" not in dashboards:
-        raise Exception(f"tron api endpoint is not defined for cluster {cluster}")
-    return dashboards["Tron"]
+    for job_config in pscl.instance_configs(
+        cluster=cluster, instance_type_class=instance_type_class
+    ):
+        if f"{service}.{namespace}" in job_config.get_registrations():
+            total_expected += job_config.get_instances()
+    return total_expected
```

### Comparing `paasta-tools-0.92.1/paasta_tools/mac_address.py` & `paasta-tools-1.0.0/paasta_tools/mac_address.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 
 class MacAddressException(Exception):
     pass
 
 
 def reserve_unique_mac_address(lock_directory):
-    """ Pick and reserve a unique mac address for a container
+    """Pick and reserve a unique mac address for a container
     returns (mac_address, lockfile)
     where the mac address is a string in the form of 00:00:00:00:00:00
     and lockfile is a file object that holds an exclusive lock
     """
     for x in range(100):
         random_hex = "{:08x}".format(random.getrandbits(32))
         mac_address = ":".join(
@@ -28,16 +28,15 @@
         if lock_file is not None:
             return (mac_address, lock_file)
 
     raise MacAddressException("Unable to pick unique MAC address")
 
 
 def obtain_lock(lock_filepath):
-    """ Open and obtain a flock on the parameter. Returns a file if successful, None if not
-    """
+    """Open and obtain a flock on the parameter. Returns a file if successful, None if not"""
     lock_file = open(lock_filepath, "w")
     try:
         fcntl.flock(lock_file, fcntl.LOCK_EX | fcntl.LOCK_NB)
         return lock_file
     except IOError as err:
         if err.errno != errno.EAGAIN:
             raise
```

### Comparing `paasta-tools-0.92.1/paasta_tools/generate_services_file.py` & `paasta-tools-1.0.0/paasta_tools/generate_services_file.py`

 * *Files 6% similar despite different names*

```diff
@@ -21,16 +21,16 @@
 import os
 import socket
 from datetime import datetime
 
 import service_configuration_lib
 import yaml
 
-from paasta_tools.marathon_tools import get_all_namespaces
-from paasta_tools.marathon_tools import get_all_namespaces_for_service
+from paasta_tools.long_running_service_tools import get_all_namespaces
+from paasta_tools.long_running_service_tools import get_all_namespaces_for_service
 from paasta_tools.utils import atomic_file_write
 from paasta_tools.utils import compose_job_id
 from paasta_tools.utils import DEFAULT_SOA_DIR
 
 
 YOCALHOST = "169.254.255.254"
```

### Comparing `paasta-tools-0.92.1/paasta_tools/synapse_srv_namespaces_fact.py` & `paasta-tools-1.0.0/paasta_tools/synapse_srv_namespaces_fact.py`

 * *Files 11% similar despite different names*

```diff
@@ -21,23 +21,22 @@
 
 Example output: mumble.canary:5019,mumble.main:111,zookeeper.hab:4921
 
 This is nice to use as a facter fact for Synapse stuff!
 """
 import sys
 
-from paasta_tools import marathon_tools
-from paasta_tools.utils import paasta_print
+from paasta_tools import long_running_service_tools
 
 
 def main():
     strings = []
-    for full_name, config in marathon_tools.get_all_namespaces():
+    for full_name, config in long_running_service_tools.get_all_namespaces():
         if "proxy_port" in config:
             strings.append("{}:{}".format(full_name, config["proxy_port"]))
     strings = sorted(strings)
-    paasta_print("synapse_srv_namespaces=" + ",".join(strings))
+    print("synapse_srv_namespaces=" + ",".join(strings))
     sys.exit(0)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/dump_locally_running_services.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/get_docker_image.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,63 +1,76 @@
 #!/usr/bin/env python
-# Copyright 2015-2019 Yelp Inc.
+# Copyright 2015-2016 Yelp Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""
-Usage: ./paasta_dump_locally_running_services.py [options]
-
-Outputs a JSON-encoded list of services that are running on this host along
-with the host port that each service is listening on.
-
-Command line options:
-
-- -d <SOA_DIR>, --soa-dir <SOA_DIR>: Specify a SOA config dir to read from
-"""
-import argparse
-import json
 import sys
 
-from paasta_tools.marathon_tools import get_marathon_services_running_here_for_nerve
-from paasta_tools.marathon_tools import get_puppet_services_running_here_for_nerve
+from paasta_tools.cli.utils import lazy_choices_completer
+from paasta_tools.cli.utils import list_deploy_groups
+from paasta_tools.cli.utils import PaastaColors
+from paasta_tools.cli.utils import validate_service_name
+from paasta_tools.deployment_utils import load_v2_deployments_json
 from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import paasta_print
+from paasta_tools.utils import get_service_docker_registry
+from paasta_tools.utils import list_services
 
 
-def parse_args(argv):
-    parser = argparse.ArgumentParser(
-        description="Dumps information about locally running services."
+def add_subparser(subparsers):
+    list_parser = subparsers.add_parser(
+        "get-docker-image",
+        help="Gets the docker image URL for the deployment of a service",
     )
-    parser.add_argument(
+    list_parser.add_argument(
+        "-s",
+        "--service",
+        help="Name of the service which you want to get the docker image for.",
+        required=True,
+    ).completer = lazy_choices_completer(list_services)
+    list_parser.add_argument(
+        "-i",
+        "-l",
+        "--deploy-group",
+        help='Name of the deploy group, like "prod".',
+        required=True,
+    ).completer = lazy_choices_completer(list_deploy_groups)
+    list_parser.add_argument(
         "-d",
         "--soa-dir",
-        dest="soa_dir",
-        metavar="SOA_DIR",
+        help="A directory from which soa-configs should be read from",
         default=DEFAULT_SOA_DIR,
-        help="define a different soa config directory",
     )
-    return parser.parse_args(argv)
 
+    list_parser.set_defaults(command=paasta_get_docker_image)
 
-def main(argv=None):
-    args = parse_args(argv)
-    soa_dir = args.soa_dir
-
-    service_dump = get_marathon_services_running_here_for_nerve(
-        cluster=None, soa_dir=soa_dir
-    ) + get_puppet_services_running_here_for_nerve(soa_dir=soa_dir)
 
-    paasta_print(json.dumps(service_dump))
-    sys.exit(0)
+def paasta_get_docker_image(args):
+    service = args.service
+    deploy_group = args.deploy_group
+    soa_dir = args.soa_dir
+    validate_service_name(service, soa_dir)
 
+    deployments = load_v2_deployments_json(service=service, soa_dir=soa_dir)
+    docker_image = deployments.get_docker_image_for_deploy_group(deploy_group)
 
-if __name__ == "__main__":
-    main()
+    if not docker_image:
+        print(
+            PaastaColors.red(
+                f"There is no {service} docker_image for {deploy_group}. Has it been deployed yet?"
+            ),
+            file=sys.stderr,
+        )
+        return 1
+    else:
+        registry_uri = get_service_docker_registry(service=service, soa_dir=soa_dir)
+        docker_url = f"{registry_uri}/{docker_image}"
+        print(docker_url)
+        return 0
```

### Comparing `paasta-tools-0.92.1/paasta_tools/generate_services_yaml.py` & `paasta-tools-1.0.0/paasta_tools/generate_services_yaml.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/mesos_maintenance.py` & `paasta-tools-1.0.0/paasta_tools/mesos_maintenance.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,27 +15,32 @@
 import argparse
 import datetime
 import json
 import logging
 from socket import gaierror
 from socket import getfqdn
 from socket import gethostbyname
+from typing import List
 from typing import NamedTuple
+from typing import Optional
 
 import a_sync
 from dateutil import parser
 from pytimeparse import timeparse
 from requests import Request
 from requests import Session
 from requests.exceptions import HTTPError
 
 from paasta_tools.mesos_tools import get_count_running_tasks_on_slave
+from paasta_tools.mesos_tools import get_mesos_config_path
 from paasta_tools.mesos_tools import get_mesos_leader
 from paasta_tools.mesos_tools import get_mesos_master
 from paasta_tools.mesos_tools import MESOS_MASTER_PORT
+from paasta_tools.utils import SystemPaastaConfig
+from paasta_tools.utils import time_cache
 from paasta_tools.utils import to_bytes
 
 
 log = logging.getLogger(__name__)
 
 
 class Hostname(NamedTuple):
@@ -53,20 +58,20 @@
     name: str
     amount: int
 
 
 MAINTENANCE_ROLE = "maintenance"
 
 
-def base_api():
+def base_api(mesos_config_path: Optional[str] = None):
     """Helper function for making all API requests
 
     :returns: a function that can be called to make a request
     """
-    leader = get_mesos_leader()
+    leader = get_mesos_leader(mesos_config_path)
 
     def execute_request(method, endpoint, timeout=(3, 2), **kwargs):
         url = "http://%s:%d%s" % (leader, MESOS_MASTER_PORT, endpoint)
         s = Session()
         s.auth = (get_principal(), get_secret())
         req = Request(method, url, **kwargs)
         prepared = s.prepare_request(req)
@@ -76,30 +81,30 @@
             return resp
         except HTTPError:
             raise HTTPError("Error executing API request calling %s." % url)
 
     return execute_request
 
 
-def master_api():
+def master_api(mesos_config_path: Optional[str] = None):
     """Helper function for making API requests to the /master API endpoints
 
     :returns: a function that can be called to make a request to /master
     """
 
     def execute_master_api_request(method, endpoint, **kwargs):
-        base_api_client = base_api()
+        base_api_client = base_api(mesos_config_path=mesos_config_path)
         return base_api_client(method, "/master%s" % endpoint, **kwargs)
 
     return execute_master_api_request
 
 
-def operator_api():
+def operator_api(mesos_config_path: Optional[str] = None):
     def execute_operator_api_request(**kwargs):
-        base_api_client = base_api()
+        base_api_client = base_api(mesos_config_path=mesos_config_path)
         if "headers" in kwargs:
             kwargs["headers"]["Content-Type"] = "application/json"
         else:
             kwargs["headers"] = {"Content-Type": "application/json"}
         data = kwargs.pop("data")
         return base_api_client("POST", "/api/v1", data=json.dumps(data), **kwargs)
 
@@ -165,60 +170,67 @@
 
     :returns: a GET_MAINTENANCE_SCHEDULE response
     """
     client_fn = operator_api()
     return client_fn(data={"type": "GET_MAINTENANCE_SCHEDULE"})
 
 
-def get_maintenance_status():
+@time_cache(ttl=10)
+def get_maintenance_status(mesos_config_path: Optional[str] = None):
     """Makes a GET_MAINTENANCE_STATUS request to the operator api
 
     :returns: a GET_MAINTENANCE_STATUS response
     """
-    client_fn = operator_api()
+    client_fn = operator_api(mesos_config_path=mesos_config_path)
     return client_fn(data={"type": "GET_MAINTENANCE_STATUS"})
 
 
 def schedule():
     """Get the Mesos maintenance schedule. This contains hostname/ip mappings and their maintenance window.
     :returns: GET_MAINTENANCE_SCHEDULE response text
     """
     try:
         schedule = get_maintenance_schedule()
     except HTTPError:
         raise HTTPError("Error getting maintenance schedule.")
     return schedule.text
 
 
-def get_hosts_with_state(state):
+def get_hosts_with_state(
+    state, system_paasta_config: Optional[SystemPaastaConfig] = None
+) -> List[str]:
     """Helper function to check the maintenance status and return all hosts
     listed as being in a current state
 
     :param state: State we are interested in ('down_machines' or 'draining_machines')
     :returns: A list of hostnames in the specified state or an empty list if no machines
     """
+
+    mesos_config_path = get_mesos_config_path(system_paasta_config)
     try:
-        status = get_maintenance_status().json()
+        status = get_maintenance_status(mesos_config_path).json()
         status = status["get_maintenance_status"]["status"]
     except HTTPError:
         raise HTTPError("Error getting maintenance status.")
     if not status or state not in status:
         return []
     if "id" in status[state][0]:
         return [machine["id"]["hostname"] for machine in status[state]]
     else:
         return [machine["hostname"] for machine in status[state]]
 
 
-def get_draining_hosts():
+def get_draining_hosts(system_paasta_config: Optional[SystemPaastaConfig] = None):
     """Returns a list of hostnames that are marked as draining
 
     :returns: a list of strings representing hostnames
     """
-    return get_hosts_with_state(state="draining_machines")
+    return get_hosts_with_state(
+        state="draining_machines", system_paasta_config=system_paasta_config
+    )
 
 
 def get_down_hosts():
     """Returns a list of hostnames that are marked as down
 
     :returns: a list of strings representing hostnames
     """
@@ -539,15 +551,15 @@
         "type": request_type.upper(),
         request_type.lower(): {"agent_id": {"value": slave_id}},
         "resources": payload,
     }
 
 
 def reserve(slave_id, resources):
-    """Dynamically reserve resources in marathon to prevent tasks from using them.
+    """Dynamically reserve resources in mesos to prevent tasks from using them.
     :param slave_id: the id of the mesos slave
     :param resources: list of Resource named tuples specifying the name and amount of the resource to (un)reserve
     :returns: boolean where 0 represents success and 1 is a failure
     """
     log.info(f"Dynamically reserving resources on {slave_id}: {resources}")
     payload = _make_operator_reservation_request_payload(
         slave_id=slave_id,
@@ -560,15 +572,15 @@
         reserve_output = client_fn(data=payload).text
     except HTTPError:
         raise HTTPError("Error adding dynamic reservation.")
     return reserve_output
 
 
 def unreserve(slave_id, resources):
-    """Dynamically unreserve resources in marathon to allow tasks to using them.
+    """Dynamically unreserve resources in mesos to allow tasks to using them.
     :param slave_id: the id of the mesos slave
     :param resources: list of Resource named tuples specifying the name and amount of the resource to (un)reserve
     :returns: boolean where 0 represents success and 1 is a failure
     """
     log.info(f"Dynamically unreserving resources on {slave_id}: {resources}")
     payload = _make_operator_reservation_request_payload(
         slave_id=slave_id,
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cleanup_maintenance.py` & `paasta-tools-1.0.0/paasta_tools/cleanup_maintenance.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/deployd/watchers.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/start_stop_restart.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,436 +1,401 @@
-import logging
-import os
-import time
-from functools import reduce
-from typing import Any
+#!/usr/bin/env python
+# Copyright 2015-2016 Yelp Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import datetime
+import socket
+import sys
 from typing import Dict
-from typing import Iterable
 from typing import List
-from typing import Optional
-from typing import Set
-from typing import Tuple
-
-import pyinotify
-from kazoo.protocol.states import EventType
-from kazoo.protocol.states import WatchedEvent
-from kazoo.protocol.states import ZnodeStat
-from kazoo.recipe.watchers import ChildrenWatch
-from kazoo.recipe.watchers import DataWatch
-from requests.exceptions import RequestException
-
-from paasta_tools.deployd.common import DelayDeadlineQueueProtocol
-from paasta_tools.deployd.common import get_marathon_clients_from_config
-from paasta_tools.deployd.common import get_service_instances_needing_update
-from paasta_tools.deployd.common import PaastaThread
-from paasta_tools.deployd.common import ServiceInstance
-from paasta_tools.long_running_service_tools import AUTOSCALING_ZK_ROOT
-from paasta_tools.marathon_tools import DEFAULT_SOA_DIR
-from paasta_tools.marathon_tools import deformat_job_id
-from paasta_tools.marathon_tools import get_marathon_apps_with_clients
-from paasta_tools.marathon_tools import MarathonServiceConfig
-from paasta_tools.mesos_maintenance import get_draining_hosts
-from paasta_tools.utils import get_services_for_cluster
-from paasta_tools.utils import list_all_instances_for_service
-from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import PATH_TO_SYSTEM_PAASTA_CONFIG_DIR
-from paasta_tools.utils import SystemPaastaConfig
-
-
-class PaastaWatcher(PaastaThread):
-    def __init__(
-        self,
-        instances_to_bounce: DelayDeadlineQueueProtocol,
-        cluster: str,
-        config: SystemPaastaConfig,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__()
-        self.daemon = True
-        self.instances_to_bounce = instances_to_bounce
-        self.cluster = cluster
-        self.config = config
-        self.is_ready = False
-
-
-class AutoscalerWatcher(PaastaWatcher):
-    def __init__(
-        self,
-        instances_to_bounce: DelayDeadlineQueueProtocol,
-        cluster: str,
-        config: SystemPaastaConfig,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(instances_to_bounce, cluster, config)
-        self.zk = kwargs.pop("zookeeper_client")
-        self.watchers: Dict[str, PaastaWatcher] = {}
-
-    def watch_folder(self, path: str, enqueue_children: bool = False) -> None:
-        """recursive nonsense"""
-        if "autoscaling.lock" in path:
-            return
-        if path.split("/")[-1] == "instances":
-            self.watch_node(path, enqueue=enqueue_children)
-            return
-        self.log.info(f"Adding folder watch on {path}")
-        watcher = ChildrenWatch(
-            self.zk, path, func=self.process_folder_event, send_event=True
-        )
-        self.watchers[path] = watcher
-        children = watcher._client.get_children(watcher._path)
-        if children:
-            for child in children:
-                self.watch_folder(f"{path}/{child}", enqueue_children=enqueue_children)
-
-    def _enqueue_service_instance(self, path: str) -> None:
-        service, instance = path.split("/")[-3:-1]
-        self.log.info(
-            f"Number of instances changed for {service}.{instance} by the autoscaler."
-        )
-        service_instance = ServiceInstance(
-            service=service,
-            instance=instance,
-            bounce_by=time.time(),
-            wait_until=time.time(),
-            watcher=type(self).__name__,
-            failures=0,
-            enqueue_time=time.time(),
-            bounce_start_time=time.time(),
-        )
-        self.instances_to_bounce.put(service_instance)
-
-    def watch_node(self, path: str, enqueue: bool = False) -> None:
-        self.log.info(f"Adding zk node watch on {path}")
-        DataWatch(self.zk, path, func=self.process_node_event, send_event=True)
-        if enqueue:
-            self._enqueue_service_instance(path)
-
-    def process_node_event(
-        self, data: Optional[bytes], stat: ZnodeStat, event: WatchedEvent
-    ) -> None:
-        self.log.debug(f"zk node change: {event}")
-        if event and (
-            event.type == EventType.CREATED or event.type == EventType.CHANGED
-        ):
-            self._enqueue_service_instance(event.path)
-
-    def process_folder_event(
-        self, children: Iterable[str], event: WatchedEvent
-    ) -> None:
-        self.log.debug(f"Folder change: {event}")
-        if event and (event.type == EventType.CHILD):
-            fq_children = [f"{event.path}/{child}" for child in children]
-            for child in fq_children:
-                if child not in self.watchers:
-                    self.watch_folder(child, enqueue_children=True)
-
-    def run(self) -> None:
-        if not self.zk.exists(AUTOSCALING_ZK_ROOT):
-            self.zk.ensure_path(AUTOSCALING_ZK_ROOT)
-        self.watch_folder(AUTOSCALING_ZK_ROOT)
-        self.is_ready = True
-        while True:
-            time.sleep(0.1)
-
-
-class SoaFileWatcher(PaastaWatcher):
-    def __init__(
-        self,
-        instances_to_bounce: DelayDeadlineQueueProtocol,
-        cluster: str,
-        config: SystemPaastaConfig,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(instances_to_bounce, cluster, config)
-        self.wm = pyinotify.WatchManager()
-        self.wm.add_watch(DEFAULT_SOA_DIR, self.mask, rec=True)
-        self.notifier = pyinotify.Notifier(
-            watch_manager=self.wm,
-            default_proc_fun=YelpSoaEventHandler(filewatcher=self),
-        )
-
-    @property
-    def mask(self) -> int:
-        boring_flags = ["IN_CLOSE_NOWRITE", "IN_OPEN", "IN_ACCESS", "IN_ATTRIB"]
-        return reduce(
-            lambda x, y: x | y,
-            [
-                v
-                for k, v in pyinotify.EventsCodes.OP_FLAGS.items()
-                if k not in boring_flags
-            ],
-        )
-
-    def run(self) -> None:
-        self.notifier.loop(callback=self.startup_checker)
 
-    def startup_checker(self, obj: Any) -> None:
-        self.is_ready = True
-
-
-class PublicConfigFileWatcher(PaastaWatcher):
-    def __init__(
-        self,
-        instances_to_bounce: DelayDeadlineQueueProtocol,
-        cluster: str,
-        config: SystemPaastaConfig,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(instances_to_bounce, cluster, config)
-        self.wm = pyinotify.WatchManager()
-        self.wm.add_watch(PATH_TO_SYSTEM_PAASTA_CONFIG_DIR, self.mask, rec=True)
-        self.notifier = pyinotify.Notifier(
-            watch_manager=self.wm,
-            default_proc_fun=PublicConfigEventHandler(filewatcher=self),
-        )
+import choice
 
-    @property
-    def mask(self) -> int:
-        boring_flags = ["IN_CLOSE_NOWRITE", "IN_OPEN", "IN_ACCESS", "IN_ATTRIB"]
-        return reduce(
-            lambda x, y: x | y,
-            [
-                v
-                for k, v in pyinotify.EventsCodes.OP_FLAGS.items()
-                if k not in boring_flags
-            ],
-        )
-
-    def run(self) -> None:
-        self.notifier.loop(callback=self.startup_checker)
+from paasta_tools import remote_git
+from paasta_tools import utils
+from paasta_tools.api.client import get_paasta_oapi_client
+from paasta_tools.cli.cmds.mark_for_deployment import can_user_deploy_service
+from paasta_tools.cli.cmds.mark_for_deployment import get_deploy_info
+from paasta_tools.cli.cmds.status import add_instance_filter_arguments
+from paasta_tools.cli.cmds.status import apply_args_filters
+from paasta_tools.cli.utils import get_instance_config
+from paasta_tools.cli.utils import get_paasta_oapi_api_clustername
+from paasta_tools.cli.utils import trigger_deploys
+from paasta_tools.flink_tools import FlinkDeploymentConfig
+from paasta_tools.flinkeks_tools import FlinkEksDeploymentConfig
+from paasta_tools.generate_deployments_for_service import get_latest_deployment_tag
+from paasta_tools.kubernetes_tools import KubernetesDeploymentConfig
+from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import load_system_paasta_config
+from paasta_tools.utils import PaastaColors
 
-    def startup_checker(self, obj: Any) -> None:
-        self.is_ready = True
 
+def add_subparser(subparsers):
+    for command, lower, upper, cmd_func in [
+        ("start", "start or restart", "Start or restart", paasta_start),
+        ("restart", "start or restart", "Start or restart", paasta_restart),
+        ("stop", "stop", "Stop", paasta_stop),
+    ]:
+        status_parser = subparsers.add_parser(
+            command,
+            help="%ss a PaaSTA service in a graceful way." % upper,
+            description=(
+                "%ss a PaaSTA service in a graceful way. This uses the Git control plane."
+                % upper
+            ),
+            epilog=(
+                "This command uses Git, and assumes access and authorization to the Git repo "
+                "for the service is available."
+            ),
+        )
+        add_instance_filter_arguments(status_parser, verb=lower)
+        status_parser.add_argument(
+            "-d",
+            "--soa-dir",
+            dest="soa_dir",
+            metavar="SOA_DIR",
+            default=DEFAULT_SOA_DIR,
+            help="define a different soa config directory",
+        )
+        status_parser.set_defaults(command=cmd_func)
+
+
+def format_tag(branch, force_bounce, desired_state):
+    return f"refs/tags/paasta-{branch}-{force_bounce}-{desired_state}"
+
+
+def make_mutate_refs_func(service_config, force_bounce, desired_state):
+    """Create a function that will inform send_pack that we want to create tags
+    corresponding to the set of branches passed, with the given force_bounce
+    and desired_state parameters. These tags will point at the current tip of
+    the branch they associate with.
+
+    dulwich's send_pack wants a function that takes a dictionary of ref name
+    to sha and returns a modified version of that dictionary. send_pack will
+    then diff what is returned versus what was passed in, and inform the remote
+    git repo of our desires."""
+
+    def mutate_refs(refs):
+        deploy_group = service_config.get_deploy_group()
+        (_, head_sha, _) = get_latest_deployment_tag(refs, deploy_group)
+        refs[
+            format_tag(service_config.get_branch(), force_bounce, desired_state)
+        ] = head_sha
+        return refs
+
+    return mutate_refs
+
+
+def log_event(service_config, desired_state):
+    user = utils.get_username()
+    host = socket.getfqdn()
+    line = "Issued request to change state of {} (an instance of {}) to '{}' by {}@{}".format(
+        service_config.get_instance(),
+        service_config.get_service(),
+        desired_state,
+        user,
+        host,
+    )
+    utils._log(
+        service=service_config.get_service(),
+        level="event",
+        cluster=service_config.get_cluster(),
+        instance=service_config.get_instance(),
+        component="deploy",
+        line=line,
+    )
+
+    utils._log_audit(
+        action=desired_state,
+        service=service_config.get_service(),
+        cluster=service_config.get_cluster(),
+        instance=service_config.get_instance(),
+    )
+
+
+def issue_state_change_for_service(service_config, force_bounce, desired_state):
+    ref_mutator = make_mutate_refs_func(
+        service_config=service_config,
+        force_bounce=force_bounce,
+        desired_state=desired_state,
+    )
+    git_url = utils.get_git_url(service_config.get_service())
+    remote_git.create_remote_refs(git_url, ref_mutator)
+    if "yelpcorp.com" in git_url:
+        trigger_deploys(service_config.get_service())
+    log_event(service_config=service_config, desired_state=desired_state)
+
+
+def print_kubernetes_message(desired_state):
+    if desired_state == "start":
+        print(
+            "This service will soon be gracefully started/restarted, replacing old instances according "
+            "to the bounce method chosen in soa-configs. "
+        )
+    elif desired_state == "stop":
+        print(
+            "This service will be gracefully stopped soon. It will be started back up again on the next deploy.\n"
+            "To stop this service permanently. Set this in the soa-configs definition:\n"
+            "\n"
+            "    instances: 0\n"
+        )
+
+
+def print_flink_message(desired_state):
+    if desired_state == "start":
+        print("'Start' will tell Flink operator to start the cluster.")
+    elif desired_state == "stop":
+        print(
+            "'Stop' will put Flink cluster in stopping mode, it may"
+            "take some time before shutdown is completed."
+        )
+
+
+def confirm_to_continue(cluster_service_instances, desired_state):
+    print(f"You are about to {desired_state} the following instances:")
+    print("Either --instances or --clusters not specified. Asking for confirmation.")
+    i_count = 0
+    for cluster, services_instances in cluster_service_instances:
+        for service, instances in services_instances.items():
+            for instance in instances.keys():
+                print(f"cluster = {cluster}, instance = {instance}")
+                i_count += 1
+    if sys.stdin.isatty():
+        return choice.Binary(
+            f"Are you sure you want to {desired_state} these {i_count} instances?",
+            False,
+        ).ask()
+    return True
+
+
+REMOTE_REFS: Dict[str, List[str]] = {}
+
+
+def get_remote_refs(service, soa_dir):
+    if service not in REMOTE_REFS:
+        REMOTE_REFS[service] = remote_git.list_remote_refs(
+            utils.get_git_url(service, soa_dir)
+        )
+    return REMOTE_REFS[service]
+
+
+def paasta_start_or_stop(args, desired_state):
+    """Requests a change of state to start or stop given branches of a service."""
+    soa_dir = args.soa_dir
+
+    pargs = apply_args_filters(args)
+    if len(pargs) == 0:
+        return 1
+
+    affected_services = {
+        s for service_list in pargs.values() for s in service_list.keys()
+    }
+    if len(affected_services) > 1:
+        print(
+            PaastaColors.red("Warning: trying to start/stop/restart multiple services:")
+        )
+
+        for cluster, services_instances in pargs.items():
+            print("Cluster %s:" % cluster)
+            for service, instances in services_instances.items():
+                print("    Service %s:" % service)
+                print("        Instances %s" % ",".join(instances.keys()))
 
-class MaintenanceWatcher(PaastaWatcher):
-    def __init__(
-        self,
-        instances_to_bounce: DelayDeadlineQueueProtocol,
-        cluster: str,
-        config: SystemPaastaConfig,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(instances_to_bounce, cluster, config)
-        self.draining: Set[str] = set()
-        self.marathon_clients = get_marathon_clients_from_config()
-
-    def get_new_draining_hosts(self) -> List[str]:
-        try:
-            draining_hosts = get_draining_hosts()
-        except RequestException as e:
-            self.log.error(f"Unable to get list of draining hosts from mesos: {e}")
-            draining_hosts = list(self.draining)
-        new_draining_hosts = [
-            host for host in draining_hosts if host not in self.draining
-        ]
-        for host in new_draining_hosts:
-            self.draining.add(host)
-        hosts_finished_draining = [
-            host for host in self.draining if host not in draining_hosts
+        if sys.stdin.isatty():
+            confirm = choice.Binary("Are you sure you want to continue?", False).ask()
+        else:
+            confirm = False
+        if not confirm:
+            print()
+            print("exiting")
+            return 1
+
+    if not all(
+        [
+            can_user_deploy_service(get_deploy_info(service, soa_dir), service)
+            for service in affected_services
         ]
-        for host in hosts_finished_draining:
-            self.draining.remove(host)
-        return new_draining_hosts
-
-    def run(self) -> None:
-        self.is_ready = True
-        while True:
-            new_draining_hosts = self.get_new_draining_hosts()
-            service_instances: List[ServiceInstance] = []
-            if new_draining_hosts:
-                self.log.info(f"Found new draining hosts: {new_draining_hosts}")
-                service_instances = self.get_at_risk_service_instances(
-                    new_draining_hosts
+    ):
+        print(PaastaColors.red("Exiting due to missing deploy permissions"))
+        return 1
+
+    invalid_deploy_groups = []
+    kubernetes_message_printed = False
+    affected_flinks = []
+
+    if args.clusters is None or args.instances is None:
+        if confirm_to_continue(pargs.items(), desired_state) is False:
+            print()
+            print("exiting")
+            return 1
+
+    for cluster, services_instances in pargs.items():
+        for service, instances in services_instances.items():
+            for instance in instances.keys():
+                service_config = get_instance_config(
+                    service=service,
+                    cluster=cluster,
+                    instance=instance,
+                    soa_dir=soa_dir,
+                    load_deployments=False,
                 )
-            for service_instance in service_instances:
-                self.instances_to_bounce.put(service_instance)
-            time.sleep(self.config.get_deployd_maintenance_polling_frequency())
-
-    def get_at_risk_service_instances(
-        self, draining_hosts: List[str]
-    ) -> List[ServiceInstance]:
-        marathon_apps_with_clients = get_marathon_apps_with_clients(
-            clients=self.marathon_clients.get_all_clients(), embed_tasks=True
-        )
-        at_risk_tasks = []
-        for app, client in marathon_apps_with_clients:
-            for task in app.tasks:
-                if task.host in draining_hosts:
-                    at_risk_tasks.append(task)
-        self.log.info(f"At risk tasks: {at_risk_tasks}")
-        service_instances: List[ServiceInstance] = []
-        for task in at_risk_tasks:
-            app_id = task.app_id.strip("/")
-            service, instance, _, __ = deformat_job_id(app_id)
-            # check we haven't already added this instance,
-            # no need to add the same instance to the bounce queue
-            # more than once
-            if not any(
-                [
-                    (service, instance) == (si.service, si.instance)
-                    for si in service_instances
-                ]
-            ):
-                service_instances.append(
-                    ServiceInstance(
-                        service=service,
-                        instance=instance,
-                        bounce_by=time.time(),
-                        wait_until=time.time(),
-                        watcher=type(self).__name__,
-                        failures=0,
-                        enqueue_time=time.time(),
-                        bounce_start_time=time.time(),
-                    )
+                if isinstance(service_config, FlinkDeploymentConfig):
+                    affected_flinks.append(service_config)
+                    continue
+
+                try:
+                    remote_refs = get_remote_refs(service, soa_dir)
+                except remote_git.LSRemoteException as e:
+                    msg = (
+                        "Error talking to the git server: %s\n"
+                        "This PaaSTA command requires access to the git server to operate.\n"
+                        "The git server may be down or not reachable from here.\n"
+                        "Try again from somewhere where the git server can be reached, "
+                        "like your developer environment."
+                    ) % str(e)
+                    print(msg)
+                    return 1
+
+                deploy_group = service_config.get_deploy_group()
+                (deploy_tag, _, _) = get_latest_deployment_tag(
+                    remote_refs, deploy_group
                 )
-        return service_instances
 
+                if deploy_tag not in remote_refs:
+                    invalid_deploy_groups.append(deploy_group)
+                else:
+                    force_bounce = utils.format_timestamp(datetime.datetime.utcnow())
+                    if (
+                        isinstance(service_config, KubernetesDeploymentConfig)
+                        and not kubernetes_message_printed
+                    ):
+                        print_kubernetes_message(desired_state)
+                        kubernetes_message_printed = True
+
+                    issue_state_change_for_service(
+                        service_config=service_config,
+                        force_bounce=force_bounce,
+                        desired_state=desired_state,
+                    )
+
+    return_val = 0
 
-class PublicConfigEventHandler(pyinotify.ProcessEvent):
-    def my_init(self, filewatcher: PublicConfigFileWatcher) -> None:
-        self.filewatcher = filewatcher
-        self.public_config = load_system_paasta_config()
-        self.marathon_clients = get_marathon_clients_from_config()
-
-    @property
-    def log(self) -> logging.Logger:
-        name = ".".join([__name__, type(self).__name__])
-        return logging.getLogger(name)
-
-    def filter_event(self, event: pyinotify.Event) -> Optional[pyinotify.Event]:
-        if event.name.endswith(".json") or event.maskname == "IN_CREATE|IN_ISDIR":
-            return event
-        return None
-
-    def watch_new_folder(self, event: pyinotify.Event) -> None:
-        if event.maskname == "IN_CREATE|IN_ISDIR" and ".~tmp~" not in event.pathname:
-            self.filewatcher.wm.add_watch(
-                event.pathname, self.filewatcher.mask, rec=True
+    # TODO: Refactor to discover if set_state is available for given
+    #       instance_type in API
+    if affected_flinks:
+        print_flink_message(desired_state)
+
+        system_paasta_config = load_system_paasta_config()
+        for service_config in affected_flinks:
+            cluster = service_config.cluster
+            service = service_config.service
+            instance = service_config.instance
+            is_eks = isinstance(service_config, FlinkEksDeploymentConfig)
+
+            client = get_paasta_oapi_client(
+                cluster=get_paasta_oapi_api_clustername(cluster=cluster, is_eks=is_eks),
+                system_paasta_config=system_paasta_config,
             )
+            if not client:
+                print("Cannot get a paasta-api client")
+                exit(1)
 
-    def process_default(self, event: pyinotify.Event) -> None:
-        self.log.debug(event)
-        self.watch_new_folder(event)
-        event = self.filter_event(event)
-        if event:
-            self.log.debug("Public config changed on disk, loading new config.")
             try:
-                new_config = load_system_paasta_config()
-            except ValueError:
-                self.log.error("Couldn't load public config, the JSON is invalid!")
-                return
-            service_instance_configs: List[Tuple[str, str, MarathonServiceConfig]] = []
-            if new_config != self.public_config:
-                self.log.info(
-                    "Public config has changed, now checking if it affects any services config shas."
-                )
-                self.public_config = new_config
-                all_service_instances = get_services_for_cluster(
-                    cluster=self.public_config.get_cluster(),
-                    instance_type="marathon",
-                    soa_dir=DEFAULT_SOA_DIR,
-                )
-                service_instance_configs = get_service_instances_needing_update(
-                    self.marathon_clients,
-                    all_service_instances,
-                    self.public_config.get_cluster(),
-                )
-            if service_instance_configs:
-                self.log.info(
-                    f"{len(service_instance_configs)} service instances affected. Doing a staggered bounce."
+                client.service.instance_set_state(
+                    service=service,
+                    instance=instance,
+                    desired_state=desired_state,
                 )
-                for service, instance, config in service_instance_configs:
-                    self.filewatcher.instances_to_bounce.put(
-                        ServiceInstance(
-                            service=service,
-                            instance=instance,
-                            watcher=type(self).__name__,
-                            bounce_by=time.time()
-                            + self.public_config.get_deployd_big_bounce_deadline(),
-                            wait_until=time.time(),
-                            enqueue_time=time.time(),
-                            bounce_start_time=time.time(),
-                        )
-                    )
+            except client.api_error as exc:
+                print(exc.reason)
+                return exc.status
 
+            return_val = 0
 
-class YelpSoaEventHandler(pyinotify.ProcessEvent):
-    def my_init(self, filewatcher: SoaFileWatcher) -> None:
-        self.filewatcher = filewatcher
-        self.marathon_clients = get_marathon_clients_from_config()
-
-    @property
-    def log(self) -> logging.Logger:
-        name = ".".join([__name__, type(self).__name__])
-        return logging.getLogger(name)
-
-    def get_service_name_from_event(self, event: pyinotify.Event) -> str:
-        """Get service_name from the file inotify event,
-        returns None if it is not an event we're interested in"""
-        starts_with = ["marathon-", "deployments.json"]
-        if any([event.name.startswith(x) for x in starts_with]):
-            service_name = event.path.split("/")[-1]
-        elif event.name.endswith(".json") and event.path.split("/")[-1] == "secrets":
-            # this is needed because we put the secrets json files in a
-            # subdirectory so the service name would be "secrets" otherwise
-            service_name = event.path.split("/")[-2]
-        else:
-            service_name = None
-        return service_name
+    if invalid_deploy_groups:
+        print(f"No deploy tags found for {', '.join(invalid_deploy_groups)}.")
+        print(f"Has {service} been deployed there yet?")
+        return_val = 1
 
-    def watch_new_folder(self, event: pyinotify.Event) -> None:
-        if event.maskname == "IN_CREATE|IN_ISDIR" and ".~tmp~" not in event.pathname:
-            self.filewatcher.wm.add_watch(
-                event.pathname, self.filewatcher.mask, rec=True
-            )
-            try:
-                file_names = os.listdir(event.pathname)
-            except OSError:
-                return
-            if any(["marathon-" in file_name for file_name in file_names]):
-                self.log.info(f"New folder with marathon files: {event.name}.")
-                self.bounce_service(event.name)
-
-    def process_default(self, event: pyinotify.Event) -> None:
-        self.log.debug(event)
-        self.watch_new_folder(event)
-        service_name = self.get_service_name_from_event(event)
-        if service_name:
-            self.log.info(
-                f"Looking for things to bounce for {service_name} because {event.path}/{event.name} changed."
-            )
-            self.bounce_service(service_name)
+    return return_val
 
-    def bounce_service(self, service_name: str) -> None:
-        self.log.info(
-            f"Checking if any marathon instances of {service_name} need bouncing."
-        )
-        instances = list_all_instances_for_service(
-            service=service_name,
-            clusters=[self.filewatcher.cluster],
-            instance_type="marathon",
-            cache=False,
-        )
-        self.log.debug(instances)
-        service_instance_configs = get_service_instances_needing_update(
-            self.marathon_clients,
-            [(service_name, instance) for instance in instances],
-            self.filewatcher.cluster,
-        )
-        for service, instance, config in service_instance_configs:
-            self.log.info(
-                f"{service}.{instance} has a new marathon app ID. Enqueuing it to be bounced."
-            )
-            now = time.time()
-            self.filewatcher.instances_to_bounce.put(
-                ServiceInstance(
+
+def paasta_start(args):
+    return paasta_start_or_stop(args, "start")
+
+
+def paasta_restart(args):
+    pargs = apply_args_filters(args)
+    soa_dir = args.soa_dir
+
+    affected_flinks = []
+    affected_non_flinks = []
+    for cluster, service_instances in pargs.items():
+        for service, instances in service_instances.items():
+            for instance in instances.keys():
+                service_config = get_instance_config(
                     service=service,
+                    cluster=cluster,
                     instance=instance,
-                    bounce_by=now + config.get_bounce_start_deadline(),
-                    wait_until=now,
-                    watcher=type(self).__name__,
-                    failures=0,
-                    enqueue_time=time.time(),
-                    bounce_start_time=time.time(),
+                    soa_dir=soa_dir,
+                    load_deployments=False,
                 )
+                if isinstance(service_config, FlinkDeploymentConfig):
+                    affected_flinks.append(service_config)
+                else:
+                    affected_non_flinks.append(service_config)
+
+    if affected_flinks:
+        flinks_info = ", ".join([f"{f.service}.{f.instance}" for f in affected_flinks])
+        print(
+            f"WARN: paasta restart is currently unsupported for Flink instances ({flinks_info})."
+        )
+        print("To restart, please run:", end="\n\n")
+        for flink in affected_flinks:
+            print(
+                f"paasta stop -s {flink.service} -i {flink.instance} -c {flink.cluster}"
+            )
+            print(
+                f"paasta start -s {flink.service} -i {flink.instance} -c {flink.cluster}",
+                end="\n\n",
             )
+
+        if not affected_non_flinks:
+            return 1
+
+        non_flinks_info = ", ".join(
+            [f"{f.service}.{f.instance}" for f in affected_non_flinks]
+        )
+        proceed = choice.Binary(
+            f"Would you like to restart the other instances ({non_flinks_info}) anyway?",
+            False,
+        ).ask()
+
+        if not proceed:
+            return 1
+
+    return paasta_start(args)
+
+
+PAASTA_STOP_UNDERSPECIFIED_ARGS_MESSAGE = PaastaColors.red(
+    "paasta stop requires explicit specification of cluster, service, and instance."
+)
+
+
+def paasta_stop(args):
+    if not args.clusters:
+        print(PAASTA_STOP_UNDERSPECIFIED_ARGS_MESSAGE)
+        return 1
+    elif not args.service_instance and not (args.service and args.instances):
+        print(PAASTA_STOP_UNDERSPECIFIED_ARGS_MESSAGE)
+        return 1
+    return paasta_start_or_stop(args, "stop")
```

### Comparing `paasta-tools-0.92.1/paasta_tools/check_services_replication_tools.py` & `paasta-tools-1.0.0/paasta_tools/frameworks/native_service_config.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,245 +1,301 @@
-# Copyright 2015-2019 Yelp Inc.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-import argparse
-import logging
-import sys
+#!/usr/bin/env python
+import copy
 from typing import Any
-from typing import Callable
+from typing import cast
 from typing import List
 from typing import Optional
-from typing import Sequence
-from typing import Tuple
-from typing import Type
-from typing import Union
-
-import a_sync
-from marathon import MarathonClient
-from marathon.models.task import MarathonTask
-from mypy_extensions import Arg
-
-from paasta_tools.kubernetes_tools import get_all_nodes
-from paasta_tools.kubernetes_tools import get_all_pods
-from paasta_tools.kubernetes_tools import KubeClient
-from paasta_tools.kubernetes_tools import V1Node
-from paasta_tools.kubernetes_tools import V1Pod
-from paasta_tools.marathon_tools import get_marathon_clients
-from paasta_tools.marathon_tools import get_marathon_servers
-from paasta_tools.mesos_tools import get_slaves
-from paasta_tools.paasta_service_config_loader import PaastaServiceConfigLoader
-from paasta_tools.smartstack_tools import KubeSmartstackReplicationChecker
-from paasta_tools.smartstack_tools import MesosSmartstackReplicationChecker
-from paasta_tools.smartstack_tools import SmartstackReplicationChecker
+
+from mypy_extensions import TypedDict
+
+from paasta_tools.long_running_service_tools import load_service_namespace_config
+from paasta_tools.long_running_service_tools import LongRunningServiceConfig
+from paasta_tools.long_running_service_tools import LongRunningServiceConfigDict
+from paasta_tools.long_running_service_tools import ServiceNamespaceConfig
+from paasta_tools.utils import BranchDictV2
+from paasta_tools.utils import compose_job_id
+from paasta_tools.utils import Constraint  # noqa, imported for typing.
 from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import InstanceConfig_T
-from paasta_tools.utils import list_services
-from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import SPACER
+from paasta_tools.utils import DockerParameter
+from paasta_tools.utils import get_code_sha_from_dockerurl
+from paasta_tools.utils import get_config_hash
+from paasta_tools.utils import load_service_instance_config
+from paasta_tools.utils import load_v2_deployments_json
 from paasta_tools.utils import SystemPaastaConfig
 
-try:
-    import yelp_meteorite
-except ImportError:
-    yelp_meteorite = None
-
-log = logging.getLogger(__name__)
-
-CheckServiceReplication = Callable[
-    [
-        Arg(InstanceConfig_T, "instance_config"),
-        Arg(Sequence[Union[MarathonTask, V1Pod]], "all_tasks_or_pods"),
-        Arg(Any, "smartstack_replication_checker"),
-    ],
-    Optional[bool],
-]
-
-
-def parse_args() -> argparse.Namespace:
-    parser = argparse.ArgumentParser()
-    parser.add_argument(
-        "-d",
-        "--soa-dir",
-        dest="soa_dir",
-        metavar="SOA_DIR",
-        default=DEFAULT_SOA_DIR,
-        help="define a different soa config directory",
-    )
-    parser.add_argument(
-        "--warn",
-        dest="under_replicated_warn_pct",
-        type=float,
-        default=5,
-        help="The percentage of under replicated service instances past which "
-        "the script will return a warning status",
-    )
-    parser.add_argument(
-        "--crit",
-        dest="under_replicated_crit_pct",
-        type=float,
-        default=10,
-        help="The percentage of under replicated service instances past which "
-        "the script will return a critical status",
-    )
-    parser.add_argument(
-        "service_instance_list",
-        nargs="*",
-        help="The list of service instances to check",
-        metavar="SERVICE%sINSTANCE" % SPACER,
-    )
-    parser.add_argument(
-        "-v", "--verbose", action="store_true", dest="verbose", default=False
-    )
-    options = parser.parse_args()
 
-    return options
+MESOS_TASK_SPACER = "."
 
 
-def check_services_replication(
-    soa_dir: str,
-    cluster: str,
-    service_instances: Sequence[str],
-    instance_type_class: Type[InstanceConfig_T],
-    check_service_replication: CheckServiceReplication,
-    replication_checker: SmartstackReplicationChecker,
-    all_tasks_or_pods: Sequence[Union[MarathonTask, V1Pod]],
-) -> float:
-    service_instances_set = set(service_instances)
-    replication_statuses: List[bool] = []
-
-    for service in list_services(soa_dir=soa_dir):
-        service_config = PaastaServiceConfigLoader(service=service, soa_dir=soa_dir)
-        for instance_config in service_config.instance_configs(
-            cluster=cluster, instance_type_class=instance_type_class
-        ):
-            if (
-                service_instances_set
-                and f"{service}{SPACER}{instance_config.instance}"
-                not in service_instances_set
-            ):
-                continue
-            if instance_config.get_docker_image():
-                is_well_replicated = check_service_replication(
-                    instance_config=instance_config,
-                    all_tasks_or_pods=all_tasks_or_pods,
-                    smartstack_replication_checker=replication_checker,
-                )
-                if is_well_replicated is not None:
-                    replication_statuses.append(is_well_replicated)
-
-            else:
-                log.debug(
-                    "%s is not deployed. Skipping replication monitoring."
-                    % instance_config.job_id
-                )
-
-    return calculate_pct_under_replicated(replication_statuses)
-
-
-def calculate_pct_under_replicated(replication_statuses: List[bool]) -> float:
-    if len(replication_statuses) == 0:
-        return 0
-    else:
-        num_under_replicated = len(
-            [status for status in replication_statuses if status is False]
-        )
-        return 100 * num_under_replicated / len(replication_statuses)
-
-
-def emit_cluster_replication_metrics(
-    pct_under_replicated: float, cluster: str, scheduler: str,
-) -> None:
-    meteorite_dims = {"paasta_cluster": cluster, "scheduler": scheduler}
-    gauge = yelp_meteorite.create_gauge(
-        "paasta.pct_services_under_replicated", meteorite_dims
-    )
-    gauge.set(pct_under_replicated)
+VolumeInfo = TypedDict(
+    "VolumeInfo", {"container_path": str, "host_path": str, "mode": str}
+)
+
+_Docker_PortMapping = TypedDict(
+    "_Docker_PortMapping", {"host_port": int, "container_port": int, "protocol": str}
+)
+
+DockerInfo = TypedDict(
+    "DockerInfo",
+    {
+        "image": str,
+        "network": str,
+        "port_mappings": List[_Docker_PortMapping],
+        "parameters": List[DockerParameter],
+    },
+)
+
+ContainerInfo = TypedDict(
+    "ContainerInfo", {"type": str, "docker": DockerInfo, "volumes": List[VolumeInfo]}
+)
+
+
+_CommandInfo_URI = TypedDict(
+    "_CommandInfo_URI",
+    {
+        "value": str,
+        "executable": bool,
+        "extract": bool,
+        "cache": bool,
+        "output_file": str,
+    },
+    total=False,
+)
+
+CommandInfo = TypedDict(
+    "CommandInfo",
+    {
+        "uris": List[_CommandInfo_URI],
+        "environment": Any,
+        "shell": bool,
+        "value": str,
+        "arguments": List[str],
+        "user": str,
+    },
+    total=False,
+)
+
+Value_Scalar = TypedDict("Value_Scalar", {"value": float})
+
+Value_Range = TypedDict("Value_Range", {"begin": int, "end": int})
+
+Value_Ranges = TypedDict("Value_Ranges", {"range": List[Value_Range]})
+
+Resource = TypedDict(
+    "Resource",
+    {"name": str, "type": str, "scalar": Value_Scalar, "ranges": Value_Ranges},
+    total=False,
+)
+
+
+TaskID = TypedDict("TaskID", {"value": str})
+
+SlaveID = TypedDict("SlaveID", {"value": str})
+
+
+class TaskInfoBase(TypedDict):
+    name: str
+    task_id: TaskID
+    agent_id: SlaveID
+    resources: List[Resource]
+
+
+class TaskInfo(TaskInfoBase):
+    container: ContainerInfo
+    command: CommandInfo
+
+
+class NativeServiceConfigDict(LongRunningServiceConfigDict):
+    pass
+
+
+class NativeServiceConfig(LongRunningServiceConfig):
+    config_dict: NativeServiceConfigDict
+    config_filename_prefix = "paasta_native"
+
+    def __init__(
+        self,
+        service: str,
+        instance: str,
+        cluster: str,
+        config_dict: NativeServiceConfigDict,
+        branch_dict: Optional[BranchDictV2],
+        soa_dir: str,
+        service_namespace_config: Optional[ServiceNamespaceConfig] = None,
+    ) -> None:
+        super().__init__(
+            cluster=cluster,
+            instance=instance,
+            service=service,
+            config_dict=config_dict,
+            branch_dict=branch_dict,
+            soa_dir=soa_dir,
+        )
+        # service_namespace_config may be omitted/set to None at first, then set
+        # after initializing. e.g. we do this in load_paasta_native_job_config
+        # so we can call get_nerve_namespace() to figure out what SNC to read.
+        # It may also be set to None if this service is not in nerve.
+        if service_namespace_config is not None:
+            self.service_namespace_config = service_namespace_config
+        else:
+            self.service_namespace_config = ServiceNamespaceConfig()
+
+    def task_name(self, base_task: TaskInfo) -> str:
+        code_sha = get_code_sha_from_dockerurl(
+            base_task["container"]["docker"]["image"]
+        )
 
+        filled_in_task = copy.deepcopy(base_task)
+        filled_in_task["name"] = ""
+        filled_in_task["task_id"] = {"value": ""}
+        filled_in_task["agent_id"] = {"value": ""}
 
-def main(
-    instance_type_class: Type[InstanceConfig_T],
-    check_service_replication: CheckServiceReplication,
-    namespace: str,
-    mesos: bool = False,
-) -> None:
-    args = parse_args()
-    if args.verbose:
-        logging.basicConfig(level=logging.DEBUG)
-    else:
-        logging.basicConfig(level=logging.WARNING)
-
-    system_paasta_config = load_system_paasta_config()
-    cluster = system_paasta_config.get_cluster()
-    replication_checker: SmartstackReplicationChecker
-
-    if mesos:
-        tasks_or_pods, slaves = get_mesos_tasks_and_slaves(system_paasta_config)
-        replication_checker = MesosSmartstackReplicationChecker(
-            mesos_slaves=slaves, system_paasta_config=system_paasta_config,
-        )
-    else:
-        tasks_or_pods, nodes = get_kubernetes_pods_and_nodes(namespace)
-        replication_checker = KubeSmartstackReplicationChecker(
-            nodes=nodes, system_paasta_config=system_paasta_config,
+        config_hash = get_config_hash(
+            filled_in_task, force_bounce=self.get_force_bounce()
         )
 
-    pct_under_replicated = check_services_replication(
-        soa_dir=args.soa_dir,
+        return compose_job_id(
+            self.service,
+            self.instance,
+            git_hash=code_sha,
+            config_hash=config_hash,
+            spacer=MESOS_TASK_SPACER,
+        )
+
+    def base_task(
+        self, system_paasta_config: SystemPaastaConfig, portMappings=True
+    ) -> TaskInfo:
+        """Return a TaskInfo Dict with all the fields corresponding to the
+        configuration filled in.
+
+        Does not include task.agent_id or a task.id; those need to be
+        computed separately.
+        """
+        docker_volumes = self.get_volumes(
+            system_volumes=system_paasta_config.get_volumes()
+        )
+        task: TaskInfo = {
+            "name": "",
+            "task_id": {"value": ""},
+            "agent_id": {"value": ""},
+            "container": {
+                "type": "DOCKER",
+                "docker": {
+                    "image": self.get_docker_url(),
+                    "parameters": [
+                        {"key": param["key"], "value": param["value"]}
+                        for param in self.format_docker_parameters()
+                    ],
+                    "network": self.get_mesos_network_mode(),
+                    "port_mappings": [],
+                },
+                "volumes": [
+                    {
+                        "container_path": volume["containerPath"],
+                        "host_path": volume["hostPath"],
+                        "mode": volume["mode"].upper(),
+                    }
+                    for volume in docker_volumes
+                ],
+            },
+            "command": {
+                "value": str(self.get_cmd()),
+                "uris": [
+                    {
+                        "value": system_paasta_config.get_dockercfg_location(),
+                        "extract": False,
+                    }
+                ],
+            },
+            "resources": [
+                {
+                    "name": "cpus",
+                    "type": "SCALAR",
+                    "scalar": {"value": self.get_cpus()},
+                },
+                {"name": "mem", "type": "SCALAR", "scalar": {"value": self.get_mem()}},
+            ],
+        }
+
+        if portMappings:
+            task["container"]["docker"]["port_mappings"] = [
+                {
+                    "container_port": self.get_container_port(),
+                    # filled by tasks_and_state_for_offer()
+                    "host_port": 0,
+                    "protocol": "tcp",
+                }
+            ]
+
+            task["resources"].append(
+                {
+                    "name": "ports",
+                    "type": "RANGES",
+                    "ranges": {
+                        # filled by tasks_and_state_for_offer
+                        "range": [{"begin": 0, "end": 0}]
+                    },
+                }
+            )
+
+        task["name"] = self.task_name(task)
+
+        return task
+
+    def get_mesos_network_mode(self) -> str:
+        return self.get_net().upper()
+
+
+def load_paasta_native_job_config(
+    service: str,
+    instance: str,
+    cluster: str,
+    load_deployments: bool = True,
+    soa_dir: str = DEFAULT_SOA_DIR,
+    instance_type: str = "paasta_native",
+    config_overrides: Optional[NativeServiceConfigDict] = None,
+) -> NativeServiceConfig:
+    instance_config_dict = cast(
+        NativeServiceConfigDict,
+        load_service_instance_config(
+            service=service,
+            instance=instance,
+            instance_type=instance_type,
+            cluster=cluster,
+            soa_dir=soa_dir,
+        ),
+    )
+    branch_dict: Optional[BranchDictV2] = None
+    instance_config_dict.update(config_overrides or {})
+    if load_deployments:
+        deployments_json = load_v2_deployments_json(service, soa_dir=soa_dir)
+        temp_instance_config = NativeServiceConfig(
+            service=service,
+            cluster=cluster,
+            instance=instance,
+            config_dict=instance_config_dict,
+            branch_dict=None,
+            soa_dir=soa_dir,
+        )
+        branch = temp_instance_config.get_branch()
+        deploy_group = temp_instance_config.get_deploy_group()
+        branch_dict = deployments_json.get_branch_dict(service, branch, deploy_group)
+
+    service_config = NativeServiceConfig(
+        service=service,
         cluster=cluster,
-        service_instances=args.service_instance_list,
-        instance_type_class=instance_type_class,
-        check_service_replication=check_service_replication,
-        replication_checker=replication_checker,
-        all_tasks_or_pods=tasks_or_pods,
+        instance=instance,
+        config_dict=instance_config_dict,
+        branch_dict=branch_dict,
+        soa_dir=soa_dir,
+    )
+
+    service_namespace_config = load_service_namespace_config(
+        service=service, namespace=service_config.get_nerve_namespace(), soa_dir=soa_dir
     )
-    if yelp_meteorite is not None:
-        emit_cluster_replication_metrics(
-            pct_under_replicated, cluster, scheduler="mesos" if mesos else "kubernetes"
-        )
-
-    if pct_under_replicated >= args.under_replicated_crit_pct:
-        log.critical(
-            f"{pct_under_replicated}% of instances are under replicated "
-            f"(past {args.under_replicated_crit_pct} is critical)!"
-        )
-        sys.exit(2)
-    elif pct_under_replicated >= args.under_replicated_warn_pct:
-        log.warning(
-            f"{pct_under_replicated}% of instances are under replicated "
-            f"(past {args.under_replicated_warn_pct} is a warning)!"
-        )
-        sys.exit(1)
-    else:
-        sys.exit(0)
-
-
-def get_mesos_tasks_and_slaves(
-    system_paasta_config: SystemPaastaConfig,
-) -> Tuple[Sequence[MarathonTask], List[Any]]:
-    clients = get_marathon_clients(get_marathon_servers(system_paasta_config))
-    all_clients: Sequence[MarathonClient] = clients.get_all_clients()
-    all_tasks: List[MarathonTask] = []
-    for client in all_clients:
-        all_tasks.extend(client.list_tasks())
-    mesos_slaves = a_sync.block(get_slaves)
-
-    return all_tasks, mesos_slaves
-
-
-def get_kubernetes_pods_and_nodes(
-    namespace: str,
-) -> Tuple[Sequence[V1Pod], Sequence[V1Node]]:
-    kube_client = KubeClient()
-    all_pods = get_all_pods(kube_client=kube_client, namespace=namespace)
-    all_nodes = get_all_nodes(kube_client)
+    service_config.service_namespace_config = service_namespace_config
+
+    return service_config
+
 
-    return all_pods, all_nodes
+class UnknownNativeServiceError(Exception):
+    pass
```

### Comparing `paasta-tools-0.92.1/paasta_tools/monitoring_tools.py` & `paasta-tools-1.0.0/paasta_tools/monitoring_tools.py`

 * *Files 20% similar despite different names*

```diff
@@ -16,37 +16,51 @@
 Getters for deriving monitoring parameters for mesos-deployed stuff.
 This leaves a place for sane defaults that might change depending
 on the framework that is asking, and still allows you to set your team
 *once* for a service in the general config.
 
 Everything in here is private, and you shouldn't worry about it.
 """
+import abc
 import json
 import logging
 import os
 from typing import Dict
+from typing import Mapping
 from typing import Optional
 from typing import Tuple
 
 import pysensu_yelp
 import service_configuration_lib
 
 from paasta_tools.long_running_service_tools import LongRunningServiceConfig
 from paasta_tools.utils import _log
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import is_under_replicated
 from paasta_tools.utils import load_system_paasta_config
 from paasta_tools.utils import PaastaNotConfiguredError
+from paasta_tools.utils import time_cache
+
+
+class ReplicationChecker(abc.ABC):
+    @abc.abstractmethod
+    def get_replication_for_instance(
+        self, instance_config: LongRunningServiceConfig
+    ) -> Dict[str, Dict[str, Dict[str, int]]]:
+        ...
+
 
 try:
     import yelp_meteorite
 except ImportError:
     yelp_meteorite = None
 
 
+DEFAULT_REPLICATION_RUNBOOK = "y/unhealthy-paasta-instances"
+
 log = logging.getLogger(__name__)
 
 
 def monitoring_defaults(key):
     defaults = {
         "runbook": 'Please set a `runbook` field in your monitoring.yaml. Like "y/rb-mesos". Docs: '
         "https://paasta.readthedocs.io/en/latest/yelpsoa_configs.html#monitoring-yaml",
@@ -134,24 +148,29 @@
     return __get_monitoring_config_value("component", overrides, service, soa_dir)
 
 
 def get_description(overrides, service, soa_dir=DEFAULT_SOA_DIR):
     return __get_monitoring_config_value("description", overrides, service, soa_dir)
 
 
+# Our typical usage pattern is that we call all the different get_* functions back to back. Applying a small amount of
+# cache here helps cut down on the number of times we re-parse service.yaml.
+_cached_read_service_configuration = time_cache(ttl=5)(
+    service_configuration_lib.read_service_configuration
+)
+
+
 def __get_monitoring_config_value(
     key,
     overrides,
     service,
     soa_dir=DEFAULT_SOA_DIR,
     monitoring_defaults=monitoring_defaults,
 ):
-    general_config = service_configuration_lib.read_service_configuration(
-        service, soa_dir=soa_dir
-    )
+    general_config = _cached_read_service_configuration(service, soa_dir=soa_dir)
     monitor_config = read_monitoring_config(service, soa_dir=soa_dir)
     service_default = general_config.get(key, monitoring_defaults(key))
     service_default = general_config.get("monitoring", {key: service_default}).get(
         key, service_default
     )
     service_default = monitor_config.get(key, service_default)
     return overrides.get(key, service_default)
@@ -180,33 +199,46 @@
             "No Sensu Team data (/etc/sensu/team_data.json) available. Using empty defaults"
         )
         team_data = {}
     return team_data
 
 
 def send_event(
-    service, check_name, overrides, status, output, soa_dir, ttl=None, cluster=None
+    service,
+    check_name,
+    overrides,
+    status,
+    output,
+    soa_dir,
+    ttl=None,
+    cluster=None,
+    system_paasta_config=None,
+    dry_run=False,
 ):
     """Send an event to sensu via pysensu_yelp with the given information.
 
     :param service: The service name the event is about
     :param check_name: The name of the check as it appears in Sensu
     :param overrides: A dictionary containing overrides for monitoring options
                       (e.g. notification_email, ticket, page)
     :param status: The status to emit for this event
     :param output: The output to emit for this event
     :param soa_dir: The service directory to read monitoring information from
+    :param ttl: TTL (optional)
     :param cluster: The cluster name (optional)
+    :param system_paasta_config: A SystemPaastaConfig object representing the system
+    :param dry_run: Print the Sensu event instead of emitting it
     """
     # This function assumes the input is a string like "mumble.main"
     team = get_team(overrides, service, soa_dir)
     if not team:
         return
 
-    system_paasta_config = load_system_paasta_config()
+    if system_paasta_config is None:
+        system_paasta_config = load_system_paasta_config()
     if cluster is None:
         try:
             cluster = system_paasta_config.get_cluster()
         except PaastaNotConfiguredError:
             cluster = "localhost"
 
     alert_after = overrides.get("alert_after", "5m")
@@ -236,301 +268,385 @@
         "ttl": ttl,
         "sensu_host": system_paasta_config.get_sensu_host(),
         "sensu_port": system_paasta_config.get_sensu_port(),
         "component": get_component(overrides, service, soa_dir),
         "description": get_description(overrides, service, soa_dir),
     }
 
-    if result_dict.get("sensu_host"):
+    if dry_run:
+        if status == pysensu_yelp.Status.OK:
+            print(f"Would've sent an OK event for check '{check_name}'")
+        else:
+            from pprint import pprint  # only import during testing
+
+            print(f"Would've sent the following alert for check '{check_name}':")
+            pprint(result_dict)
+
+    elif result_dict.get("sensu_host"):
         pysensu_yelp.send_event(**result_dict)
 
 
+@time_cache(ttl=5)
 def read_monitoring_config(service, soa_dir=DEFAULT_SOA_DIR):
     """Read a service's monitoring.yaml file.
 
     :param service: The service name
     :param soa_dir: THe SOA configuration directory to read from
     :returns: A dictionary of whatever was in soa_dir/name/monitoring.yaml"""
     rootdir = os.path.abspath(soa_dir)
     monitoring_file = os.path.join(rootdir, service, "monitoring.yaml")
     monitor_conf = service_configuration_lib.read_monitoring(monitoring_file)
     return monitor_conf
 
 
-def list_teams(**kwargs):
+def list_teams():
     """Loads team data from the system. Returns a set of team names (or empty
     set).
     """
     team_data = _load_sensu_team_data()
     teams = set(team_data.get("team_data", {}).keys())
     return teams
 
 
-def send_replication_event(instance_config, status, output):
+def send_replication_event(
+    instance_config,
+    status,
+    output,
+    description,
+    dry_run=False,
+):
     """Send an event to sensu via pysensu_yelp with the given information.
 
     :param instance_config: an instance of LongRunningServiceConfig
     :param status: The status to emit for this event
-    :param output: The output to emit for this event"""
+    :param output: The output to emit for this event
+    :param dry_run: Print the event instead of emitting it
+    """
     # This function assumes the input is a string like "mumble.main"
     monitoring_overrides = instance_config.get_monitoring()
     if "alert_after" not in monitoring_overrides:
         monitoring_overrides["alert_after"] = "2m"
     monitoring_overrides["check_every"] = "1m"
-    monitoring_overrides["runbook"] = get_runbook(
-        monitoring_overrides, instance_config.service, soa_dir=instance_config.soa_dir
+    monitoring_overrides["runbook"] = __get_monitoring_config_value(
+        "runbook",
+        monitoring_overrides,
+        instance_config.service,
+        soa_dir=instance_config.soa_dir,
+        monitoring_defaults=lambda _: DEFAULT_REPLICATION_RUNBOOK,
     )
+    monitoring_overrides["tip"] = __get_monitoring_config_value(
+        "tip",
+        monitoring_overrides,
+        instance_config.service,
+        soa_dir=instance_config.soa_dir,
+        monitoring_defaults=lambda _: (
+            f"Check the instance with: `paasta status -s {instance_config.service} "
+            f"-i {instance_config.instance} -c {instance_config.cluster} -vv`"
+        ),
+    )
+    monitoring_overrides["description"] = description
 
     check_name = "check_paasta_services_replication.%s" % instance_config.job_id
     send_event(
         service=instance_config.service,
         check_name=check_name,
         overrides=monitoring_overrides,
         status=status,
         output=output,
         soa_dir=instance_config.soa_dir,
         cluster=instance_config.cluster,
+        dry_run=dry_run,
     )
     _log(
         service=instance_config.service,
         line="Replication: %s" % output,
         component="monitoring",
         level="debug",
         cluster=instance_config.cluster,
         instance=instance_config.instance,
     )
 
 
 def emit_replication_metrics(
-    smartstack_replication_info: Dict[str, Dict[str, int]],
+    replication_infos: Mapping[str, Mapping[str, Mapping[str, int]]],
     instance_config: LongRunningServiceConfig,
     expected_count: int,
+    dry_run: bool = False,
 ) -> None:
-    meteorite_dims = {
-        "paasta_service": instance_config.service,
-        "paasta_cluster": instance_config.cluster,
-        "paasta_instance": instance_config.instance,
-        "paasta_pool": instance_config.get_pool(),
-    }
+    for provider, replication_info in replication_infos.items():
+        meteorite_dims = {
+            "paasta_service": instance_config.service,
+            "paasta_cluster": instance_config.cluster,
+            "paasta_instance": instance_config.instance,
+            "paasta_pool": instance_config.get_pool(),
+            "service_discovery_provider": provider,
+        }
 
-    num_available_backends = 0
-    for available_backends in smartstack_replication_info.values():
-        num_available_backends += available_backends.get(instance_config.job_id, 0)
-    available_backends_gauge = yelp_meteorite.create_gauge(
-        "paasta.service.available_backends", meteorite_dims
-    )
-    available_backends_gauge.set(num_available_backends)
+        num_available_backends = 0
+        for available_backends in replication_info.values():
+            num_available_backends += available_backends.get(instance_config.job_id, 0)
+        available_backends_metric = "paasta.service.available_backends"
+        if dry_run:
+            print(
+                f"Would've sent value {num_available_backends} for metric '{available_backends_metric}'"
+            )
+        else:
+            available_backends_gauge = yelp_meteorite.create_gauge(
+                available_backends_metric, meteorite_dims
+            )
+            available_backends_gauge.set(num_available_backends)
 
-    critical_percentage = instance_config.get_replication_crit_percentage()
-    num_critical_backends = critical_percentage * expected_count / 100.0
-    critical_backends_gauge = yelp_meteorite.create_gauge(
-        "paasta.service.critical_backends", meteorite_dims
-    )
-    critical_backends_gauge.set(num_critical_backends)
+        critical_percentage = instance_config.get_replication_crit_percentage()
+        num_critical_backends = critical_percentage * expected_count / 100.0
+        critical_backends_metric = "paasta.service.critical_backends"
+        if dry_run:
+            print(
+                f"Would've sent value {num_critical_backends} for metric '{critical_backends_metric}'"
+            )
+        else:
+            critical_backends_gauge = yelp_meteorite.create_gauge(
+                critical_backends_metric, meteorite_dims
+            )
+            critical_backends_gauge.set(num_critical_backends)
 
-    expected_backends_gauge = yelp_meteorite.create_gauge(
-        "paasta.service.expected_backends", meteorite_dims
-    )
-    expected_backends_gauge.set(expected_count)
+        expected_backends_metric = "paasta.service.expected_backends"
+        if dry_run:
+            print(
+                f"Would've sent value {expected_count} for metric '{expected_backends_metric}'"
+            )
+        else:
+            expected_backends_gauge = yelp_meteorite.create_gauge(
+                "paasta.service.expected_backends", meteorite_dims
+            )
+            expected_backends_gauge.set(expected_count)
 
 
-def check_smartstack_replication_for_instance(
-    instance_config, expected_count, smartstack_replication_checker
+def check_replication_for_instance(
+    instance_config: LongRunningServiceConfig,
+    expected_count: int,
+    replication_checker: ReplicationChecker,
+    dry_run: bool = False,
 ) -> bool:
     """Check a set of namespaces to see if their number of available backends is too low,
     emitting events to Sensu based on the fraction available and the thresholds defined in
     the corresponding yelpsoa config.
 
-    :param instance_config: an instance of MarathonServiceConfig
-    :param smartstack_replication_checker: an instance of SmartstackReplicationChecker
+    :param instance_config: an instance of LongRunningServiceConfig
+    :param replication_checker: an instance of ReplicationChecker
+    :param dry_run: Print Sensu event and metrics instead of emitting them
     """
 
     crit_threshold = instance_config.get_replication_crit_percentage()
 
-    log.info("Checking instance %s in smartstack", instance_config.job_id)
-    smartstack_replication_info = smartstack_replication_checker.get_replication_for_instance(
+    log.info(
+        "Checking instance %s in service discovery providers", instance_config.job_id
+    )
+    replication_infos = replication_checker.get_replication_for_instance(
         instance_config
     )
 
-    log.debug(
-        "Got smartstack replication info for %s: %s"
-        % (instance_config.job_id, smartstack_replication_info)
-    )
+    log.debug(f"Got replication info for {instance_config.job_id}: {replication_infos}")
     if yelp_meteorite is not None:
         emit_replication_metrics(
-            smartstack_replication_info, instance_config, expected_count,
+            replication_infos,
+            instance_config,
+            expected_count,
+            dry_run=dry_run,
         )
 
-    if len(smartstack_replication_info) == 0:
-        status = pysensu_yelp.Status.CRITICAL
-        output = (
-            "Service %s has no Smartstack replication info. Make sure the discover key in your smartstack.yaml "
-            "is valid!\n"
-        ) % instance_config.job_id
-        log.error(output)
-        service_is_under_replicated = True
-    else:
-        expected_count_per_location = int(
-            expected_count / len(smartstack_replication_info)
-        )
-        output = ""
-        output_critical = ""
-        output_ok = ""
-        under_replication_per_location = []
-
-        for location, available_backends in sorted(smartstack_replication_info.items()):
-            num_available_in_location = available_backends.get(
-                instance_config.job_id, 0
-            )
-            under_replicated, ratio = is_under_replicated(
-                num_available_in_location, expected_count_per_location, crit_threshold
-            )
-            if under_replicated:
-                output_critical += (
-                    "- Service %s has %d out of %d expected instances in %s (CRITICAL: %d%%)\n"
-                    % (
-                        instance_config.job_id,
-                        num_available_in_location,
-                        expected_count_per_location,
-                        location,
-                        ratio,
-                    )
+    service_is_under_replicated = False
+    failed_service_discovery_providers = set()
+    for service_discovery_provider, replication_info in replication_infos.items():
+        if len(replication_info) == 0:
+            output = (
+                "Service %s has no %s replication info. Make sure the discover key in the corresponding config (e.g. smartstack.yaml for Smartstack) is valid!\n"
+            ) % (instance_config.job_id, service_discovery_provider)
+            log.error(output)
+            service_is_under_replicated = True
+            failed_service_discovery_providers.add(service_discovery_provider)
+        else:
+            expected_count_per_location = int(expected_count / len(replication_info))
+            output_critical = []
+            output_ok = []
+            under_replication_per_location = []
+
+            for location, available_backends in sorted(replication_info.items()):
+                num_available_in_location = available_backends.get(
+                    instance_config.job_id, 0
                 )
-            else:
-                output_ok += (
-                    "- Service %s has %d out of %d expected instances in %s (OK: %d%%)\n"
-                    % (
-                        instance_config.job_id,
-                        num_available_in_location,
-                        expected_count_per_location,
-                        location,
-                        ratio,
-                    )
+                under_replicated, ratio = is_under_replicated(
+                    num_available_in_location,
+                    expected_count_per_location,
+                    crit_threshold,
                 )
-            under_replication_per_location.append(under_replicated)
+                if under_replicated:
+                    output_critical.append(
+                        "{} has {}/{} replicas in {} according to {} (CRITICAL: {}%)\n".format(
+                            instance_config.job_id,
+                            num_available_in_location,
+                            expected_count_per_location,
+                            location,
+                            service_discovery_provider,
+                            ratio,
+                        )
+                    )
+                    failed_service_discovery_providers.add(service_discovery_provider)
+                else:
+                    output_ok.append(
+                        "{} has {}/{} replicas in {} according to {} (OK: {}%)\n".format(
+                            instance_config.job_id,
+                            num_available_in_location,
+                            expected_count_per_location,
+                            location,
+                            service_discovery_provider,
+                            ratio,
+                        )
+                    )
+                under_replication_per_location.append(under_replicated)
+
+            output = ", ".join(output_critical)
+            if output_critical and output_ok:
+                output += ". The following locations are OK: "
+            output += ", ".join(output_ok)
+
+            service_is_under_replicated_anywhere = any(under_replication_per_location)
+            service_is_under_replicated |= service_is_under_replicated_anywhere
+            if service_is_under_replicated_anywhere:
+                log.error(output)
+            else:
+                log.info(output)
+
+    if service_is_under_replicated:
+        failed_service_discovery_providers_list = ",".join(
+            failed_service_discovery_providers
+        )
+        description = (
+            "This replication alert means that a {service_discovery_provider} powered loadbalancer\n"
+            "doesn't have enough healthy backends. Not having enough healthy backends\n"
+            "means that clients of that service will get 503s (http) or connection refused\n"
+            "(tcp) when trying to connect to it.\n"
+            "\n"
+            "Reasons this might be happening:\n"
+            "\n"
+            "  The service may simply not have enough copies or it could simply be\n"
+            "  unhealthy in that location. There also may not be enough resources\n"
+            "  in the cluster to support the requested instance count.\n"
+            "\n"
+            "Things you can do:\n"
+            "\n"
+            "  * You can view the logs for the job with:\n"
+            "      paasta logs -s {service} -i {instance} -c {cluster}\n"
+            "\n"
+            "  * Fix the cause of the unhealthy service. Try running:\n"
+            "\n"
+            "      paasta status -s {service} -i {instance} -c {cluster} -vv\n"
+            "\n"
+            "  * Widen {service_discovery_provider} discovery settings\n"
+            "  * Increase the instance count\n"
+            "\n"
+        ).format(
+            service=instance_config.service,
+            instance=instance_config.instance,
+            cluster=instance_config.cluster,
+            service_discovery_provider=failed_service_discovery_providers_list,
+        )
+        status = pysensu_yelp.Status.CRITICAL
+    else:
+        description = (
+            "{} is well-replicated because it has over {}% of its "
+            "expected replicas up."
+        ).format(instance_config.job_id, crit_threshold)
+        status = pysensu_yelp.Status.OK
 
-        output += output_critical
-        if output_critical and output_ok:
-            output += "\n\n"
-            output += "The following locations are OK:\n"
-        output += output_ok
-
-        service_is_under_replicated = any(under_replication_per_location)
-        if service_is_under_replicated:
-            status = pysensu_yelp.Status.CRITICAL
-            output += (
-                "\n\n"
-                "What this alert means:\n"
-                "\n"
-                "  This replication alert means that a SmartStack powered loadbalancer (haproxy)\n"
-                "  doesn't have enough healthy backends. Not having enough healthy backends\n"
-                "  means that clients of that service will get 503s (http) or connection refused\n"
-                "  (tcp) when trying to connect to it.\n"
-                "\n"
-                "Reasons this might be happening:\n"
-                "\n"
-                "  The service may simply not have enough copies or it could simply be\n"
-                "  unhealthy in that location. There also may not be enough resources\n"
-                "  in the cluster to support the requested instance count.\n"
-                "\n"
-                "Things you can do:\n"
-                "\n"
-                "  * You can view the logs for the job with:\n"
-                "      paasta logs -s %(service)s -i %(instance)s -c %(cluster)s\n"
-                "\n"
-                "  * Fix the cause of the unhealthy service. Try running:\n"
-                "\n"
-                "      paasta status -s %(service)s -i %(instance)s -c %(cluster)s -vv\n"
-                "\n"
-                "  * Widen SmartStack discovery settings\n"
-                "  * Increase the instance count\n"
-                "\n"
-            ) % {
-                "service": instance_config.service,
-                "instance": instance_config.instance,
-                "cluster": instance_config.cluster,
-            }
-            log.error(output)
-        else:
-            status = pysensu_yelp.Status.OK
-            log.info(output)
     send_replication_event(
-        instance_config=instance_config, status=status, output=output
+        instance_config=instance_config,
+        status=status,
+        output=output,
+        description=description,
+        dry_run=dry_run,
     )
-
     return not service_is_under_replicated
 
 
 def check_under_replication(
-    instance_config,
+    instance_config: LongRunningServiceConfig,
     expected_count: int,
     num_available: int,
     sub_component: Optional[str] = None,
-) -> Tuple[bool, str]:
+) -> Tuple[bool, str, str]:
     """Check if a component/sub_component is under-replicated and returns both the result of the check in the form of a
     boolean and a human-readable text to be used in logging or monitoring events.
     """
     crit_threshold = instance_config.get_replication_crit_percentage()
+
+    # Keep output short, with rest of context in description. This is because
+    # by default, Slack-Sensu messages have a 400 char limit, incl. the output.
+    # If it is too long, the runbook and tip won't show up.
     if sub_component is not None:
-        output = (
-            "Service %s has %d out of %d expected instances of %s available!\n"
-            + "(threshold: %d%%)"
-        ) % (
+        output = ("{} has {}/{} replicas of {} available (threshold: {}%)").format(
             instance_config.job_id,
             num_available,
             expected_count,
             sub_component,
             crit_threshold,
         )
     else:
-        output = (
-            "Service %s has %d out of %d expected instances available!\n"
-            + "(threshold: %d%%)"
-        ) % (instance_config.job_id, num_available, expected_count, crit_threshold)
+        output = ("{} has {}/{} replicas available (threshold: {}%)").format(
+            instance_config.job_id, num_available, expected_count, crit_threshold
+        )
+
     under_replicated, _ = is_under_replicated(
         num_available, expected_count, crit_threshold
     )
     if under_replicated:
-        output += (
-            "\n\n"
-            "What this alert means:\n"
-            "\n"
-            "  This replication alert means that the service PaaSTA can't keep the\n"
-            "  requested number of copies up and healthy in the cluster.\n"
+        description = (
+            "This replication alert means that PaaSTA can't keep the\n"
+            "requested number of replicas up and healthy in the cluster for "
+            "the instance {service}.{instance}.\n"
             "\n"
             "Reasons this might be happening:\n"
             "\n"
             "  The service may simply be unhealthy. There also may not be enough resources\n"
             "  in the cluster to support the requested instance count.\n"
             "\n"
             "Things you can do:\n"
             "\n"
             "  * Increase the instance count\n"
             "  * Fix the cause of the unhealthy service. Try running:\n"
             "\n"
-            "      paasta status -s %(service)s -i %(instance)s -c %(cluster)s -vv\n"
-        ) % {
-            "service": instance_config.service,
-            "instance": instance_config.instance,
-            "cluster": instance_config.cluster,
-        }
-    return (under_replicated, output)
+            "      paasta status -s {service} -i {instance} -c {cluster} -vv\n"
+        ).format(
+            service=instance_config.service,
+            instance=instance_config.instance,
+            cluster=instance_config.cluster,
+        )
+    else:
+        description = (
+            "{} is well-replicated because it has over {}% of its "
+            "expected replicas up."
+        ).format(instance_config.job_id, crit_threshold)
+    return under_replicated, output, description
 
 
 def send_replication_event_if_under_replication(
-    instance_config,
+    instance_config: LongRunningServiceConfig,
     expected_count: int,
     num_available: int,
     sub_component: Optional[str] = None,
+    dry_run: bool = False,
 ):
-    under_replicated, output = check_under_replication(
+    under_replicated, output, description = check_under_replication(
         instance_config, expected_count, num_available, sub_component
     )
     if under_replicated:
         log.error(output)
         status = pysensu_yelp.Status.CRITICAL
     else:
         log.info(output)
         status = pysensu_yelp.Status.OK
     send_replication_event(
-        instance_config=instance_config, status=status, output=output
+        instance_config=instance_config,
+        status=status,
+        output=output,
+        description=description,
+        dry_run=dry_run,
     )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/async_utils.py` & `paasta-tools-1.0.0/paasta_tools/async_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -70,27 +70,31 @@
                 return await call_or_get_from_cache(
                     self_cache, wrapped, args, (self,) + args, kwargs
                 )
 
             return inner
 
     else:
-        cache2: Dict = cache if cache is not None else {}  # Should be Dict[Any, T] but that doesn't work.
+        cache2: Dict = (
+            cache if cache is not None else {}
+        )  # Should be Dict[Any, T] but that doesn't work.
 
         def outer(wrapped):
             @functools.wraps(wrapped)
             async def inner(*args, **kwargs):
                 return await call_or_get_from_cache(cache2, wrapped, args, args, kwargs)
 
             return inner
 
     return outer
 
 
-async def aiter_to_list(aiter: AsyncIterable[T],) -> List[T]:
+async def aiter_to_list(
+    aiter: AsyncIterable[T],
+) -> List[T]:
     return [x async for x in aiter]
 
 
 def async_timeout(
     seconds: int = 10,
 ) -> Callable[
     [Callable[..., Awaitable[T]]], Callable[..., Awaitable[T]]  # wrapped  # inner
```

### Comparing `paasta-tools-0.92.1/paasta_tools/tron/tron_command_context.py` & `paasta-tools-1.0.0/paasta_tools/tron/tron_command_context.py`

 * *Files 0% similar despite different names*

```diff
@@ -40,17 +40,17 @@
         base.__getattr__(name),
         next[name],
         next.__getattr__(name)
     """
 
     def __init__(self, base=None, next=None):
         """
-          base - Object to look for attributes in
-          next - Next place to look for more pieces of context
-                 Generally this will be another instance of CommandContext
+        base - Object to look for attributes in
+        next - Next place to look for more pieces of context
+               Generally this will be another instance of CommandContext
         """
         self.base = base or {}
         self.next = next or {}
 
     def get(self, name, default=None):
         try:
             return self.__getitem__(name)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/tron/tron_timeutils.py` & `paasta-tools-1.0.0/paasta_tools/tron/tron_timeutils.py`

 * *Files 5% similar despite different names*

```diff
@@ -18,18 +18,17 @@
 
 def to_timestamp(time_val):
     """Generate a unix timestamp for the given datetime instance"""
     return time.mktime(time_val.timetuple())
 
 
 def delta_total_seconds(td):
-    """Equivalent to timedelta.total_seconds() available in Python 2.7.
-    """
+    """Equivalent to timedelta.total_seconds() available in Python 2.7."""
     microseconds, seconds, days = td.microseconds, td.seconds, td.days
-    return (microseconds + (seconds + days * 24 * 3600) * 10 ** 6) / 10 ** 6
+    return (microseconds + (seconds + days * 24 * 3600) * 10**6) / 10**6
 
 
 def macro_timedelta(start_date, years=0, months=0, days=0, hours=0):
     """Since datetime doesn't provide timedeltas at the year or month level,
     this function generates timedeltas of the appropriate sizes.
     """
     delta = datetime.timedelta(days=days, hours=hours)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/tron/client.py` & `paasta-tools-1.0.0/paasta_tools/tron/client.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/frameworks/native_service_config.py` & `paasta-tools-1.0.0/paasta_tools/contrib/service_shard_update.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,320 +1,262 @@
-#!/usr/bin/env python
-import copy
-from typing import Any
-from typing import List
-from typing import Optional
-
-import service_configuration_lib
-from mypy_extensions import TypedDict
-
-from paasta_tools.long_running_service_tools import load_service_namespace_config
-from paasta_tools.long_running_service_tools import LongRunningServiceConfig
-from paasta_tools.long_running_service_tools import LongRunningServiceConfigDict
-from paasta_tools.long_running_service_tools import ServiceNamespaceConfig
-from paasta_tools.utils import BranchDictV2
-from paasta_tools.utils import compose_job_id
-from paasta_tools.utils import Constraint  # noqa, imported for typing.
-from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import DockerParameter
-from paasta_tools.utils import get_code_sha_from_dockerurl
-from paasta_tools.utils import get_config_hash
-from paasta_tools.utils import load_v2_deployments_json
-from paasta_tools.utils import paasta_print
-from paasta_tools.utils import SystemPaastaConfig
-
-
-MESOS_TASK_SPACER = "."
-
-
-VolumeInfo = TypedDict(
-    "VolumeInfo", {"container_path": str, "host_path": str, "mode": str}
-)
-
-_Docker_PortMapping = TypedDict(
-    "_Docker_PortMapping", {"host_port": int, "container_port": int, "protocol": str}
-)
-
-DockerInfo = TypedDict(
-    "DockerInfo",
-    {
-        "image": str,
-        "network": str,
-        "port_mappings": List[_Docker_PortMapping],
-        "parameters": List[DockerParameter],
-    },
-)
-
-ContainerInfo = TypedDict(
-    "ContainerInfo", {"type": str, "docker": DockerInfo, "volumes": List[VolumeInfo]}
-)
-
-
-_CommandInfo_URI = TypedDict(
-    "_CommandInfo_URI",
-    {
-        "value": str,
-        "executable": bool,
-        "extract": bool,
-        "cache": bool,
-        "output_file": str,
-    },
-    total=False,
-)
-
-CommandInfo = TypedDict(
-    "CommandInfo",
-    {
-        "uris": List[_CommandInfo_URI],
-        "environment": Any,
-        "shell": bool,
-        "value": str,
-        "arguments": List[str],
-        "user": str,
-    },
-    total=False,
-)
-
-Value_Scalar = TypedDict("Value_Scalar", {"value": float})
-
-Value_Range = TypedDict("Value_Range", {"begin": int, "end": int})
-
-Value_Ranges = TypedDict("Value_Ranges", {"range": List[Value_Range]})
-
-Resource = TypedDict(
-    "Resource",
-    {"name": str, "type": str, "scalar": Value_Scalar, "ranges": Value_Ranges},
-    total=False,
-)
-
-
-TaskID = TypedDict("TaskID", {"value": str})
-
-SlaveID = TypedDict("SlaveID", {"value": str})
-
-
-class TaskInfoBase(TypedDict):
-    name: str
-    task_id: TaskID
-    agent_id: SlaveID
-    resources: List[Resource]
-
-
-class TaskInfo(TaskInfoBase):
-    container: ContainerInfo
-    command: CommandInfo
-
-
-class NativeServiceConfigDict(LongRunningServiceConfigDict):
-    pass
-
-
-class NativeServiceConfig(LongRunningServiceConfig):
-    config_dict: NativeServiceConfigDict
-    config_filename_prefix = "paasta_native"
-
-    def __init__(
-        self,
-        service: str,
-        instance: str,
-        cluster: str,
-        config_dict: NativeServiceConfigDict,
-        branch_dict: Optional[BranchDictV2],
-        soa_dir: str,
-        service_namespace_config: Optional[ServiceNamespaceConfig] = None,
-    ) -> None:
-        super().__init__(
-            cluster=cluster,
-            instance=instance,
-            service=service,
-            config_dict=config_dict,
-            branch_dict=branch_dict,
-            soa_dir=soa_dir,
-        )
-        # service_namespace_config may be omitted/set to None at first, then set
-        # after initializing. e.g. we do this in load_paasta_native_job_config
-        # so we can call get_nerve_namespace() to figure out what SNC to read.
-        # It may also be set to None if this service is not in nerve.
-        if service_namespace_config is not None:
-            self.service_namespace_config = service_namespace_config
-        else:
-            self.service_namespace_config = ServiceNamespaceConfig()
+import argparse
+import logging
+import sys
+
+from paasta_tools.cli.utils import trigger_deploys
+from paasta_tools.config_utils import AutoConfigUpdater
+from paasta_tools.utils import DEFAULT_SOA_CONFIGS_GIT_URL
+from paasta_tools.utils import format_git_url
+from paasta_tools.utils import load_system_paasta_config
+
+log = logging.getLogger(__name__)
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description="")
+    parser.add_argument(
+        "--git-remote",
+        help="Master git repo for soaconfigs",
+        default=None,
+        dest="git_remote",
+    )
+    parser.add_argument(
+        "--branch",
+        help="Branch name to push to",
+        required=True,
+        dest="branch",
+    )
+    parser.add_argument(
+        "--local-dir",
+        help="Act on configs in the local directory rather than cloning the git_remote",
+        required=False,
+        default=None,
+        dest="local_dir",
+    )
+    parser.add_argument(
+        "-v",
+        "--verbose",
+        help="Logging verbosity",
+        action="store_true",
+        dest="verbose",
+    )
+    parser.add_argument(
+        "--source-id",
+        help="String to attribute the changes in the commit message.",
+        required=False,
+        default=None,
+        dest="source_id",
+    )
+    parser.add_argument(
+        "--service",
+        help="Service to modify",
+        required=True,
+        dest="service",
+    )
+    parser.add_argument(
+        "--min-instance-count",
+        help="If a deploy group is added, the min_instance count to create it with",
+        required=False,
+        default=1,
+        dest="min_instance_count",
+    )
+    parser.add_argument(
+        "--prod-max-instance-count",
+        help="If a deploy group is added, the prod max_instance count to create it with",
+        required=False,
+        default=100,
+        type=int,
+        dest="prod_max_instance_count",
+    )
+    parser.add_argument(
+        "--non-prod-max-instance-count",
+        help="If a deploy group is added, the non-prod max_instance count to create it with",
+        required=False,
+        default=5,
+        type=int,
+        dest="non_prod_max_instance_count",
+    )
+    parser.add_argument(
+        "--cpus",
+        help="If a deploy group is added, the cpu value to create it with",
+        required=False,
+        type=float,
+        dest="cpus",
+    )
+    parser.add_argument(
+        "--mem",
+        help="If a deploy group is added, the mem value to create it with",
+        required=False,
+        type=int,
+        dest="mem",
+    )
+    parser.add_argument(
+        "--setpoint",
+        help="If a deploy group is added, the autoscaling.setpoint value to create it with",
+        required=False,
+        type=float,
+        dest="setpoint",
+    )
+    parser.add_argument(
+        "--shard-name",
+        help="Shard name to add if it does not exist",
+        required=True,
+        dest="shard_name",
+    )
+    parser.add_argument(
+        "--metrics-provider",
+        help="Autoscaling metrics provider",
+        required=False,
+        dest="metrics_provider",
+    )
+    parser.add_argument(
+        "--timeout-server-ms",
+        help="smartstack server timeout",
+        required=False,
+        type=int,
+        dest="timeout_server_ms",
+    )
+    return parser.parse_args()
+
+
+def get_default_git_remote():
+    system_paasta_config = load_system_paasta_config()
+    repo_config = system_paasta_config.get_git_repo_config("yelpsoa-configs")
+    default_git_remote = format_git_url(
+        system_paasta_config.get_git_config()["git_user"],
+        repo_config.get("git_server", DEFAULT_SOA_CONFIGS_GIT_URL),
+        repo_config["repo_name"],
+    )
+    return default_git_remote
+
+
+DEPLOY_MAPPINGS = {
+    "dev": ["norcal-devc"],
+    "stage": ["norcal-stagef", "norcal-stageg"],
+    "prod": ["nova-prod", "pnw-prod"],
+}
+
+
+def main(args):
+    changes_made = False
+    updater = AutoConfigUpdater(
+        config_source=args.source_id,
+        git_remote=args.git_remote or get_default_git_remote(),
+        branch=args.branch,
+        working_dir=args.local_dir or "/nail/tmp",
+        do_clone=args.local_dir is None,
+    )
+    with updater:
+        deploy_file = updater.get_existing_configs(args.service, "deploy")
+        smartstack_file = updater.get_existing_configs(args.service, "smartstack")
+        shard_deploy_groups = {
+            f"{prefix}.{args.shard_name}" for prefix in DEPLOY_MAPPINGS.keys()
+        }
+        pipeline_steps = {step["step"] for step in deploy_file["pipeline"]}
 
-    def task_name(self, base_task: TaskInfo) -> str:
-        code_sha = get_code_sha_from_dockerurl(
-            base_task["container"]["docker"]["image"]
-        )
-
-        filled_in_task = copy.deepcopy(base_task)
-        filled_in_task["name"] = ""
-        filled_in_task["task_id"] = {"value": ""}
-        filled_in_task["agent_id"] = {"value": ""}
-
-        config_hash = get_config_hash(
-            filled_in_task, force_bounce=self.get_force_bounce()
-        )
-
-        return compose_job_id(
-            self.service,
-            self.instance,
-            git_hash=code_sha,
-            config_hash=config_hash,
-            spacer=MESOS_TASK_SPACER,
-        )
-
-    def base_task(
-        self, system_paasta_config: SystemPaastaConfig, portMappings=True
-    ) -> TaskInfo:
-        """Return a TaskInfo Dict with all the fields corresponding to the
-        configuration filled in.
-
-        Does not include task.agent_id or a task.id; those need to be
-        computed separately.
-        """
-        docker_volumes = self.get_volumes(
-            system_volumes=system_paasta_config.get_volumes()
-        )
-        task: TaskInfo = {
-            "name": "",
-            "task_id": {"value": ""},
-            "agent_id": {"value": ""},
-            "container": {
-                "type": "DOCKER",
-                "docker": {
-                    "image": self.get_docker_url(),
-                    "parameters": [
-                        {"key": param["key"], "value": param["value"]}
-                        for param in self.format_docker_parameters()
-                    ],
-                    "network": self.get_mesos_network_mode(),
-                    "port_mappings": [],
-                },
-                "volumes": [
+        if not shard_deploy_groups.issubset(pipeline_steps):
+            changes_made = True
+            steps_to_add = shard_deploy_groups - pipeline_steps
+
+            # If the pipeline does not contain deploy groups for the service shard
+            # Add the missing steps and write to deploy config
+            for step in steps_to_add:
+                deploy_file["pipeline"].append(
                     {
-                        "container_path": volume["containerPath"],
-                        "host_path": volume["hostPath"],
-                        "mode": volume["mode"].upper(),
+                        "step": step,
+                        "wait_for_deployment": True,
+                        "disabled": True,
                     }
-                    for volume in docker_volumes
-                ],
-            },
-            "command": {
-                "value": str(self.get_cmd()),
-                "uris": [
-                    {
-                        "value": system_paasta_config.get_dockercfg_location(),
-                        "extract": False,
+                )
+                log.info(f"{step} added to deploy config")
+            updater.write_configs(args.service, "deploy", deploy_file)
+
+            for deploy_prefix, config_paths in DEPLOY_MAPPINGS.items():
+                for config_path in config_paths:
+                    # Determine configuration suffix (PAASTA-18216)
+                    eks_config = updater.get_existing_configs(
+                        args.service, f"eks-{config_path}"
+                    )
+                    kube_config = updater.get_existing_configs(
+                        args.service, f"kubernetes-{config_path}"
+                    )
+
+                    if eks_config:
+                        config_file = eks_config
+                        config_prefix = "eks-"
+                    elif kube_config:
+                        config_file = kube_config
+                        config_prefix = "kubernetes-"
+                    else:
+                        log.error(
+                            f"No EKS or Kubernetes config found for {args.service}"
+                        )
+                        continue
+
+                    instance_config = {
+                        "deploy_group": f"{deploy_prefix}.{args.shard_name}",
+                        "min_instances": args.min_instance_count,
+                        "max_instances": args.prod_max_instance_count
+                        if deploy_prefix == "prod"
+                        else args.non_prod_max_instance_count,
+                        "env": {
+                            "PAASTA_SECRET_BUGSNAG_API_KEY": "SECRET(bugsnag_api_key)",
+                        },
                     }
-                ],
-            },
-            "resources": [
-                {
-                    "name": "cpus",
-                    "type": "SCALAR",
-                    "scalar": {"value": self.get_cpus()},
-                },
-                {"name": "mem", "type": "SCALAR", "scalar": {"value": self.get_mem()}},
-            ],
-        }
+                    if args.metrics_provider is not None or args.setpoint is not None:
+                        instance_config["autoscaling"] = {}
+                        if args.metrics_provider is not None:
+                            instance_config["autoscaling"][
+                                "metrics_provider"
+                            ] = args.metrics_provider
+                        if args.setpoint is not None:
+                            instance_config["autoscaling"]["setpoint"] = args.setpoint
+                    if args.cpus is not None:
+                        instance_config["cpus"] = args.cpus
+                    if args.mem is not None:
+                        instance_config["mem"] = args.mem
+                    # If the service config does not contain definitions for the shard in each ecosystem
+                    # Add the missing definition and write to the corresponding config
+                    if args.shard_name not in config_file.keys():
+                        config_file[args.shard_name] = instance_config
+                        updater.write_configs(
+                            args.service, f"{config_prefix}{config_path}", config_file
+                        )
+                        log.info(
+                            f"{deploy_prefix}.{args.shard_name} added to {config_prefix}{config_path}"
+                        )
+        else:
+            log.info(f"{args.shard_name} is in deploy config already.")
 
-        if portMappings:
-            task["container"]["docker"]["port_mappings"] = [
-                {
-                    "container_port": self.get_container_port(),
-                    # filled by tasks_and_state_for_offer()
-                    "host_port": 0,
-                    "protocol": "tcp",
-                }
-            ]
-
-            task["resources"].append(
-                {
-                    "name": "ports",
-                    "type": "RANGES",
-                    "ranges": {
-                        # filled by tasks_and_state_for_offer
-                        "range": [{"begin": 0, "end": 0}]
-                    },
-                }
-            )
-
-        task["name"] = self.task_name(task)
-
-        return task
-
-    def get_mesos_network_mode(self) -> str:
-        return self.get_net().upper()
-
-
-def load_paasta_native_job_config(
-    service,
-    instance,
-    cluster,
-    load_deployments=True,
-    soa_dir=DEFAULT_SOA_DIR,
-    instance_type="paasta_native",
-    config_overrides=None,
-) -> NativeServiceConfig:
-    service_paasta_native_jobs = read_service_config(
-        service=service,
-        instance=instance,
-        instance_type=instance_type,
-        cluster=cluster,
-        soa_dir=soa_dir,
-    )
-    branch_dict: Optional[BranchDictV2] = None
-    instance_config_dict = service_paasta_native_jobs[instance].copy()
-    instance_config_dict.update(config_overrides or {})
-    if load_deployments:
-        deployments_json = load_v2_deployments_json(service, soa_dir=soa_dir)
-        temp_instance_config = NativeServiceConfig(
-            service=service,
-            cluster=cluster,
-            instance=instance,
-            config_dict=instance_config_dict,
-            branch_dict=None,
-            soa_dir=soa_dir,
-        )
-        branch = temp_instance_config.get_branch()
-        deploy_group = temp_instance_config.get_deploy_group()
-        branch_dict = deployments_json.get_branch_dict(service, branch, deploy_group)
-
-    service_config = NativeServiceConfig(
-        service=service,
-        cluster=cluster,
-        instance=instance,
-        config_dict=instance_config_dict,
-        branch_dict=branch_dict,
-        soa_dir=soa_dir,
-    )
-
-    service_namespace_config = load_service_namespace_config(
-        service=service, namespace=service_config.get_nerve_namespace(), soa_dir=soa_dir
-    )
-    service_config.service_namespace_config = service_namespace_config
-
-    return service_config
-
-
-def read_service_config(
-    service, instance, instance_type, cluster, soa_dir=DEFAULT_SOA_DIR
-):
-    conf_file = f"{instance_type}-{cluster}"
-    full_path = f"{soa_dir}/{service}/{conf_file}.yaml"
-    paasta_print("Reading paasta-remote configuration file: %s" % full_path)
-
-    config = service_configuration_lib.read_extra_service_information(
-        service, conf_file, soa_dir=soa_dir
-    )
-
-    if instance not in config:
-        raise UnknownNativeServiceError(
-            'No job named "{}" in config file {}: \n{}'.format(
-                instance, full_path, open(full_path).read()
-            )
-        )
+        # If the service shard is not defined in smartstack
+        # Add the definition with a suggested proxy port
+        if args.shard_name not in smartstack_file.keys():
+            changes_made = True
+            smartstack_file[args.shard_name] = {
+                "proxy_port": None,
+                "extra_advertise": {"ecosystem:devc": ["ecosystem:devc"]},
+            }
+            if args.timeout_server_ms:
+                smartstack_file[args.shard_name][
+                    "timeout_server_ms"
+                ] = args.timeout_server_ms
+            updater.write_configs(args.service, "smartstack", smartstack_file)
+        else:
+            log.info(f"{args.shard_name} is in smartstack config already, skipping.")
+
+        # Only commit to remote if changes were made
+        if changes_made:
+            updater.commit_to_remote()
+            trigger_deploys(args.service)
+        else:
+            # exit with code to indicate nothing was changed
+            sys.exit(129)
 
-    return config
 
+if __name__ == "__main__":
+    args = parse_args()
+    if args.verbose:
+        logging.basicConfig(level=logging.DEBUG)
+    else:
+        logging.basicConfig(level=logging.WARNING)
 
-class UnknownNativeServiceError(Exception):
-    pass
+    main(args)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/frameworks/native_scheduler.py` & `paasta-tools-1.0.0/paasta_tools/frameworks/native_scheduler.py`

 * *Files 0% similar despite different names*

```diff
@@ -23,14 +23,15 @@
 from paasta_tools import drain_lib
 from paasta_tools import mesos_tools
 from paasta_tools.frameworks.constraints import check_offer_constraints
 from paasta_tools.frameworks.constraints import ConstraintState
 from paasta_tools.frameworks.constraints import update_constraint_state
 from paasta_tools.frameworks.native_service_config import load_paasta_native_job_config
 from paasta_tools.frameworks.native_service_config import NativeServiceConfig
+from paasta_tools.frameworks.native_service_config import NativeServiceConfigDict
 from paasta_tools.frameworks.native_service_config import TaskInfo
 from paasta_tools.frameworks.task_store import MesosTaskParameters
 from paasta_tools.frameworks.task_store import TaskStore
 from paasta_tools.frameworks.task_store import ZKTaskStore
 from paasta_tools.utils import _log
 from paasta_tools.utils import DEFAULT_LOGLEVEL
 from paasta_tools.utils import DEFAULT_SOA_DIR
@@ -70,15 +71,15 @@
         cluster: str,
         system_paasta_config: SystemPaastaConfig,
         staging_timeout: float,
         soa_dir: str = DEFAULT_SOA_DIR,
         service_config: Optional[NativeServiceConfig] = None,
         reconcile_backoff: float = 30,
         instance_type: str = "paasta_native",
-        service_config_overrides: Optional[Dict] = None,
+        service_config_overrides: Optional[NativeServiceConfigDict] = None,
         reconcile_start_time: float = float("inf"),
         task_store_type=ZKTaskStore,
     ) -> None:
         self.service_name = service_name
         self.instance_name = instance_name
         self.instance_type = instance_type
         self.cluster = cluster
```

### Comparing `paasta-tools-0.92.1/paasta_tools/frameworks/adhoc_scheduler.py` & `paasta-tools-1.0.0/paasta_tools/frameworks/adhoc_scheduler.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,14 @@
 from pymesos import MesosSchedulerDriver
 
 from paasta_tools.frameworks.constraints import ConstraintState
 from paasta_tools.frameworks.native_scheduler import LIVE_TASK_STATES
 from paasta_tools.frameworks.native_scheduler import NativeScheduler
 from paasta_tools.frameworks.native_service_config import TaskInfo
 from paasta_tools.frameworks.native_service_config import UnknownNativeServiceError
-from paasta_tools.utils import paasta_print
 
 
 class AdhocScheduler(NativeScheduler):
     def __init__(self, *args, **kwargs):
         self.dry_run = kwargs.pop("dry_run")
 
         if kwargs.get("service_config_overrides") is None:
@@ -54,15 +53,15 @@
     def tasks_and_state_for_offer(
         self, driver: MesosSchedulerDriver, offer, state: ConstraintState
     ) -> Tuple[List[TaskInfo], ConstraintState]:
         # In dry run satisfy exit-conditions after we got the offer
         if self.dry_run or self.need_to_stop():
             if self.dry_run:
                 tasks, _ = super().tasks_and_state_for_offer(driver, offer, state)
-                paasta_print("Would have launched: ", tasks)
+                print("Would have launched: ", tasks)
             driver.stop()
             return [], state
 
         return super().tasks_and_state_for_offer(driver, offer, state)
 
     def kill_tasks_if_necessary(self, *args, **kwargs):
         return
```

### Comparing `paasta-tools-0.92.1/paasta_tools/frameworks/task_store.py` & `paasta-tools-1.0.0/paasta_tools/frameworks/task_store.py`

 * *Files 0% similar despite different names*

```diff
@@ -170,15 +170,15 @@
         self.zk_client.close()
 
     def get_task(self, task_id: str) -> MesosTaskParameters:
         params, stat = self._get_task(task_id)
         return params
 
     def _get_task(self, task_id: str) -> Tuple[MesosTaskParameters, ZnodeStat]:
-        """Like get_task, but also returns the ZnodeStat that self.zk_client.get() returns """
+        """Like get_task, but also returns the ZnodeStat that self.zk_client.get() returns"""
         try:
             data, stat = self.zk_client.get("/%s" % task_id)
             return MesosTaskParameters.deserialize(data), stat
         except NoNodeError:
             return None, None
         except json.decoder.JSONDecodeError:
             _log(
```

### Comparing `paasta-tools-0.92.1/paasta_tools/frameworks/constraints.py` & `paasta-tools-1.0.0/paasta_tools/frameworks/constraints.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,15 +1,13 @@
 #!/usr/bin/env python
 import re
 from typing import Any
 from typing import Callable
 from typing import Dict
 
-from paasta_tools.utils import paasta_print
-
 
 ConstraintState = Dict[str, Dict[str, Any]]
 ConstraintOp = Callable[[str, str, str, ConstraintState], bool]
 
 
 def max_per(constraint_value, offer_value, attribute, state: ConstraintState):
     if not constraint_value:
@@ -56,25 +54,25 @@
     """Returns True if all constraints are satisfied by offer's attributes,
     returns False otherwise. Prints a error message and re-raises if an error
     was thrown."""
     for (attr, op, val) in constraints:
         try:
             offer_attr = next((x for x in offer.attributes if x.name == attr), None)
             if offer_attr is None:
-                paasta_print("Attribute not found for a constraint: %s" % attr)
+                print("Attribute not found for a constraint: %s" % attr)
                 return False
             elif not (CONS_OPS[op](val, offer_attr.text.value, offer_attr.name, state)):
-                paasta_print(
+                print(
                     "Constraint not satisfied: [{} {} {}] for {} with {}".format(
                         attr, op, val, offer_attr.text.value, state
                     )
                 )
                 return False
         except Exception as err:
-            paasta_print(
+            print(
                 "Error while matching constraint: [{} {} {}] {}".format(
                     attr, op, val, str(err)
                 )
             )
             raise err
 
     return True
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/spark_run.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/spark_run.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,145 +1,258 @@
 import argparse
 import json
 import logging
 import os
+import re
+import shlex
 import socket
 import sys
-import time
-
-from boto3.exceptions import Boto3Error
-from ruamel.yaml import YAML
+from typing import Any
+from typing import Dict
+from typing import List
+from typing import Mapping
+from typing import Optional
+from typing import Tuple
+from typing import Union
+
+from service_configuration_lib import read_service_configuration
+from service_configuration_lib import read_yaml_file
+from service_configuration_lib import spark_config
+from service_configuration_lib.spark_config import get_aws_credentials
+from service_configuration_lib.spark_config import get_grafana_url
+from service_configuration_lib.spark_config import get_resources_requested
+from service_configuration_lib.spark_config import get_spark_hourly_cost
+from service_configuration_lib.spark_config import UnsupportedClusterManagerException
 
 from paasta_tools.cli.cmds.check import makefile_responds_to
 from paasta_tools.cli.cmds.cook_image import paasta_cook_image
 from paasta_tools.cli.utils import get_instance_config
 from paasta_tools.cli.utils import lazy_choices_completer
 from paasta_tools.cli.utils import list_instances
-from paasta_tools.cli.utils import pick_random_port
 from paasta_tools.clusterman import get_clusterman_metrics
-from paasta_tools.mesos_tools import find_mesos_leader
+from paasta_tools.spark_tools import auto_add_timeout_for_spark_job
+from paasta_tools.spark_tools import create_spark_config_str
+from paasta_tools.spark_tools import DEFAULT_SPARK_RUNTIME_TIMEOUT
 from paasta_tools.spark_tools import DEFAULT_SPARK_SERVICE
-from paasta_tools.spark_tools import get_aws_credentials
-from paasta_tools.spark_tools import get_default_event_log_dir
-from paasta_tools.spark_tools import load_mesos_secret_for_spark
+from paasta_tools.spark_tools import get_volumes_from_spark_k8s_configs
+from paasta_tools.spark_tools import get_webui_url
+from paasta_tools.spark_tools import inject_spark_conf_str
 from paasta_tools.utils import _run
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import filter_templates_from_config
+from paasta_tools.utils import get_k8s_url_for_cluster
 from paasta_tools.utils import get_possible_launched_by_user_variable_from_env
 from paasta_tools.utils import get_username
 from paasta_tools.utils import InstanceConfig
+from paasta_tools.utils import is_using_unprivileged_containers
 from paasta_tools.utils import list_services
 from paasta_tools.utils import load_system_paasta_config
 from paasta_tools.utils import NoConfigurationForServiceError
 from paasta_tools.utils import NoDeploymentsAvailable
 from paasta_tools.utils import NoDockerImageError
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import PaastaNotConfiguredError
+from paasta_tools.utils import PoolsNotConfiguredError
 from paasta_tools.utils import SystemPaastaConfig
+from paasta_tools.utils import validate_pool
 
 
 DEFAULT_AWS_REGION = "us-west-2"
 DEFAULT_SPARK_WORK_DIR = "/spark_driver"
 DEFAULT_SPARK_DOCKER_IMAGE_PREFIX = "paasta-spark-run"
 DEFAULT_SPARK_DOCKER_REGISTRY = "docker-dev.yelpcorp.com"
-SENSITIVE_ENV = ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
+SENSITIVE_ENV = ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY", "AWS_SESSION_TOKEN"]
 clusterman_metrics, CLUSTERMAN_YAML_FILE_PATH = get_clusterman_metrics()
+CLUSTER_MANAGER_K8S = "kubernetes"
+CLUSTER_MANAGER_LOCAL = "local"
+CLUSTER_MANAGERS = {CLUSTER_MANAGER_K8S, CLUSTER_MANAGER_LOCAL}
+DEFAULT_DOCKER_SHM_SIZE = "64m"
+# Reference: https://spark.apache.org/docs/latest/configuration.html#application-properties
+DEFAULT_DRIVER_CORES_BY_SPARK = 1
+DEFAULT_DRIVER_MEMORY_BY_SPARK = "1g"
+# Extra room for memory overhead and for any other running inside container
+DOCKER_RESOURCE_ADJUSTMENT_FACTOR = 2
 
+DEFAULT_AWS_PROFILE = "default"
 
-deprecated_opts = {
+DEPRECATED_OPTS = {
     "j": "spark.jars",
     "jars": "spark.jars",
-    "max-cores": "spark.cores.max",
-    "executor-cores": "spark.executor.cores",
-    "executor-memory": "spark.executor.memory",
-    "driver-max-result-size": "spark.driver.maxResultSize",
-    "driver-cores": "spark.driver.cores",
-    "driver-memory": "spark.driver.memory",
 }
 
 SPARK_COMMANDS = {"pyspark", "spark-submit"}
 
 log = logging.getLogger(__name__)
 
 
 class DeprecatedAction(argparse.Action):
+    def __init__(self, option_strings, dest, nargs="?", **kwargs):
+        super().__init__(option_strings, dest, nargs=nargs, **kwargs)
+
     def __call__(self, parser, namespace, values, option_string=None):
-        paasta_print(
+        print(
             PaastaColors.red(
-                "Use of {} is deprecated. Please use {}=value in --spark-args.".format(
-                    option_string, deprecated_opts[option_string.strip("-")]
+                f"Use of {option_string} is deprecated. "
+                + (
+                    f"Please use {DEPRECATED_OPTS.get(option_string.strip('-'), '')}=value in --spark-args."
+                    if option_string.strip("-") in DEPRECATED_OPTS
+                    else ""
                 )
             )
         )
-        sys.exit(1)
 
 
 def add_subparser(subparsers):
     list_parser = subparsers.add_parser(
         "spark-run",
         help="Run Spark on the PaaSTA cluster",
         description=(
             "'paasta spark-run' launches a Spark cluster on PaaSTA. "
             "It analyzes soa-configs and command line arguments to invoke "
             "a 'docker run'. By default, it will pull the Spark service "
             "image from the registry unless the --build option is used.\n\n"
         ),
         formatter_class=argparse.ArgumentDefaultsHelpFormatter,
     )
+    # Deprecated args kept to avoid failures
+    # TODO: Remove these deprecated args later
+    list_parser.add_argument(
+        "--jars",
+        help=argparse.SUPPRESS,
+        action=DeprecatedAction,
+    )
+    list_parser.add_argument(
+        "--executor-memory",
+        help=argparse.SUPPRESS,
+        action=DeprecatedAction,
+    )
+    list_parser.add_argument(
+        "--executor-cores",
+        help=argparse.SUPPRESS,
+        action=DeprecatedAction,
+    )
+    list_parser.add_argument(
+        "--max-cores",
+        help=argparse.SUPPRESS,
+        action=DeprecatedAction,
+    )
+    list_parser.add_argument(
+        "-e",
+        "--enable-compact-bin-packing",
+        help=argparse.SUPPRESS,
+        action=DeprecatedAction,
+    )
+    list_parser.add_argument(
+        "--enable-dra",
+        help=argparse.SUPPRESS,
+        action=DeprecatedAction,
+    )
+    list_parser.add_argument(
+        "--force-use-eks",
+        help=argparse.SUPPRESS,
+        action=DeprecatedAction,
+    )
 
     group = list_parser.add_mutually_exclusive_group()
     group.add_argument(
         "-b",
         "--build",
         help="Build the docker image from scratch using the local Makefile's cook-image target.",
         action="store_true",
         default=False,
     )
     group.add_argument(
         "-I",
         "--image",
         help="Use the provided image to start the Spark driver and executors.",
     )
+    list_parser.add_argument(
+        "--docker-memory-limit",
+        help=(
+            "Set docker memory limit. Should be greater than driver memory. Defaults to 2x spark.driver.memory. Example: 2g, 500m, Max: 64g"
+            " Note: If memory limit provided is greater than associated with the batch instance, it will default to max memory of the box."
+        ),
+        default=None,
+    )
+    list_parser.add_argument(
+        "--docker-cpu-limit",
+        help=(
+            "Set docker cpus limit. Should be greater than driver cores. Defaults to 1x spark.driver.cores."
+            " Note: The job will fail if the limit provided is greater than number of cores present on batch box (8 for production batch boxes)."
+        ),
+        default=None,
+    )
 
     list_parser.add_argument(
+        "--docker-shm-size",
+        help=(
+            "Set docker shared memory size limit for the driver's container. This is the same as setting docker run --shm-size and the shared"
+            " memory is mounted to /dev/shm in the container. Anything written to the shared memory mount point counts towards the docker memory"
+            " limit for the driver's container. Therefore, this should be less than --docker-memory-limit."
+            f" Defaults to {DEFAULT_DOCKER_SHM_SIZE}. Example: 8g, 256m"
+            " Note: this option is mainly useful when training TensorFlow models in the driver, with multiple GPUs using NCCL. The shared memory"
+            f" space is used to sync gradient updates between GPUs during training. The default value of {DEFAULT_DOCKER_SHM_SIZE} is typically not large enough for"
+            " this inter-gpu communication to run efficiently. We recommend a starting value of 8g to ensure that the entire set of model parameters"
+            " can fit in the shared memory. This can be less if you are training a smaller model (<1g parameters) or more if you are using a larger model (>2.5g parameters)"
+            " If you are observing low, average GPU utilization during epoch training (<65-70 percent) you can also try increasing this value; you may be"
+            " resource constrained when GPUs sync training weights between mini-batches (there are other potential bottlenecks that could cause this as well)."
+            " A tool such as nvidia-smi can be use to check GPU utilization."
+            " This option also adds the --ulimit memlock=-1 to the docker run command since this is recommended for TensorFlow applications that use NCCL."
+            " Please refer to docker run documentation for more details on --shm-size and --ulimit memlock=-1."
+        ),
+        default=None,
+    )
+    list_parser.add_argument(
+        "--force-spark-resource-configs",
+        help=(
+            "Skip the resource/instances recalculation. "
+            "This is strongly not recommended."
+        ),
+        action="store_true",
+        default=False,
+    )
+    list_parser.add_argument(
         "--docker-registry",
         help="Docker registry to push the Spark image built.",
-        default=DEFAULT_SPARK_DOCKER_REGISTRY,
+        default=None,
     )
 
     list_parser.add_argument(
         "-s",
         "--service",
         help="The name of the service from which the Spark image is built.",
         default=DEFAULT_SPARK_SERVICE,
     ).completer = lazy_choices_completer(list_services)
 
     list_parser.add_argument(
         "-i",
         "--instance",
-        help=("Start a docker run for a particular instance of the service."),
+        help="Start a docker run for a particular instance of the service.",
         default="adhoc",
     ).completer = lazy_choices_completer(list_instances)
 
     try:
         system_paasta_config = load_system_paasta_config()
+        valid_clusters = system_paasta_config.get_clusters()
         default_spark_cluster = system_paasta_config.get_spark_run_config().get(
             "default_cluster"
         )
         default_spark_pool = system_paasta_config.get_spark_run_config().get(
             "default_pool"
         )
     except PaastaNotConfiguredError:
         default_spark_cluster = "pnw-devc"
         default_spark_pool = "batch"
+        valid_clusters = ["spark-pnw-prod", "pnw-devc"]
 
     list_parser.add_argument(
         "-c",
         "--cluster",
-        help=("The name of the cluster you wish to run Spark on."),
+        help="The name of the cluster you wish to run Spark on.",
+        choices=valid_clusters,
         default=default_spark_cluster,
     )
 
     list_parser.add_argument(
         "-p",
         "--pool",
         help="Name of the resource pool to run the Spark job.",
@@ -164,26 +277,34 @@
     list_parser.add_argument(
         "-C",
         "--cmd",
         help="Run the spark-shell, pyspark, spark-submit, jupyter-lab, or history-server command.",
     )
 
     list_parser.add_argument(
+        "--timeout-job-runtime",
+        type=str,
+        help="Timeout value which will be added before spark-submit. Job will exit if it doesn't finish in given "
+        "runtime. Recommended value: 2 * expected runtime. Example: 1h, 30m",
+        default=DEFAULT_SPARK_RUNTIME_TIMEOUT,
+    )
+
+    list_parser.add_argument(
         "-d",
         "--dry-run",
         help="Shows the arguments supplied to docker as json.",
         action="store_true",
         default=False,
     )
 
     list_parser.add_argument(
         "--spark-args",
-        help="Spark configurations documented in https://spark.apache.org/docs/latest/configuration.html. "
-        r'For example, --spark-args "spark.mesos.constraints=pool:default\;instance_type:m4.10xlarge '
-        'spark.executor.cores=4".',
+        help="Spark configurations documented in https://spark.apache.org/docs/latest/configuration.html, "
+        'separated by space. For example, --spark-args "spark.executor.cores=1 spark.executor.memory=7g '
+        'spark.executor.instances=2".',
     )
 
     list_parser.add_argument(
         "--nvidia",
         help="Use nvidia docker runtime for Spark driver process (requires GPU)",
         action="store_true",
         default=False,
@@ -192,48 +313,34 @@
     list_parser.add_argument(
         "--mrjob",
         help="Pass Spark arguments to invoked command in the format expected by mrjobs",
         action="store_true",
         default=False,
     )
 
-    if clusterman_metrics:
-        list_parser.add_argument(
-            "--suppress-clusterman-metrics-errors",
-            help="Continue even if sending resource requirements to Clusterman fails. This may result in the job "
-            "failing to acquire resources.",
-            action="store_true",
-        )
-
-    list_parser.add_argument(
-        "-j", "--jars", help=argparse.SUPPRESS, action=DeprecatedAction
-    )
-
-    list_parser.add_argument(
-        "--executor-memory", help=argparse.SUPPRESS, action=DeprecatedAction
-    )
-
-    list_parser.add_argument(
-        "--executor-cores", help=argparse.SUPPRESS, action=DeprecatedAction
-    )
-
     list_parser.add_argument(
-        "--max-cores", help=argparse.SUPPRESS, action=DeprecatedAction
+        "--cluster-manager",
+        help="Specify which cluster manager to use. Support for certain cluster managers may be experimental",
+        dest="cluster_manager",
+        choices=CLUSTER_MANAGERS,
+        default=CLUSTER_MANAGER_K8S,
     )
 
     list_parser.add_argument(
-        "--driver-max-result-size", help=argparse.SUPPRESS, action=DeprecatedAction
+        "--tronfig",
+        help="Load the Tron config yaml. Use with --job-id.",
+        type=str,
+        default=None,
     )
 
     list_parser.add_argument(
-        "--driver-memory", help=argparse.SUPPRESS, action=DeprecatedAction
-    )
-
-    list_parser.add_argument(
-        "--driver-cores", help=argparse.SUPPRESS, action=DeprecatedAction
+        "--job-id",
+        help="Tron job id <job_name>.<action_name> in the Tronfig to run. Use wuth --tronfig.",
+        type=str,
+        default=None,
     )
 
     aws_group = list_parser.add_argument_group(
         title="AWS credentials options",
         description="If --aws-credentials-yaml is specified, it overrides all "
         "other options. Otherwise, if -s/--service is specified, spark-run "
         "looks for service credentials in /etc/boto_cfg/[service].yaml. If "
@@ -250,32 +357,54 @@
 
     aws_group.add_argument(
         "--aws-profile",
         help="Name of the AWS profile to load credentials from. Only used when "
         "--aws-credentials-yaml is not specified and --service is either "
         "not specified or the service does not have credentials in "
         "/etc/boto_cfg",
-        default="default",
-    )
-
-    aws_group.add_argument(
-        "--no-aws-credentials",
-        help="Do not load any AWS credentials; allow the Spark job to use its "
-        "own logic to load credentials",
-        action="store_true",
-        default=False,
+        default=DEFAULT_AWS_PROFILE,
     )
 
     aws_group.add_argument(
         "--aws-region",
         help=f"Specify an aws region. If the region is not specified, we will"
         f"default to using {DEFAULT_AWS_REGION}.",
         default=DEFAULT_AWS_REGION,
     )
 
+    aws_group.add_argument(
+        "--assume-aws-role",
+        help=(
+            "Takes an AWS IAM role ARN and attempts to create a session using "
+            "spark_role_assumer"
+        ),
+    )
+
+    aws_group.add_argument(
+        "--aws-role-duration",
+        help=(
+            "Duration in seconds for the role if --assume-aws-role provided. "
+            "The maximum is 43200, but by default, roles may only allow 3600."
+        ),
+        type=int,
+        default=43200,
+    )
+
+    aws_group.add_argument(
+        "--use-web-identity",
+        help=(
+            "If the current environment contains AWS_ROLE_ARN and "
+            "AWS_WEB_IDENTITY_TOKEN_FILE, creates a session to use. These "
+            "ENV vars must be present, and will be in the context of a pod-"
+            "identity enabled pod."
+        ),
+        action="store_true",
+        default=False,
+    )
+
     jupyter_group = list_parser.add_argument_group(
         title="Jupyter kernel culling options",
         description="Idle kernels will be culled by default. Idle "
         "kernels with connections can be overridden not to be culled.",
     )
 
     jupyter_group.add_argument(
@@ -293,49 +422,127 @@
         help="By default, connected idle kernels are culled after timeout. "
         "They can be skipped if not-cull-connected is specified.",
     )
 
     list_parser.set_defaults(command=paasta_spark_run)
 
 
-def get_docker_run_cmd(container_name, volumes, env, docker_img, docker_cmd, nvidia):
+def sanitize_container_name(container_name):
+    # container_name only allows [a-zA-Z0-9][a-zA-Z0-9_.-]
+    return re.sub("[^a-zA-Z0-9_.-]", "_", re.sub("^[^a-zA-Z0-9]+", "", container_name))
+
+
+def get_docker_run_cmd(
+    container_name,
+    volumes,
+    env,
+    docker_img,
+    docker_cmd,
+    nvidia,
+    docker_memory_limit,
+    docker_shm_size,
+    docker_cpu_limit,
+):
+    print(
+        f"Setting docker memory, shared memory, and cpu limits as {docker_memory_limit}, {docker_shm_size}, and {docker_cpu_limit} core(s) respectively."
+    )
     cmd = ["paasta_docker_wrapper", "run"]
+    cmd.append(f"--memory={docker_memory_limit}")
+    if docker_shm_size is not None:
+        cmd.append(f"--shm-size={docker_shm_size}")
+        cmd.append("--ulimit")
+        cmd.append("memlock=-1")
+    cmd.append(f"--cpus={docker_cpu_limit}")
     cmd.append("--rm")
     cmd.append("--net=host")
 
-    sensitive_env = {}
-
     non_interactive_cmd = ["spark-submit", "history-server"]
     if not any(c in docker_cmd for c in non_interactive_cmd):
         cmd.append("--interactive=true")
         if sys.stdout.isatty():
             cmd.append("--tty=true")
 
-    cmd.append("--user=%d:%d" % (os.geteuid(), os.getegid()))
-    cmd.append("--name=%s" % container_name)
+    container_user = (
+        # root inside container == current user outside
+        (0, 0)
+        if is_using_unprivileged_containers()
+        else (os.geteuid(), os.getegid())
+    )
+    cmd.append("--user=%d:%d" % container_user)
+    cmd.append("--name=%s" % sanitize_container_name(container_name))
     for k, v in env.items():
         cmd.append("--env")
         if k in SENSITIVE_ENV:
-            sensitive_env[k] = v
             cmd.append(k)
         else:
             cmd.append(f"{k}={v}")
+    if is_using_unprivileged_containers():
+        cmd.append("--env")
+        cmd.append(f"HOME=/nail/home/{get_username()}")
     if nvidia:
         cmd.append("--env")
         cmd.append("NVIDIA_VISIBLE_DEVICES=all")
         cmd.append("--runtime=nvidia")
     for volume in volumes:
         cmd.append("--volume=%s" % volume)
     cmd.append("%s" % docker_img)
     cmd.extend(("sh", "-c", docker_cmd))
-    cmd.append(sensitive_env)
 
     return cmd
 
 
+def get_docker_image(
+    args: argparse.Namespace, instance_config: InstanceConfig
+) -> Optional[str]:
+    """
+    Since the Docker image digest used to launch the Spark cluster is obtained by inspecting local
+    Docker images, we need to ensure that the Docker image exists locally or is pulled in all scenarios.
+    """
+    # docker image is built locally then pushed
+    if args.build:
+        return build_and_push_docker_image(args)
+
+    docker_url = ""
+    if args.image:
+        docker_url = args.image
+    else:
+        try:
+            docker_url = instance_config.get_docker_url()
+        except NoDockerImageError:
+            print(
+                PaastaColors.red(
+                    "Error: No sha has been marked for deployment for the %s deploy group.\n"
+                    "Please ensure this service has either run through a jenkins pipeline "
+                    "or paasta mark-for-deployment has been run for %s\n"
+                    % (instance_config.get_deploy_group(), args.service)
+                ),
+                sep="",
+                file=sys.stderr,
+            )
+            return None
+
+    print(
+        "Please wait while the image (%s) is pulled (times out after 5m)..."
+        % docker_url,
+        file=sys.stderr,
+    )
+    # Need sudo for credentials when pulling images from paasta docker registry (docker-paasta.yelpcorp.com)
+    # However, in CI env, we can't connect to docker via root and we can pull with user `jenkins`
+    is_ci_env = "CI" in os.environ
+    cmd_prefix = "" if is_ci_env else "sudo -H "
+    retcode, _ = _run(f"{cmd_prefix}docker pull {docker_url}", stream=True, timeout=300)
+    if retcode != 0:
+        print(
+            "\nPull failed. Are you authorized to run docker commands?",
+            file=sys.stderr,
+        )
+        return None
+    return docker_url
+
+
 def get_smart_paasta_instance_name(args):
     if os.environ.get("TRON_JOB_NAMESPACE"):
         tron_job = os.environ.get("TRON_JOB_NAME")
         tron_action = os.environ.get("TRON_ACTION")
         return f"{tron_job}.{tron_action}"
     else:
         how_submitted = None
@@ -346,395 +553,322 @@
                 if spark_cmd in args.cmd:
                     how_submitted = spark_cmd
                     break
         how_submitted = how_submitted or "other"
         return f"{args.instance}_{get_username()}_{how_submitted}"
 
 
-def get_spark_env(args, spark_conf, spark_ui_port, access_key, secret_key):
-    spark_env = {}
+def get_spark_env(
+    args: argparse.Namespace,
+    spark_conf_str: str,
+    aws_creds: Tuple[Optional[str], Optional[str], Optional[str]],
+    ui_port: str,
+    system_paasta_config: SystemPaastaConfig,
+) -> Dict[str, str]:
+    """Create the env config dict to configure on the docker container"""
 
-    if access_key is not None:
+    spark_env = {}
+    access_key, secret_key, session_token = aws_creds
+    if access_key:
         spark_env["AWS_ACCESS_KEY_ID"] = access_key
         spark_env["AWS_SECRET_ACCESS_KEY"] = secret_key
-        spark_env["AWS_DEFAULT_REGION"] = args.aws_region
+        if session_token is not None:
+            spark_env["AWS_SESSION_TOKEN"] = session_token
+
+    spark_env["AWS_DEFAULT_REGION"] = args.aws_region
     spark_env["PAASTA_LAUNCHED_BY"] = get_possible_launched_by_user_variable_from_env()
     spark_env["PAASTA_INSTANCE_TYPE"] = "spark"
 
     # Run spark (and mesos framework) as root.
     spark_env["SPARK_USER"] = "root"
-    spark_env["SPARK_OPTS"] = spark_conf
+    spark_env["SPARK_OPTS"] = spark_conf_str
 
     # Default configs to start the jupyter notebook server
     if args.cmd == "jupyter-lab":
         spark_env["JUPYTER_RUNTIME_DIR"] = "/source/.jupyter"
         spark_env["JUPYTER_DATA_DIR"] = "/source/.jupyter"
         spark_env["JUPYTER_CONFIG_DIR"] = "/source/.jupyter"
     elif args.cmd == "history-server":
         dirs = args.work_dir.split(":")
         spark_env["SPARK_LOG_DIR"] = dirs[1]
         if not args.spark_args or not args.spark_args.startswith(
             "spark.history.fs.logDirectory"
         ):
-            paasta_print(
+            print(
                 "history-server requires spark.history.fs.logDirectory in spark-args",
                 file=sys.stderr,
             )
             sys.exit(1)
-        spark_env["SPARK_HISTORY_OPTS"] = "-D%s -Dspark.history.ui.port=%d" % (
-            args.spark_args,
-            spark_ui_port,
+        spark_env["SPARK_HISTORY_OPTS"] = (
+            f"-D{args.spark_args} " f"-Dspark.history.ui.port={ui_port}"
         )
         spark_env["SPARK_DAEMON_CLASSPATH"] = "/opt/spark/extra_jars/*"
         spark_env["SPARK_NO_DAEMONIZE"] = "true"
 
-    return spark_env
-
-
-def get_spark_config(
-    args,
-    spark_app_name,
-    spark_ui_port,
-    docker_img,
-    system_paasta_config,
-    volumes,
-    access_key,
-    secret_key,
-):
-    # User configurable Spark options
-    user_args = {
-        "spark.app.name": spark_app_name,
-        "spark.cores.max": "4",
-        "spark.executor.cores": "2",
-        "spark.executor.memory": "4g",
-        # Use \; for multiple constraints. e.g.
-        # instance_type:m4.10xlarge\;pool:default
-        "spark.mesos.constraints": "pool:%s" % args.pool,
-        "spark.mesos.executor.docker.forcePullImage": "true",
-    }
-
-    default_event_log_dir = get_default_event_log_dir(
-        access_key=access_key, secret_key=secret_key
-    )
-    if default_event_log_dir is not None:
-        user_args["spark.eventLog.enabled"] = "true"
-        user_args["spark.eventLog.dir"] = default_event_log_dir
+    spark_env["KUBECONFIG"] = system_paasta_config.get_spark_kubeconfig()
 
-    # Spark options managed by PaaSTA
-    mesos_address = find_mesos_leader(args.cluster)
-    paasta_instance = get_smart_paasta_instance_name(args)
-    non_user_args = {
-        "spark.master": "mesos://%s" % mesos_address,
-        "spark.ui.port": spark_ui_port,
-        "spark.executorEnv.PAASTA_SERVICE": args.service,
-        "spark.executorEnv.PAASTA_INSTANCE": paasta_instance,
-        "spark.executorEnv.PAASTA_CLUSTER": args.cluster,
-        "spark.executorEnv.PAASTA_INSTANCE_TYPE": "spark",
-        "spark.mesos.executor.docker.parameters": f"label=paasta_service={args.service},label=paasta_instance={paasta_instance}",
-        "spark.mesos.executor.docker.volumes": ",".join(volumes),
-        "spark.mesos.executor.docker.image": docker_img,
-        "spark.mesos.principal": "spark",
-        "spark.mesos.secret": load_mesos_secret_for_spark(),
-    }
+    return spark_env
 
-    if not args.build and not args.image:
-        non_user_args["spark.mesos.uris"] = "file:///root/.dockercfg"
 
-    if args.spark_args:
-        spark_args = args.spark_args.split()
-        for spark_arg in spark_args:
+def _parse_user_spark_args(
+    spark_args: str,
+) -> Dict[str, str]:
+
+    user_spark_opts = {}
+    if spark_args:
+        for spark_arg in spark_args.split():
             fields = spark_arg.split("=", 1)
             if len(fields) != 2:
-                paasta_print(
+                print(
                     PaastaColors.red(
                         "Spark option %s is not in format option=value." % spark_arg
                     ),
                     file=sys.stderr,
                 )
                 sys.exit(1)
+            user_spark_opts[fields[0]] = fields[1]
 
-            if fields[0] in non_user_args:
-                paasta_print(
-                    PaastaColors.red(
-                        "Spark option {} is set by PaaSTA with {}.".format(
-                            fields[0], non_user_args[fields[0]]
-                        )
-                    ),
-                    file=sys.stderr,
-                )
-                sys.exit(1)
-            # Update default configuration
-            user_args[fields[0]] = fields[1]
-
-    if "spark.sql.shuffle.partitions" not in user_args:
-        num_partitions = str(2 * int(user_args["spark.cores.max"]))
-        user_args["spark.sql.shuffle.partitions"] = num_partitions
-        paasta_print(
-            PaastaColors.yellow(
-                f"Warning: spark.sql.shuffle.partitions has been set to"
-                f" {num_partitions} to be equal to twice the number of "
-                f"requested cores, but you should consider setting a "
-                f"higher value if necessary."
-            )
-        )
-
-    if int(user_args["spark.cores.max"]) < int(user_args["spark.executor.cores"]):
-        paasta_print(
-            PaastaColors.red(
-                "Total number of cores {} is less than per-executor cores {}.".format(
-                    user_args["spark.cores.max"], user_args["spark.executor.cores"]
-                )
-            ),
-            file=sys.stderr,
-        )
-        sys.exit(1)
-
-    exec_mem = user_args["spark.executor.memory"]
-    if exec_mem[-1] != "g" or not exec_mem[:-1].isdigit() or int(exec_mem[:-1]) > 32:
-        paasta_print(
-            PaastaColors.red(
-                "Executor memory {} not in format dg (d<=32).".format(
-                    user_args["spark.executor.memory"]
-                )
-            ),
-            file=sys.stderr,
-        )
-        sys.exit(1)
-
-    # Limit a container's cpu usage
-    non_user_args["spark.mesos.executor.docker.parameters"] += ",cpus={}".format(
-        user_args["spark.executor.cores"]
-    )
-
-    return dict(non_user_args, **user_args)
-
-
-def create_spark_config_str(spark_config_dict, is_mrjob):
-    conf_option = "--jobconf" if is_mrjob else "--conf"
-    spark_config_entries = list()
-
-    if is_mrjob:
-        spark_master = spark_config_dict.pop("spark.master")
-        spark_config_entries.append(f"--spark-master={spark_master}")
-
-    for opt, val in spark_config_dict.items():
-        spark_config_entries.append(f"{conf_option} {opt}={val}")
-    return " ".join(spark_config_entries)
-
-
-def emit_resource_requirements(spark_config_dict, paasta_cluster, webui_url):
-    num_executors = int(spark_config_dict["spark.cores.max"]) / int(
-        spark_config_dict["spark.executor.cores"]
-    )
-    memory_per_executor = calculate_memory_per_executor(
-        spark_config_dict["spark.executor.memory"],
-        spark_config_dict.get("spark.mesos.executor.memoryOverhead"),
-    )
-
-    desired_resources = {
-        "cpus": int(spark_config_dict["spark.cores.max"]),
-        "mem": memory_per_executor * num_executors,
-        "disk": memory_per_executor
-        * num_executors,  # rough guess since spark does not collect this information
-    }
-    dimensions = {
-        "framework_name": spark_config_dict["spark.app.name"],
-        "webui_url": webui_url,
-    }
-
-    constraints = parse_constraints_string(spark_config_dict["spark.mesos.constraints"])
-    pool = constraints["pool"]
-
-    paasta_print("Sending resource request metrics to Clusterman")
-    aws_region = get_aws_region_for_paasta_cluster(paasta_cluster)
-    metrics_client = clusterman_metrics.ClustermanMetricsBotoClient(
-        region_name=aws_region, app_identifier=pool
-    )
-
-    estimated_cost = clusterman_metrics.util.costs.estimate_cost_per_hour(
-        cluster=paasta_cluster,
-        pool=pool,
-        cpus=desired_resources["cpus"],
-        mem=desired_resources["mem"],
-    )
-    message = "Resource request ({} cpus and {} MB memory total) is estimated to cost ${} per hour".format(
-        desired_resources["cpus"], desired_resources["mem"], estimated_cost
-    )
-    if clusterman_metrics.util.costs.should_warn(estimated_cost):
-        message = "WARNING: " + message
-        paasta_print(PaastaColors.red(message))
-    else:
-        paasta_print(message)
-
-    with metrics_client.get_writer(
-        clusterman_metrics.APP_METRICS, aggregate_meteorite_dims=True
-    ) as writer:
-        for resource, desired_quantity in desired_resources.items():
-            metric_key = clusterman_metrics.generate_key_with_dimensions(
-                f"requested_{resource}", dimensions
-            )
-            writer.send((metric_key, int(time.time()), desired_quantity))
-
-
-def get_aws_region_for_paasta_cluster(paasta_cluster):
-    with open(CLUSTERMAN_YAML_FILE_PATH, "r") as clusterman_yaml_file:
-        clusterman_yaml = YAML().load(clusterman_yaml_file.read())
-        return clusterman_yaml["clusters"][paasta_cluster]["aws_region"]
-
-
-def calculate_memory_per_executor(spark_memory_string, memory_overhead):
-    # expected to be in format "dg" where d is an integer
-    base_memory_per_executor = 1024 * int(spark_memory_string[:-1])
-
-    # by default, spark adds an overhead of 10% of the executor memory, with
-    # a minimum of 384mb
-    if memory_overhead is None:
-        memory_overhead = max(384, int(0.1 * base_memory_per_executor))
-    else:
-        memory_overhead = int(memory_overhead)
-
-    return base_memory_per_executor + memory_overhead
-
-
-def parse_constraints_string(constraints_string):
-    constraints = {}
-    for constraint in constraints_string.split(";"):
-        if constraint[-1] == "\\":
-            constraint = constraint[:-1]
-        k, v = constraint.split(":")
-        constraints[k] = v
-
-    return constraints
+    return user_spark_opts
 
 
 def run_docker_container(
-    container_name, volumes, environment, docker_img, docker_cmd, dry_run, nvidia
+    container_name,
+    volumes,
+    environment,
+    docker_img,
+    docker_cmd,
+    dry_run,
+    nvidia,
+    docker_memory_limit,
+    docker_shm_size,
+    docker_cpu_limit,
 ) -> int:
+
     docker_run_args = dict(
         container_name=container_name,
         volumes=volumes,
         env=environment,
         docker_img=docker_img,
         docker_cmd=docker_cmd,
         nvidia=nvidia,
+        docker_memory_limit=docker_memory_limit,
+        docker_shm_size=docker_shm_size,
+        docker_cpu_limit=docker_cpu_limit,
     )
     docker_run_cmd = get_docker_run_cmd(**docker_run_args)
-
     if dry_run:
-        paasta_print(json.dumps(docker_run_cmd))
+        print(json.dumps(docker_run_cmd))
         return 0
 
-    os.execlpe("paasta_docker_wrapper", *docker_run_cmd)
+    merged_env = {**os.environ, **environment}
+    os.execlpe("paasta_docker_wrapper", *docker_run_cmd, merged_env)
     return 0
 
 
+def get_spark_app_name(original_docker_cmd: Union[Any, str, List[str]]) -> str:
+    """Use submitted batch name as default spark_run job name"""
+    docker_cmds = (
+        shlex.split(original_docker_cmd)
+        if isinstance(original_docker_cmd, str)
+        else original_docker_cmd
+    )
+    spark_app_name = None
+    after_spark_submit = False
+    for arg in docker_cmds:
+        if arg == "spark-submit":
+            after_spark_submit = True
+        elif after_spark_submit and arg.endswith(".py"):
+            batch_name = arg.split("/")[-1].replace(".py", "")
+            spark_app_name = "paasta_" + batch_name
+            break
+        elif arg == "jupyter-lab":
+            spark_app_name = "paasta_jupyter"
+            break
+
+    if spark_app_name is None:
+        spark_app_name = "paasta_spark_run"
+
+    spark_app_name += f"_{get_username()}"
+
+    return spark_app_name
+
+
+def _calculate_docker_memory_limit(
+    spark_conf: Mapping[str, str], memory_limit: Optional[str]
+) -> str:
+    """In Order of preference:
+    1. Argument: --docker-memory-limit
+    2. --spark-args or spark-submit: spark.driver.memory
+    3. Default
+    """
+    if memory_limit:
+        return memory_limit
+
+    try:
+        docker_memory_limit_str = spark_conf.get(
+            "spark.driver.memory", DEFAULT_DRIVER_MEMORY_BY_SPARK
+        )
+        adjustment_factor = DOCKER_RESOURCE_ADJUSTMENT_FACTOR
+        match = re.match(r"([0-9]+)([a-z]*)", docker_memory_limit_str)
+        memory_val = int(match[1]) * adjustment_factor
+        memory_unit = match[2]
+        docker_memory_limit = f"{memory_val}{memory_unit}"
+    except Exception as e:
+        # For any reason it fails, continue with default value
+        print(
+            f"ERROR: Failed to parse docker memory limit. Error: {e}. Example values: 1g, 200m."
+        )
+        raise
+
+    return docker_memory_limit
+
+
+def _calculate_docker_shared_memory_size(shm_size: Optional[str]) -> str:
+    """In Order of preference:
+    1. Argument: --docker-shm-size
+    3. Default
+    """
+    if shm_size:
+        return shm_size
+
+    return DEFAULT_DOCKER_SHM_SIZE
+
+
+def _calculate_docker_cpu_limit(
+    spark_conf: Mapping[str, str], cpu_limit: Optional[str]
+) -> str:
+    """In Order of preference:
+    1. Argument: --docker-cpu-limit
+    2. --spark-args or spark-submit: spark.driver.cores
+    3. Default
+    """
+    return (
+        cpu_limit
+        if cpu_limit
+        else spark_conf.get("spark.driver.cores", str(DEFAULT_DRIVER_CORES_BY_SPARK))
+    )
+
+
 def configure_and_run_docker_container(
     args: argparse.Namespace,
     docker_img: str,
     instance_config: InstanceConfig,
     system_paasta_config: SystemPaastaConfig,
+    spark_conf: Dict[str, str],
+    aws_creds: Tuple[Optional[str], Optional[str], Optional[str]],
+    cluster_manager: str,
+    pod_template_path: str,
+    extra_driver_envs: Dict[str, str] = dict(),
 ) -> int:
-    volumes = list()
-    for volume in instance_config.get_volumes(system_paasta_config.get_volumes()):
-        if os.path.exists(volume["hostPath"]):
-            volumes.append(
-                "{}:{}:{}".format(
-                    volume["hostPath"], volume["containerPath"], volume["mode"].lower()
-                )
-            )
-        else:
-            paasta_print(
-                PaastaColors.yellow(
-                    "Warning: Path %s does not exist on this host. Skipping this binding."
-                    % volume["hostPath"]
-                ),
-                file=sys.stderr,
-            )
-
-    spark_ui_port = pick_random_port(args.service + str(os.getpid()))
-    spark_app_name = "paasta_spark_run_{}".format(get_username())
-    container_name = spark_app_name + "_" + str(spark_ui_port)
-    original_docker_cmd = args.cmd or instance_config.get_cmd()
-    if "jupyter" not in original_docker_cmd:
-        spark_app_name = container_name
-
-    access_key, secret_key = get_aws_credentials(
-        service=args.service,
-        no_aws_credentials=args.no_aws_credentials,
-        aws_credentials_yaml=args.aws_credentials_yaml,
-        profile_name=args.aws_profile,
+    docker_memory_limit = _calculate_docker_memory_limit(
+        spark_conf, args.docker_memory_limit
     )
-    spark_config_dict = get_spark_config(
-        args=args,
-        spark_app_name=spark_app_name,
-        spark_ui_port=spark_ui_port,
-        docker_img=docker_img,
-        system_paasta_config=system_paasta_config,
-        volumes=volumes,
-        access_key=access_key,
-        secret_key=secret_key,
+    docker_shm_size = _calculate_docker_shared_memory_size(args.docker_shm_size)
+    docker_cpu_limit = _calculate_docker_cpu_limit(
+        spark_conf,
+        args.docker_cpu_limit,
     )
-    spark_conf_str = create_spark_config_str(spark_config_dict, is_mrjob=args.mrjob)
 
-    # Spark client specific volumes
+    if cluster_manager in {CLUSTER_MANAGER_K8S, CLUSTER_MANAGER_LOCAL}:
+        # service_configuration_lib puts volumes into the k8s
+        # configs for local mode
+        volumes = get_volumes_from_spark_k8s_configs(spark_conf)
+    else:
+        raise UnsupportedClusterManagerException(cluster_manager)
+
     volumes.append("%s:rw" % args.work_dir)
-    volumes.append("/etc/passwd:/etc/passwd:ro")
-    volumes.append("/etc/group:/etc/group:ro")
     volumes.append("/nail/home:/nail/home:rw")
 
-    environment = instance_config.get_env_dictionary()
-    environment.update(
-        get_spark_env(args, spark_conf_str, spark_ui_port, access_key, secret_key)
+    if pod_template_path:
+        volumes.append(f"{pod_template_path}:{pod_template_path}:rw")
+
+    volumes.append(
+        f"{system_paasta_config.get_spark_kubeconfig()}:{system_paasta_config.get_spark_kubeconfig()}:ro"
     )
 
-    webui_url = f"http://{socket.getfqdn()}:{spark_ui_port}"
+    environment = instance_config.get_env_dictionary()  # type: ignore
+    spark_conf_str = create_spark_config_str(spark_conf, is_mrjob=args.mrjob)
+    environment.update(
+        get_spark_env(
+            args=args,
+            spark_conf_str=spark_conf_str,
+            aws_creds=aws_creds,
+            ui_port=spark_conf["spark.ui.port"],
+            system_paasta_config=system_paasta_config,
+        )
+    )  # type:ignore
+    environment.update(extra_driver_envs)
+
+    webui_url = get_webui_url(spark_conf["spark.ui.port"])
+    webui_url_msg = PaastaColors.green(f"\nSpark monitoring URL: ") + f"{webui_url}\n"
 
     docker_cmd = get_docker_cmd(args, instance_config, spark_conf_str)
     if "history-server" in docker_cmd:
-        paasta_print(f"\nSpark history server URL {webui_url}\n")
+        print(PaastaColors.green(f"\nSpark history server URL: ") + f"{webui_url}\n")
     elif any(c in docker_cmd for c in ["pyspark", "spark-shell", "spark-submit"]):
-        paasta_print(f"\nSpark monitoring URL {webui_url}\n")
-
-    if clusterman_metrics and _should_emit_resource_requirements(
-        docker_cmd, args.mrjob
-    ):
-        try:
-            emit_resource_requirements(spark_config_dict, args.cluster, webui_url)
-        except Boto3Error as e:
-            paasta_print(
-                PaastaColors.red(
-                    f"Encountered {e} while attempting to send resource requirements to Clusterman."
-                )
+        grafana_url = get_grafana_url(spark_conf)
+        dashboard_url_msg = (
+            PaastaColors.green(f"\nGrafana dashboard: ") + f"{grafana_url}\n"
+        )
+        print(webui_url_msg)
+        print(dashboard_url_msg)
+        log.info(webui_url_msg)
+        log.info(dashboard_url_msg)
+        spark_conf_builder = spark_config.SparkConfBuilder()
+        history_server_url = spark_conf_builder.get_history_url(spark_conf)
+        if history_server_url:
+            history_server_url_msg = (
+                f"\nAfter the job is finished, you can find the spark UI from {history_server_url}\n"
+                "Check y/spark-recent-history for faster access to prod logs\n"
             )
-            if args.suppress_clusterman_metrics_errors:
-                paasta_print(
-                    "Continuing anyway since --suppress-clusterman-metrics-errors was passed"
-                )
-            else:
-                raise
+            print(history_server_url_msg)
+            log.info(history_server_url_msg)
+    print(f"Selected cluster manager: {cluster_manager}\n")
+
+    if clusterman_metrics and _should_get_resource_requirements(docker_cmd, args.mrjob):
+        resources = get_resources_requested(spark_conf)
+        hourly_cost = get_spark_hourly_cost(
+            clusterman_metrics,
+            resources,
+            spark_conf["spark.executorEnv.PAASTA_CLUSTER"],
+            args.pool,
+        )
+        message = (
+            f"Resource request ({resources['cpus']} cpus and {resources['mem']} MB memory total)"
+            f" is estimated to cost ${hourly_cost} per hour"
+        )
+        if clusterman_metrics.util.costs.should_warn(hourly_cost):
+            print(PaastaColors.red(f"WARNING: {message}"))
+        else:
+            print(message)
 
     return run_docker_container(
-        container_name=container_name,
+        container_name=spark_conf["spark.app.name"],
         volumes=volumes,
         environment=environment,
         docker_img=docker_img,
         docker_cmd=docker_cmd,
         dry_run=args.dry_run,
         nvidia=args.nvidia,
+        docker_memory_limit=docker_memory_limit,
+        docker_shm_size=docker_shm_size,
+        docker_cpu_limit=docker_cpu_limit,
     )
 
 
-def _should_emit_resource_requirements(docker_cmd, is_mrjob):
+def _should_get_resource_requirements(docker_cmd: str, is_mrjob: bool) -> bool:
     return is_mrjob or any(
         c in docker_cmd for c in ["pyspark", "spark-shell", "spark-submit"]
     )
 
 
-def get_docker_cmd(args, instance_config, spark_conf_str):
-    original_docker_cmd = args.cmd or instance_config.get_cmd()
+def get_docker_cmd(
+    args: argparse.Namespace, instance_config: InstanceConfig, spark_conf_str: str
+) -> str:
+    original_docker_cmd = str(args.cmd or instance_config.get_cmd())
 
     if args.mrjob:
         return original_docker_cmd + " " + spark_conf_str
     # Default cli options to start the jupyter notebook server.
     elif original_docker_cmd == "jupyter-lab":
         cull_opts = (
             "--MappingKernelManager.cull_idle_timeout=%s " % args.cull_idle_timeout
@@ -746,30 +880,33 @@
             get_username(), socket.getfqdn(), cull_opts
         )
     elif original_docker_cmd == "history-server":
         return "start-history-server.sh"
     # Spark options are passed as options to pyspark and spark-shell.
     # For jupyter, environment variable SPARK_OPTS is set instead.
     else:
-        for base_cmd in ("pyspark", "spark-shell", "spark-submit"):
-            if base_cmd in original_docker_cmd:
-                return original_docker_cmd.replace(
-                    base_cmd, base_cmd + " " + spark_conf_str, 1
-                )
-        return original_docker_cmd
+        return inject_spark_conf_str(original_docker_cmd, spark_conf_str)
 
 
-def build_and_push_docker_image(args):
+def _get_adhoc_docker_registry(service: str, soa_dir: str = DEFAULT_SOA_DIR) -> str:
+    if service is None:
+        raise NotImplementedError('"None" is not a valid service')
+
+    service_configuration = read_service_configuration(service, soa_dir)
+    return service_configuration.get("docker_registry", DEFAULT_SPARK_DOCKER_REGISTRY)
+
+
+def build_and_push_docker_image(args: argparse.Namespace) -> Optional[str]:
     """
     Build an image if the default Spark service image is not preferred.
     The image needs to be pushed to a registry for the Spark executors
     to pull.
     """
     if not makefile_responds_to("cook-image"):
-        paasta_print(
+        print(
             "A local Makefile with a 'cook-image' target is required for --build",
             file=sys.stderr,
         )
         return None
 
     default_tag = "{}-{}".format(DEFAULT_SPARK_DOCKER_IMAGE_PREFIX, get_username())
     docker_tag = os.environ.get("DOCKER_TAG", default_tag)
@@ -777,143 +914,337 @@
 
     cook_return = paasta_cook_image(
         args=None, service=args.service, soa_dir=args.yelpsoa_config_root
     )
     if cook_return != 0:
         return None
 
-    docker_url = f"{args.docker_registry}/{docker_tag}"
+    registry_uri = args.docker_registry or _get_adhoc_docker_registry(
+        service=args.service,
+        soa_dir=args.yelpsoa_config_root,
+    )
+
+    docker_url = f"{registry_uri}/{docker_tag}"
     command = f"docker tag {docker_tag} {docker_url}"
-    paasta_print(PaastaColors.grey(command))
+    print(PaastaColors.grey(command))
     retcode, _ = _run(command, stream=True)
     if retcode != 0:
         return None
 
-    if args.docker_registry != DEFAULT_SPARK_DOCKER_REGISTRY:
+    if registry_uri != DEFAULT_SPARK_DOCKER_REGISTRY:
         command = "sudo -H docker push %s" % docker_url
     else:
         command = "docker push %s" % docker_url
 
-    paasta_print(PaastaColors.grey(command))
-    retcode, output = _run(command, stream=True)
+    print(PaastaColors.grey(command))
+    retcode, output = _run(command, stream=False)
     if retcode != 0:
         return None
 
-    return docker_url
+    # With unprivileged docker, the digest on the remote registry may not match the digest
+    # in the local environment. Because of this, we have to parse the digest message from the
+    # server response and use downstream when launching spark executors
+
+    # Output from `docker push` with unprivileged docker looks like
+    #  Using default tag: latest
+    #  The push refers to repository [docker-dev.yelpcorp.com/paasta-spark-run-dpopes:latest]
+    #  latest: digest: sha256:0a43aa65174a400bd280d48d460b73eb49b0ded4072c9e173f919543bf693557
+
+    # With privileged docker, the last line has an extra "size: 123"
+    #  latest: digest: sha256:0a43aa65174a400bd280d48d460b73eb49b0ded4072c9e173f919543bf693557 size: 52
+
+    digest_line = output.split("\n")[-1]
+    digest_match = re.match(r"[^:]*: [^:]*: (?P<digest>[^\s]*)", digest_line)
+    if not digest_match:
+        raise ValueError(f"Could not determine digest from output: {output}")
+    digest = digest_match.group("digest")
+
+    image_url = f"{docker_url}@{digest}"
+
+    # If the local digest doesn't match the remote digest AND the registry is
+    # non-default (which requires requires authentication, and consequently sudo),
+    # downstream `docker run` commands will fail trying to authenticate.
+    # To work around this, we can proactively `sudo docker pull` here so that
+    # the image exists locally and can be `docker run` without sudo
+    if registry_uri != DEFAULT_SPARK_DOCKER_REGISTRY:
+        command = f"sudo -H docker pull {image_url}"
+        print(PaastaColors.grey(command))
+        retcode, output = _run(command, stream=False)
+        if retcode != 0:
+            raise NoDockerImageError(f"Could not pull {image_url}: {output}")
+
+    return image_url
 
 
 def validate_work_dir(s):
     dirs = s.split(":")
     if len(dirs) != 2:
-        paasta_print(
+        print(
             "work-dir %s is not in format local_abs_dir:container_abs_dir" % s,
             file=sys.stderr,
         )
         sys.exit(1)
 
     for d in dirs:
         if not os.path.isabs(d):
-            paasta_print("%s is not an absolute path" % d, file=sys.stderr)
+            print("%s is not an absolute path" % d, file=sys.stderr)
             sys.exit(1)
 
 
-def paasta_spark_run(args):
+def parse_tronfig(tronfig_path: str, job_id: str) -> Optional[Dict[str, Any]]:
+    splitted = job_id.split(".")
+    if len(splitted) != 2:
+        return None
+    job_name, action_name = splitted
+
+    file_content = read_yaml_file(tronfig_path)
+    jobs = filter_templates_from_config(file_content)
+    if job_name not in jobs or action_name not in jobs[job_name].get("actions", {}):
+        return None
+    return jobs[job_name]["actions"][action_name]
+
+
+def update_args_from_tronfig(args: argparse.Namespace) -> Optional[Dict[str, str]]:
+    """
+    Load and check the following config fields from the provided Tronfig.
+      - executor
+      - pool
+      - iam_role
+      - iam_role_provider
+      - force_spark_resource_configs
+      - max_runtime
+      - command
+      - env
+      - spark_args
+
+    Returns: environment variables dictionary or None if failed.
+    """
+    action_dict = parse_tronfig(args.tronfig, args.job_id)
+    if action_dict is None:
+        print(
+            PaastaColors.red(f"Unable to get configs from job-id: {args.job_id}"),
+            file=sys.stderr,
+        )
+        return None
+
+    # executor === spark
+    if action_dict.get("executor", "") != "spark":
+        print(
+            PaastaColors.red("Invalid Tronfig: executor should be 'spark'"),
+            file=sys.stderr,
+        )
+        return None
+
+    # iam_role / aws_profile
+    if (
+        "iam_role" in action_dict
+        and action_dict.get("iam_role_provider", "aws") != "aws"
+    ):
+        print(
+            PaastaColors.red("Invalid Tronfig: iam_role_provider should be 'aws'"),
+            file=sys.stderr,
+        )
+        return None
+
+    # Other args: map Tronfig YAML fields to spark-run CLI args
+    fields_to_args = {
+        "pool": "pool",
+        "iam_role": "assume_aws_role",
+        "force_spark_resource_configs": "force_spark_resource_configs",
+        "max_runtime": "timeout_job_runtime",
+        "command": "cmd",
+        "spark_args": "spark_args",
+    }
+    for field_name, arg_name in fields_to_args.items():
+        if field_name in action_dict:
+            value = action_dict[field_name]
+
+            # Convert spark_args values from dict to a string "k1=v1 k2=v2"
+            if field_name == "spark_args":
+                value = " ".join([f"{k}={v}" for k, v in dict(value).items()])
+
+            # Befutify for printing
+            arg_name_str = (f"--{arg_name.replace('_', '-')}").ljust(31, " ")
+            field_name_str = field_name.ljust(28)
+
+            # Only load iam_role value if --aws-profile is not set
+            if field_name == "iam_role" and args.aws_profile != DEFAULT_AWS_PROFILE:
+                print(
+                    PaastaColors.yellow(
+                        f"Overwriting args with Tronfig: {arg_name_str} => {field_name_str} : IGNORE, "
+                        "since --aws-profile is provided"
+                    ),
+                )
+                continue
+
+            if hasattr(args, arg_name):
+                print(
+                    PaastaColors.yellow(
+                        f"Overwriting args with Tronfig: {arg_name_str} => {field_name_str} : {value}"
+                    ),
+                )
+            setattr(args, arg_name, value)
+
+    # env (currently paasta spark-run does not support Spark driver secrets environment variables)
+    return action_dict.get("env", dict())
+
+
+def paasta_spark_run(args: argparse.Namespace) -> int:
+    driver_envs_from_tronfig: Dict[str, str] = dict()
+    if args.tronfig is not None:
+        if args.job_id is None:
+            print(
+                PaastaColors.red("Missing --job-id when --tronfig is provided"),
+                file=sys.stderr,
+            )
+            return False
+        driver_envs_from_tronfig = update_args_from_tronfig(args)
+        if driver_envs_from_tronfig is None:
+            return False
+
     # argparse does not work as expected with both default and
     # type=validate_work_dir.
     validate_work_dir(args.work_dir)
 
     try:
         system_paasta_config = load_system_paasta_config()
     except PaastaNotConfiguredError:
-        paasta_print(
+        print(
             PaastaColors.yellow(
                 "Warning: Couldn't load config files from '/etc/paasta'. This indicates"
                 "PaaSTA is not configured locally on this host, and local-run may not behave"
                 "the same way it would behave on a server configured for PaaSTA."
             ),
             sep="\n",
         )
         system_paasta_config = SystemPaastaConfig({"volumes": []}, "/etc/paasta")
 
+    if args.cmd == "jupyter-lab" and not args.build and not args.image:
+        print(
+            PaastaColors.red(
+                "The jupyter-lab command requires a prebuilt image with -I or --image."
+            ),
+            file=sys.stderr,
+        )
+        return 1
+
+    # validate pool
+    try:
+        if not validate_pool(args.cluster, args.pool, system_paasta_config):
+            print(
+                PaastaColors.red(
+                    f"Invalid --pool value. List of valid pools for cluster `{args.cluster}`: "
+                    f"{system_paasta_config.get_pools_for_cluster(args.cluster)}"
+                ),
+                file=sys.stderr,
+            )
+            return 1
+    except PoolsNotConfiguredError:
+        log.warning(
+            PaastaColors.yellow(
+                f"Could not fetch allowed_pools for `{args.cluster}`. Skipping pool validation.\n"
+            )
+        )
+
+    # annoyingly, there's two layers of aliases: one for the soaconfigs to read from
+    # (that's this alias lookup) - and then another layer later when figuring out what
+    # k8s server url to use ;_;
+    cluster = system_paasta_config.get_cluster_aliases().get(args.cluster, args.cluster)
     # Use the default spark:client instance configs if not provided
     try:
         instance_config = get_instance_config(
             service=args.service,
             instance=args.instance,
-            cluster=args.cluster,
+            cluster=cluster,
             load_deployments=args.build is False and args.image is None,
             soa_dir=args.yelpsoa_config_root,
         )
     except NoConfigurationForServiceError as e:
-        paasta_print(str(e), file=sys.stderr)
+        print(str(e), file=sys.stderr)
         return 1
     except NoDeploymentsAvailable:
-        paasta_print(
+        print(
             PaastaColors.red(
                 "Error: No deployments.json found in %(soa_dir)s/%(service)s."
                 "You can generate this by running:"
                 "generate_deployments_for_service -d %(soa_dir)s -s %(service)s"
                 % {"soa_dir": args.yelpsoa_config_root, "service": args.service}
             ),
             sep="\n",
             file=sys.stderr,
         )
         return 1
 
     if not args.cmd and not instance_config.get_cmd():
-        paasta_print(
+        print(
             "A command is required, pyspark, spark-shell, spark-submit or jupyter",
             file=sys.stderr,
         )
         return 1
 
-    if args.build:
-        docker_url = build_and_push_docker_image(args)
-        if docker_url is None:
-            return 1
-    elif args.image:
-        docker_url = args.image
-    else:
-        if args.cmd == "jupyter-lab":
-            paasta_print(
-                PaastaColors.red(
-                    "The jupyter-lab command requires a prebuilt image with -I or --image."
-                ),
-                file=sys.stderr,
-            )
-            return 1
+    aws_creds = get_aws_credentials(
+        service=args.service,
+        aws_credentials_yaml=args.aws_credentials_yaml,
+        profile_name=args.aws_profile,
+        assume_aws_role_arn=args.assume_aws_role,
+        session_duration=args.aws_role_duration,
+        use_web_identity=args.use_web_identity,
+    )
+    docker_image_digest = get_docker_image(args, instance_config)
+    if docker_image_digest is None:
+        return 1
 
-        try:
-            docker_url = instance_config.get_docker_url()
-        except NoDockerImageError:
-            paasta_print(
-                PaastaColors.red(
-                    "Error: No sha has been marked for deployment for the %s deploy group.\n"
-                    "Please ensure this service has either run through a jenkins pipeline "
-                    "or paasta mark-for-deployment has been run for %s\n"
-                    % (instance_config.get_deploy_group(), args.service)
-                ),
-                sep="",
-                file=sys.stderr,
-            )
-            return 1
-        paasta_print(
-            "Please wait while the image (%s) is pulled (times out after 5m)..."
-            % docker_url,
-            file=sys.stderr,
-        )
-        retcode, _ = _run(
-            "sudo -H docker pull %s" % docker_url, stream=True, timeout=300
-        )
-        if retcode != 0:
-            paasta_print(
-                "\nPull failed. Are you authorized to run docker commands?",
-                file=sys.stderr,
-            )
-            return 1
+    volumes = instance_config.get_volumes(system_paasta_config.get_volumes())
+    app_base_name = get_spark_app_name(args.cmd or instance_config.get_cmd())
+
+    user_spark_opts = _parse_user_spark_args(args.spark_args)
+
+    args.cmd = auto_add_timeout_for_spark_job(args.cmd, args.timeout_job_runtime)
+
+    # This is required if configs are provided as part of `spark-submit`
+    # Other way to provide is with --spark-args
+    sub_cmds = args.cmd.split(" ")  # spark.driver.memory=10g
+    for cmd in sub_cmds:
+        if cmd.startswith("spark.driver.memory") or cmd.startswith(
+            "spark.driver.cores"
+        ):
+            key, value = cmd.split("=")
+            user_spark_opts[key] = value
+
+    paasta_instance = get_smart_paasta_instance_name(args)
+
+    k8s_server_address = get_k8s_url_for_cluster(args.cluster)
+    paasta_cluster = system_paasta_config.get_eks_cluster_aliases().get(
+        args.cluster, args.cluster
+    )
+
+    spark_conf_builder = spark_config.SparkConfBuilder()
+    spark_conf = spark_conf_builder.get_spark_conf(
+        cluster_manager=args.cluster_manager,
+        spark_app_base_name=app_base_name,
+        docker_img=docker_image_digest,
+        user_spark_opts=user_spark_opts,
+        paasta_cluster=paasta_cluster,
+        paasta_pool=args.pool,
+        paasta_service=args.service,
+        paasta_instance=paasta_instance,
+        extra_volumes=volumes,
+        aws_creds=aws_creds,
+        aws_region=args.aws_region,
+        force_spark_resource_configs=args.force_spark_resource_configs,
+        use_eks=True,
+        k8s_server_address=k8s_server_address,
+    )
 
     return configure_and_run_docker_container(
         args,
-        docker_img=docker_url,
+        docker_img=docker_image_digest,
         instance_config=instance_config,
         system_paasta_config=system_paasta_config,
+        spark_conf=spark_conf,
+        aws_creds=aws_creds,
+        cluster_manager=args.cluster_manager,
+        pod_template_path=spark_conf.get(
+            "spark.kubernetes.executor.podTemplateFile", ""
+        ),
+        extra_driver_envs=driver_envs_from_tronfig,
     )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/security_check.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/security_check.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from paasta_tools.utils import _run
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 
 
 def add_subparser(subparsers):
     list_parser = subparsers.add_parser(
         "security-check",
         description="Performs a security check consisting of a few tests.",
         help="Performs a security check",
@@ -41,20 +40,20 @@
     If you are at Yelp, please visit https://confluence.yelpcorp.com/display/PAASTA/PaaSTA+security-check+explained
     to learn more.
     :param args: service - the name of the service; commit - upstream git commit.
     :return: 0 if the security-check passed, non-zero if it failed.
     """
     security_check_command = load_system_paasta_config().get_security_check_command()
     if not security_check_command:
-        paasta_print("Nothing to be executed during the security-check step")
+        print("Nothing to be executed during the security-check step")
         return 0
 
     command = f"{security_check_command} {args.service} {args.commit}"
 
     ret_code, output = _run(command, timeout=3600, stream=True)
     if ret_code != 0:
-        paasta_print(
+        print(
             "The security-check failed. Please visit y/security-check-runbook to learn how to fix it ("
             "including whitelisting safe versions of packages and docker images)."
         )
 
     return ret_code
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/local_run.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/local_run.py`

 * *Files 16% similar despite different names*

```diff
@@ -13,59 +13,76 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import datetime
 import json
 import os
 import shutil
 import socket
+import subprocess
 import sys
+import tempfile
 import threading
 import time
 import uuid
 from os import execlpe
 from random import randint
+from typing import Optional
 from urllib.parse import urlparse
 
+import boto3
 import requests
 from docker import errors
+from mypy_extensions import TypedDict
 
 from paasta_tools.adhoc_tools import get_default_interactive_config
 from paasta_tools.cli.cmds.check import makefile_responds_to
 from paasta_tools.cli.cmds.cook_image import paasta_cook_image
 from paasta_tools.cli.utils import figure_out_service_name
 from paasta_tools.cli.utils import get_instance_config
 from paasta_tools.cli.utils import lazy_choices_completer
 from paasta_tools.cli.utils import list_instances
 from paasta_tools.cli.utils import pick_random_port
 from paasta_tools.generate_deployments_for_service import build_docker_image_name
+from paasta_tools.kubernetes_tools import get_kubernetes_secret_env_variables
+from paasta_tools.kubernetes_tools import get_kubernetes_secret_volumes
+from paasta_tools.kubernetes_tools import KUBE_CONFIG_USER_PATH
+from paasta_tools.kubernetes_tools import KubeClient
 from paasta_tools.long_running_service_tools import get_healthcheck_for_instance
 from paasta_tools.paasta_execute_docker_command import execute_in_container
 from paasta_tools.secret_tools import decrypt_secret_environment_variables
+from paasta_tools.secret_tools import decrypt_secret_volumes
 from paasta_tools.tron_tools import parse_time_variables
 from paasta_tools.utils import _run
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import get_docker_client
 from paasta_tools.utils import get_possible_launched_by_user_variable_from_env
 from paasta_tools.utils import get_username
+from paasta_tools.utils import InstanceConfig
+from paasta_tools.utils import is_secrets_for_teams_enabled
 from paasta_tools.utils import list_clusters
 from paasta_tools.utils import list_services
 from paasta_tools.utils import load_system_paasta_config
 from paasta_tools.utils import NoConfigurationForServiceError
 from paasta_tools.utils import NoDeploymentsAvailable
 from paasta_tools.utils import NoDockerImageError
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import PaastaNotConfiguredError
 from paasta_tools.utils import SystemPaastaConfig
-from paasta_tools.utils import timed_flock
 from paasta_tools.utils import Timeout
 from paasta_tools.utils import TimeoutError
 from paasta_tools.utils import validate_service_instance
 
 
+class AWSSessionCreds(TypedDict):
+    AWS_ACCESS_KEY_ID: str
+    AWS_SECRET_ACCESS_KEY: str
+    AWS_SESSION_TOKEN: str
+    AWS_SECURITY_TOKEN: str
+
+
 def parse_date(date_string):
     return datetime.datetime.strptime(date_string, "%Y-%m-%d")
 
 
 def perform_http_healthcheck(url, timeout):
     """Returns true if healthcheck on url succeeds, false otherwise
 
@@ -79,15 +96,15 @@
                 res = requests.get(url, verify=False)
             except requests.ConnectionError:
                 return (False, "http request failed: connection failed")
     except TimeoutError:
         return (False, "http request timed out after %d seconds" % timeout)
 
     if "content-type" in res.headers and "," in res.headers["content-type"]:
-        paasta_print(
+        print(
             PaastaColors.yellow(
                 "Multiple content-type headers detected in response."
                 " The Mesos healthcheck system will treat this as a failure!"
             )
         )
         return (False, "http request succeeded, code %d" % res.status_code)
     # check if response code is valid per https://mesosphere.github.io/marathon/docs/health-checks.html
@@ -150,15 +167,15 @@
             docker_client, container_id, healthcheck_data, timeout
         )
     elif healthcheck_mode == "http" or healthcheck_mode == "https":
         healthcheck_result = perform_http_healthcheck(healthcheck_data, timeout)
     elif healthcheck_mode == "tcp":
         healthcheck_result = perform_tcp_healthcheck(healthcheck_data, timeout)
     else:
-        paasta_print(
+        print(
             PaastaColors.yellow(
                 "Healthcheck mode '%s' is not currently supported!" % healthcheck_mode
             )
         )
         sys.exit(1)
     return healthcheck_result
 
@@ -183,15 +200,15 @@
     healthcheck_link = PaastaColors.cyan(healthcheck_data)
     if healthcheck_enabled:
         grace_period = instance_config.get_healthcheck_grace_period_seconds()
         timeout = instance_config.get_healthcheck_timeout_seconds()
         interval = instance_config.get_healthcheck_interval_seconds()
         max_failures = instance_config.get_healthcheck_max_consecutive_failures()
 
-        paasta_print(
+        print(
             "\nStarting health check via %s (waiting %s seconds before "
             "considering failures due to grace period):"
             % (healthcheck_link, grace_period)
         )
 
         # silently start performing health checks until grace period ends or first check succeeds
         graceperiod_end_time = time.time() + grace_period
@@ -200,15 +217,15 @@
 
         def _stream_docker_logs(container_id, generator):
             while healthchecking:
                 try:
                     # the generator will block until another log line is available
                     log_line = next(generator).decode("utf-8").rstrip("\n")
                     if healthchecking:
-                        paasta_print(f"container [{container_id[:12]}]: {log_line}")
+                        print(f"container [{container_id[:12]}]: {log_line}")
                     else:
                         # stop streaming at first opportunity, since generator.close()
                         # cant be used until the container is dead
                         break
                 except StopIteration:  # natural end of logs
                     break
 
@@ -221,15 +238,15 @@
             args=(container_id, docker_logs_generator),
         ).start()
 
         while True:
             # First inspect the container for early exits
             container_state = docker_client.inspect_container(container_id)
             if not container_state["State"]["Running"]:
-                paasta_print(
+                print(
                     PaastaColors.red(
                         "Container exited with code {}".format(
                             container_state["State"]["ExitCode"]
                         )
                     )
                 )
                 healthcheck_passed = False
@@ -237,15 +254,15 @@
 
             healthcheck_passed, healthcheck_output = run_healthcheck_on_container(
                 docker_client, container_id, healthcheck_mode, healthcheck_data, timeout
             )
 
             # Yay, we passed the healthcheck
             if healthcheck_passed:
-                paasta_print(
+                print(
                     "{}'{}' (via {})".format(
                         PaastaColors.green("Healthcheck succeeded!: "),
                         healthcheck_output,
                         healthcheck_link,
                     )
                 )
                 break
@@ -260,23 +277,23 @@
                 after_grace_period_attempts += 1
                 color = PaastaColors.red
                 msg = "(Attempt {} of {})".format(
                     after_grace_period_attempts, max_failures
                 )
                 extra_msg = f" (via: {healthcheck_link}. Output: {healthcheck_output})"
 
-            paasta_print("{}{}".format(color(f"Healthcheck failed! {msg}"), extra_msg))
+            print("{}{}".format(color(f"Healthcheck failed! {msg}"), extra_msg))
 
             if after_grace_period_attempts == max_failures:
                 break
 
             time.sleep(interval)
         healthchecking = False  # end docker logs stream
     else:
-        paasta_print(
+        print(
             "\nPaaSTA would have healthchecked your service via\n%s" % healthcheck_link
         )
         healthcheck_passed = True
     return healthcheck_passed
 
 
 def read_local_dockerfile_lines():
@@ -453,24 +470,68 @@
         help="Skip decrypting secrets, useful if running non-interactively",
         dest="skip_secrets",
         required=False,
         action="store_true",
         default=False,
     )
     list_parser.add_argument(
+        "--assume-role-aws-account",
+        "--aws-account",
+        "-a",
+        help="Specify AWS account from which to source credentials",
+    )
+    list_parser.add_argument(
+        "--assume-role-arn",
+        help=(
+            "role ARN to assume before launching the service. "
+            "Example format: arn:aws:iam::01234567890:role/rolename"
+        ),
+        type=str,
+        dest="assume_role_arn",
+        required=False,
+        default="",
+    )
+    list_parser.add_argument(
+        "--assume-pod-identity",
+        help="If pod identity is set via yelpsoa-configs, attempt to assume it",
+        action="store_true",
+        dest="assume_pod_identity",
+        required=False,
+        default=False,
+    )
+    list_parser.add_argument(
+        "--use-okta-role",
+        help="Call aws-okta and run the service within the context of the returned credentials",
+        dest="use_okta_role",
+        action="store_true",
+        required=False,
+        default=False,
+    )
+    list_parser.add_argument(
         "--sha",
         help=(
             "SHA to run instead of the currently marked-for-deployment SHA. Ignored when used with --build."
             " Must be a version that exists in the registry, i.e. it has been built by Jenkins."
         ),
         type=str,
         dest="sha",
         required=False,
         default=None,
     )
+    list_parser.add_argument(
+        "--volume",
+        dest="volumes",
+        action="append",
+        type=str,
+        default=[],
+        required=False,
+        help=(
+            "Same as the -v / --volume parameter to docker run: hostPath:containerPath[:mode]"
+        ),
+    )
 
     list_parser.set_defaults(command=paasta_local_run)
 
 
 def get_container_name():
     return "paasta_local_run_{}_{}".format(get_username(), randint(1, 999999))
 
@@ -524,31 +585,29 @@
 
 
 def docker_pull_image(docker_url):
     """Pull an image via ``docker pull``. Uses the actual pull command instead of the python
     bindings due to the docker auth/registry transition. Once we are past Docker 1.6
     we can use better credential management, but for now this function assumes the
     user running the command has already been authorized for the registry"""
-    paasta_print(
+    print(
         "Please wait while the image (%s) is pulled (times out after 30m)..."
         % docker_url,
         file=sys.stderr,
     )
-    DEVNULL = open(os.devnull, "wb")
-    with open("/tmp/paasta-local-run-pull.lock", "w") as f:
-        with timed_flock(f, seconds=1800):
-            ret, output = _run(
-                "docker pull %s" % docker_url, stream=True, stdin=DEVNULL
+    with Timeout(
+        seconds=1800, error_message=f"Timed out pulling docker image from {docker_url}"
+    ), open(os.devnull, mode="wb") as DEVNULL:
+        ret, _ = _run("docker pull %s" % docker_url, stream=True, stdin=DEVNULL)
+        if ret != 0:
+            print(
+                "\nPull failed. Are you authorized to run docker commands?",
+                file=sys.stderr,
             )
-            if ret != 0:
-                paasta_print(
-                    "\nPull failed. Are you authorized to run docker commands?",
-                    file=sys.stderr,
-                )
-                sys.exit(ret)
+            sys.exit(ret)
 
 
 def get_container_id(docker_client, container_name):
     """Use 'docker_client' to find the container we started, identifiable by
     its 'container_name'. If we can't find the id, raise
     LostContainerException.
     """
@@ -562,29 +621,29 @@
         "Here were all the containers:\n"
         "%s" % (container_name, containers)
     )
 
 
 def _cleanup_container(docker_client, container_id):
     if docker_client.inspect_container(container_id)["State"].get("OOMKilled", False):
-        paasta_print(
+        print(
             PaastaColors.red(
                 "Your service was killed by the OOM Killer!\n"
                 "You've exceeded the memory limit, try increasing the mem parameter in your soa_configs"
             ),
             file=sys.stderr,
         )
-    paasta_print("\nStopping and removing the old container %s..." % container_id)
-    paasta_print("(Please wait or you may leave an orphaned container.)")
+    print("\nStopping and removing the old container %s..." % container_id)
+    print("(Please wait or you may leave an orphaned container.)")
     try:
         docker_client.stop(container_id)
         docker_client.remove_container(container_id)
-        paasta_print("...done")
+        print("...done")
     except errors.APIError:
-        paasta_print(
+        print(
             PaastaColors.yellow(
                 "Could not clean up container! You should stop and remove container '%s' manually."
                 % container_id
             )
         )
 
 
@@ -593,38 +652,22 @@
     a paasta service running in a container"""
     hostname = socket.getfqdn()
     docker_image = instance_config.get_docker_image()
     if docker_image == "":
         # In a local_run environment, the docker_image may not be available
         # so we can fall-back to the injected DOCKER_TAG per the paasta contract
         docker_image = os.environ["DOCKER_TAG"]
-    fake_taskid = uuid.uuid4()
     env = {
         "HOST": hostname,
-        "MESOS_SANDBOX": "/mnt/mesos/sandbox",
-        "MESOS_CONTAINER_NAME": "localrun-%s" % fake_taskid,
-        "MESOS_TASK_ID": str(fake_taskid),
         "PAASTA_DOCKER_IMAGE": docker_image,
         "PAASTA_LAUNCHED_BY": get_possible_launched_by_user_variable_from_env(),
+        "PAASTA_HOST": hostname,
+        # Kubernetes instances remove PAASTA_CLUSTER, so we need to re-add it ourselves
+        "PAASTA_CLUSTER": instance_config.get_cluster(),
     }
-    if framework == "marathon":
-        env["MARATHON_PORT"] = str(port0)
-        env["MARATHON_PORT0"] = str(port0)
-        env["MARATHON_PORTS"] = str(port0)
-        env["MARATHON_PORT_%d" % instance_config.get_container_port()] = str(port0)
-        env["MARATHON_APP_VERSION"] = "simulated_marathon_app_version"
-        env["MARATHON_APP_RESOURCE_CPUS"] = str(instance_config.get_cpus())
-        env["MARATHON_APP_DOCKER_IMAGE"] = docker_image
-        env["MARATHON_APP_RESOURCE_MEM"] = str(instance_config.get_mem())
-        env["MARATHON_APP_RESOURCE_DISK"] = str(instance_config.get_disk())
-        env["MARATHON_APP_LABELS"] = ""
-        env["MARATHON_APP_ID"] = "/simulated_marathon_app_id"
-        env["MARATHON_HOST"] = hostname
-        env["PAASTA_HOST"] = hostname
-        env["PAASTA_PORT"] = str(port0)
 
     return env
 
 
 def check_if_port_free(port):
     temp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
     try:
@@ -632,14 +675,129 @@
     except socket.error:
         return False
     finally:
         temp_socket.close()
     return True
 
 
+def resolve_aws_account_from_runtimeenv() -> str:
+    try:
+        with open("/nail/etc/runtimeenv") as runtimeenv_file:
+            runtimeenv = runtimeenv_file.read()
+    except FileNotFoundError:
+        print(
+            "Unable to determine environment for AWS account name. Using 'dev'",
+            file=sys.stderr,
+        )
+        runtimeenv = "dev"
+
+    runtimeenv_to_account_overrides = {
+        "stage": "dev",
+        "corp": "corpprod",
+    }
+    return runtimeenv_to_account_overrides.get(runtimeenv, runtimeenv)
+
+
+def assume_aws_role(
+    instance_config: InstanceConfig,
+    service: str,
+    assume_role_arn: str,
+    assume_pod_identity: bool,
+    use_okta_role: bool,
+    aws_account: str,
+) -> AWSSessionCreds:
+    """Runs AWS cli to assume into the correct role, then extract and return the ENV variables from that session"""
+    pod_identity = instance_config.get_iam_role()
+    if assume_role_arn:
+        pod_identity = assume_role_arn
+    if assume_pod_identity and not pod_identity:
+        print(
+            f"Error: --assume-pod-identity passed but no pod identity was found for this instance ({instance_config.instance})",
+            file=sys.stderr,
+        )
+        sys.exit(1)
+
+    if pod_identity and (assume_pod_identity or assume_role_arn):
+        print(
+            "Calling aws-okta to assume role {} using account {}".format(
+                pod_identity, aws_account
+            )
+        )
+    elif use_okta_role:
+        print(f"Calling aws-okta using account {aws_account}")
+    elif "AWS_ROLE_ARN" in os.environ and "AWS_WEB_IDENTITY_TOKEN_FILE" in os.environ:
+        # Get a session using the current pod identity
+        print(
+            f"Found Pod Identity token in env. Assuming into role {os.environ['AWS_ROLE_ARN']}."
+        )
+        boto_session = boto3.Session()
+        credentials = boto_session.get_credentials()
+        assumed_creds_dict: AWSSessionCreds = {
+            "AWS_ACCESS_KEY_ID": credentials.access_key,
+            "AWS_SECRET_ACCESS_KEY": credentials.secret_key,
+            "AWS_SESSION_TOKEN": credentials.token,
+            "AWS_SECURITY_TOKEN": credentials.token,
+        }
+        return assumed_creds_dict
+    else:
+        # use_okta_role, assume_pod_identity, and assume_role are all empty, and there's no
+        # pod identity (web identity token) in the env. This shouldn't happen
+        print(
+            "Error: assume_aws_role called without required arguments and no pod identity env",
+            file=sys.stderr,
+        )
+        sys.exit(1)
+    # local-run will sometimes run as root - make sure that we get the actual
+    # users AWS credentials instead of looking for non-existent root AWS
+    # credentials
+    if os.getuid() == 0:
+        aws_okta_cmd = [
+            "sudo",
+            "-u",
+            get_username(),
+            f"HOME=/nail/home/{get_username()}",
+            "aws-okta",
+            "-a",
+            aws_account,
+            "-o",
+            "json",
+        ]
+    else:
+        aws_okta_cmd = ["aws-okta", "-a", aws_account, "-o", "json"]
+    cmd = subprocess.run(aws_okta_cmd, stdout=subprocess.PIPE)
+    if cmd.returncode != 0:
+        print(
+            "Error calling aws-okta. Remove --assume-pod-identity to run without pod identity role",
+            file=sys.stderr,
+        )
+        sys.exit(1)
+    cmd_output = json.loads(cmd.stdout.decode("utf-8"))
+
+    if not use_okta_role:
+        boto_session = boto3.Session(
+            aws_access_key_id=cmd_output["AccessKeyId"],
+            aws_secret_access_key=cmd_output["SecretAccessKey"],
+            aws_session_token=cmd_output["SessionToken"],
+        )
+        sts_client = boto_session.client("sts")
+        assumed_role = sts_client.assume_role(
+            RoleArn=pod_identity, RoleSessionName=f"{get_username()}-local-run"
+        )
+        # The contents of "Credentials" key from assume_role is the same as from aws-okta
+        cmd_output = assumed_role["Credentials"]
+
+    creds_dict: AWSSessionCreds = {
+        "AWS_ACCESS_KEY_ID": cmd_output["AccessKeyId"],
+        "AWS_SECRET_ACCESS_KEY": cmd_output["SecretAccessKey"],
+        "AWS_SESSION_TOKEN": cmd_output["SessionToken"],
+        "AWS_SECURITY_TOKEN": cmd_output["SessionToken"],
+    }
+    return creds_dict
+
+
 def run_docker_container(
     docker_client,
     service,
     instance,
     docker_url,
     volumes,
     interactive,
@@ -651,54 +809,107 @@
     secret_provider_name,
     soa_dir=DEFAULT_SOA_DIR,
     dry_run=False,
     json_dict=False,
     framework=None,
     secret_provider_kwargs={},
     skip_secrets=False,
+    assume_pod_identity=False,
+    assume_role_arn="",
+    use_okta_role=False,
+    assume_role_aws_account: Optional[str] = None,
 ):
     """docker-py has issues running a container with a TTY attached, so for
     consistency we execute 'docker run' directly in both interactive and
     non-interactive modes.
 
     In non-interactive mode when the run is complete, stop the container and
     remove it (with docker-py).
     """
     if user_port:
         if check_if_port_free(user_port):
             chosen_port = user_port
         else:
-            paasta_print(
+            print(
                 PaastaColors.red(
                     "The chosen port is already in use!\n"
                     "Try specifying another one, or omit (--port|-o) and paasta will find a free one for you"
                 ),
                 file=sys.stderr,
             )
             sys.exit(1)
     else:
         chosen_port = pick_random_port(service)
-    environment = instance_config.get_env_dictionary()
+    environment = instance_config.get_env()
+    secret_volumes = {}  # type: ignore
     if not skip_secrets:
-        try:
-            secret_environment = decrypt_secret_environment_variables(
-                secret_provider_name=secret_provider_name,
-                environment=environment,
-                soa_dir=soa_dir,
-                service_name=service,
-                cluster_name=instance_config.cluster,
-                secret_provider_kwargs=secret_provider_kwargs,
-            )
-        except Exception as e:
-            paasta_print(f"Failed to retrieve secrets with {e.__class__.__name__}: {e}")
-            paasta_print(
-                "If you don't need the secrets for local-run, you can add --skip-secrets"
-            )
-            sys.exit(1)
+        # if secrets_for_owner_team enabled in yelpsoa for service
+        if is_secrets_for_teams_enabled(service, soa_dir):
+            try:
+                kube_client = KubeClient(
+                    config_file=KUBE_CONFIG_USER_PATH, context=instance_config.cluster
+                )
+                secret_environment = get_kubernetes_secret_env_variables(
+                    kube_client, environment, service, instance_config.get_namespace()
+                )
+                secret_volumes = get_kubernetes_secret_volumes(
+                    kube_client,
+                    instance_config.get_secret_volumes(),
+                    service,
+                    instance_config.get_namespace(),
+                )
+            except Exception as e:
+                print(
+                    f"Failed to retrieve kubernetes secrets with {e.__class__.__name__}: {e}"
+                )
+                print(
+                    "If you don't need the secrets for local-run, you can add --skip-secrets"
+                )
+                sys.exit(1)
+        else:
+            try:
+                secret_environment = decrypt_secret_environment_variables(
+                    secret_provider_name=secret_provider_name,
+                    environment=environment,
+                    soa_dir=soa_dir,
+                    service_name=instance_config.get_service(),
+                    cluster_name=instance_config.cluster,
+                    secret_provider_kwargs=secret_provider_kwargs,
+                )
+                secret_volumes = decrypt_secret_volumes(
+                    secret_provider_name=secret_provider_name,
+                    secret_volumes_config=instance_config.get_secret_volumes(),
+                    soa_dir=soa_dir,
+                    service_name=instance_config.get_service(),
+                    cluster_name=instance_config.cluster,
+                    secret_provider_kwargs=secret_provider_kwargs,
+                )
+            except Exception as e:
+                print(f"Failed to decrypt secrets with {e.__class__.__name__}: {e}")
+                print(
+                    "If you don't need the secrets for local-run, you can add --skip-secrets"
+                )
+                sys.exit(1)
         environment.update(secret_environment)
+    if (
+        assume_role_arn
+        or assume_pod_identity
+        or use_okta_role
+        or "AWS_WEB_IDENTITY_TOKEN_FILE" in os.environ
+    ):
+        aws_creds = assume_aws_role(
+            instance_config,
+            service,
+            assume_role_arn,
+            assume_pod_identity,
+            use_okta_role,
+            assume_role_aws_account,
+        )
+        environment.update(aws_creds)
+
     local_run_environment = get_local_run_environment_vars(
         instance_config=instance_config, port0=chosen_port, framework=framework
     )
     environment.update(local_run_environment)
     net = instance_config.get_net()
     memory = instance_config.get_mem()
     container_name = get_container_name()
@@ -718,14 +929,36 @@
         except AttributeError:
             container_port = None
 
     simulate_healthcheck = (
         healthcheck_only or healthcheck
     ) and healthcheck_mode is not None
 
+    for container_mount_path, secret_content in secret_volumes.items():
+        temp_secret_folder = tempfile.mktemp(dir=os.environ.get("TMPDIR", "/nail/tmp"))
+        os.makedirs(temp_secret_folder, exist_ok=True)
+        temp_secret_filename = os.path.join(temp_secret_folder, str(uuid.uuid4()))
+        # write the secret contents
+        # Permissions will automatically be set to readable by "users" group
+        # TODO: Make this readable only by "nobody" user? What about other non-standard users that people sometimes use inside the container?
+        # -rw-r--r-- 1 dpopes users 3.2K Nov 28 19:16 854bdbad-30b8-4681-ae4e-854cb28075c5
+        try:
+            # First try to write the file as a string
+            # This is for text like config files
+            with open(temp_secret_filename, "w") as f:
+                f.write(secret_content)
+        except TypeError:
+            # If that fails, try to write it as bytes
+            # This is for binary files like TLS keys
+            with open(temp_secret_filename, "wb") as fb:
+                fb.write(secret_content)
+
+        # Append this to the list of volumes passed to docker run
+        volumes.append(f"{temp_secret_filename}:{container_mount_path}:ro")
+
     docker_run_args = dict(
         memory=memory,
         chosen_port=chosen_port,
         container_port=container_port,
         container_name=container_name,
         volumes=volumes,
         env=environment,
@@ -737,22 +970,20 @@
         docker_params=docker_params,
     )
     docker_run_cmd = get_docker_run_cmd(**docker_run_args)
     joined_docker_run_cmd = " ".join(docker_run_cmd)
 
     if dry_run:
         if json_dict:
-            paasta_print(json.dumps(docker_run_args))
+            print(json.dumps(docker_run_args))
         else:
-            paasta_print(json.dumps(docker_run_cmd))
+            print(json.dumps(docker_run_cmd))
         return 0
     else:
-        paasta_print(
-            "Running docker command:\n%s" % PaastaColors.grey(joined_docker_run_cmd)
-        )
+        print("Running docker command:\n%s" % PaastaColors.grey(joined_docker_run_cmd))
 
     merged_env = {**os.environ, **environment}
 
     if interactive or not simulate_healthcheck:
         # NOTE: This immediately replaces us with the docker run cmd. Docker
         # run knows how to clean up the running container in this situation.
         wrapper_path = shutil.which("paasta_docker_wrapper")
@@ -765,29 +996,29 @@
         return 0
 
     container_started = False
     container_id = None
     try:
         (returncode, output) = _run(docker_run_cmd, env=merged_env)
         if returncode != 0:
-            paasta_print(
+            print(
                 "Failure trying to start your container!"
                 "Returncode: %d"
                 "Output:"
                 "%s"
                 ""
                 "Fix that problem and try again."
                 "http://y/paasta-troubleshooting" % (returncode, output),
                 sep="\n",
             )
             # Container failed to start so no need to cleanup; just bail.
             sys.exit(1)
         container_started = True
         container_id = get_container_id(docker_client, container_name)
-        paasta_print("Found our container running with CID %s" % container_id)
+        print("Found our container running with CID %s" % container_id)
 
         if simulate_healthcheck:
             healthcheck_result = simulate_healthcheck_on_service(
                 instance_config=instance_config,
                 docker_client=docker_client,
                 container_id=container_id,
                 healthcheck_mode=healthcheck_mode,
@@ -795,37 +1026,43 @@
                 healthcheck_enabled=healthcheck,
             )
 
         def _output_exit_code():
             returncode = docker_client.inspect_container(container_id)["State"][
                 "ExitCode"
             ]
-            paasta_print(f"Container exited: {returncode})")
+            print(f"Container exited: {returncode})")
 
         if healthcheck_only:
             if container_started:
                 _output_exit_code()
                 _cleanup_container(docker_client, container_id)
             if healthcheck_mode is None:
-                paasta_print(
+                print(
                     "--healthcheck-only, but no healthcheck is defined for this instance!"
                 )
                 sys.exit(1)
             elif healthcheck_result is True:
                 sys.exit(0)
             else:
                 sys.exit(1)
 
         running = docker_client.inspect_container(container_id)["State"]["Running"]
         if running:
-            paasta_print("Your service is now running! Tailing stdout and stderr:")
-            for line in docker_client.attach(
-                container_id, stderr=True, stream=True, logs=True
+            print("Your service is now running! Tailing stdout and stderr:")
+            for line in docker_client.logs(
+                container_id,
+                stderr=True,
+                stream=True,
             ):
-                paasta_print(line)
+                # writing to sys.stdout.buffer lets us write the raw bytes we
+                # get from the docker client without having to convert them to
+                # a utf-8 string
+                sys.stdout.buffer.write(line)
+                sys.stdout.flush()
         else:
             _output_exit_code()
             returncode = 3
 
     except KeyboardInterrupt:
         returncode = 3
 
@@ -853,36 +1090,35 @@
     docker_url,
     docker_sha,
     service,
     instance,
     cluster,
     system_paasta_config,
     args,
+    assume_role_aws_account,
     pull_image=False,
     dry_run=False,
 ):
     """
     Run Docker container by image hash with args set in command line.
     Function prints the output of run command in stdout.
     """
 
     if instance is None and args.healthcheck_only:
-        paasta_print(
-            "With --healthcheck-only, --instance MUST be provided!", file=sys.stderr
-        )
+        print("With --healthcheck-only, --instance MUST be provided!", file=sys.stderr)
         return 1
     if instance is None and not sys.stdin.isatty():
-        paasta_print(
+        print(
             "--instance and --cluster must be specified when using paasta local-run without a tty!",
             file=sys.stderr,
         )
         return 1
 
     soa_dir = args.yelpsoa_config_root
-    volumes = list()
+    volumes = args.volumes
     load_deployments = (docker_url is None or pull_image) and not docker_sha
     interactive = args.interactive
 
     try:
         if instance is None:
             instance_type = "adhoc"
             instance = "interactive"
@@ -901,18 +1137,18 @@
                 service=service,
                 instance=instance,
                 cluster=cluster,
                 load_deployments=load_deployments,
                 soa_dir=soa_dir,
             )
     except NoConfigurationForServiceError as e:
-        paasta_print(str(e), file=sys.stderr)
+        print(str(e), file=sys.stderr)
         return 1
     except NoDeploymentsAvailable:
-        paasta_print(
+        print(
             PaastaColors.red(
                 "Error: No deployments.json found in %(soa_dir)s/%(service)s. "
                 "You can generate this by running: "
                 "generate_deployments_for_service -d %(soa_dir)s -s %(service)s"
                 % {"soa_dir": soa_dir, "service": service}
             ),
             sep="\n",
@@ -929,24 +1165,24 @@
         }
 
     if docker_url is None:
         try:
             docker_url = instance_config.get_docker_url()
         except NoDockerImageError:
             if instance_config.get_deploy_group() is None:
-                paasta_print(
+                print(
                     PaastaColors.red(
                         f"Error: {service}.{instance} has no 'deploy_group' set. Please set one so "
                         "the proper image can be used to run for this service."
                     ),
                     sep="",
                     file=sys.stderr,
                 )
             else:
-                paasta_print(
+                print(
                     PaastaColors.red(
                         "Error: No sha has been marked for deployment for the %s deploy group.\n"
                         "Please ensure this service has either run through a jenkins pipeline "
                         "or paasta mark-for-deployment has been run for %s\n"
                         % (instance_config.get_deploy_group(), service)
                     ),
                     sep="",
@@ -961,15 +1197,15 @@
         if os.path.exists(volume["hostPath"]):
             volumes.append(
                 "{}:{}:{}".format(
                     volume["hostPath"], volume["containerPath"], volume["mode"].lower()
                 )
             )
         else:
-            paasta_print(
+            print(
                 PaastaColors.yellow(
                     "Warning: Path %s does not exist on this host. Skipping this binding."
                     % volume["hostPath"]
                 ),
                 file=sys.stderr,
             )
 
@@ -1007,72 +1243,83 @@
         soa_dir=args.yelpsoa_config_root,
         dry_run=dry_run,
         json_dict=args.dry_run_json_dict,
         framework=instance_type,
         secret_provider_name=system_paasta_config.get_secret_provider_name(),
         secret_provider_kwargs=secret_provider_kwargs,
         skip_secrets=args.skip_secrets,
+        assume_pod_identity=args.assume_pod_identity,
+        assume_role_arn=args.assume_role_arn,
+        assume_role_aws_account=assume_role_aws_account,
+        use_okta_role=args.use_okta_role,
     )
 
 
 def docker_config_available():
     home = os.path.expanduser("~")
     oldconfig = os.path.join(home, ".dockercfg")
     newconfig = os.path.join(home, ".docker", "config.json")
     return (os.path.isfile(oldconfig) and os.access(oldconfig, os.R_OK)) or (
         os.path.isfile(newconfig) and os.access(newconfig, os.R_OK)
     )
 
 
 def paasta_local_run(args):
     if args.action == "pull" and os.geteuid() != 0 and not docker_config_available():
-        paasta_print("Re-executing paasta local-run --pull with sudo..")
+        print("Re-executing paasta local-run --pull with sudo..")
         os.execvp("sudo", ["sudo", "-H"] + sys.argv)
     if args.action == "build" and not makefile_responds_to("cook-image"):
-        paasta_print(
+        print(
             "A local Makefile with a 'cook-image' target is required for --build",
             file=sys.stderr,
         )
-        paasta_print(
+        print(
             "If you meant to pull the docker image from the registry, explicitly pass --pull",
             file=sys.stderr,
         )
         return 1
 
     try:
         system_paasta_config = load_system_paasta_config()
     except PaastaNotConfiguredError:
-        paasta_print(
+        print(
             PaastaColors.yellow(
                 "Warning: Couldn't load config files from '/etc/paasta'. This indicates"
                 "PaaSTA is not configured locally on this host, and local-run may not behave"
                 "the same way it would behave on a server configured for PaaSTA."
             ),
             sep="\n",
         )
         system_paasta_config = SystemPaastaConfig({"volumes": []}, "/etc/paasta")
 
     local_run_config = system_paasta_config.get_local_run_config()
 
     service = figure_out_service_name(args, soa_dir=args.yelpsoa_config_root)
+
     if args.cluster:
         cluster = args.cluster
     else:
         try:
             cluster = local_run_config["default_cluster"]
         except KeyError:
-            paasta_print(
+            print(
                 PaastaColors.red(
                     "PaaSTA on this machine has not been configured with a default cluster."
                     "Please pass one to local-run using '-c'."
                 ),
                 sep="\n",
                 file=sys.stderr,
             )
             return 1
+    assume_role_aws_account = args.assume_role_aws_account or (
+        system_paasta_config.get_kube_clusters()
+        .get(cluster, {})
+        .get("aws_account", resolve_aws_account_from_runtimeenv())
+    )
+
     instance = args.instance
     docker_client = get_docker_client()
 
     docker_sha = None
     docker_url = None
 
     if args.action == "build":
@@ -1102,11 +1349,12 @@
             service=service,
             instance=instance,
             cluster=cluster,
             args=args,
             pull_image=pull_image,
             system_paasta_config=system_paasta_config,
             dry_run=args.action == "dry_run",
+            assume_role_aws_account=assume_role_aws_account,
         )
     except errors.APIError as e:
-        paasta_print("Can't run Docker container. Error: %s" % str(e), file=sys.stderr)
+        print("Can't run Docker container. Error: %s" % str(e), file=sys.stderr)
         return 1
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/check.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/check.py`

 * *Files 9% similar despite different names*

```diff
@@ -24,26 +24,25 @@
 from paasta_tools.cli.utils import is_file_in_dir
 from paasta_tools.cli.utils import lazy_choices_completer
 from paasta_tools.cli.utils import NoSuchService
 from paasta_tools.cli.utils import PaastaCheckMessages
 from paasta_tools.cli.utils import success
 from paasta_tools.cli.utils import validate_service_name
 from paasta_tools.cli.utils import x_mark
-from paasta_tools.marathon_tools import get_all_namespaces_for_service
+from paasta_tools.long_running_service_tools import get_all_namespaces_for_service
 from paasta_tools.monitoring_tools import get_team
 from paasta_tools.utils import _run
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import get_git_url
 from paasta_tools.utils import get_pipeline_config
 from paasta_tools.utils import get_pipeline_deploy_groups
 from paasta_tools.utils import get_service_instance_list
 from paasta_tools.utils import INSTANCE_TYPES
 from paasta_tools.utils import list_clusters
 from paasta_tools.utils import list_services
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 
 
 def add_subparser(subparsers):
     help_text = (
         "Determine whether service in pwd is 'paasta ready', checking for common "
         "mistakes in the soa-configs directory and the local service directory. This "
@@ -67,48 +66,53 @@
 
 def deploy_check(service_path):
     """Check whether deploy.yaml exists in service directory. Prints success or
     error message.
 
     :param service_path: path to a directory containing deploy.yaml"""
     if is_file_in_dir("deploy.yaml", service_path):
-        paasta_print(PaastaCheckMessages.DEPLOY_YAML_FOUND)
+        print(PaastaCheckMessages.DEPLOY_YAML_FOUND)
     else:
-        paasta_print(PaastaCheckMessages.DEPLOY_YAML_MISSING)
+        print(PaastaCheckMessages.DEPLOY_YAML_MISSING)
 
 
 def deploy_has_security_check(service, soa_dir):
     pipeline = get_pipeline_config(service=service, soa_dir=soa_dir)
-    steps = [step["step"] for step in pipeline]
+    steps = [step["step"] for step in pipeline if not step.get("parallel")]
+    steps += [
+        substep["step"]
+        for step in pipeline
+        if step.get("parallel")
+        for substep in step.get("parallel")
+    ]
     if "security-check" in steps:
-        paasta_print(PaastaCheckMessages.DEPLOY_SECURITY_FOUND)
+        print(PaastaCheckMessages.DEPLOY_SECURITY_FOUND)
         return True
     else:
-        paasta_print(PaastaCheckMessages.DEPLOY_SECURITY_MISSING)
+        print(PaastaCheckMessages.DEPLOY_SECURITY_MISSING)
         return False
 
 
 def docker_check():
     """Check whether Dockerfile exists in service directory, and is valid.
     Prints suitable message depending on outcome"""
     docker_file_path = is_file_in_dir("Dockerfile", os.getcwd())
     if docker_file_path:
-        paasta_print(PaastaCheckMessages.DOCKERFILE_FOUND)
+        print(PaastaCheckMessages.DOCKERFILE_FOUND)
     else:
-        paasta_print(PaastaCheckMessages.DOCKERFILE_MISSING)
+        print(PaastaCheckMessages.DOCKERFILE_MISSING)
 
 
 def makefile_responds_to(target):
-    """Runs `make --dry-run <target>` to detect if a makefile responds to the
+    """Runs `make --question <target>` to detect if a makefile responds to the
     specified target."""
-    cmd = "make --dry-run %s" % target
     # According to http://www.gnu.org/software/make/manual/make.html#index-exit-status-of-make,
-    # 0 means OK, and 2 means error
-    returncode, _ = _run(cmd, timeout=5)
-    return returncode == 0
+    # 0 means OK, 1 means the target is not up to date, and 2 means error
+    returncode, _ = _run(["make", "--question", target], timeout=5)
+    return returncode != 2
 
 
 def makefile_has_a_tab(makefile_path):
     contents = get_file_contents(makefile_path)
     return "\t" in contents
 
 
@@ -118,61 +122,61 @@
 
 
 def makefile_check():
     """Detects if you have a makefile and runs some sanity tests against
     it to ensure it is paasta-ready"""
     makefile_path = is_file_in_dir("Makefile", os.getcwd())
     if makefile_path:
-        paasta_print(PaastaCheckMessages.MAKEFILE_FOUND)
+        print(PaastaCheckMessages.MAKEFILE_FOUND)
 
         if makefile_has_a_tab(makefile_path):
-            paasta_print(PaastaCheckMessages.MAKEFILE_HAS_A_TAB)
+            print(PaastaCheckMessages.MAKEFILE_HAS_A_TAB)
         else:
-            paasta_print(PaastaCheckMessages.MAKEFILE_HAS_NO_TABS)
+            print(PaastaCheckMessages.MAKEFILE_HAS_NO_TABS)
 
         if makefile_has_docker_tag(makefile_path):
-            paasta_print(PaastaCheckMessages.MAKEFILE_HAS_DOCKER_TAG)
+            print(PaastaCheckMessages.MAKEFILE_HAS_DOCKER_TAG)
         else:
-            paasta_print(PaastaCheckMessages.MAKEFILE_HAS_NO_DOCKER_TAG)
+            print(PaastaCheckMessages.MAKEFILE_HAS_NO_DOCKER_TAG)
 
         if makefile_responds_to("cook-image"):
-            paasta_print(PaastaCheckMessages.MAKEFILE_RESPONDS_BUILD_IMAGE)
+            print(PaastaCheckMessages.MAKEFILE_RESPONDS_BUILD_IMAGE)
         else:
-            paasta_print(PaastaCheckMessages.MAKEFILE_RESPONDS_BUILD_IMAGE_FAIL)
+            print(PaastaCheckMessages.MAKEFILE_RESPONDS_BUILD_IMAGE_FAIL)
 
         if makefile_responds_to("itest"):
-            paasta_print(PaastaCheckMessages.MAKEFILE_RESPONDS_ITEST)
+            print(PaastaCheckMessages.MAKEFILE_RESPONDS_ITEST)
         else:
-            paasta_print(PaastaCheckMessages.MAKEFILE_RESPONDS_ITEST_FAIL)
+            print(PaastaCheckMessages.MAKEFILE_RESPONDS_ITEST_FAIL)
 
         if makefile_responds_to("test"):
-            paasta_print(PaastaCheckMessages.MAKEFILE_RESPONDS_TEST)
+            print(PaastaCheckMessages.MAKEFILE_RESPONDS_TEST)
         else:
-            paasta_print(PaastaCheckMessages.MAKEFILE_RESPONDS_TEST_FAIL)
+            print(PaastaCheckMessages.MAKEFILE_RESPONDS_TEST_FAIL)
     else:
-        paasta_print(PaastaCheckMessages.MAKEFILE_MISSING)
+        print(PaastaCheckMessages.MAKEFILE_MISSING)
 
 
 def git_repo_check(service, soa_dir):
     git_url = get_git_url(service, soa_dir)
     cmd = "git ls-remote %s" % git_url
     returncode, _ = _run(cmd, timeout=5)
     if returncode == 0:
-        paasta_print(PaastaCheckMessages.GIT_REPO_FOUND)
+        print(PaastaCheckMessages.GIT_REPO_FOUND)
     else:
-        paasta_print(PaastaCheckMessages.git_repo_missing(git_url))
+        print(PaastaCheckMessages.git_repo_missing(git_url))
 
 
 def get_deploy_groups_used_by_framework(instance_type, service, soa_dir):
     """This is a kind of funny function that gets all the instances for specified
     service and framework, and massages it into a form that matches up with what
     deploy.yaml's steps look like. This is only so we can compare it 1-1
     with what deploy.yaml has for linting.
 
-    :param instance_type: one of 'marathon', 'adhoc'
+    :param instance_type: one of the entries in utils.INSTANCE_TYPES
     :param service: the service name
     :param soa_dir: The SOA configuration directory to read from
 
     :returns: a list of deploy group names used by the service.
     """
 
     deploy_groups = []
@@ -195,15 +199,15 @@
                 deploy_groups.append(config.get_deploy_group())
             except NotImplementedError:
                 pass
     return set(filter(None, deploy_groups))
 
 
 def deployments_check(service, soa_dir):
-    """Checks for consistency between deploy.yaml and the marathon yamls"""
+    """Checks for consistency between deploy.yaml and the kubernetes/etc yamls"""
     the_return = True
     pipeline_deploy_groups = get_pipeline_deploy_groups(
         service=service, soa_dir=soa_dir
     )
 
     framework_deploy_groups = {}
     in_deploy_not_frameworks = set(pipeline_deploy_groups)
@@ -212,106 +216,102 @@
             it, service, soa_dir
         )
         in_framework_not_deploy = set(framework_deploy_groups[it]) - set(
             pipeline_deploy_groups
         )
         in_deploy_not_frameworks -= set(framework_deploy_groups[it])
         if len(in_framework_not_deploy) > 0:
-            paasta_print(
+            print(
                 "{} There are some instance(s) you have asked to run in {} that".format(
                     x_mark(), it
                 )
             )
-            paasta_print("  do not have a corresponding entry in deploy.yaml:")
-            paasta_print("  %s" % PaastaColors.bold(", ".join(in_framework_not_deploy)))
-            paasta_print(
-                "  You should probably configure these to use a 'deploy_group' or"
-            )
-            paasta_print(
+            print("  do not have a corresponding entry in deploy.yaml:")
+            print("  %s" % PaastaColors.bold(", ".join(in_framework_not_deploy)))
+            print("  You should probably configure these to use a 'deploy_group' or")
+            print(
                 "  add entries to deploy.yaml for them so they are deployed to those clusters."
             )
             the_return = False
 
     if len(in_deploy_not_frameworks) > 0:
-        paasta_print(
+        print(
             "%s There are some instance(s) in deploy.yaml that are not referenced"
             % x_mark()
         )
-        paasta_print("  by any marathon or adhoc instance:")
-        paasta_print("  %s" % PaastaColors.bold((", ".join(in_deploy_not_frameworks))))
-        paasta_print(
+        print("  by any instance:")
+        print("  %s" % PaastaColors.bold((", ".join(in_deploy_not_frameworks))))
+        print(
             "  You should probably delete these deploy.yaml entries if they are unused."
         )
         the_return = False
 
     if the_return is True:
-        paasta_print(
-            success("All entries in deploy.yaml correspond to a paasta instance")
-        )
+        print(success("All entries in deploy.yaml correspond to a paasta instance"))
         for it in INSTANCE_TYPES:
             if len(framework_deploy_groups[it]) > 0:
-                paasta_print(
+                print(
                     success(
                         "All %s instances have a corresponding deploy.yaml entry" % it
                     )
                 )
     return the_return
 
 
 def sensu_check(service, service_path, soa_dir):
     """Check whether monitoring.yaml exists in service directory,
     and that the team name is declared.
 
     :param service: name of service currently being examined
     :param service_path: path to location of monitoring.yaml file"""
     if is_file_in_dir("monitoring.yaml", service_path):
-        paasta_print(PaastaCheckMessages.SENSU_MONITORING_FOUND)
+        print(PaastaCheckMessages.SENSU_MONITORING_FOUND)
         team = get_team(service=service, overrides={}, soa_dir=soa_dir)
         if team is None:
-            paasta_print(PaastaCheckMessages.SENSU_TEAM_MISSING)
+            print(PaastaCheckMessages.SENSU_TEAM_MISSING)
         else:
-            paasta_print(PaastaCheckMessages.sensu_team_found(team))
+            print(PaastaCheckMessages.sensu_team_found(team))
     else:
-        paasta_print(PaastaCheckMessages.SENSU_MONITORING_MISSING)
+        print(PaastaCheckMessages.SENSU_MONITORING_MISSING)
 
 
 def service_dir_check(service, soa_dir):
     """Check whether directory service exists in /nail/etc/services
     :param service: string of service name we wish to inspect
     """
     try:
         validate_service_name(service, soa_dir)
-        paasta_print(PaastaCheckMessages.service_dir_found(service, soa_dir))
+        print(PaastaCheckMessages.service_dir_found(service, soa_dir))
     except NoSuchService:
-        paasta_print(PaastaCheckMessages.service_dir_missing(service, soa_dir))
+        print(PaastaCheckMessages.service_dir_missing(service, soa_dir))
 
 
 def smartstack_check(service, service_path, soa_dir):
     """Check whether smartstack.yaml exists in service directory, and the proxy
     ports are declared.  Print appropriate message depending on outcome.
 
     :param service: name of service currently being examined
     :param service_path: path to location of smartstack.yaml file"""
     if is_file_in_dir("smartstack.yaml", service_path):
-        paasta_print(PaastaCheckMessages.SMARTSTACK_YAML_FOUND)
+        print(PaastaCheckMessages.SMARTSTACK_YAML_FOUND)
         instances = get_all_namespaces_for_service(service=service, soa_dir=soa_dir)
         if len(instances) > 0:
             for namespace, config in get_all_namespaces_for_service(
                 service=service, soa_dir=soa_dir, full_name=False
             ):
                 if "proxy_port" in config:
-                    paasta_print(
+                    print(
                         PaastaCheckMessages.smartstack_port_found(
                             namespace, config.get("proxy_port")
                         )
                     )
                 else:
-                    paasta_print(PaastaCheckMessages.SMARTSTACK_PORT_MISSING)
+                    print(PaastaCheckMessages.SMARTSTACK_PORT_MISSING)
         else:
-            paasta_print(PaastaCheckMessages.SMARTSTACK_PORT_MISSING)
+            print(PaastaCheckMessages.SMARTSTACK_PORT_MISSING)
 
 
 def paasta_check(args):
     """Analyze the service in the PWD to determine if it is paasta ready
     :param args: argparse.Namespace obj created from sys.args by cli"""
     soa_dir = args.yelpsoa_config_root
     service = figure_out_service_name(args, soa_dir)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/get_latest_deployment.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/list.py`

 * *Files 19% similar despite different names*

```diff
@@ -8,67 +8,59 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import sys
+from service_configuration_lib import DEFAULT_SOA_DIR
 
-from paasta_tools.cli.utils import lazy_choices_completer
-from paasta_tools.cli.utils import list_deploy_groups
-from paasta_tools.cli.utils import PaastaColors
-from paasta_tools.cli.utils import validate_service_name
-from paasta_tools.deployment_utils import get_currently_deployed_sha
-from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.cli.utils import list_paasta_services
+from paasta_tools.cli.utils import list_service_instances
 from paasta_tools.utils import list_services
-from paasta_tools.utils import paasta_print
+from paasta_tools.utils import SPACER
 
 
 def add_subparser(subparsers):
     list_parser = subparsers.add_parser(
-        "get-latest-deployment",
-        help="Gets the Git SHA for the latest deployment of a service",
+        "list",
+        help="Display a list of PaaSTA services",
+        description=(
+            "'paasta list' inspects the soa-configs directory and lists all of the "
+            "PaaSTA services that are declared."
+        ),
     )
     list_parser.add_argument(
-        "-s",
-        "--service",
-        help="Name of the service which you want to get the latest deployment for.",
-        required=True,
-    ).completer = lazy_choices_completer(list_services)
+        "-a",
+        "--all",
+        action="store_true",
+        help="Display all services, even if not on PaaSTA.",
+    )
     list_parser.add_argument(
         "-i",
-        "-l",
-        "--deploy-group",
-        help="Name of the deploy group which you want to get the latest deployment for.",
-        required=True,
-    ).completer = lazy_choices_completer(list_deploy_groups)
+        "--print-instances",
+        action="store_true",
+        help="Display all service%sinstance values, which only PaaSTA services have."
+        % SPACER,
+    )
     list_parser.add_argument(
-        "-d",
-        "--soa-dir",
-        help="A directory from which soa-configs should be read from",
+        "-y",
+        "--yelpsoa-config-root",
+        dest="soa_dir",
+        help="A directory from which yelpsoa-configs should be read from",
         default=DEFAULT_SOA_DIR,
     )
-
-    list_parser.set_defaults(command=paasta_get_latest_deployment)
+    list_parser.set_defaults(command=paasta_list)
 
 
-def paasta_get_latest_deployment(args):
-    service = args.service
-    deploy_group = args.deploy_group
-    soa_dir = args.soa_dir
-    validate_service_name(service, soa_dir)
-
-    git_sha = get_currently_deployed_sha(
-        service=service, deploy_group=deploy_group, soa_dir=soa_dir
-    )
-    if not git_sha:
-        paasta_print(
-            PaastaColors.red(
-                f"A deployment could not be found for {deploy_group} in {service}"
-            ),
-            file=sys.stderr,
-        )
-        return 1
+def paasta_list(args):
+    """Print a list of Yelp services currently running
+    :param args: argparse.Namespace obj created from sys.args by cli"""
+    if args.print_instances:
+        services = list_service_instances(args.soa_dir)
+    elif args.all:
+        services = list_services(args.soa_dir)
     else:
-        paasta_print(git_sha)
-        return 0
+        services = list_paasta_services(args.soa_dir)
+
+    for service in services:
+        print(service)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/cook_image.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/itest.py`

 * *Files 18% similar despite different names*

```diff
@@ -8,103 +8,110 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Contains methods used by the paasta client to build a docker image."""
 import os
-import sys
 
-from paasta_tools.cli.cmds.check import makefile_responds_to
+from paasta_tools.cli.utils import get_jenkins_build_output_url
+from paasta_tools.cli.utils import lazy_choices_completer
 from paasta_tools.cli.utils import validate_service_name
 from paasta_tools.utils import _log
-from paasta_tools.utils import _log_audit
 from paasta_tools.utils import _run
+from paasta_tools.utils import build_docker_tag
+from paasta_tools.utils import check_docker_image
 from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import get_username
-from paasta_tools.utils import paasta_print
+from paasta_tools.utils import list_services
 
 
 def add_subparser(subparsers):
     list_parser = subparsers.add_parser(
-        "cook-image",
-        description="Calls 'make cook-image' as part of the PaaSTA contract",
-        help=(
-            "'paasta cook-image' calls 'make cook-image' as part of the PaaSTA contract.\n\n"
-            "The PaaSTA contract specifies that a service MUST respond to 'cook-image' and produce "
-            "a docker image as a result. This command is often run as part of the normal build pipeline "
-            "('paasta itest'), or via a 'paasta local-run --build'."
+        "itest",
+        help="Runs 'make itest' as part of the PaaSTA contract.",
+        description=(
+            "'paasta itest' runs 'make itest' in the root of a service directory. "
+            "It is designed to be used in conjunction with the 'Jenkins' workflow: "
+            "http://paasta.readthedocs.io/en/latest/about/contract.html#jenkins-pipeline-recommended"
         ),
-        epilog="This command assumes that the Makefile is in the current working directory.",
     )
     list_parser.add_argument(
         "-s",
         "--service",
-        help=(
-            "Build docker image for this service. Leading "
-            '"services-", as included in a Jenkins job name, '
-            "will be stripped."
-        ),
+        help="Test and build docker image for this service. Leading "
+        '"services-", as included in a Jenkins job name, '
+        "will be stripped.",
         required=True,
     )
     list_parser.add_argument(
-        "-y",
-        "--yelpsoa-config-root",
-        dest="yelpsoa_config_root",
-        help="A directory from which yelpsoa-configs should be read from",
+        "-c",
+        "--commit",
+        help="Git sha used to construct tag for built image",
+        required=True,
+    )
+    list_parser.add_argument(
+        "--image-version",
+        type=str,
+        required=False,
+        default=None,
+        help="Extra version metadata used to construct tag for built image",
+    )
+    list_parser.add_argument(
+        "-d",
+        "--soa-dir",
+        dest="soa_dir",
+        help="A directory from which soa-configs should be read from",
         default=DEFAULT_SOA_DIR,
+    ).completer = lazy_choices_completer(list_services)
+    list_parser.add_argument(
+        "--timeout",
+        dest="timeout",
+        help="How many seconds before this command times out",
+        default=3600,
+        type=float,
     )
-    list_parser.set_defaults(command=paasta_cook_image)
+    list_parser.set_defaults(command=paasta_itest)
 
 
-def paasta_cook_image(args, service=None, soa_dir=None):
-    """Build a docker image"""
-    if not service:
-        service = args.service
-    if service.startswith("services-"):
+def paasta_itest(args):
+    """Build and test a docker image"""
+    service = args.service
+    soa_dir = args.soa_dir
+    if service and service.startswith("services-"):
         service = service.split("services-", 1)[1]
-    if not soa_dir:
-        soa_dir = args.yelpsoa_config_root
-    validate_service_name(service, soa_dir)
+    validate_service_name(service, soa_dir=soa_dir)
 
+    tag = build_docker_tag(service, args.commit, args.image_version)
     run_env = os.environ.copy()
-    default_tag = "paasta-cook-image-{}-{}".format(service, get_username())
-    tag = run_env.get("DOCKER_TAG", default_tag)
     run_env["DOCKER_TAG"] = tag
+    cmd = "make itest"
+    loglines = []
 
-    if not makefile_responds_to("cook-image"):
-        paasta_print(
-            "ERROR: local-run now requires a cook-image target to be present in the Makefile. See"
-            "http://paasta.readthedocs.io/en/latest/about/contract.html",
-            file=sys.stderr,
-        )
-        return 1
-
-    try:
-        cmd = "make cook-image"
-        returncode, output = _run(
-            cmd,
-            env=run_env,
-            log=True,
-            component="build",
-            service=service,
-            loglevel="debug",
-        )
-        if returncode != 0:
-            _log(
-                service=service,
-                line="ERROR: make cook-image failed for %s." % service,
-                component="build",
-                level="event",
-            )
-        else:
-            action_details = {"tag": tag}
-            _log_audit(
-                action="cook-image", action_details=action_details, service=service
-            )
-        return returncode
-
-    except KeyboardInterrupt:
-        paasta_print("\nProcess interrupted by the user. Cancelling.", file=sys.stderr)
-        return 2
+    _log(
+        service=service,
+        line="starting itest for %s." % args.commit,
+        component="build",
+        level="event",
+    )
+    returncode, output = _run(
+        cmd,
+        env=run_env,
+        timeout=args.timeout,
+        log=True,
+        component="build",
+        service=service,
+        loglevel="debug",
+    )
+    if returncode != 0:
+        loglines.append("ERROR: itest failed for %s." % args.commit)
+        output = get_jenkins_build_output_url()
+        if output:
+            loglines.append("See output: %s" % output)
+    else:
+        loglines.append("itest passed for %s." % args.commit)
+        if not check_docker_image(service, args.commit, args.image_version):
+            loglines.append("ERROR: itest has not created %s" % tag)
+            returncode = 1
+    for logline in loglines:
+        _log(service=service, line=logline, component="build", level="event")
+    return returncode
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/remote_run.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/remote_run.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,14 @@
 
 from paasta_tools.cli.utils import lazy_choices_completer
 from paasta_tools.cli.utils import list_clusters
 from paasta_tools.cli.utils import list_instances
 from paasta_tools.cli.utils import run_on_master
 from paasta_tools.utils import list_services
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import PaastaNotConfiguredError
 from paasta_tools.utils import SystemPaastaConfig
 
 
 ARG_DEFAULTS = dict(
     common=dict(
@@ -52,15 +51,15 @@
 )
 
 
 def get_system_paasta_config():
     try:
         return load_system_paasta_config()
     except PaastaNotConfiguredError:
-        paasta_print(
+        print(
             PaastaColors.yellow(
                 "Warning: Couldn't load config files from '/etc/paasta'. This "
                 "indicates PaaSTA is not configured locally on this host, and "
                 "remote-run may not behave the same way it would behave on a "
                 "server configured for PaaSTA."
             ),
             sep="\n",
@@ -289,15 +288,15 @@
     system_paasta_config = get_system_paasta_config()
 
     if not args.cluster:
         default_cluster = system_paasta_config.get_remote_run_config().get(
             "default_cluster"
         )
         if not default_cluster:
-            paasta_print(
+            print(
                 PaastaColors.red(
                     "Error: no cluster specified and no default cluster available"
                 )
             )
             return 1
         args.cluster = default_cluster
 
@@ -309,10 +308,10 @@
         cmd_parts=cmd_parts,
         graceful_exit=graceful_exit,
     )
 
     # Status results are streamed. This print is for possible error messages.
     if status is not None:
         for line in status.rstrip().split("\n"):
-            paasta_print("    %s" % line)
+            print("    %s" % line)
 
     return return_code
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/push_to_registry.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/cook_image.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,193 +8,140 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Contains methods used by the paasta client to upload a docker
-image to a registry.
-"""
-import base64
-import binascii
-import json
+"""Contains methods used by the paasta client to build a docker image."""
+import argparse
 import os
+import sys
+from typing import Optional
 
-import requests
-from requests.exceptions import RequestException
-from requests.exceptions import SSLError
-
-from paasta_tools.cli.utils import get_jenkins_build_output_url
-from paasta_tools.cli.utils import validate_full_git_sha
+from paasta_tools.cli.cmds.check import makefile_responds_to
 from paasta_tools.cli.utils import validate_service_name
-from paasta_tools.generate_deployments_for_service import build_docker_image_name
 from paasta_tools.utils import _log
 from paasta_tools.utils import _log_audit
 from paasta_tools.utils import _run
 from paasta_tools.utils import build_docker_tag
 from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import get_service_docker_registry
-from paasta_tools.utils import paasta_print
+from paasta_tools.utils import get_username
 
 
-def add_subparser(subparsers):
+def add_subparser(subparsers: argparse._SubParsersAction) -> None:
     list_parser = subparsers.add_parser(
-        "push-to-registry",
-        help="Uploads a docker image to a registry",
-        description=(
-            "'paasta push-to-registry' is a tool to upload a local docker image "
-            "to the configured PaaSTA docker registry with a predictable and "
-            "well-constructed image name. The image name must be predictable because "
-            "the other PaaSTA components are expecting a particular format for the docker "
-            "image name."
-        ),
-        epilog=(
-            "Note: Uploading to a docker registry often requires access to the local "
-            "docker socket as well as credentials to the remote registry"
+        "cook-image",
+        description="Calls 'make cook-image' as part of the PaaSTA contract",
+        help=(
+            "'paasta cook-image' calls 'make cook-image' as part of the PaaSTA contract.\n\n"
+            "The PaaSTA contract specifies that a service MUST respond to 'cook-image' and produce "
+            "a docker image as a result. This command is often run as part of the normal build pipeline "
+            "('paasta itest'), or via a 'paasta local-run --build'."
         ),
+        epilog="This command assumes that the Makefile is in the current working directory.",
     )
     list_parser.add_argument(
         "-s",
         "--service",
-        help='Name of service for which you wish to upload a docker image. Leading "services-", '
-        "as included in a Jenkins job name, will be stripped.",
-        required=True,
-    )
-    list_parser.add_argument(
-        "-c",
-        "--commit",
-        help="Git sha after which to name the remote image",
+        help=(
+            "Build docker image for this service. Leading "
+            '"services-", as included in a Jenkins job name, '
+            "will be stripped."
+        ),
         required=True,
-        type=validate_full_git_sha,
     )
     list_parser.add_argument(
-        "-d",
-        "--soa-dir",
-        dest="soa_dir",
-        metavar="SOA_DIR",
+        "-y",
+        "--yelpsoa-config-root",
+        dest="yelpsoa_config_root",
+        help="A directory from which yelpsoa-configs should be read from",
         default=DEFAULT_SOA_DIR,
-        help="define a different soa config directory",
     )
     list_parser.add_argument(
-        "-f",
-        "--force",
-        help=(
-            "Do not check if the image is already in the PaaSTA docker registry. "
-            "Push it anyway."
-        ),
-        action="store_true",
+        "-c",
+        "--commit",
+        help="Git sha used to construct tag for built image",
     )
-    list_parser.set_defaults(command=paasta_push_to_registry)
-
-
-def build_command(upstream_job_name, upstream_git_commit):
-    # This is kinda dumb since we just cleaned the 'services-' off of the
-    # service so we could validate it, but the Docker image will have the full
-    # name with 'services-' so add it back.
-    tag = build_docker_tag(upstream_job_name, upstream_git_commit)
-    cmd = f"docker push {tag}"
-    return cmd
-
-
-def paasta_push_to_registry(args):
-    """Upload a docker image to a registry"""
-    service = args.service
+    list_parser.add_argument(
+        "--image-version",
+        type=str,
+        required=False,
+        default=None,
+        help="Extra version metadata used to construct tag for built image",
+    )
+    list_parser.set_defaults(command=paasta_cook_image)
+
+
+def paasta_cook_image(
+    args: Optional[argparse.Namespace],
+    service: Optional[str] = None,
+    soa_dir: Optional[str] = None,
+) -> int:
+    """Build a docker image"""
+    if not service:
+        if args is None:
+            print(
+                "ERROR: No arguments or service passed to cook-image - unable to determine what service to cook an image for",
+                file=sys.stderr,
+            )
+            return 1
+        service = args.service
     if service and service.startswith("services-"):
         service = service.split("services-", 1)[1]
-    validate_service_name(service, args.soa_dir)
-
-    if not args.force:
-        try:
-            if is_docker_image_already_in_registry(service, args.soa_dir, args.commit):
-                paasta_print(
-                    "The docker image is already in the PaaSTA docker registry. "
-                    "I'm NOT overriding the existing image. "
-                    "Add --force to override the image in the registry if you are sure what you are doing."
-                )
-                return 0
-        except RequestException as e:
-            registry_uri = get_service_docker_registry(service, args.soa_dir)
-            paasta_print(
-                "Can not connect to the PaaSTA docker registry '%s' to verify if this image exists.\n"
-                "%s" % (registry_uri, str(e))
+    if not soa_dir:
+        if args is None:
+            print(
+                "ERROR: No arguments or soadir passed to cook-image - unable to determine where to look for soa-configs",
+                file=sys.stderr,
             )
             return 1
+        soa_dir = args.yelpsoa_config_root
 
-    cmd = build_command(service, args.commit)
-    loglines = []
-    returncode, output = _run(
-        cmd,
-        timeout=3600,
-        log=True,
-        component="build",
-        service=service,
-        loglevel="debug",
-    )
-    if returncode != 0:
-        loglines.append("ERROR: Failed to promote image for %s." % args.commit)
-        output = get_jenkins_build_output_url()
-        if output:
-            loglines.append("See output: %s" % output)
+    validate_service_name(service, soa_dir)
+
+    run_env = os.environ.copy()
+    if args is not None and args.commit is not None:
+        # if we're given a commit, we're likely being called by Jenkins or someone
+        # trying to push the cooked image to our registry - as such, we should tag
+        # the cooked image as `paasta itest` would.
+        tag = build_docker_tag(service, args.commit, args.image_version)
     else:
-        loglines.append("Successfully pushed image for %s to registry" % args.commit)
-        _log_audit(
-            action="push-to-registry",
-            action_details={"commit": args.commit},
-            service=service,
+        default_tag = "paasta-cook-image-{}-{}".format(service, get_username())
+        tag = run_env.get("DOCKER_TAG", default_tag)
+    run_env["DOCKER_TAG"] = tag
+
+    if not makefile_responds_to("cook-image"):
+        print(
+            "ERROR: local-run now requires a cook-image target to be present in the Makefile. See "
+            "http://paasta.readthedocs.io/en/latest/about/contract.html.",
+            file=sys.stderr,
         )
-    for logline in loglines:
-        _log(service=service, line=logline, component="build", level="event")
-    return returncode
+        return 1
 
-
-def read_docker_registry_creds(registry_uri):
-    dockercfg_path = os.path.expanduser("~/.dockercfg")
     try:
-        with open(dockercfg_path) as f:
-            dockercfg = json.load(f)
-            auth = base64.b64decode(dockercfg[registry_uri]["auth"]).decode("utf-8")
-            first_colon = auth.find(":")
-            if first_colon != -1:
-                return (auth[:first_colon], auth[first_colon + 1 : -2])
-    except IOError:  # Can't open ~/.dockercfg
-        pass
-    except json.scanner.JSONDecodeError:  # JSON decoder error
-        pass
-    except binascii.Error:  # base64 decode error
-        pass
-    return (None, None)
-
-
-def is_docker_image_already_in_registry(service, soa_dir, sha):
-    """Verifies that docker image exists in the paasta registry.
-
-    :param service: name of the service
-    :param sha: git sha
-    :returns: True, False or raises requests.exceptions.RequestException
-    """
-    registry_uri = get_service_docker_registry(service, soa_dir)
-    repository, tag = build_docker_image_name(service, sha).split(":")
-
-    creds = read_docker_registry_creds(registry_uri)
-    uri = f"{registry_uri}/v2/{repository}/manifests/paasta-{sha}"
-
-    with requests.Session() as s:
-        try:
-            url = "https://" + uri
-            r = (
-                s.head(url, timeout=30)
-                if creds[0] is None
-                else s.head(url, auth=creds, timeout=30)
+        cmd = "make cook-image"
+        returncode, output = _run(
+            cmd,
+            env=run_env,
+            log=True,
+            component="build",
+            service=service,
+            loglevel="debug",
+        )
+        if returncode != 0:
+            _log(
+                service=service,
+                line="ERROR: make cook-image failed for %s." % service,
+                component="build",
+                level="event",
+            )
+        else:
+            action_details = {"tag": tag}
+            _log_audit(
+                action="cook-image", action_details=action_details, service=service
             )
-        except SSLError:
-            # If no auth creds, fallback to trying http
-            if creds[0] is not None:
-                raise
-            url = "http://" + uri
-            r = s.head(url, timeout=30)
-
-        if r.status_code == 200:
-            return True
-        elif r.status_code == 404:
-            return False  # No Such Repository Error
-        r.raise_for_status()
+        return returncode
+
+    except KeyboardInterrupt:
+        print("\nProcess interrupted by the user. Cancelling.", file=sys.stderr)
+        return 2
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/secret.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/wait_for_deployment.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,244 +1,275 @@
 #!/usr/bin/env python
-# Copyright 2015-2018 Yelp Inc.
+# Copyright 2015-2016 Yelp Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import argparse
-import os
-import re
-import sys
-
+"""Contains methods used by the paasta client to wait for deployment
+of a docker image to a cluster.instance.
+"""
+import asyncio
+import logging
+from typing import Optional
+
+from paasta_tools.cli.cmds.mark_for_deployment import NoSuchCluster
+from paasta_tools.cli.cmds.mark_for_deployment import report_waiting_aborted
+from paasta_tools.cli.cmds.mark_for_deployment import wait_for_deployment
 from paasta_tools.cli.utils import lazy_choices_completer
-from paasta_tools.secret_tools import get_secret_provider
-from paasta_tools.secret_tools import SHARED_SECRET_SERVICE
-from paasta_tools.utils import _log_audit
-from paasta_tools.utils import list_clusters
+from paasta_tools.cli.utils import list_deploy_groups
+from paasta_tools.cli.utils import NoSuchService
+from paasta_tools.cli.utils import validate_git_sha
+from paasta_tools.cli.utils import validate_given_deploy_groups
+from paasta_tools.cli.utils import validate_service_name
+from paasta_tools.cli.utils import validate_short_git_sha
+from paasta_tools.remote_git import list_remote_refs
+from paasta_tools.remote_git import LSRemoteException
+from paasta_tools.utils import _log
+from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import DeploymentVersion
+from paasta_tools.utils import get_git_url
+from paasta_tools.utils import get_latest_deployment_tag
 from paasta_tools.utils import list_services
-from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
+from paasta_tools.utils import PaastaColors
+from paasta_tools.utils import TimeoutError
 
+DEFAULT_DEPLOYMENT_TIMEOUT = 3600  # seconds
 
-SECRET_NAME_REGEX = r"([A-Za-z0-9_-]*)"
 
+log = logging.getLogger(__name__)
 
-def check_secret_name(secret_name_arg: str):
-    pattern = re.compile(SECRET_NAME_REGEX)
-    if (
-        not secret_name_arg.startswith("-")
-        and not secret_name_arg.startswith("_")
-        and "".join(pattern.findall(secret_name_arg)) == secret_name_arg
-    ):
-        return secret_name_arg
-    raise argparse.ArgumentTypeError(
-        "--secret-name argument should only contain letters, numbers, "
-        "dashes and underscores characters and cannot start from latter two"
-    )
+
+class VersionError(Exception):
+    pass
+
+
+class DeployGroupError(Exception):
+    pass
 
 
 def add_subparser(subparsers):
-    secret_parser = subparsers.add_parser(
-        "secret",
-        help="Add/update PaaSTA service secrets",
+    list_parser = subparsers.add_parser(
+        "wait-for-deployment",
+        help="Wait a service to be deployed to deploy_group",
         description=(
-            "This script allows you to add secrets to your services "
-            "as environment variables. This script modifies your local "
-            "checkout of yelpsoa-configs and you must then commit and "
-            "push the changes back to git."
+            "'paasta wait-for-deployment' waits for a previously marked for "
+            "deployment service to be deployed to deploy_group."
+        ),
+        epilog=(
+            "Note: Access and credentials to the Git repo of a service "
+            "are required for this command to work."
         ),
     )
-    secret_parser.add_argument(
-        "action", help="should be add/update", choices=["add", "update", "decrypt"]
-    )
-    secret_parser.add_argument(
-        "-n",
-        "--secret-name",
-        type=check_secret_name,
-        required=True,
-        help="The name of the secret to create/update, "
-        "this is the name you will reference in your "
-        "services yaml files and should "
-        "be unique per service.",
-    )
-
-    # Must choose valid service or act on a shared secret
-    service_group = secret_parser.add_mutually_exclusive_group(required=True)
-    service_group.add_argument(
-        "-s", "--service", help="The name of the service on which you wish to act"
-    ).completer = lazy_choices_completer(list_services)
-    service_group.add_argument(
-        "--shared",
-        help="Act on a secret that can be shared by all services",
-        action="store_true",
+    list_parser.add_argument(
+        "-u",
+        "--git-url",
+        help=(
+            "Git url for service. Defaults to the normal git URL for " "the service."
+        ),
+        default=None,
     )
-
-    secret_parser.add_argument(
+    list_parser.add_argument(
         "-c",
-        "--clusters",
-        help="A comma-separated list of clusters to create secrets for. "
-        "Note: this is translated to ecosystems because Vault is run "
-        "at an ecosystem level. As a result you can only have different "
-        "secrets per ecosystem. (it is not possible for example to encrypt "
-        "a different value for norcal-prod vs nova-prod. "
-        "Defaults to all clusters in which the service runs. "
-        "For example: --clusters norcal-prod,nova-prod ",
-    ).completer = lazy_choices_completer(list_clusters)
-    secret_parser.add_argument(
-        "-p",
-        "--plain-text",
-        required=False,
-        type=str,
-        help="Optionally specify the secret as a command line argument",
+        "-k",
+        "--commit",
+        help="Git sha to wait for deployment",
+        required=True,
+        type=validate_short_git_sha,
     )
-    secret_parser.add_argument(
+    list_parser.add_argument(
         "-i",
-        "--stdin",
+        "--image-version",
+        help="Extra version metadata to mark for deployment",
         required=False,
-        action="store_true",
-        default=False,
-        help="Optionally pass the plaintext from stdin",
-    )
-    secret_parser.set_defaults(command=paasta_secret)
-
-
-def secret_name_for_env(secret_name):
-    secret_name = secret_name.upper()
-    valid_parts = re.findall(r"[a-zA-Z0-9_]+", secret_name)
-    return "_".join(valid_parts)
-
-
-def print_paasta_helper(secret_path, secret_name, is_shared):
-    print(
-        "\nYou have successfully encrypted your new secret and it\n"
-        "has been stored at {}\n"
-        "To use the secret in a service you can add it to your PaaSTA service\n"
-        "as an environment variable.\n"
-        "You do so by referencing it in the env dict in your yaml config:\n\n"
-        "main:\n"
-        "  cpus: 1\n"
-        "  env:\n"
-        "    PAASTA_SECRET_{}: {}SECRET({})\n\n"
-        "Once you have referenced the secret you must commit the newly\n"
-        "created/updated json file and your changes to your yaml config. When\n"
-        "you push to master PaaSTA will bounce your service and the new\n"
-        "secrets plaintext will be in the environment variable you have\n"
-        "specified. The PAASTA_SECRET_ prefix is optional but necessary\n"
-        "for the yelp_servlib client library".format(
-            secret_path,
-            secret_name_for_env(secret_name),
-            "SHARED_" if is_shared else "",
-            secret_name,
-        )
+        default=None,
+    )
+    list_parser.add_argument(
+        "-l",
+        "--deploy-group",
+        help="deploy group (e.g. cluster1.canary, cluster2.main).",
+        required=True,
+    ).completer = lazy_choices_completer(list_deploy_groups)
+    list_parser.add_argument(
+        "-s",
+        "--service",
+        help="Name of the service which you wish to wait for deployment. "
+        'Leading "services-" will be stripped.',
+        required=True,
+    ).completer = lazy_choices_completer(list_services)
+    list_parser.add_argument(
+        "-t",
+        "--timeout",
+        dest="timeout",
+        type=int,
+        default=DEFAULT_DEPLOYMENT_TIMEOUT,
+        help=(
+            "Time in seconds to wait for paasta to deploy the service. "
+            "If the timeout is exceeded we return 1. "
+            "Default is %(default)s seconds."
+        ),
     )
+    list_parser.add_argument(
+        "-d",
+        "--soa-dir",
+        dest="soa_dir",
+        metavar="SOA_DIR",
+        default=DEFAULT_SOA_DIR,
+        help="define a different soa config directory",
+    )
+    list_parser.add_argument(
+        "-v",
+        "--verbose",
+        action="count",
+        dest="verbose",
+        default=0,
+        help="Print out more output.",
+    )
+    list_parser.add_argument(
+        "--polling-interval",
+        dest="polling_interval",
+        type=float,
+        default=None,
+        help="How long to wait between each time we check to see if an instance is done deploying.",
+    )
+    list_parser.add_argument(
+        "--diagnosis-interval",
+        dest="diagnosis_interval",
+        type=float,
+        default=None,
+        help="How long to wait between diagnoses of why the bounce isn't done.",
+    )
+    list_parser.add_argument(
+        "--time-before-first-diagnosis",
+        dest="time_before_first_diagnosis",
+        type=float,
+        default=None,
+        help="Wait this long before trying to diagnose why the bounce isn't done.",
+    )
+
+    list_parser.set_defaults(command=paasta_wait_for_deployment)
+
+
+def get_latest_marked_version(
+    git_url: str, deploy_group: str
+) -> Optional[DeploymentVersion]:
+    """Return the latest marked for deployment version or None"""
+    # TODO: correct this function for new tag format
+    refs = list_remote_refs(git_url)
+    _, sha, image_version = get_latest_deployment_tag(refs, deploy_group)
+    if sha:
+        return DeploymentVersion(sha=sha, image_version=image_version)
+    # We did not find a ref for this deploy group
+    return None
+
+
+def validate_version_is_latest(
+    version: DeploymentVersion, git_url: str, deploy_group: str, service: str
+):
+    """Verify if the requested version  is the latest marked for deployment.
+
+    Raise exception when the provided version is not the latest
+    marked for deployment in 'deploy_group' for 'service'.
+    """
+    try:
+        marked_version = get_latest_marked_version(git_url, deploy_group)
+    except LSRemoteException as e:
+        print(
+            "Error talking to the git server: {}\n"
+            "It is not possible to verify that {} is marked for deployment in {}, "
+            "but I assume that it is marked and will continue waiting..".format(
+                e, version, deploy_group
+            )
+        )
+        return
+    if marked_version is None:
+        raise VersionError(
+            "ERROR: Nothing is marked for deployment "
+            "in {} for {}".format(deploy_group, service)
+        )
+    if version != marked_version:
+        raise VersionError(
+            "ERROR: The latest version marked for "
+            "deployment in {} is {}".format(deploy_group, marked_version)
+        )
 
 
-def get_plaintext_input(args):
-    if args.stdin:
-        plaintext = sys.stdin.buffer.read()
-    elif args.plain_text:
-        plaintext = args.plain_text.encode("utf-8")
-    else:
-        print(
-            "Please enter the plaintext for the secret, then enter a newline and Ctrl-D when done."
+def validate_deploy_group(deploy_group: str, service: str, soa_dir: str):
+    """Validate deploy_group.
+
+    Raise exception if the specified deploy group is not used anywhere.
+    """
+    in_use_deploy_groups = list_deploy_groups(service=service, soa_dir=soa_dir)
+    _, invalid_deploy_groups = validate_given_deploy_groups(
+        in_use_deploy_groups, [deploy_group]
+    )
+
+    if len(invalid_deploy_groups) == 1:
+        raise DeployGroupError(
+            "ERROR: These deploy groups are not currently "
+            "used anywhere: {}.\n"
+            "You probably need one of these in-use deploy "
+            "groups?:\n   {}".format(
+                ",".join(invalid_deploy_groups), ",".join(in_use_deploy_groups)
+            )
         )
-        lines = []
-        while True:
-            try:
-                line = input()
-            except EOFError:
-                break
-            lines.append(line)
-        plaintext = "\n".join(lines).encode("utf-8")
-        print("The secret as a Python string is:", repr(plaintext))
-        print("Please make sure this is correct.")
-    return plaintext
-
-
-def is_service_folder(soa_dir, service_name):
-    return os.path.isfile(os.path.join(soa_dir, service_name, "service.yaml"))
-
-
-def _get_secret_provider_for_service(service_name, cluster_names=None):
-    if not is_service_folder(os.getcwd(), service_name):
-        paasta_print(
-            "{} not found.\n"
-            "You must run this tool from the root of your local yelpsoa checkout\n"
-            "The tool modifies files in yelpsoa-configs that you must then commit\n"
-            "and push back to git.".format(os.path.join(service_name, "service.yaml"))
-        )
-        sys.exit(1)
-    system_paasta_config = load_system_paasta_config()
-    secret_provider_kwargs = {
-        "vault_cluster_config": system_paasta_config.get_vault_cluster_config()
-    }
-    clusters = (
-        cluster_names.split(",")
-        if cluster_names
-        else list_clusters(service=service_name, soa_dir=os.getcwd())
-    )
-
-    return get_secret_provider(
-        secret_provider_name=system_paasta_config.get_secret_provider_name(),
-        soa_dir=os.getcwd(),
-        service_name=service_name,
-        cluster_names=clusters,
-        secret_provider_kwargs=secret_provider_kwargs,
-    )
 
 
-def paasta_secret(args):
-    if args.shared:
-        service = SHARED_SECRET_SERVICE
-        if not args.clusters:
-            print("A list of clusters is required for shared secrets.")
-            sys.exit(1)
+def paasta_wait_for_deployment(args):
+    """Wrapping wait_for_deployment"""
+    if args.verbose:
+        log.setLevel(level=logging.DEBUG)
     else:
-        service = args.service
-    secret_provider = _get_secret_provider_for_service(
-        service, cluster_names=args.clusters
-    )
-    if args.action in ["add", "update"]:
-        plaintext = get_plaintext_input(args)
-        if not plaintext:
-            print("Warning: Given plaintext is an empty string.")
-        secret_provider.write_secret(
-            action=args.action, secret_name=args.secret_name, plaintext=plaintext
-        )
-        secret_path = os.path.join(
-            secret_provider.secret_dir, f"{args.secret_name}.json"
-        )
-        _log_audit(
-            action=f"{args.action}-secret",
-            action_details={"secret_name": args.secret_name, "clusters": args.clusters},
-            service=service,
-        )
+        log.setLevel(level=logging.INFO)
 
-        print_paasta_helper(secret_path, args.secret_name, args.shared)
-    elif args.action == "decrypt":
-        print(
-            decrypt_secret(
-                secret_provider=secret_provider, secret_name=args.secret_name
-            ),
-            end="",
+    service = args.service
+    if service and service.startswith("services-"):
+        service = service.split("services-", 1)[1]
+
+    if args.git_url is None:
+        args.git_url = get_git_url(service=service, soa_dir=args.soa_dir)
+
+    args.commit = validate_git_sha(sha=args.commit, git_url=args.git_url)
+
+    version = DeploymentVersion(sha=args.commit, image_version=args.image_version)
+
+    try:
+        validate_service_name(service, soa_dir=args.soa_dir)
+        validate_deploy_group(args.deploy_group, service, args.soa_dir)
+        validate_version_is_latest(version, args.git_url, args.deploy_group, service)
+    except (VersionError, DeployGroupError, NoSuchService) as e:
+        print(PaastaColors.red(f"{e}"))
+        return 1
+
+    try:
+        asyncio.run(
+            wait_for_deployment(
+                service=service,
+                deploy_group=args.deploy_group,
+                git_sha=args.commit,
+                image_version=args.image_version,
+                soa_dir=args.soa_dir,
+                timeout=args.timeout,
+                polling_interval=args.polling_interval,
+                diagnosis_interval=args.diagnosis_interval,
+                time_before_first_diagnosis=args.time_before_first_diagnosis,
+            )
+        )
+        _log(
+            service=service,
+            component="deploy",
+            line=(f"Deployment of {version} for {args.deploy_group} complete"),
+            level="event",
         )
-    else:
-        print("Unknown action")
-        sys.exit(1)
 
+    except (KeyboardInterrupt, TimeoutError, NoSuchCluster):
+        report_waiting_aborted(service, args.deploy_group)
+        return 1
 
-def decrypt_secret(secret_provider, secret_name):
-    if len(secret_provider.cluster_names) > 1:
-        paasta_print(
-            "Can only decrypt for one cluster at a time!\nFor example, try '-c norcal-devc'"
-            " to decrypt the secret for this service in norcal-devc."
-        )
-        sys.exit(1)
-    return secret_provider.decrypt_secret(secret_name)
+    return 0
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/start_stop_restart.py` & `paasta-tools-1.0.0/paasta_tools/setup_kubernetes_job.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,324 +1,352 @@
 #!/usr/bin/env python
-# Copyright 2015-2016 Yelp Inc.
+# Copyright 2015-2018 Yelp Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import datetime
-import socket
-import sys
-from collections import defaultdict
-from typing import Dict
-from typing import List
+"""
+Usage: ./setup_kubernetes_job.py <service.instance> [options]
 
-import choice
-from bravado.exception import HTTPError
+Command line options:
 
-from paasta_tools import remote_git
-from paasta_tools import utils
-from paasta_tools.api.client import get_paasta_api_client
-from paasta_tools.cli.cmds.status import add_instance_filter_arguments
-from paasta_tools.cli.cmds.status import apply_args_filters
-from paasta_tools.cli.utils import get_instance_config
-from paasta_tools.flink_tools import FlinkDeploymentConfig
-from paasta_tools.generate_deployments_for_service import get_latest_deployment_tag
-from paasta_tools.marathon_tools import MarathonServiceConfig
+- -d <SOA_DIR>, --soa-dir <SOA_DIR>: Specify a SOA config dir to read from
+- -v, --verbose: Verbose output
+"""
+import argparse
+import logging
+import sys
+import traceback
+from typing import List
+from typing import Optional
+from typing import Sequence
+from typing import Tuple
+from typing import Union
+
+from paasta_tools.eks_tools import EksDeploymentConfig
+from paasta_tools.eks_tools import load_eks_service_config_no_cache
+from paasta_tools.kubernetes.application.controller_wrappers import Application
+from paasta_tools.kubernetes.application.controller_wrappers import (
+    get_application_wrapper,
+)
+from paasta_tools.kubernetes_tools import ensure_namespace
+from paasta_tools.kubernetes_tools import InvalidKubernetesConfig
+from paasta_tools.kubernetes_tools import KubeClient
+from paasta_tools.kubernetes_tools import KubernetesDeploymentConfig
+from paasta_tools.kubernetes_tools import list_all_paasta_deployments
+from paasta_tools.kubernetes_tools import load_kubernetes_service_config_no_cache
+from paasta_tools.metrics import metrics_lib
+from paasta_tools.utils import decompose_job_id
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import InvalidJobNameError
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
-from paasta_tools.utils import PaastaColors
-
-
-def add_subparser(subparsers):
-    for command, lower, upper, cmd_func in [
-        ("start", "start or restart", "Start or restart", paasta_start),
-        ("restart", "start or restart", "Start or restart", paasta_start),
-        ("stop", "stop", "Stop", paasta_stop),
-    ]:
-        status_parser = subparsers.add_parser(
-            command,
-            help="%ss a PaaSTA service in a graceful way." % upper,
-            description=(
-                "%ss a PaaSTA service in a graceful way. This uses the Git control plane."
-                % upper
-            ),
-            epilog=(
-                "This command uses Git, and assumes access and authorization to the Git repo "
-                "for the service is available."
-            ),
-        )
-        add_instance_filter_arguments(status_parser, verb=lower)
-        status_parser.add_argument(
-            "-d",
-            "--soa-dir",
-            dest="soa_dir",
-            metavar="SOA_DIR",
-            default=DEFAULT_SOA_DIR,
-            help="define a different soa config directory",
-        )
-        status_parser.set_defaults(command=cmd_func)
-
+from paasta_tools.utils import NoConfigurationForServiceError
+from paasta_tools.utils import NoDeploymentsAvailable
+from paasta_tools.utils import SPACER
+
+log = logging.getLogger(__name__)
 
-def format_tag(branch, force_bounce, desired_state):
-    return f"refs/tags/paasta-{branch}-{force_bounce}-{desired_state}"
 
-
-def make_mutate_refs_func(service_config, force_bounce, desired_state):
-    """Create a function that will inform send_pack that we want to create tags
-    corresponding to the set of branches passed, with the given force_bounce
-    and desired_state parameters. These tags will point at the current tip of
-    the branch they associate with.
-
-    dulwich's send_pack wants a function that takes a dictionary of ref name
-    to sha and returns a modified version of that dictionary. send_pack will
-    then diff what is returned versus what was passed in, and inform the remote
-    git repo of our desires."""
-
-    def mutate_refs(refs):
-        deploy_group = service_config.get_deploy_group()
-        (_, head_sha) = get_latest_deployment_tag(refs, deploy_group)
-        refs[
-            format_tag(service_config.get_branch(), force_bounce, desired_state)
-        ] = head_sha
-        return refs
-
-    return mutate_refs
-
-
-def log_event(service_config, desired_state):
-    user = utils.get_username()
-    host = socket.getfqdn()
-    line = "Issued request to change state of {} (an instance of {}) to '{}' by {}@{}".format(
-        service_config.get_instance(),
-        service_config.get_service(),
-        desired_state,
-        user,
-        host,
+def parse_args() -> argparse.Namespace:
+    parser = argparse.ArgumentParser(description="Creates Kubernetes jobs.")
+    parser.add_argument(
+        "service_instance_list",
+        nargs="+",
+        help="The list of Kubernetes service instances to create or update",
+        metavar="SERVICE%sINSTANCE" % SPACER,
     )
-    utils._log(
-        service=service_config.get_service(),
-        level="event",
-        cluster=service_config.get_cluster(),
-        instance=service_config.get_instance(),
-        component="deploy",
-        line=line,
+    parser.add_argument(
+        "-d",
+        "--soa-dir",
+        dest="soa_dir",
+        metavar="SOA_DIR",
+        default=DEFAULT_SOA_DIR,
+        help="define a different soa config directory",
     )
-
-    utils._log_audit(
-        action=desired_state,
-        service=service_config.get_service(),
-        cluster=service_config.get_cluster(),
-        instance=service_config.get_instance(),
+    parser.add_argument(
+        "-c",
+        "--cluster",
+        dest="cluster",
+        help="paasta cluster",
     )
-
-
-def issue_state_change_for_service(service_config, force_bounce, desired_state):
-    ref_mutator = make_mutate_refs_func(
-        service_config=service_config,
-        force_bounce=force_bounce,
-        desired_state=desired_state,
+    parser.add_argument(
+        "-v",
+        "--verbose",
+        action="store_true",
+        dest="verbose",
+        default=False,
     )
-    remote_git.create_remote_refs(
-        utils.get_git_url(service_config.get_service()), ref_mutator
+    parser.add_argument(
+        "-l",
+        "--rate-limit",
+        dest="rate_limit",
+        default=0,
+        metavar="LIMIT",
+        type=int,
+        help="Update or create up to this number of service instances. Default is 0 (no limit).",
     )
-    log_event(service_config=service_config, desired_state=desired_state)
-
-
-def print_marathon_message(desired_state):
-    if desired_state == "start":
-        paasta_print(
-            "This service will soon be gracefully started/restarted, replacing old instances according "
-            "to the bounce method chosen in soa-configs. "
-        )
-    elif desired_state == "stop":
-        paasta_print(
-            "This service will be gracefully stopped soon. It will be started back up again on the next deploy.\n"
-            "To stop this service permanently. Set this in the soa-configs definition:\n"
-            "\n"
-            "    instances: 0\n"
-        )
+    parser.add_argument(
+        "--eks",
+        help="This flag deploys only k8 services that should run on EKS",
+        dest="eks",
+        action="store_true",
+        default=False,
+    )
+    args = parser.parse_args()
+    return args
 
 
-def print_flink_message(desired_state):
-    if desired_state == "start":
-        paasta_print("'Start' will tell Flink operator to start the cluster.")
-    elif desired_state == "stop":
-        paasta_print(
-            "'Stop' will put Flink cluster in stopping mode, it may"
-            "take some time before shutdown is completed."
-        )
+def main() -> None:
+    args = parse_args()
+    soa_dir = args.soa_dir
+    if args.verbose:
+        logging.basicConfig(level=logging.DEBUG)
+    else:
+        # filter out unwanted zookeeper messages in the log
+        logging.getLogger("kazoo").setLevel(logging.WARN)
+        logging.basicConfig(level=logging.INFO)
+
+    # emit deploy events for updated jobs
+    deploy_metrics = metrics_lib.get_metrics_interface("paasta")
+
+    # emit timing metrics for s_k_j
+    cluster = args.cluster or load_system_paasta_config().get_cluster()
+    timer = metrics_lib.system_timer(
+        dimensions=dict(
+            cluster=cluster,
+            eks=args.eks,
+        ),
+    )
+    timer.start()
 
+    kube_client = KubeClient()
+    service_instances_valid = True
 
-def confirm_to_continue(cluster_service_instances, desired_state):
-    paasta_print(f"You are about to {desired_state} the following instances:")
-    paasta_print(
-        "Either --instances or --clusters not specified. Asking for confirmation."
+    # validate the service_instance names
+    service_instances_with_valid_names = get_service_instances_with_valid_names(
+        service_instances=args.service_instance_list
     )
-    i_count = 0
-    for cluster, services_instances in cluster_service_instances:
-        for service, instances in services_instances.items():
-            for instance in instances.keys():
-                paasta_print(f"cluster = {cluster}, instance = {instance}")
-                i_count += 1
-    if sys.stdin.isatty():
-        return choice.Binary(
-            f"Are you sure you want to {desired_state} these {i_count} instances?",
-            False,
-        ).ask()
-    return True
-
 
-REMOTE_REFS: Dict[str, List[str]] = {}
+    # returns a list of pairs of (No error?, KubernetesDeploymentConfig | EksDeploymentConfig) for every service_instance
+    service_instance_configs_list = get_kubernetes_deployment_config(
+        service_instances_with_valid_names=service_instances_with_valid_names,
+        cluster=cluster,
+        soa_dir=soa_dir,
+        eks=args.eks,
+    )
 
+    if ((False, None) in service_instance_configs_list) or (
+        len(service_instances_with_valid_names) != len(args.service_instance_list)
+    ):
+        service_instances_valid = False
+
+    if service_instance_configs_list:
+        for _, service_instance_config in service_instance_configs_list:
+            if service_instance_config:
+                ensure_namespace(
+                    kube_client, namespace=service_instance_config.get_namespace()
+                )
 
-def get_remote_refs(service, soa_dir):
-    if service not in REMOTE_REFS:
-        REMOTE_REFS[service] = remote_git.list_remote_refs(
-            utils.get_git_url(service, soa_dir)
+        setup_kube_succeeded = setup_kube_deployments(
+            kube_client=kube_client,
+            cluster=args.cluster or load_system_paasta_config().get_cluster(),
+            service_instance_configs_list=service_instance_configs_list,
+            rate_limit=args.rate_limit,
+            soa_dir=soa_dir,
+            metrics_interface=deploy_metrics,
+            eks=args.eks,
         )
-    return REMOTE_REFS[service]
-
-
-def paasta_start_or_stop(args, desired_state):
-    """Requests a change of state to start or stop given branches of a service."""
-    soa_dir = args.soa_dir
+    else:
+        setup_kube_succeeded = False
+    exit_code = 0 if setup_kube_succeeded and service_instances_valid else 1
+
+    timer.stop(tmp_dimensions={"result": exit_code})
+    logging.info(
+        f"Stopping timer for {cluster} (eks={args.eks}) with result {exit_code}: {timer()}ms elapsed"
+    )
+    sys.exit(exit_code)
 
-    pargs = apply_args_filters(args)
-    if len(pargs) == 0:
-        return 1
 
-    affected_services = {
-        s for service_list in pargs.values() for s in service_list.keys()
-    }
-    if len(affected_services) > 1:
-        paasta_print(
-            PaastaColors.red("Warning: trying to start/stop/restart multiple services:")
+def get_service_instances_with_valid_names(
+    service_instances: Sequence[str],
+) -> List[Tuple[str, str, str, str]]:
+    service_instances_with_valid_names = [
+        decompose_job_id(service_instance)
+        for service_instance in service_instances
+        if validate_job_name(service_instance)
+    ]
+    return service_instances_with_valid_names
+
+
+def validate_job_name(service_instance: str) -> bool:
+    try:
+        service, instance, _, __ = decompose_job_id(service_instance)
+    except InvalidJobNameError:
+        log.error(
+            "Invalid service instance specified. Format is service%sinstance." % SPACER
         )
+        return False
+    return True
+
 
-        for cluster, services_instances in pargs.items():
-            paasta_print("Cluster %s:" % cluster)
-            for service, instances in services_instances.items():
-                paasta_print("    Service %s:" % service)
-                paasta_print("        Instances %s" % ",".join(instances.keys()))
-
-        if sys.stdin.isatty():
-            confirm = choice.Binary("Are you sure you want to continue?", False).ask()
-        else:
-            confirm = False
-        if not confirm:
-            paasta_print()
-            paasta_print("exiting")
-            return 1
-
-    invalid_deploy_groups = []
-    marathon_message_printed = False
-    affected_flinks = []
-
-    if args.clusters is None or args.instances is None:
-        if confirm_to_continue(pargs.items(), desired_state) is False:
-            paasta_print()
-            paasta_print("exiting")
-            return 1
-
-    for cluster, services_instances in pargs.items():
-        for service, instances in services_instances.items():
-            for instance in instances.keys():
-                service_config = get_instance_config(
-                    service=service,
+def get_kubernetes_deployment_config(
+    service_instances_with_valid_names: list,
+    cluster: str,
+    soa_dir: str = DEFAULT_SOA_DIR,
+    eks: bool = False,
+) -> List[Tuple[bool, Union[KubernetesDeploymentConfig, EksDeploymentConfig]]]:
+    service_instance_configs_list = []
+    for service_instance in service_instances_with_valid_names:
+        try:
+            service_instance_config: Union[
+                KubernetesDeploymentConfig, EksDeploymentConfig
+            ]
+            if eks:
+                service_instance_config = load_eks_service_config_no_cache(
+                    service=service_instance[0],
+                    instance=service_instance[1],
                     cluster=cluster,
-                    instance=instance,
                     soa_dir=soa_dir,
-                    load_deployments=False,
                 )
-                if isinstance(service_config, FlinkDeploymentConfig):
-                    affected_flinks.append(service_config)
-                    continue
-
-                try:
-                    remote_refs = get_remote_refs(service, soa_dir)
-                except remote_git.LSRemoteException as e:
-                    msg = (
-                        "Error talking to the git server: %s\n"
-                        "This PaaSTA command requires access to the git server to operate.\n"
-                        "The git server may be down or not reachable from here.\n"
-                        "Try again from somewhere where the git server can be reached, "
-                        "like your developer environment."
-                    ) % str(e)
-                    paasta_print(msg)
-                    return 1
-
-                deploy_group = service_config.get_deploy_group()
-                (deploy_tag, _) = get_latest_deployment_tag(remote_refs, deploy_group)
+            else:
+                service_instance_config = load_kubernetes_service_config_no_cache(
+                    service=service_instance[0],
+                    instance=service_instance[1],
+                    cluster=cluster,
+                    soa_dir=soa_dir,
+                )
+            service_instance_configs_list.append((True, service_instance_config))
+        except NoDeploymentsAvailable:
+            log.debug(
+                "No deployments found for %s.%s in cluster %s. Skipping."
+                % (service_instance[0], service_instance[1], cluster)
+            )
+            service_instance_configs_list.append((True, None))
+        except NoConfigurationForServiceError:
+            error_msg = (
+                f"Could not read kubernetes configuration file for %s.%s in cluster %s"
+                % (service_instance[0], service_instance[1], cluster)
+            )
+            log.error(error_msg)
+            service_instance_configs_list.append((False, None))
+    return service_instance_configs_list
+
+
+def setup_kube_deployments(
+    kube_client: KubeClient,
+    cluster: str,
+    service_instance_configs_list: List[
+        Tuple[bool, Union[KubernetesDeploymentConfig, EksDeploymentConfig]]
+    ],
+    rate_limit: int = 0,
+    soa_dir: str = DEFAULT_SOA_DIR,
+    metrics_interface: metrics_lib.BaseMetrics = metrics_lib.NoMetrics("paasta"),
+    eks: bool = False,
+) -> bool:
+
+    if not service_instance_configs_list:
+        return True
+
+    existing_kube_deployments = set(list_all_paasta_deployments(kube_client))
+    existing_apps = {
+        (deployment.service, deployment.instance, deployment.namespace)
+        for deployment in existing_kube_deployments
+    }
 
-                if deploy_tag not in remote_refs:
-                    invalid_deploy_groups.append(deploy_group)
-                else:
-                    force_bounce = utils.format_timestamp(datetime.datetime.utcnow())
-                    if (
-                        isinstance(service_config, MarathonServiceConfig)
-                        and not marathon_message_printed
-                    ):
-                        print_marathon_message(desired_state)
-                        marathon_message_printed = True
-
-                    issue_state_change_for_service(
-                        service_config=service_config,
-                        force_bounce=force_bounce,
-                        desired_state=desired_state,
+    applications = [
+        create_application_object(
+            cluster=cluster,
+            soa_dir=soa_dir,
+            service_instance_config=service_instance,
+            eks=eks,
+        )
+        if service_instance
+        else (_, None)
+        for _, service_instance in service_instance_configs_list
+    ]
+    api_updates = 0
+    for _, app in applications:
+        if app:
+            app_dimensions = {
+                "paasta_service": app.kube_deployment.service,
+                "paasta_instance": app.kube_deployment.instance,
+                "paasta_cluster": cluster,
+                "paasta_namespace": app.kube_deployment.namespace,
+            }
+            try:
+                if (
+                    app.kube_deployment.service,
+                    app.kube_deployment.instance,
+                    app.kube_deployment.namespace,
+                ) not in existing_apps:
+                    if app.soa_config.get_bounce_method() == "downthenup":
+                        if any(
+                            (
+                                existing_app[:2]
+                                == (
+                                    app.kube_deployment.service,
+                                    app.kube_deployment.instance,
+                                )
+                            )
+                            for existing_app in existing_apps
+                        ):
+                            # For downthenup, we don't want to create until cleanup_kubernetes_job has cleaned up the instance in the other namespace.
+                            continue
+                    log.info(f"Creating {app} because it does not exist yet.")
+                    app.create(kube_client)
+                    app_dimensions["deploy_event"] = "create"
+                    metrics_interface.emit_event(
+                        name="deploy",
+                        dimensions=app_dimensions,
                     )
+                    api_updates += 1
+                elif app.kube_deployment not in existing_kube_deployments:
+                    log.info(f"Updating {app} because configs have changed.")
+                    app.update(kube_client)
+                    app_dimensions["deploy_event"] = "update"
+                    metrics_interface.emit_event(
+                        name="deploy",
+                        dimensions=app_dimensions,
+                    )
+                    api_updates += 1
+                else:
+                    log.info(f"{app} is up-to-date!")
 
-    return_val = 0
-
-    # TODO: Refactor to discover if set_state is available for given
-    #       instance_type in API
-    if affected_flinks:
-        print_flink_message(desired_state)
-        csi = defaultdict(lambda: defaultdict(list))
-        for service_config in affected_flinks:
-            csi[service_config.cluster][service_config.service].append(
-                service_config.instance
+                log.info(f"Ensuring related API objects for {app} are in sync")
+                app.update_related_api_objects(kube_client)
+            except Exception:
+                log.exception(f"Error while processing: {app}")
+        if rate_limit > 0 and api_updates >= rate_limit:
+            log.info(
+                f"Not doing any further updates as we reached the limit ({api_updates})"
             )
-
-        system_paasta_config = load_system_paasta_config()
-        for cluster, services_instances in csi.items():
-            client = get_paasta_api_client(cluster, system_paasta_config)
-            if not client:
-                paasta_print("Cannot get a paasta-api client")
-                exit(1)
-
-            for service, instances in services_instances.items():
-                for instance in instances:
-                    try:
-                        client.service.instance_set_state(
-                            service=service,
-                            instance=instance,
-                            desired_state=desired_state,
-                        ).result()
-                    except HTTPError as exc:
-                        paasta_print(exc.response.text)
-                        return exc.status_code
-
-                return_val = 0
-
-    if invalid_deploy_groups:
-        paasta_print(f"No deploy tags found for {', '.join(invalid_deploy_groups)}.")
-        paasta_print(f"Has {service} been deployed there yet?")
-        return_val = 1
-
-    return return_val
+            break
+    return (False, None) not in applications
 
 
-def paasta_start(args):
-    return paasta_start_or_stop(args, "start")
+def create_application_object(
+    cluster: str,
+    soa_dir: str,
+    service_instance_config: Union[KubernetesDeploymentConfig, EksDeploymentConfig],
+    eks: bool = False,
+) -> Tuple[bool, Optional[Application]]:
+    try:
+        formatted_application = service_instance_config.format_kubernetes_app()
+    except InvalidKubernetesConfig:
+        log.error(traceback.format_exc())
+        return False, None
+
+    app = get_application_wrapper(formatted_application)
+    app.load_local_config(soa_dir, cluster, eks)
+    return True, app
 
 
-def paasta_stop(args):
-    return paasta_start_or_stop(args, "stop")
+if __name__ == "__main__":
+    main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/logs.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/logs.py`

 * *Files 13% similar despite different names*

```diff
@@ -11,93 +11,115 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """PaaSTA log reader for humans"""
 import argparse
 import datetime
+import json
 import logging
 import re
 import sys
 from collections import namedtuple
 from contextlib import contextmanager
 from multiprocessing import Process
 from multiprocessing import Queue
 from queue import Empty
 from time import sleep
 from typing import Any
+from typing import Callable
+from typing import ContextManager
 from typing import Dict
+from typing import Iterable
 from typing import List
+from typing import Mapping
+from typing import MutableSequence
+from typing import Sequence
 from typing import Set
+from typing import Tuple
+from typing import Type
 
 import isodate
 import pytz
-import ujson as json
 from dateutil import tz
 
-from paasta_tools.utils import paasta_print
-
 try:
     from scribereader import scribereader
     from scribereader.scribereader import StreamTailerSetupError
 except ImportError:
     scribereader = None
 
 from pytimeparse.timeparse import timeparse
 
-from paasta_tools.marathon_tools import format_job_id
 from paasta_tools.cli.utils import figure_out_service_name
 from paasta_tools.cli.utils import guess_service_name
 from paasta_tools.cli.utils import lazy_choices_completer
+from paasta_tools.cli.utils import verify_instances
 from paasta_tools.utils import list_services
 from paasta_tools.utils import ANY_CLUSTER
 from paasta_tools.utils import datetime_convert_timezone
 from paasta_tools.utils import datetime_from_utc_to_local
 from paasta_tools.utils import DEFAULT_LOGLEVEL
 from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import format_log_line
 from paasta_tools.utils import load_system_paasta_config
 from paasta_tools.utils import list_clusters
 from paasta_tools.utils import LOG_COMPONENTS
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import get_log_name_for_service
 
 
-DEFAULT_COMPONENTS = ["build", "deploy", "monitoring", "oom", "stdout", "stderr"]
+DEFAULT_COMPONENTS = ["stdout", "stderr"]
 
 log = logging.getLogger(__name__)
 
 
-def add_subparser(subparsers):
+def add_subparser(subparsers) -> None:
     status_parser = subparsers.add_parser(
         "logs",
         help="Streams logs relevant to a service across the PaaSTA components",
         description=(
             "'paasta logs' works by streaming PaaSTA-related event messages "
             "in a human-readable way."
         ),
         formatter_class=argparse.RawDescriptionHelpFormatter,
     )
     status_parser.add_argument(
         "-s",
         "--service",
         help="The name of the service you wish to inspect. Defaults to autodetect.",
     ).completer = lazy_choices_completer(list_services)
-    components_help = "A comma separated list of the components you want logs for."
-    status_parser.add_argument(
-        "-C", "--components", help=components_help
-    ).completer = lazy_choices_completer(LOG_COMPONENTS.keys)
-    cluster_help = "The clusters to see relevant logs for. Defaults to all clusters to which this service is deployed."
     status_parser.add_argument(
-        "-c", "--clusters", help=cluster_help
+        "-c",
+        "--cluster",
+        help="The cluster to see relevant logs for. Defaults to all clusters to which this service is deployed.",
+        nargs=1,
     ).completer = completer_clusters
-    instance_help = "The instances to see relevant logs for. Defaults to all instances for this service."
     status_parser.add_argument(
-        "-i", "--instances", help=instance_help
+        "-i",
+        "--instance",
+        help="The instance to see relevant logs for. Defaults to all instances for this service.",
+        type=str,
     ).completer = completer_clusters
+    pod_help = (
+        "The pods to see relevant logs for. Defaults to all pods for this service."
+    )
+    status_parser.add_argument("-p", "--pods", help=pod_help)
+    status_parser.add_argument(
+        "-C",
+        "--components",
+        type=lambda s: set(s.split(",")),
+        default=set(DEFAULT_COMPONENTS),
+        help=(
+            "A comma-separated list of the components you want logs for. "
+            "PaaSTA consists of 'components' such as builds and deployments, "
+            "for each of which we collect logs for per service. "
+            "See below for a list of components. "
+            "Defaults to %(default)s."
+        ),
+    ).completer = lazy_choices_completer(LOG_COMPONENTS.keys)
     status_parser.add_argument(
         "-f",
         "-F",
         "--tail",
         dest="tail",
         action="store_true",
         default=False,
@@ -121,98 +143,114 @@
     )
     status_parser.add_argument(
         "-d",
         "--soa-dir",
         dest="soa_dir",
         metavar="SOA_DIR",
         default=DEFAULT_SOA_DIR,
-        help="define a different soa config directory",
+        help=f"Define a different soa config directory. Defaults to %(default)s.",
     )
 
     status_parser.add_argument(
         "-a",
         "--from",
         "--after",
         dest="time_from",
-        help="The time to start getting logs from. This can be an ISO-8601 timestamp or a human readable duration "
-        'parsable by pytimeparse such as "5m", "1d3h" etc. For example: --from "3m" would start retrieving logs '
-        "from 3 minutes ago",
+        help=(
+            "The time to start getting logs from. "
+            'This can be an ISO-8601 timestamp or a human readable duration parsable by pytimeparse such as "5m", "1d3h" etc. '
+            'For example: --from "3m" would start retrieving logs from 3 minutes ago. '
+            "Incompatible with --line-offset and --lines."
+        ),
     )
     status_parser.add_argument(
         "-t",
         "--to",
         dest="time_to",
-        help="The time to get logs up to. This can be an ISO-8601 timestamp or a human readable duration"
-        'parsable by pytimeparse such as "5m", "1d3h" etc. Defaults to right now',
+        help=(
+            "The time to get logs up to. "
+            'This can be an ISO-8601 timestamp or a human readable duration parsable by pytimeparse such as "5m", "1d3h" etc. '
+            "Incompatiable with --line-offset and --lines. "
+            "Defaults to now."
+        ),
     )
     status_parser.add_argument(
         "-l",
         "-n",
         "--lines",
         dest="line_count",
-        help='The number of lines to retrieve from the specified offset. May optionally be prefixed with a "+" or "-" '
-        'to specify which direction from the offset, defaults to "-100"',
+        help=(
+            "The number of lines to retrieve from the specified offset. "
+            'May optionally be prefixed with a "+" or "-" to specify which direction from the offset. '
+            "Incompatiable with --from and --to. "
+            'Defaults to "-100".'
+        ),
         type=int,
     )
     status_parser.add_argument(
         "-o",
         "--line-offset",
         dest="line_offset",
-        help="The offset at which line to start grabbing logs from. For example 1 would be the first line. Paired with "
-        "--lines +100 would give you the first 100 lines of logs. Defaults to the latest line's offset",
+        help=(
+            "The offset at which line to start grabbing logs from. "
+            "For example, --line-offset 1 would be the first line. "
+            "Paired with --lines, --line-offset +100 would give you the first 100 lines of logs. "
+            "Some logging backends may not support line offsetting by time or lines. "
+            "Incompatiable with --from and --to. "
+            "Defaults to the latest line's offset."
+        ),
         type=int,
     )
-    default_component_string = ",".join(DEFAULT_COMPONENTS)
-    component_descriptions = build_component_descriptions(LOG_COMPONENTS)
-    epilog = (
-        "TIME/LINE PARAMETERS\n"
-        "The args for time and line based offsetting are mutually exclusive, they cannot be used together. "
-        "Additionally, some logging backends may not support offsetting by time or offsetting by lines."
-        "\n"
-        "\n"
+    status_parser.add_argument(
+        "-S",
+        "--strip-headers",
+        dest="strip_headers",
+        help="Print log lines without header information.",
+        action="store_true",
+    )
+    status_parser.epilog = (
         "COMPONENTS\n"
-        "There are many possible components of Paasta logs that you might be interested in:\n"
-        "Run --list-components to see all available log components.\n"
-        "If unset, the default components are:\n\t%s\n"
-        "So the default behavior of `paasta logs` will be to tail those logs.\n\n"
-        "Here is a list of all components and what they are:\n%s\n\n"
-        % (default_component_string, component_descriptions)
+        "Here is a list of all components and what they are:\n"
+        f"{build_component_descriptions(LOG_COMPONENTS)}"
     )
-    status_parser.epilog = epilog
     status_parser.set_defaults(command=paasta_logs)
 
 
 def completer_clusters(prefix, parsed_args, **kwargs):
     service = parsed_args.service or guess_service_name()
     if service in list_services():
         return list_clusters(service)
     else:
         return list_clusters()
 
 
-def build_component_descriptions(components):
+def build_component_descriptions(components: Mapping[str, Mapping[str, Any]]) -> str:
     """Returns a colored description string for every log component
     based on its help attribute"""
     output = []
     for k, v in components.items():
-        output.append("     {}: {}".format(v["color"](k), v["help"]))
+        output.append("    {}: {}".format(v["color"](k), v["help"]))
     return "\n".join(output)
 
 
-def prefix(input_string, component):
+def prefix(input_string: str, component: str) -> str:
     """Returns a colored string with the right colored prefix with a given component"""
     return "{}: {}".format(LOG_COMPONENTS[component]["color"](component), input_string)
 
 
 # The reason this returns true if start_time or end_time are None is because
 # filtering by time is optional, and it allows us to simply do
 # if not check_timestamp_in_range(...): return False
 # The default arguments for start_time and end_time are None when we aren't
 # filtering by time
-def check_timestamp_in_range(timestamp, start_time, end_time):
+def check_timestamp_in_range(
+    timestamp: datetime.datetime,
+    start_time: datetime.datetime,
+    end_time: datetime.datetime,
+) -> bool:
     """A convenience function to check if a datetime.datetime timestamp is within the given start and end times,
     returns true if start_time or end_time is None
 
     :param timestamp: The timestamp to check
     :param start_time: The start of the interval
     :param end_time: The end of the interval
     :return: True if timestamp is within start_time and end_time range, False otherwise
@@ -222,33 +260,35 @@
             timestamp = pytz.utc.localize(timestamp)
         return start_time < timestamp < end_time
     else:
         return True
 
 
 def paasta_log_line_passes_filter(
-    line,
-    levels,
-    service,
-    components,
-    clusters,
-    instances,
-    start_time=None,
-    end_time=None,
-):
+    line: str,
+    levels: Sequence[str],
+    service: str,
+    components: Iterable[str],
+    clusters: Sequence[str],
+    instances: List[str],
+    pods: Iterable[str] = None,
+    start_time: datetime.datetime = None,
+    end_time: datetime.datetime = None,
+) -> bool:
     """Given a (JSON-formatted) log line, return True if the line should be
     displayed given the provided levels, components, and clusters; return False
     otherwise.
+
+    NOTE: Pods are optional as services that use Mesos do not operate with pods.
     """
     try:
         parsed_line = json.loads(line)
     except ValueError:
         log.debug("Trouble parsing line as json. Skipping. Line: %r" % line)
         return False
-
     timestamp = isodate.parse_datetime(parsed_line.get("timestamp"))
     if not check_timestamp_in_range(timestamp, start_time, end_time):
         return False
     return (
         parsed_line.get("level") in levels
         and parsed_line.get("component") in components
         and (
@@ -256,23 +296,24 @@
             or parsed_line.get("cluster") == ANY_CLUSTER
         )
         and (instances is None or parsed_line.get("instance") in instances)
     )
 
 
 def paasta_app_output_passes_filter(
-    line,
-    levels,
-    service,
-    components,
-    clusters,
-    instances,
-    start_time=None,
-    end_time=None,
-):
+    line: str,
+    levels: Sequence[str],
+    service: str,
+    components: Iterable[str],
+    clusters: Sequence[str],
+    instances: List[str],
+    pods: Iterable[str] = None,
+    start_time: datetime.datetime = None,
+    end_time: datetime.datetime = None,
+) -> bool:
     try:
         parsed_line = json.loads(line)
     except ValueError:
         log.debug("Trouble parsing line as json. Skipping. Line: %r" % line)
         return False
     try:
         timestamp = isodate.parse_datetime(parsed_line.get("timestamp"))
@@ -284,18 +325,19 @@
     return (
         parsed_line.get("component") in components
         and (
             parsed_line.get("cluster") in clusters
             or parsed_line.get("cluster") == ANY_CLUSTER
         )
         and (instances is None or parsed_line.get("instance") in instances)
+        and (pods is None or parsed_line.get("pod_name") in pods)
     )
 
 
-def extract_utc_timestamp_from_log_line(line):
+def extract_utc_timestamp_from_log_line(line: str) -> datetime.datetime:
     """
     Extracts the timestamp from a log line of the format "<timestamp> <other data>" and returns a UTC datetime object
     or None if it could not parse the line
     """
     # Extract ISO 8601 date per http://www.pelagodesign.com/blog/2009/05/20/iso-8601-date-validation-that-doesnt-suck/
     iso_re = (
         r"^([\+-]?\d{4}(?!\d{2}\b))((-?)((0[1-9]|1[0-2])(\3([12]\d|0[1-9]|3[01]))?|W([0-4]\d|5[0-2])(-?[1-7])?|"
@@ -310,84 +352,50 @@
         return None
     timestamp = tokens.group(0).strip()
     dt = isodate.parse_datetime(timestamp)
     utc_timestamp = datetime_convert_timezone(dt, dt.tzinfo, tz.tzutc())
     return utc_timestamp
 
 
-def parse_marathon_log_line(line, clusters, service):
-    utc_timestamp = extract_utc_timestamp_from_log_line(line)
-    if not utc_timestamp:
-        return ""
-    else:
-        return format_log_line(
-            level="event",
-            cluster=clusters[0],
-            service=service,
-            instance="ALL",
-            component="marathon",
-            line=line.strip(),
-            timestamp=utc_timestamp.strftime("%Y-%m-%dT%H:%M:%S.%f"),
-        )
-
-
-def marathon_log_line_passes_filter(
-    line,
-    levels,
-    service,
-    components,
-    clusters,
-    instances,
-    start_time=None,
-    end_time=None,
-):
-    """Given a (JSON-formatted) log line where the message is a Marathon log line,
-    return True if the line should be displayed given the provided service; return False
-    otherwise."""
-    try:
-        parsed_line = json.loads(line)
-    except ValueError:
-        log.debug("Trouble parsing line as json. Skipping. Line: %r" % line)
-        return False
-
-    timestamp = isodate.parse_datetime(parsed_line.get("timestamp"))
-    if not check_timestamp_in_range(timestamp, start_time, end_time):
-        return False
-    return format_job_id(service, "") in parsed_line.get("message", "")
-
-
-def print_log(line, requested_levels, raw_mode=False):
+def print_log(
+    line: str,
+    requested_levels: Sequence[str],
+    raw_mode: bool = False,
+    strip_headers: bool = False,
+) -> None:
     """Mostly a stub to ease testing. Eventually this may do some formatting or
     something.
     """
     if raw_mode:
-        paasta_print(
-            line, end=" "
-        )  # suppress trailing newline since scribereader already attached one
+        # suppress trailing newline since scribereader already attached one
+        print(line, end=" ", flush=True)
     else:
-        paasta_print(prettify_log_line(line, requested_levels))
+        print(
+            prettify_log_line(line, requested_levels, strip_headers),
+            flush=True,
+        )
 
 
-def prettify_timestamp(timestamp):
+def prettify_timestamp(timestamp: datetime.datetime) -> str:
     """Returns more human-friendly form of 'timestamp' without microseconds and
     in local time.
     """
     dt = isodate.parse_datetime(timestamp)
     pretty_timestamp = datetime_from_utc_to_local(dt)
     return pretty_timestamp.strftime("%Y-%m-%d %H:%M:%S")
 
 
-def prettify_component(component):
+def prettify_component(component: str) -> str:
     try:
         return LOG_COMPONENTS[component]["color"]("[%s]" % component)
     except KeyError:
         return "UNPRETTIFIABLE COMPONENT %s" % component
 
 
-def prettify_level(level, requested_levels):
+def prettify_level(level: str, requested_levels: Sequence[str]) -> str:
     """Colorize level. 'event' is special and gets bolded; everything else gets
     lightened.
 
     requested_levels is an iterable of levels that will be displayed. If only
     one level will be displayed, don't bother to print it (return empty string).
     If multiple levels will be displayed, emit the (prettified) level so the
     resulting log output is not ambiguous.
@@ -397,39 +405,42 @@
         if level == "event":
             pretty_level = PaastaColors.bold("[%s] " % level)
         else:
             pretty_level = PaastaColors.grey("[%s] " % level)
     return pretty_level
 
 
-def prettify_log_line(line, requested_levels):
+def prettify_log_line(
+    line: str, requested_levels: Sequence[str], strip_headers: bool
+) -> str:
     """Given a line from the log, which is expected to be JSON and have all the
     things we expect, return a pretty formatted string containing relevant values.
     """
     try:
         parsed_line = json.loads(line)
     except ValueError:
         log.debug("Trouble parsing line as json. Skipping. Line: %r" % line)
         return "Invalid JSON: %s" % line
 
     try:
-        pretty_level = prettify_level(parsed_line["level"], requested_levels)
-        return (
-            "%(timestamp)s %(component)s %(cluster)s %(instance)s - %(level)s%(message)s"
-            % (
+        if strip_headers:
+            return "%(timestamp)s %(message)s" % (
+                {
+                    "timestamp": prettify_timestamp(parsed_line["timestamp"]),
+                    "message": parsed_line["message"],
+                }
+            )
+        else:
+            return "%(timestamp)s %(component)s - %(message)s" % (
                 {
                     "timestamp": prettify_timestamp(parsed_line["timestamp"]),
                     "component": prettify_component(parsed_line["component"]),
-                    "cluster": "[%s]" % parsed_line["cluster"],
-                    "instance": "[%s]" % parsed_line["instance"],
-                    "level": "%s" % pretty_level,
                     "message": parsed_line["message"],
                 }
             )
-        )
     except KeyError:
         log.debug(
             "JSON parsed correctly but was missing a key. Skipping. Line: %r" % line
         )
         return "JSON missing keys: %s" % line
 
 
@@ -444,23 +455,23 @@
     def outer(log_reader_class):
         _log_reader_classes[name] = log_reader_class
         return log_reader_class
 
     return outer
 
 
-def get_log_reader_class(name):
+def get_log_reader_class(name: str) -> Type["ScribeLogReader"]:
     return _log_reader_classes[name]
 
 
-def list_log_readers():
+def list_log_readers() -> Iterable[str]:
     return _log_reader_classes.keys()
 
 
-def get_log_reader():
+def get_log_reader() -> "ScribeLogReader":
     log_reader_config = load_system_paasta_config().get_log_reader()
     log_reader_class = get_log_reader_class(log_reader_config["driver"])
     return log_reader_class(**log_reader_config.get("options", {}))
 
 
 class LogReader:
     # Tailing, i.e actively viewing logs as they come in
@@ -470,46 +481,67 @@
     # Getting the last/prev n lines of logs from line #34013 for example
     SUPPORTS_LINE_OFFSET = False
     # Getting the logs between two given times
     SUPPORTS_TIME = False
     # Supporting at least one of these log retrieval modes is required
 
     def tail_logs(
-        self, service, levels, components, clusters, instances, raw_mode=False
+        self,
+        service,
+        levels,
+        components,
+        clusters,
+        instances,
+        pods,
+        raw_mode=False,
+        strip_headers=False,
     ):
         raise NotImplementedError("tail_logs is not implemented")
 
     def print_logs_by_time(
         self,
         service,
         start_time,
         end_time,
         levels,
         components,
         clusters,
         instances,
+        pods,
         raw_mode,
+        strip_headers,
     ):
         raise NotImplementedError("print_logs_by_time is not implemented")
 
     def print_last_n_logs(
-        self, service, line_count, levels, components, clusters, instances, raw_mode
+        self,
+        service,
+        line_count,
+        levels,
+        components,
+        clusters,
+        instances,
+        pods,
+        raw_mode,
+        strip_headers,
     ):
         raise NotImplementedError("print_last_n_logs is not implemented")
 
     def print_logs_by_offset(
         self,
         service,
         line_count,
-        offset,
+        line_offset,
         levels,
         components,
         clusters,
         instances,
+        pods,
         raw_mode,
+        strip_headers,
     ):
         raise NotImplementedError("print_logs_by_offset is not implemented")
 
 
 ScribeComponentStreamInfo = namedtuple(
     "ScribeComponentStreamInfo", "per_cluster, stream_name_fn, filter_fn, parse_fn"
 )
@@ -540,32 +572,40 @@
             per_cluster=False,
             stream_name_fn=lambda service: get_log_name_for_service(
                 service, prefix="app_output"
             ),
             filter_fn=paasta_app_output_passes_filter,
             parse_fn=None,
         ),
-        "marathon": ScribeComponentStreamInfo(
-            per_cluster=True,
-            stream_name_fn=lambda service, cluster: "stream_marathon_%s" % cluster,
-            filter_fn=marathon_log_line_passes_filter,
-            parse_fn=parse_marathon_log_line,
-        ),
     }
 
-    def __init__(self, cluster_map):
+    def __init__(self, cluster_map: Mapping[str, Any]) -> None:
         super().__init__()
 
         if scribereader is None:
             raise Exception(
                 "scribereader package must be available to use scribereader log reading backend"
             )
         self.cluster_map = cluster_map
 
-    def run_code_over_scribe_envs(self, clusters, components, callback):
+    def get_scribereader_selector(self, scribe_env: str) -> str:
+        # this is kinda silly, but until the scribereader cli becomes more ergonomic
+        # we'll need to do a little bit of string munging to let humans use scribereader
+        # in the same way we are (tl;dr: scribereader has sorta confusing behavior between
+        # what can be use for --ecosystem, --region, and --superregion and the fastest/least
+        # hacky thing to figure out which we wanna use is that any env with a - in it is a region
+        # and any without one is an ecosystem)
+        return "-e" if "-" in scribe_env else "-r"
+
+    def run_code_over_scribe_envs(
+        self,
+        clusters: Sequence[str],
+        components: Iterable[str],
+        callback: Callable[..., None],
+    ) -> None:
         """Iterates over the scribe environments for a given set of clusters and components, executing
         functions for each component
 
         :param clusters: The set of clusters
         :param components: The set of components
         :param callback: The callback function. Gets called with (component_name, stream_info, scribe_env, cluster)
                          The cluster field will only be set if the component is set to per_cluster
@@ -577,35 +617,43 @@
 
         for scribe_env in scribe_envs:
             # These components all get grouped in one call for backwards compatibility
             grouped_components = {"build", "deploy", "monitoring"}
 
             if any([component in components for component in grouped_components]):
                 stream_info = self.get_stream_info("default")
-                callback("default", stream_info, scribe_env, cluster=None)
+                callback(components, stream_info, scribe_env, cluster=None)
 
             non_defaults = set(components) - grouped_components
             for component in non_defaults:
                 stream_info = self.get_stream_info(component)
 
                 if stream_info.per_cluster:
                     for cluster in clusters:
-                        callback(component, stream_info, scribe_env, cluster=cluster)
+                        callback([component], stream_info, scribe_env, cluster=cluster)
                 else:
-                    callback(component, stream_info, scribe_env, cluster=None)
+                    callback([component], stream_info, scribe_env, cluster=None)
 
-    def get_stream_info(self, component):
+    def get_stream_info(self, component: str) -> ScribeComponentStreamInfo:
         if component in self.COMPONENT_STREAM_INFO:
             return self.COMPONENT_STREAM_INFO[component]
         else:
             return self.COMPONENT_STREAM_INFO["default"]
 
     def tail_logs(
-        self, service, levels, components, clusters, instances, raw_mode=False
-    ):
+        self,
+        service: str,
+        levels: Sequence[str],
+        components: Iterable[str],
+        clusters: Sequence[str],
+        instances: List[str],
+        pods: Iterable[str] = None,
+        raw_mode: bool = False,
+        strip_headers: bool = False,
+    ) -> None:
         """Sergeant function for spawning off all the right log tailing functions.
 
         NOTE: This function spawns concurrent processes and doesn't necessarily
         worry about cleaning them up! That's because we expect to just exit the
         main process when this function returns (as main() does). Someone calling
         this function directly with something like "while True: tail_paasta_logs()"
         may be very sad.
@@ -625,37 +673,45 @@
         things.
 
         It's possible this whole multiprocessing strategy is wrong-headed. If you
         are reading this code to curse whoever wrote it, see discussion in
         PAASTA-214 and https://reviewboard.yelpcorp.com/r/87320/ and feel free to
         implement one of the other options.
         """
-        queue = Queue()
+        queue: Queue = Queue()
         spawned_processes = []
 
-        def callback(component, stream_info, scribe_env, cluster):
+        def callback(
+            components: Iterable[str],
+            stream_info: ScribeComponentStreamInfo,
+            scribe_env: str,
+            cluster: str,
+        ) -> None:
             kw = {
                 "scribe_env": scribe_env,
                 "service": service,
                 "levels": levels,
                 "components": components,
                 "clusters": clusters,
                 "instances": instances,
+                "pods": pods,
                 "queue": queue,
                 "filter_fn": stream_info.filter_fn,
             }
 
             if stream_info.per_cluster:
                 kw["stream_name"] = stream_info.stream_name_fn(service, cluster)
                 kw["clusters"] = [cluster]
             else:
                 kw["stream_name"] = stream_info.stream_name_fn(service)
             log.debug(
-                "Running the equivalent of 'scribereader -e {} {}'".format(
-                    scribe_env, kw["stream_name"]
+                "Running the equivalent of 'scribereader {} {} {}'".format(
+                    self.get_scribereader_selector(scribe_env),
+                    scribe_env,
+                    kw["stream_name"],
                 )
             )
             process = Process(target=self.scribe_tail, kwargs=kw)
             spawned_processes.append(process)
             process.start()
 
         self.run_code_over_scribe_envs(
@@ -696,15 +752,15 @@
                 # will surely be fine.
                 #
                 # UPDATE: Actually this is leading to a test failure rate of about
                 # 1/10 even with timeout of 1s. I'm adding a sleep to the threads
                 # in test code to smooth this out, then pulling the trigger on
                 # moving that test to integration land where it belongs.
                 line = queue.get(block=True, timeout=0.1)
-                print_log(line, levels, raw_mode)
+                print_log(line, levels, raw_mode, strip_headers)
             except Empty:
                 try:
                     # If there's nothing in the queue, take this opportunity to make
                     # sure all the tailers are still running.
                     running_processes = [tt.is_alive() for tt in spawned_processes]
                     if not running_processes or not all(running_processes):
                         log.warn(
@@ -728,35 +784,33 @@
                 # Die peacefully rather than printing N threads worth of stack
                 # traces.
                 log.warn("Terminating.")
                 break
 
     def print_logs_by_time(
         self,
-        service,
-        start_time,
-        end_time,
-        levels,
-        components,
-        clusters,
-        instances,
-        raw_mode,
-    ):
+        service: str,
+        start_time: datetime.datetime,
+        end_time: datetime.datetime,
+        levels: Sequence[str],
+        components: Iterable[str],
+        clusters: Sequence[str],
+        instances: List[str],
+        pods: Iterable[str],
+        raw_mode: bool,
+        strip_headers: bool,
+    ) -> None:
         aggregated_logs: List[Dict[str, Any]] = []
 
-        if "marathon" in components:
-            paasta_print(
-                PaastaColors.red(
-                    "Warning, you have chosen to get marathon logs based "
-                    "on time. This command may take a dozen minutes or so to run.\n"
-                ),
-                file=sys.stderr,
-            )
-
-        def callback(component, stream_info, scribe_env, cluster):
+        def callback(
+            components: Iterable[str],
+            stream_info: ScribeComponentStreamInfo,
+            scribe_env: str,
+            cluster: str,
+        ) -> None:
             if stream_info.per_cluster:
                 stream_name = stream_info.stream_name_fn(service, cluster)
             else:
                 stream_name = stream_info.stream_name_fn(service)
 
             ctx = self.scribe_get_from_time(
                 scribe_env, stream_name, start_time, end_time
@@ -767,38 +821,53 @@
                 stream_name=stream_name,
                 levels=levels,
                 service=service,
                 components=components,
                 clusters=clusters,
                 instances=instances,
                 aggregated_logs=aggregated_logs,
+                pods=pods,
                 filter_fn=stream_info.filter_fn,
                 parser_fn=stream_info.parse_fn,
                 start_time=start_time,
                 end_time=end_time,
             )
 
         self.run_code_over_scribe_envs(
             clusters=clusters, components=components, callback=callback
         )
 
         aggregated_logs = list(
             {line["raw_line"]: line for line in aggregated_logs}.values()
         )
         aggregated_logs.sort(key=lambda log_line: log_line["sort_key"])
+
         for line in aggregated_logs:
-            print_log(line["raw_line"], levels, raw_mode)
+            print_log(line["raw_line"], levels, raw_mode, strip_headers)
 
     def print_last_n_logs(
-        self, service, line_count, levels, components, clusters, instances, raw_mode
-    ):
+        self,
+        service: str,
+        line_count: int,
+        levels: Sequence[str],
+        components: Iterable[str],
+        clusters: Sequence[str],
+        instances: List[str],
+        pods: Iterable[str],
+        raw_mode: bool,
+        strip_headers: bool,
+    ) -> None:
         aggregated_logs: List[Dict[str, Any]] = []
 
-        def callback(component, stream_info, scribe_env, cluster):
-            stream_info = self.get_stream_info(component)
+        def callback(
+            components: Iterable[str],
+            stream_info: ScribeComponentStreamInfo,
+            scribe_env: str,
+            cluster: str,
+        ) -> None:
 
             if stream_info.per_cluster:
                 stream_name = stream_info.stream_name_fn(service, cluster)
             else:
                 stream_name = stream_info.stream_name_fn(service)
 
             ctx = self.scribe_get_last_n_lines(scribe_env, stream_name, line_count)
@@ -808,44 +877,47 @@
                 stream_name=stream_name,
                 levels=levels,
                 service=service,
                 components=components,
                 clusters=clusters,
                 instances=instances,
                 aggregated_logs=aggregated_logs,
+                pods=pods,
                 filter_fn=stream_info.filter_fn,
                 parser_fn=stream_info.parse_fn,
             )
 
         self.run_code_over_scribe_envs(
             clusters=clusters, components=components, callback=callback
         )
         aggregated_logs = list(
             {line["raw_line"]: line for line in aggregated_logs}.values()
         )
         aggregated_logs.sort(key=lambda log_line: log_line["sort_key"])
+
         for line in aggregated_logs:
-            print_log(line["raw_line"], levels, raw_mode)
+            print_log(line["raw_line"], levels, raw_mode, strip_headers)
 
     def filter_and_aggregate_scribe_logs(
         self,
-        scribe_reader_ctx,
-        scribe_env,
-        stream_name,
-        levels,
-        service,
-        components,
-        clusters,
-        instances,
-        aggregated_logs,
-        parser_fn=None,
-        filter_fn=None,
-        start_time=None,
-        end_time=None,
-    ):
+        scribe_reader_ctx: ContextManager,
+        scribe_env: str,
+        stream_name: str,
+        levels: Sequence[str],
+        service: str,
+        components: Iterable[str],
+        clusters: Sequence[str],
+        instances: List[str],
+        aggregated_logs: MutableSequence[Dict[str, Any]],
+        pods: Iterable[str] = None,
+        parser_fn: Callable = None,
+        filter_fn: Callable = None,
+        start_time: datetime.datetime = None,
+        end_time: datetime.datetime = None,
+    ) -> None:
         with scribe_reader_ctx as scribe_reader:
             try:
                 for line in scribe_reader:
                     # temporary until all log lines are strings not byte strings
                     if isinstance(line, bytes):
                         line = line.decode("utf-8")
                     if parser_fn:
@@ -854,14 +926,15 @@
                         if filter_fn(
                             line,
                             levels,
                             service,
                             components,
                             clusters,
                             instances,
+                            pods,
                             start_time=start_time,
                             end_time=end_time,
                         ):
                             try:
                                 parsed_line = json.loads(line)
                                 timestamp = isodate.parse_datetime(
                                     parsed_line.get("timestamp")
@@ -884,19 +957,25 @@
                     log.warning(
                         f"Couldn't connect to Scribe to tail {stream_name} in {scribe_env}"
                     )
                     log.warning(f"Please be in {scribe_env} to tail this log.")
                 else:
                     raise
 
-    def scribe_get_from_time(self, scribe_env, stream_name, start_time, end_time):
+    def scribe_get_from_time(
+        self,
+        scribe_env: str,
+        stream_name: str,
+        start_time: datetime.datetime,
+        end_time: datetime.datetime,
+    ) -> ContextManager:
         # Scribe connection details
-        host_and_port = scribereader.get_env_scribe_host(scribe_env, tail=False)
-        host = host_and_port["host"]
-        port = host_and_port["port"]
+        host, port = scribereader.get_tail_host_and_port(
+            **scribe_env_to_locations(scribe_env),
+        )
 
         # Recent logs might not be archived yet. Log warning message.
         warning_end_time = datetime.datetime.utcnow().replace(
             tzinfo=pytz.utc
         ) - datetime.timedelta(hours=4)
         if end_time > warning_end_time:
             log.warn("Recent logs might be incomplete. Consider tailing instead.")
@@ -905,78 +984,92 @@
         # dates instead.
         start_date_yst = start_time.astimezone(
             pytz.timezone("America/Los_Angeles")
         ).date()
         end_date_yst = end_time.astimezone(pytz.timezone("America/Los_Angeles")).date()
 
         log.debug(
-            "Running the equivalent of 'scribereader -e %s %s --min-date %s --max-date %s"
-            % (scribe_env, stream_name, start_date_yst, end_date_yst)
+            "Running the equivalent of 'scribereader %s %s %s --min-date %s --max-date %s"
+            % (
+                self.get_scribereader_selector(scribe_env),
+                scribe_env,
+                stream_name,
+                start_date_yst,
+                end_date_yst,
+            )
         )
         return scribereader.get_stream_reader(
             stream_name=stream_name,
             reader_host=host,
             reader_port=port,
             min_date=start_date_yst,
             max_date=end_date_yst,
         )
 
-    def scribe_get_last_n_lines(self, scribe_env, stream_name, line_count):
+    def scribe_get_last_n_lines(
+        self, scribe_env: str, stream_name: str, line_count: int
+    ) -> ContextManager:
         # Scribe connection details
-        host_and_port = scribereader.get_env_scribe_host(scribe_env, tail=True)
-        host = host_and_port["host"]
-        port = host_and_port["port"]
+        host, port = scribereader.get_tail_host_and_port(
+            **scribe_env_to_locations(scribe_env),
+        )
 
         # The reason we need a fake context here is because scribereader is a bit inconsistent in its
         # returns. get_stream_reader returns a context that needs to be acquired for cleanup code but
         # get_stream_tailer simply returns an object that can be iterated over. We'd still like to have
         # the cleanup code for get_stream_reader to be executed by this function's caller and this is
         # one of the simpler ways to achieve it without having 2 if statements everywhere that calls
         # this method
         @contextmanager
         def fake_context():
             log.debug(
-                f"Running the equivalent of 'scribereader -e {scribe_env} {stream_name}'"
+                f"Running the equivalent of 'scribereader -n {line_count} {self.get_scribereader_selector(scribe_env)} {scribe_env} {stream_name}'"
             )
             yield scribereader.get_stream_tailer(
-                stream_name, host, port, True, line_count
+                stream_name=stream_name,
+                tailing_host=host,
+                tailing_port=port,
+                lines=line_count,
             )
 
         return fake_context()
 
     def scribe_tail(
         self,
-        scribe_env,
-        stream_name,
-        service,
-        levels,
-        components,
-        clusters,
-        instances,
-        queue,
-        filter_fn,
-        parse_fn=None,
-    ):
+        scribe_env: str,
+        stream_name: str,
+        service: str,
+        levels: Sequence[str],
+        components: Iterable[str],
+        clusters: Sequence[str],
+        instances: List[str],
+        pods: Iterable[str],
+        queue: Queue,
+        filter_fn: Callable,
+        parse_fn: Callable = None,
+    ) -> None:
         """Creates a scribetailer for a particular environment.
 
         When it encounters a line that it should report, it sticks it into the
         provided queue.
 
         This code is designed to run in a thread as spawned by tail_paasta_logs().
         """
         try:
             log.debug(f"Going to tail {stream_name} scribe stream in {scribe_env}")
-            host_and_port = scribereader.get_env_scribe_host(scribe_env, True)
-            host = host_and_port["host"]
-            port = host_and_port["port"]
+            host, port = scribereader.get_tail_host_and_port(
+                **scribe_env_to_locations(scribe_env),
+            )
             tailer = scribereader.get_stream_tailer(stream_name, host, port)
             for line in tailer:
                 if parse_fn:
                     line = parse_fn(line, clusters, service)
-                if filter_fn(line, levels, service, components, clusters, instances):
+                if filter_fn(
+                    line, levels, service, components, clusters, instances, pods
+                ):
                     queue.put(line)
         except KeyboardInterrupt:
             # Die peacefully rather than printing N threads worth of stack
             # traces.
             pass
         except StreamTailerSetupError as e:
             if "No data in stream" in str(e):
@@ -984,19 +1077,21 @@
                 log.warning(
                     "Don't Panic! This may or may not be a problem depending on if you expect there to be"
                 )
                 log.warning("output within this stream.")
                 # Enter a wait so the process isn't considered dead.
                 # This is just a large number, since apparently some python interpreters
                 # don't like being passed sys.maxsize.
-                sleep(2 ** 16)
+                sleep(2**16)
             else:
                 raise
 
-    def determine_scribereader_envs(self, components, cluster):
+    def determine_scribereader_envs(
+        self, components: Iterable[str], cluster: str
+    ) -> Set[str]:
         """Returns a list of environments that scribereader needs to connect
         to based on a given list of components and the cluster involved.
 
         Some components are in certain environments, regardless of the cluster.
         Some clusters do not match up with the scribe environment names, so
         we figure that out here"""
         envs: List[str] = []
@@ -1007,32 +1102,48 @@
                 "source_env", self.cluster_to_scribe_env(cluster)
             )
             if "additional_source_envs" in LOG_COMPONENTS[component]:
                 envs += LOG_COMPONENTS[component]["additional_source_envs"]
             envs.append(env)
         return set(envs)
 
-    def cluster_to_scribe_env(self, cluster):
+    def cluster_to_scribe_env(self, cluster: str) -> str:
         """Looks up the particular scribe env associated with a given paasta cluster.
 
         Scribe has its own "environment" key, which doesn't always map 1:1 with our
         cluster names, so we have to maintain a manual mapping.
 
         This mapping is deployed as a config file via puppet as part of the public
         config deployed to every server.
         """
         env = self.cluster_map.get(cluster, None)
         if env is None:
-            paasta_print("I don't know where scribe logs for %s live?" % cluster)
+            print("I don't know where scribe logs for %s live?" % cluster)
             sys.exit(1)
         else:
             return env
 
 
-def generate_start_end_time(from_string="30m", to_string=None):
+def scribe_env_to_locations(scribe_env):
+    """Converts a scribe environment to a dictionary of locations. The
+    return value is meant to be used as kwargs for `scribereader.get_tail_host_and_port`.
+    """
+    locations = {"ecosystem": None, "region": None, "superregion": None}
+    if scribe_env in scribereader.PROD_REGIONS:
+        locations["region"] = scribe_env
+    elif scribe_env in scribereader.PROD_SUPERREGIONS:
+        locations["superregion"] = scribe_env
+    else:  # non-prod envs are expressed as ecosystems
+        locations["ecosystem"] = scribe_env
+    return locations
+
+
+def generate_start_end_time(
+    from_string: str = "30m", to_string: str = None
+) -> Tuple[datetime.datetime, datetime.datetime]:
     """Parses the --from and --to command line arguments to create python
     datetime objects representing the start and end times for log retrieval
 
     :param from_string: The --from argument, defaults to 30 minutes
     :param to_string: The --to argument, defaults to the time right now
     :return: A tuple containing start_time, end_time, which specify the interval of log retrieval
     """
@@ -1073,188 +1184,212 @@
 
     if start_time > end_time:
         raise ValueError("Start time bigger than end time")
 
     return start_time, end_time
 
 
-def validate_filtering_args(args, log_reader):
+def validate_filtering_args(
+    args: argparse.Namespace, log_reader: ScribeLogReader
+) -> bool:
     if not log_reader.SUPPORTS_LINE_OFFSET and args.line_offset is not None:
-        paasta_print(
+        print(
             PaastaColors.red(
                 log_reader.__class__.__name__ + " does not support line based offsets"
             ),
             file=sys.stderr,
         )
         return False
     if not log_reader.SUPPORTS_LINE_COUNT and args.line_count is not None:
-        paasta_print(
+        print(
             PaastaColors.red(
                 log_reader.__class__.__name__
                 + " does not support line count based log retrieval"
             ),
             file=sys.stderr,
         )
         return False
     if not log_reader.SUPPORTS_TAILING and args.tail:
-        paasta_print(
+        print(
             PaastaColors.red(
                 log_reader.__class__.__name__ + " does not support tailing"
             ),
             file=sys.stderr,
         )
         return False
     if not log_reader.SUPPORTS_TIME and (
         args.time_from is not None or args.time_to is not None
     ):
-        paasta_print(
+        print(
             PaastaColors.red(
                 log_reader.__class__.__name__ + " does not support time based offsets"
             ),
             file=sys.stderr,
         )
         return False
 
     if args.tail and (
         args.line_count is not None
         or args.time_from is not None
         or args.time_to is not None
         or args.line_offset is not None
     ):
-        paasta_print(
+        print(
             PaastaColors.red(
                 "You cannot specify line/time based filtering parameters when tailing"
             ),
             file=sys.stderr,
         )
         return False
 
     # Can't have both
     if args.line_count is not None and args.time_from is not None:
-        paasta_print(
+        print(
             PaastaColors.red("You cannot filter based on both line counts and time"),
             file=sys.stderr,
         )
         return False
 
     return True
 
 
 def pick_default_log_mode(
-    args, log_reader, service, levels, components, clusters, instances
-):
+    args: argparse.Namespace,
+    log_reader: ScribeLogReader,
+    service: str,
+    levels: Sequence[str],
+    components: Iterable[str],
+    clusters: Sequence[str],
+    instances: List[str],
+    pods: Iterable[str],
+) -> int:
     if log_reader.SUPPORTS_LINE_COUNT:
-        paasta_print(
+        print(
             PaastaColors.cyan(
                 "Fetching 100 lines and applying filters. Try -n 1000 for more lines..."
             ),
             file=sys.stderr,
         )
         log_reader.print_last_n_logs(
             service=service,
             line_count=100,
             levels=levels,
             components=components,
             clusters=clusters,
             instances=instances,
+            pods=pods,
             raw_mode=args.raw_mode,
+            strip_headers=args.strip_headers,
         )
         return 0
     elif log_reader.SUPPORTS_TIME:
         start_time, end_time = generate_start_end_time()
-        paasta_print(
+        print(
             PaastaColors.cyan(
                 "Fetching a specific time period and applying filters..."
             ),
             file=sys.stderr,
         )
         log_reader.print_logs_by_time(
             service=service,
             start_time=start_time,
             end_time=end_time,
             levels=levels,
             components=components,
             clusters=clusters,
             instances=instances,
+            pods=pods,
             raw_mode=args.raw_mode,
+            strip_headers=args.strip_headers,
         )
         return 0
     elif log_reader.SUPPORTS_TAILING:
-        paasta_print(
+        print(
             PaastaColors.cyan("Tailing logs and applying filters..."), file=sys.stderr
         )
         log_reader.tail_logs(
             service=service,
             levels=levels,
             components=components,
             clusters=clusters,
             instances=instances,
+            pods=pods,
             raw_mode=args.raw_mode,
+            strip_headers=args.strip_headers,
         )
         return 0
+    return 1
 
 
-def paasta_logs(args):
+def paasta_logs(args: argparse.Namespace) -> int:
     """Print the logs for as Paasta service.
     :param args: argparse.Namespace obj created from sys.args by cli"""
     soa_dir = args.soa_dir
+
     service = figure_out_service_name(args, soa_dir)
 
-    if args.clusters is None:
-        clusters = list_clusters(service, soa_dir=soa_dir)
-    else:
-        clusters = args.clusters.split(",")
+    cluster = args.cluster
+    if (
+        args.cluster is None
+        or args.instance is None
+        or len(args.instance.split(",")) > 2
+    ):
+        print(
+            PaastaColors.red("You must specify one cluster and one instance."),
+            file=sys.stderr,
+        )
+        return 1
 
-    if args.instances is None:
-        instances = None
-    else:
-        instances = args.instances.split(",")
+    if verify_instances(args.instance, service, cluster, soa_dir):
+        return 1
+
+    instance = args.instance
 
-    if args.components is not None:
-        components = args.components.split(",")
+    if args.pods is None:
+        pods = None
     else:
-        components = DEFAULT_COMPONENTS
-    components = set(components)
+        pods = args.pods.split(",")
 
-    if "app_output" in components:
+    components = args.components
+    if "app_output" in args.components:
         components.remove("app_output")
         components.add("stdout")
         components.add("stderr")
 
     if args.verbose:
         log.setLevel(logging.DEBUG)
     else:
         log.setLevel(logging.INFO)
 
     levels = [DEFAULT_LOGLEVEL, "debug"]
 
-    log.debug(f"Going to get logs for {service} on clusters {clusters}")
+    log.debug(f"Going to get logs for {service} on cluster {cluster}")
 
     log_reader = get_log_reader()
 
     if not validate_filtering_args(args, log_reader):
         return 1
-
     # They haven't specified what kind of filtering they want, decide for them
     if args.line_count is None and args.time_from is None and not args.tail:
         return pick_default_log_mode(
-            args, log_reader, service, levels, components, clusters, instances
+            args, log_reader, service, levels, components, cluster, instance, pods
         )
-
     if args.tail:
-        paasta_print(
+        print(
             PaastaColors.cyan("Tailing logs and applying filters..."), file=sys.stderr
         )
         log_reader.tail_logs(
             service=service,
             levels=levels,
             components=components,
-            clusters=clusters,
-            instances=instances,
+            clusters=cluster,
+            instances=[instance],
+            pods=pods,
             raw_mode=args.raw_mode,
+            strip_headers=args.strip_headers,
         )
         return 0
 
     # If the logger doesn't support offsetting the number of lines by a particular line number
     # there is no point in distinguishing between a positive/negative number of lines since it
     # can only get the last N lines
     if not log_reader.SUPPORTS_LINE_OFFSET and args.line_count is not None:
@@ -1263,42 +1398,49 @@
     # Handle line based filtering
     if args.line_count is not None and args.line_offset is None:
         log_reader.print_last_n_logs(
             service=service,
             line_count=args.line_count,
             levels=levels,
             components=components,
-            clusters=clusters,
-            instances=instances,
+            clusters=cluster,
+            instances=[instance],
+            pods=pods,
             raw_mode=args.raw_mode,
+            strip_headers=args.strip_headers,
         )
         return 0
     elif args.line_count is not None and args.line_offset is not None:
         log_reader.print_logs_by_offset(
             service=service,
             line_count=args.line_count,
             line_offset=args.line_offset,
             levels=levels,
             components=components,
-            clusters=clusters,
-            instances=instances,
+            clusters=cluster,
+            instances=[instance],
+            pods=pods,
             raw_mode=args.raw_mode,
+            strip_headers=args.strip_headers,
         )
         return 0
 
     # Handle time based filtering
     try:
         start_time, end_time = generate_start_end_time(args.time_from, args.time_to)
     except ValueError as e:
-        paasta_print(PaastaColors.red(str(e)), file=sys.stderr)
+        print(PaastaColors.red(str(e)), file=sys.stderr)
         return 1
 
     log_reader.print_logs_by_time(
         service=service,
         start_time=start_time,
         end_time=end_time,
         levels=levels,
         components=components,
-        clusters=clusters,
-        instances=instances,
+        clusters=cluster,
+        instances=[instance],
+        pods=pods,
         raw_mode=args.raw_mode,
+        strip_headers=args.strip_headers,
     )
+    return 0
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/list_clusters.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/list_clusters.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import list_clusters
-from paasta_tools.utils import paasta_print
 
 
 def add_subparser(subparsers):
     list_parser = subparsers.add_parser(
         "list-clusters",
         help="Display a list of all PaaSTA clusters",
         description=(
@@ -36,8 +35,8 @@
         help="define a different soa config directory",
     )
     list_parser.set_defaults(command=paasta_list_clusters)
 
 
 def paasta_list_clusters(args, **kwargs):
     for cluster in list_clusters(soa_dir=args.soa_dir):
-        paasta_print(cluster)
+        print(cluster)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/autoscale.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/pause_service_autoscaler.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,82 +8,100 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import logging
-
-from paasta_tools.api import client
-from paasta_tools.cli.utils import figure_out_service_name
-from paasta_tools.cli.utils import lazy_choices_completer
-from paasta_tools.cli.utils import list_instances
+from paasta_tools.autoscaling.pause_service_autoscaler import (
+    delete_service_autoscale_pause_time,
+)
+from paasta_tools.autoscaling.pause_service_autoscaler import (
+    get_service_autoscale_pause_time,
+)
+from paasta_tools.autoscaling.pause_service_autoscaler import (
+    update_service_autoscale_pause_time,
+)
 from paasta_tools.utils import _log_audit
-from paasta_tools.utils import list_clusters
-from paasta_tools.utils import list_services
-from paasta_tools.utils import paasta_print
-
 
-log = logging.getLogger(__name__)
+MAX_PAUSE_DURATION = 320
 
 
 def add_subparser(subparsers):
-    autoscale_parser = subparsers.add_parser(
-        "autoscale",
-        help="Manually scale a service up and down manually, bypassing the normal autoscaler",
+    status_parser = subparsers.add_parser(
+        "pause_service_autoscaler",
+        help="Pause the service autoscaler for an entire cluster",
+        description=(
+            "'paasta pause_service_autoscaler is used to pause the paasta service autoscaler "
+            "for an entire paasta cluster. "
+        ),
     )
-
-    autoscale_parser.add_argument(
-        "-s", "--service", help="Service that you want to stop. Like 'example_service'."
-    ).completer = lazy_choices_completer(list_services)
-    autoscale_parser.add_argument(
-        "-i",
-        "--instance",
-        help="Instance of the service that you want to stop. Like 'main' or 'canary'.",
-        required=True,
-    ).completer = lazy_choices_completer(list_instances)
-    autoscale_parser.add_argument(
+    status_parser.add_argument(
         "-c",
         "--cluster",
-        help="The PaaSTA cluster that has the service instance you want to stop. Like 'norcal-prod'.",
-        required=True,
-    ).completer = lazy_choices_completer(list_clusters)
-    autoscale_parser.add_argument(
-        "--set", help="Set the number to scale to. Must be an Int.", type=int
-    )
-    autoscale_parser.set_defaults(command=paasta_autoscale)
-
-
-def paasta_autoscale(args):
-    log.setLevel(logging.DEBUG)
-    service = figure_out_service_name(args)
-    api = client.get_paasta_api_client(cluster=args.cluster, http_res=True)
-    if not api:
-        paasta_print(
-            "Could not connect to paasta api. Maybe you misspelled the cluster?"
-        )
-        return 1
+        dest="cluster",
+        help="which cluster to pause autoscaling in. ie. pnw-prod",
+    )
+    status_parser.add_argument(
+        "-d",
+        "--pause-duration",
+        default=120,
+        dest="duration",
+        type=int,
+        help="How long to pause the autoscaler for, defaults to %(default)s minutes",
+    )
+    status_parser.add_argument(
+        "-f",
+        "--force",
+        help="Force pause for longer than max duration",
+        action="store_true",
+        dest="force",
+        default=False,
+    )
+    status_parser.add_argument(
+        "-i",
+        "--info",
+        help="Print when the autoscaler is paused until",
+        action="store_true",
+        dest="info",
+        default=False,
+    )
+    status_parser.add_argument(
+        "-r",
+        "--resume",
+        help="Resume autoscaling (unpause) in a cluster",
+        action="store_true",
+        dest="resume",
+        default=False,
+    )
+
+    status_parser.set_defaults(command=paasta_pause_service_autoscaler)
 
-    if args.set is None:
-        log.debug("Getting the current autoscaler count...")
-        res, http = api.autoscaler.get_autoscaler_count(
-            service=service, instance=args.instance
-        ).result()
-    else:
-        log.debug(f"Setting desired instances to {args.set}.")
-        body = {"desired_instances": int(args.set)}
-        res, http = api.autoscaler.update_autoscaler_count(
-            service=service, instance=args.instance, json_body=body
-        ).result()
 
+def paasta_pause_service_autoscaler(args):
+    """With a given cluster and duration, pauses the paasta service autoscaler
+    in that cluster for duration minutes"""
+    if args.duration > MAX_PAUSE_DURATION:
+        if not args.force:
+            print(
+                "Specified duration: {d} longer than max: {m}".format(
+                    d=args.duration, m=MAX_PAUSE_DURATION
+                )
+            )
+            print("If you are really sure, run again with --force")
+            return 3
+
+    if args.info:
+        return_code = get_service_autoscale_pause_time(args.cluster)
+    elif args.resume:
+        return_code = delete_service_autoscale_pause_time(args.cluster)
+        _log_audit(action="resume-service-autoscaler", cluster=args.cluster)
+    else:
+        minutes = args.duration
+        return_code = update_service_autoscale_pause_time(args.cluster, minutes)
         _log_audit(
-            action="manual-scale",
-            action_details=body,
-            service=service,
-            instance=args.instance,
+            action="pause-service-autoscaler",
+            action_details={"duration": minutes},
             cluster=args.cluster,
         )
 
-    log.debug(f"Res: {res} Http: {http}")
-    print(res["desired_instances"])
-    return 0
+    return return_code
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/__init__.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/__init__.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/itest.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/get_latest_deployment.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,102 +8,86 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import os
+import sys
 
-from paasta_tools.cli.utils import get_jenkins_build_output_url
 from paasta_tools.cli.utils import lazy_choices_completer
+from paasta_tools.cli.utils import list_deploy_groups
+from paasta_tools.cli.utils import PaastaColors
 from paasta_tools.cli.utils import validate_service_name
-from paasta_tools.utils import _log
-from paasta_tools.utils import _run
-from paasta_tools.utils import build_docker_tag
-from paasta_tools.utils import check_docker_image
+from paasta_tools.deployment_utils import get_currently_deployed_version
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import list_services
 
 
 def add_subparser(subparsers):
     list_parser = subparsers.add_parser(
-        "itest",
-        help="Runs 'make itest' as part of the PaaSTA contract.",
-        description=(
-            "'paasta itest' runs 'make itest' in the root of a service directory. "
-            "It is designed to be used in conjunction with the 'Jenkins' workflow: "
-            "http://paasta.readthedocs.io/en/latest/about/contract.html#jenkins-pipeline-recommended"
-        ),
+        "get-latest-deployment",
+        help="Gets the Git SHA for the latest deployment of a service",
     )
     list_parser.add_argument(
         "-s",
         "--service",
-        help="Test and build docker image for this service. Leading "
-        '"services-", as included in a Jenkins job name, '
-        "will be stripped.",
+        help="Name of the service which you want to get the latest deployment for.",
         required=True,
-    )
+    ).completer = lazy_choices_completer(list_services)
     list_parser.add_argument(
-        "-c",
-        "--commit",
-        help="Git sha used to construct tag for built image",
+        "-i",
+        "-l",
+        "--deploy-group",
+        help="Name of the deploy group which you want to get the latest deployment for.",
         required=True,
-    )
+    ).completer = lazy_choices_completer(list_deploy_groups)
     list_parser.add_argument(
         "-d",
         "--soa-dir",
-        dest="soa_dir",
         help="A directory from which soa-configs should be read from",
         default=DEFAULT_SOA_DIR,
-    ).completer = lazy_choices_completer(list_services)
-    list_parser.add_argument(
-        "--timeout",
-        dest="timeout",
-        help="How many seconds before this command times out",
-        default=3600,
     )
-    list_parser.set_defaults(command=paasta_itest)
+    format_group = list_parser.add_mutually_exclusive_group()
+    format_group.add_argument(
+        "--sha-only",
+        help="Return only the latest sha for this deploy group, not the full deployed version",
+        action="store_true",
+        default=False,
+    )
+    format_group.add_argument(
+        "-j",
+        "--json",
+        help="Return result in json format instead of raw string",
+        action="store_true",
+        default=False,
+    )
+
+    list_parser.set_defaults(command=paasta_get_latest_deployment)
 
 
-def paasta_itest(args):
-    """Build and test a docker image"""
+def paasta_get_latest_deployment(args):
     service = args.service
+    deploy_group = args.deploy_group
     soa_dir = args.soa_dir
-    if service and service.startswith("services-"):
-        service = service.split("services-", 1)[1]
-    validate_service_name(service, soa_dir=soa_dir)
-
-    tag = build_docker_tag(service, args.commit)
-    run_env = os.environ.copy()
-    run_env["DOCKER_TAG"] = tag
-    cmd = "make itest"
-    loglines = []
-
-    _log(
-        service=service,
-        line="starting itest for %s." % args.commit,
-        component="build",
-        level="event",
-    )
-    returncode, output = _run(
-        cmd,
-        env=run_env,
-        timeout=args.timeout,
-        log=True,
-        component="build",
-        service=service,
-        loglevel="debug",
+    validate_service_name(service, soa_dir)
+
+    version = get_currently_deployed_version(
+        service=service, deploy_group=deploy_group, soa_dir=soa_dir
     )
-    if returncode != 0:
-        loglines.append("ERROR: itest failed for %s." % args.commit)
-        output = get_jenkins_build_output_url()
-        if output:
-            loglines.append("See output: %s" % output)
+    if not version:
+        print(
+            PaastaColors.red(
+                f"A deployment could not be found for {deploy_group} in {service}"
+            ),
+            file=sys.stderr,
+        )
+        return 1
     else:
-        loglines.append("itest passed for %s." % args.commit)
-        if not check_docker_image(service, args.commit):
-            loglines.append("ERROR: itest has not created %s" % tag)
-            returncode = 1
-    for logline in loglines:
-        _log(service=service, line=logline, component="build", level="event")
-    return returncode
+        if args.sha_only:
+            print(version.sha)
+        else:
+            if args.json:
+                print(version.json())
+            else:
+                print(version)
+        return 0
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/info.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/info.py`

 * *Files 8% similar despite different names*

```diff
@@ -16,26 +16,25 @@
 from typing import List
 
 from service_configuration_lib import read_service_configuration
 
 from paasta_tools.cli.cmds.status import get_actual_deployments
 from paasta_tools.cli.utils import figure_out_service_name
 from paasta_tools.cli.utils import lazy_choices_completer
-from paasta_tools.marathon_tools import get_all_namespaces_for_service
-from paasta_tools.marathon_tools import load_service_namespace_config
+from paasta_tools.long_running_service_tools import get_all_namespaces_for_service
+from paasta_tools.long_running_service_tools import load_service_namespace_config
 from paasta_tools.monitoring_tools import get_runbook
 from paasta_tools.monitoring_tools import get_team
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import get_git_url
 from paasta_tools.utils import list_services
 from paasta_tools.utils import NoDeploymentsAvailable
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 
-NO_DESCRIPTION_MESSAGE = "No 'description' entry in service.yaml. Please a one line sentence that describes this service"
+NO_DESCRIPTION_MESSAGE = "No 'description' entry in service.yaml. Please add a one line sentence that describes this service"
 NO_EXTERNAL_LINK_MESSAGE = (
     "No 'external_link' entry in service.yaml. "
     "Please add one that points to a reference doc for your service"
 )
 
 
 def add_subparser(subparsers):
@@ -150,8 +149,8 @@
     return "\n".join(output)
 
 
 def paasta_info(args):
     """Prints general information about a service"""
     soa_dir = args.soa_dir
     service = figure_out_service_name(args, soa_dir=soa_dir)
-    paasta_print(get_service_info(service, soa_dir))
+    print(get_service_info(service, soa_dir))
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/boost.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/boost.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,23 +14,22 @@
 # limitations under the License.
 from paasta_tools.autoscaling import load_boost
 from paasta_tools.cli.utils import execute_paasta_cluster_boost_on_remote_master
 from paasta_tools.cli.utils import lazy_choices_completer
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import list_clusters
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 
 
 def add_subparser(subparsers):
     boost_parser = subparsers.add_parser(
         "boost",
         help="Set, print the status, or clear a capacity boost for a given region in a PaaSTA cluster",
         description=(
-            "'paasta boost' is used to temporary provision more capacity in a given cluster "
+            "'paasta boost' is used to temporarily provision more capacity in a given cluster "
             "It operates by ssh'ing to a Mesos master of a remote cluster, and "
             "interacting with the boost in the local zookeeper cluster. If you set or clear "
             "a boost, you may want to run the cluster autoscaler manually afterward."
         ),
         epilog=(
             "The boost command may time out during heavy load. When that happens "
             "users may execute the ssh command directly, in order to bypass the timeout."
@@ -99,15 +98,15 @@
 def paasta_boost(args):
     soa_dir = args.soa_dir
     system_paasta_config = load_system_paasta_config()
     all_clusters = list_clusters(soa_dir=soa_dir)
     clusters = args.cluster.split(",")
     for cluster in clusters:
         if cluster not in all_clusters:
-            paasta_print(
+            print(
                 f"Error: {cluster} doesn't look like a valid cluster. "
                 + "Here is a list of valid paasta clusters:\n"
                 + "\n".join(all_clusters)
             )
             return 1
 
     return_code, output = execute_paasta_cluster_boost_on_remote_master(
@@ -116,9 +115,9 @@
         action=args.action,
         pool=args.pool,
         duration=args.duration if args.action == "set" else None,
         override=args.override if args.action == "set" else None,
         boost=args.boost if args.action == "set" else None,
         verbose=args.verbose,
     )
-    paasta_print(output)
+    print(output)
     return return_code
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cmds/metastatus.py` & `paasta-tools-1.0.0/paasta_tools/cli/cmds/metastatus.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,29 +11,29 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Sequence
 from typing import Tuple
 
-from bravado.exception import HTTPError
-
-from paasta_tools.api.client import get_paasta_api_client
+from paasta_tools import paastaapi
+from paasta_tools.api.client import get_paasta_oapi_client
 from paasta_tools.cli.utils import get_paasta_metastatus_cmd_args
 from paasta_tools.cli.utils import lazy_choices_completer
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import list_clusters
 from paasta_tools.utils import list_services
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import SystemPaastaConfig
 
 
-def add_subparser(subparsers,) -> None:
+def add_subparser(
+    subparsers,
+) -> None:
     status_parser = subparsers.add_parser(
         "metastatus",
         help="Display the status for an entire PaaSTA cluster",
         description=(
             "'paasta metastatus' is used to get the vital statistics about a PaaSTA "
             "cluster as a whole. This tool is helpful when answering the question: 'Is "
             "it just my service or the whole cluster that is broken?'\n\n"
@@ -52,15 +52,15 @@
         dest="verbose",
         default=0,
         help="""Print out more output regarding the state of the cluster.
         Multiple v options increase verbosity. Maximum is 3.""",
     )
     clusters_help = (
         "A comma separated list of clusters to view. Defaults to view all clusters. "
-        "Try: --clusters norcal-prod,nova-prod"
+        "Try: --clusters pnw-prod,nova-prod"
     )
     status_parser.add_argument(
         "-c", "--clusters", help=clusters_help
     ).completer = lazy_choices_completer(list_clusters)
     status_parser.add_argument(
         "-d",
         "--soa-dir",
@@ -118,32 +118,30 @@
     cluster: str,
     system_paasta_config: SystemPaastaConfig,
     groupings: Sequence[str],
     verbose: int,
     autoscaling_info: bool = False,
     use_mesos_cache: bool = False,
 ) -> Tuple[int, str]:
-    client = get_paasta_api_client(cluster, system_paasta_config)
+    client = get_paasta_oapi_client(cluster, system_paasta_config)
     if not client:
-        paasta_print("Cannot get a paasta-api client")
+        print("Cannot get a paasta-api client")
         exit(1)
 
     try:
         cmd_args, _ = get_paasta_metastatus_cmd_args(
             groupings=groupings,
             verbose=verbose,
             autoscaling_info=autoscaling_info,
             use_mesos_cache=use_mesos_cache,
         )
-        res = client.metastatus.metastatus(
-            cmd_args=[str(arg) for arg in cmd_args]
-        ).result()
+        res = client.default.metastatus(cmd_args=[str(arg) for arg in cmd_args])
         output, exit_code = res.output, res.exit_code
-    except HTTPError as exc:
-        output, exit_code = exc.response.text, exc.status_code
+    except paastaapi.ApiException as exc:
+        output, exit_code = exc.body, exc.status
 
     return exit_code, output
 
 
 def print_cluster_status(
     cluster: str,
     system_paasta_config: SystemPaastaConfig,
@@ -159,55 +157,62 @@
         system_paasta_config=system_paasta_config,
         groupings=groupings,
         verbose=verbose,
         autoscaling_info=autoscaling_info,
         use_mesos_cache=use_mesos_cache,
     )
 
-    paasta_print("Cluster: %s" % cluster)
-    paasta_print(get_cluster_dashboards(cluster))
-    paasta_print(output)
-    paasta_print()
+    print("Cluster: %s" % cluster)
+    print(get_cluster_dashboards(cluster))
+    print(output)
+    print()
 
     return return_code
 
 
 def figure_out_clusters_to_inspect(args, all_clusters) -> Sequence[str]:
     if args.clusters is not None:
         clusters_to_inspect = args.clusters.split(",")
     else:
         clusters_to_inspect = all_clusters
     return clusters_to_inspect
 
 
-def get_cluster_dashboards(cluster: str,) -> str:
+def get_cluster_dashboards(
+    cluster: str,
+) -> str:
     """Returns the direct dashboards for humans to use for a given cluster"""
     SPACER = " "
     try:
         dashboards = load_system_paasta_config().get_dashboard_links()[cluster]
     except KeyError as e:
         if e.args[0] == cluster:
             output = [PaastaColors.red("No dashboards configured for %s!" % cluster)]
         else:
             output = [PaastaColors.red("No dashboards configured!")]
     else:
+        if len(dashboards) == 0:
+            return PaastaColors.red("No dashboards configured!")
+
         output = ["Dashboards:"]
         spacing = max((len(label) for label in dashboards.keys())) + 1
         for label, urls in dashboards.items():
             if isinstance(urls, list):
                 urls = "\n    %s" % "\n    ".join(urls)
             output.append(
                 "  {}:{}{}".format(
                     label, SPACER * (spacing - len(label)), PaastaColors.cyan(urls)
                 )
             )
     return "\n".join(output)
 
 
-def paasta_metastatus(args,) -> int:
+def paasta_metastatus(
+    args,
+) -> int:
     """Print the status of a PaaSTA clusters"""
     soa_dir = args.soa_dir
     system_paasta_config = load_system_paasta_config()
 
     all_clusters = list_clusters(soa_dir=soa_dir)
     clusters_to_inspect = figure_out_clusters_to_inspect(args, all_clusters)
     return_codes = []
@@ -220,12 +225,10 @@
                     groupings=args.groupings,
                     verbose=args.verbose,
                     autoscaling_info=args.autoscaling_info,
                     use_mesos_cache=args.use_mesos_cache,
                 )
             )
         else:
-            paasta_print(
-                "Cluster %s doesn't look like a valid cluster?" % args.clusters
-            )
-            paasta_print("Try using tab completion to help complete the cluster name")
+            print("Cluster %s doesn't look like a valid cluster?" % args.clusters)
+            print("Try using tab completion to help complete the cluster name")
     return 0 if all([return_code == 0 for return_code in return_codes]) else 1
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/paasta_tabcomplete.sh` & `paasta-tools-1.0.0/paasta_tools/cli/paasta_tabcomplete.sh`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/cli.py` & `paasta-tools-1.0.0/paasta_tools/cli/cli.py`

 * *Files 25% similar despite different names*

```diff
@@ -13,34 +13,58 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # PYTHON_ARGCOMPLETE_OK
 """A command line tool for viewing information from the PaaSTA stack."""
 import argparse
 import logging
 import os
+import pkgutil
 import subprocess
 import sys
 import warnings
+from typing import Any
+from typing import List
+from typing import Tuple
 
 import argcomplete
-import pkg_resources
 
+import paasta_tools
 from paasta_tools.cli import cmds
-from paasta_tools.cli.utils import load_method
-from paasta_tools.cli.utils import modules_in_pkg as paasta_commands_dir
-from paasta_tools.utils import paasta_print
+
+
+def load_method(module_name, method_name):
+    """Return a function given a module and method name.
+
+    :param module_name: a string
+    :param method_name: a string
+    :return: a function
+    """
+    module = __import__(module_name, fromlist=[method_name])
+    method = getattr(module, method_name)
+    return method
+
+
+def modules_in_pkg(pkg):
+    """Return the list of modules in a python package (a module with a
+    __init__.py file.)
+
+    :return: a list of strings such as `['list', 'check']` that correspond to
+             the module names in the package.
+    """
+    for _, module_name, _ in pkgutil.walk_packages(pkg.__path__):
+        yield module_name
 
 
 class PrintsHelpOnErrorArgumentParser(argparse.ArgumentParser):
     """Overriding the error method allows us to print the whole help page,
     otherwise the python arg parser prints a not-so-useful usage message that
     is way too terse"""
 
     def error(self, message):
-        paasta_print("Argument parse error: %s" % message)
+        print(f"Argument parse error: {message}\n")
         self.print_help()
         sys.exit(1)
 
 
 def list_external_commands():
     p = subprocess.check_output(["/bin/bash", "-p", "-c", "compgen -A command paasta-"])
     lines = p.decode("utf-8").strip().split("\n")
@@ -50,24 +74,14 @@
 def calling_external_command():
     if len(sys.argv) > 1:
         return sys.argv[1] in list_external_commands()
     else:
         return False
 
 
-def get_command_help(command):
-    return f"(run 'paasta {command} -h' for usage)"
-
-
-def external_commands_items():
-    for command in list_external_commands():
-        command_help = get_command_help(command)
-        yield command, command_help
-
-
 def exec_subcommand(argv):
     command = sys.argv[1]
     os.execlp(f"paasta-{command}", *argv[1:])
 
 
 def add_subparser(command, subparsers):
     """Given a command name, paasta_cmd, execute the add_subparser method
@@ -81,15 +95,57 @@
     :param command: a simple string - e.g. 'list'
     :param subparsers: an ArgumentParser object"""
     module_name = "paasta_tools.cli.cmds.%s" % command
     add_subparser_fn = load_method(module_name, "add_subparser")
     add_subparser_fn(subparsers)
 
 
-def get_argparser():
+PAASTA_SUBCOMMANDS = {
+    "autoscale": "autoscale",
+    "boost": "boost",
+    "check": "check",
+    "cook-image": "cook_image",
+    "get-docker-image": "get_docker_image",
+    "get-image-version": "get_image_version",
+    "get-latest-deployment": "get_latest_deployment",
+    "info": "info",
+    "itest": "itest",
+    "list-clusters": "list_clusters",
+    "list-deploy-queue": "list_deploy_queue",
+    "list": "list",
+    "local-run": "local_run",
+    "logs": "logs",
+    "mark-for-deployment": "mark_for_deployment",
+    "mesh-status": "mesh_status",
+    "metastatus": "metastatus",
+    "pause_service_autoscaler": "pause_service_autoscaler",
+    "push-to-registry": "push_to_registry",
+    "remote-run": "remote_run",
+    "rollback": "rollback",
+    "secret": "secret",
+    "security-check": "security_check",
+    "spark-run": "spark_run",
+    "start": "start_stop_restart",
+    "stop": "start_stop_restart",
+    "restart": "start_stop_restart",
+    "status": "status",
+    "validate": "validate",
+    "wait-for-deployment": "wait_for_deployment",
+}
+
+
+def get_argparser(commands=None):
+    """Create and return argument parser for a set of subcommands.
+
+    :param commands: Union[None, List[str]] If `commands` argument is `None`,
+    add full parsers for all subcommands, if `commands` is empty list -
+    add thin parsers for all subcommands, otherwise - add full parsers for
+    subcommands in the argument.
+    """
+
     parser = PrintsHelpOnErrorArgumentParser(
         description=(
             "The PaaSTA command line tool. The 'paasta' command is the entry point "
             "to multiple subcommands, see below.\n\n"
             "You can see more help for individual commands by appending them with '--help', "
             "for example, 'paasta status --help' or see the man page with 'man paasta status'."
         ),
@@ -103,46 +159,78 @@
     )
 
     # http://stackoverflow.com/a/8521644/812183
     parser.add_argument(
         "-V",
         "--version",
         action="version",
-        version="paasta-tools {}".format(
-            pkg_resources.get_distribution("paasta-tools").version
-        ),
+        version=f"paasta-tools {paasta_tools.__version__}",
     )
 
-    subparsers = parser.add_subparsers(
-        help="[-h, --help] for subcommand help", dest="command"
-    )
+    subparsers = parser.add_subparsers(dest="command", metavar="")
     subparsers.required = True
 
     # Adding a separate help subparser allows us to respond to "help" without --help
-    help_parser = subparsers.add_parser("help", add_help=False)
+    help_parser = subparsers.add_parser(
+        "help", help=f"run `paasta <subcommand> -h` for help"
+    )
     help_parser.set_defaults(command=None)
 
-    for command in sorted(paasta_commands_dir(cmds)):
-        add_subparser(command, subparsers)
+    # Build a list of subcommands to add them in alphabetical order later
+    command_choices: List[Tuple[str, Any]] = []
+    if commands is None:
+        for command in sorted(modules_in_pkg(cmds)):
+            command_choices.append(
+                (command, (add_subparser, [command, subparsers], {}))
+            )
+    elif commands:
+        for command in commands:
+            if command not in PAASTA_SUBCOMMANDS:
+                # could be external subcommand
+                continue
+            command_choices.append(
+                (
+                    command,
+                    (add_subparser, [PAASTA_SUBCOMMANDS[command], subparsers], {}),
+                )
+            )
+    else:
+        for command in PAASTA_SUBCOMMANDS.keys():
+            command_choices.append(
+                (
+                    command,
+                    (subparsers.add_parser, [command], dict(help="", add_help=False)),
+                )
+            )
+
+    for command in list_external_commands():
+        command_choices.append(
+            (command, (subparsers.add_parser, [command], dict(help="")))
+        )
 
-    for command, command_help in external_commands_items():
-        subparsers.add_parser(command, help=command_help)
+    for (_, (fn, args, kwds)) in sorted(command_choices, key=lambda e: e[0]):
+        fn(*args, **kwds)
 
     return parser
 
 
 def parse_args(argv):
     """Initialize autocompletion and configure the argument parser.
 
     :return: an argparse.Namespace object mapping parameter names to the inputs
              from sys.argv
     """
-    parser = get_argparser()
+    parser = get_argparser(commands=[])
     argcomplete.autocomplete(parser)
 
+    args, _ = parser.parse_known_args(argv)
+    if args.command:
+        parser = get_argparser(commands=[args.command])
+
+    argcomplete.autocomplete(parser)
     return parser.parse_args(argv), parser
 
 
 def main(argv=None):
     """Perform a paasta call. Read args from sys.argv and pass parsed args onto
     appropriate command in paasta_cli/cmds directory.
 
@@ -161,13 +249,13 @@
         args, parser = parse_args(argv)
         if args.command is None:
             parser.print_help()
             return_code = 0
         else:
             return_code = args.command(args)
     except KeyboardInterrupt:
-        return_code = 1
+        return_code = 130
     sys.exit(return_code)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/utils.py` & `paasta-tools-1.0.0/paasta_tools/cli/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,93 +8,82 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import argparse
+import difflib
 import fnmatch
 import getpass
 import hashlib
 import logging
 import os
-import pkgutil
 import random
 import re
+import socket
 import subprocess
-import sys
 from collections import defaultdict
 from shlex import quote
-from socket import gaierror
-from socket import gethostbyname_ex
 from typing import Callable
+from typing import Collection
+from typing import Generator
 from typing import Iterable
 from typing import List
 from typing import Mapping
 from typing import NamedTuple
 from typing import Optional
 from typing import Sequence
 from typing import Set
 from typing import Tuple
 
 import ephemeral_port_reserve
-from bravado.exception import HTTPError
-from bravado.exception import HTTPNotFound
 from mypy_extensions import NamedArg
 
 from paasta_tools import remote_git
 from paasta_tools.adhoc_tools import load_adhoc_job_config
-from paasta_tools.api import client
 from paasta_tools.cassandracluster_tools import load_cassandracluster_instance_config
+from paasta_tools.eks_tools import EksDeploymentConfig
+from paasta_tools.eks_tools import load_eks_service_config
 from paasta_tools.flink_tools import load_flink_instance_config
+from paasta_tools.flinkeks_tools import load_flinkeks_instance_config
 from paasta_tools.kafkacluster_tools import load_kafkacluster_instance_config
+from paasta_tools.kubernetes_tools import KubernetesDeploymentConfig
 from paasta_tools.kubernetes_tools import load_kubernetes_service_config
 from paasta_tools.long_running_service_tools import LongRunningServiceConfig
-from paasta_tools.marathon_tools import load_marathon_service_config
+from paasta_tools.monkrelaycluster_tools import load_monkrelaycluster_instance_config
+from paasta_tools.nrtsearchservice_tools import load_nrtsearchservice_instance_config
+from paasta_tools.nrtsearchserviceeks_tools import (
+    load_nrtsearchserviceeks_instance_config,
+)
+from paasta_tools.paasta_service_config_loader import PaastaServiceConfigLoader
 from paasta_tools.tron_tools import load_tron_instance_config
+from paasta_tools.utils import _log
 from paasta_tools.utils import _log_audit
 from paasta_tools.utils import _run
 from paasta_tools.utils import compose_job_id
+from paasta_tools.utils import DEFAULT_SOA_CONFIGS_GIT_URL
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import get_service_instance_list
+from paasta_tools.utils import INSTANCE_TYPE_TO_K8S_NAMESPACE
+from paasta_tools.utils import INSTANCE_TYPES
 from paasta_tools.utils import InstanceConfig
 from paasta_tools.utils import list_all_instances_for_service
 from paasta_tools.utils import list_clusters
 from paasta_tools.utils import list_services
-from paasta_tools.utils import paasta_print
+from paasta_tools.utils import load_system_paasta_config
+from paasta_tools.utils import PAASTA_K8S_INSTANCE_TYPES
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import SystemPaastaConfig
 from paasta_tools.utils import validate_service_instance
+from paasta_tools.vitesscluster_tools import load_vitess_instance_config
 
 log = logging.getLogger(__name__)
 
 
-def load_method(module_name, method_name):
-    """Return a function given a module and method name.
-
-    :param module_name: a string
-    :param method_name: a string
-    :return: a function
-    """
-    module = __import__(module_name, fromlist=[method_name])
-    method = getattr(module, method_name)
-    return method
-
-
-def modules_in_pkg(pkg):
-    """Return the list of modules in a python package (a module with a
-    __init__.py file.)
-
-    :return: a list of strings such as `['list', 'check']` that correspond to
-             the module names in the package.
-    """
-    for _, module_name, _ in pkgutil.walk_packages(pkg.__path__):
-        yield module_name
-
-
 def is_file_in_dir(file_name, path):
     """Recursively search path for file_name.
 
     :param file_name: a string of a file name to find
     :param path: a string path
     :param file_ext: a string of a file extension
     :return: a boolean
@@ -125,14 +114,21 @@
 def x_mark():
     """
     :return: string that can print an x-mark
     """
     return PaastaColors.red("\u2717")
 
 
+def info_mark() -> str:
+    """
+    :return: string that can print an info symbol
+    """
+    return PaastaColors.blue("\u2139")
+
+
 def success(msg):
     """Format a paasta check success message.
 
     :param msg: a string
     :return: a beautiful string
     """
     return "{} {}".format(check_mark(), msg)
@@ -143,14 +139,18 @@
 
     :param msg: a string
     :return: a beautiful string
     """
     return "{} {} {}".format(x_mark(), msg, PaastaColors.blue(link))
 
 
+def info_message(msg: str) -> str:
+    return f"{info_mark()} {msg}"
+
+
 class PaastaCheckMessages:
 
     """Collection of message printed out by 'paasta check'.
     Helpful as it avoids cumbersome maintenance of the unit tests.
     """
 
     DEPLOY_YAML_FOUND = success("deploy.yaml exists for a Jenkins pipeline")
@@ -190,16 +190,14 @@
         "using the local mirrors.\n"
         "More info:",
         "http://y/base-docker-images",
     )
 
     GIT_REPO_FOUND = success("Git repo found in the expected location.")
 
-    MARATHON_YAML_FOUND = success("Found marathon.yaml file.")
-
     ADHOC_YAML_FOUND = success("Found adhoc.yaml file.")
 
     MAKEFILE_FOUND = success("A Makefile is present")
     MAKEFILE_MISSING = failure(
         "No Makefile available. Please make a Makefile that responds\n"
         "to the proper targets. More info:",
         "http://paasta.readthedocs.io/en/latest/about/contract.html",
@@ -350,30 +348,32 @@
     :raises: NoSuchService exception
     """
     if not service or not os.path.isdir(os.path.join(soa_dir, service)):
         raise NoSuchService(service)
     return True
 
 
-def list_paasta_services():
+def list_paasta_services(soa_dir: str = DEFAULT_SOA_DIR):
     """Returns a sorted list of services that happen to have at
     least one service.instance, which indicates it is on PaaSTA
     """
     the_list = []
-    for service in list_services():
-        if list_all_instances_for_service(service):
+    for service in list_services(soa_dir):
+        if list_all_instances_for_service(service, soa_dir=soa_dir):
             the_list.append(service)
     return the_list
 
 
-def list_service_instances():
+def list_service_instances(soa_dir: str = DEFAULT_SOA_DIR):
     """Returns a sorted list of service<SPACER>instance names"""
     the_list = []
-    for service in list_services():
-        for instance in list_all_instances_for_service(service):
+    for service in list_services(soa_dir):
+        for instance in list_all_instances_for_service(
+            service=service, soa_dir=soa_dir
+        ):
             the_list.append(compose_job_id(service, instance))
     return the_list
 
 
 def list_instances(**kwargs):
     """Returns a sorted list of all possible instance names
     for tab completion. We try to guess what service you might be
@@ -399,17 +399,17 @@
     Return IPs of those Mesos masters.
     """
 
     cluster_fqdn = system_paasta_config.get_cluster_fqdn_format().format(
         cluster=cluster
     )
     try:
-        _, _, ips = gethostbyname_ex(cluster_fqdn)
+        _, _, ips = socket.gethostbyname_ex(cluster_fqdn)
         output = None
-    except gaierror as e:
+    except socket.gaierror as e:
         output = f"ERROR while doing DNS lookup of {cluster_fqdn}:\n{e.strerror}\n "
         ips = []
     return (ips, output)
 
 
 def find_connectable_master(
     masters: Sequence[str],
@@ -697,15 +697,15 @@
 
 def figure_out_service_name(args, soa_dir=DEFAULT_SOA_DIR):
     """Figures out and validates the input service name"""
     service = args.service or guess_service_name()
     try:
         validate_service_name(service, soa_dir=soa_dir)
     except NoSuchService as service_not_found:
-        paasta_print(service_not_found)
+        print(service_not_found)
         exit(1)
     return service
 
 
 def get_jenkins_build_output_url():
     """Returns the URL for Jenkins job's output.
     Returns None if it's not available.
@@ -767,63 +767,91 @@
 class LongRunningInstanceTypeHandler(NamedTuple):
     lister: LongRunningServiceListerSig
     loader: LongRunningServiceLoaderSig
 
 
 INSTANCE_TYPE_HANDLERS: Mapping[str, InstanceTypeHandler] = defaultdict(
     lambda: InstanceTypeHandler(None, None),
-    marathon=InstanceTypeHandler(
-        get_service_instance_list, load_marathon_service_config
-    ),
     adhoc=InstanceTypeHandler(get_service_instance_list, load_adhoc_job_config),
     kubernetes=InstanceTypeHandler(
         get_service_instance_list, load_kubernetes_service_config
     ),
+    eks=InstanceTypeHandler(get_service_instance_list, load_eks_service_config),
     tron=InstanceTypeHandler(get_service_instance_list, load_tron_instance_config),
     flink=InstanceTypeHandler(get_service_instance_list, load_flink_instance_config),
+    flinkeks=InstanceTypeHandler(
+        get_service_instance_list, load_flinkeks_instance_config
+    ),
     cassandracluster=InstanceTypeHandler(
         get_service_instance_list, load_cassandracluster_instance_config
     ),
     kafkacluster=InstanceTypeHandler(
         get_service_instance_list, load_kafkacluster_instance_config
     ),
+    vitesscluster=InstanceTypeHandler(
+        get_service_instance_list, load_vitess_instance_config
+    ),
+    nrtsearchservice=InstanceTypeHandler(
+        get_service_instance_list, load_nrtsearchservice_instance_config
+    ),
+    nrtsearchserviceeks=InstanceTypeHandler(
+        get_service_instance_list, load_nrtsearchserviceeks_instance_config
+    ),
+    monkrelays=InstanceTypeHandler(
+        get_service_instance_list, load_monkrelaycluster_instance_config
+    ),
 )
 
 LONG_RUNNING_INSTANCE_TYPE_HANDLERS: Mapping[
     str, LongRunningInstanceTypeHandler
 ] = defaultdict(
     lambda: LongRunningInstanceTypeHandler(None, None),
-    marathon=LongRunningInstanceTypeHandler(
-        get_service_instance_list, load_marathon_service_config
-    ),
     kubernetes=LongRunningInstanceTypeHandler(
         get_service_instance_list, load_kubernetes_service_config
     ),
     flink=LongRunningInstanceTypeHandler(
         get_service_instance_list, load_flink_instance_config
     ),
+    flinkeks=LongRunningInstanceTypeHandler(
+        get_service_instance_list, load_flinkeks_instance_config
+    ),
     cassandracluster=LongRunningInstanceTypeHandler(
         get_service_instance_list, load_cassandracluster_instance_config
     ),
     kafkacluster=LongRunningInstanceTypeHandler(
         get_service_instance_list, load_kafkacluster_instance_config
     ),
+    vitesscluster=LongRunningInstanceTypeHandler(
+        get_service_instance_list, load_vitess_instance_config
+    ),
+    nrtsearchservice=LongRunningInstanceTypeHandler(
+        get_service_instance_list, load_nrtsearchservice_instance_config
+    ),
+    nrtsearchserviceeks=LongRunningInstanceTypeHandler(
+        get_service_instance_list, load_nrtsearchserviceeks_instance_config
+    ),
+    monkrelays=LongRunningInstanceTypeHandler(
+        get_service_instance_list, load_monkrelaycluster_instance_config
+    ),
+    eks=LongRunningInstanceTypeHandler(
+        get_service_instance_list, load_eks_service_config
+    ),
 )
 
 
 def get_instance_config(
     service: str,
     instance: str,
     cluster: str,
     soa_dir: str = DEFAULT_SOA_DIR,
     load_deployments: bool = False,
     instance_type: Optional[str] = None,
 ) -> InstanceConfig:
-    """ Returns the InstanceConfig object for whatever type of instance
-    it is. (marathon) """
+    """Returns the InstanceConfig object for whatever type of instance
+    it is. (kubernetes)"""
     if instance_type is None:
         instance_type = validate_service_instance(
             service=service, instance=instance, cluster=cluster, soa_dir=soa_dir
         )
 
     instance_config_loader = INSTANCE_TYPE_HANDLERS[instance_type].loader
     if instance_config_loader is None:
@@ -837,17 +865,72 @@
         instance=instance,
         cluster=cluster,
         load_deployments=load_deployments,
         soa_dir=soa_dir,
     )
 
 
-def extract_tags(paasta_tag):
+def get_namespaces_for_secret(
+    service: str, cluster: str, secret_name: str, soa_dir: str = DEFAULT_SOA_DIR
+) -> Set[str]:
+    secret_to_k8s_namespace = set()
+
+    k8s_instance_type_classes = {
+        "kubernetes": KubernetesDeploymentConfig,
+        "eks": EksDeploymentConfig,
+    }
+    for instance_type in INSTANCE_TYPES:
+        if instance_type in PAASTA_K8S_INSTANCE_TYPES:
+            config_loader = PaastaServiceConfigLoader(service, soa_dir)
+            for service_instance_config in config_loader.instance_configs(
+                cluster=cluster,
+                instance_type_class=k8s_instance_type_classes[instance_type],
+            ):
+                secret_to_k8s_namespace.add(service_instance_config.get_namespace())
+        else:
+            instances = get_service_instance_list(
+                service=service,
+                instance_type=instance_type,
+                cluster=cluster,
+                soa_dir=soa_dir,
+            )
+
+            for serv, instance in instances:
+                config = get_instance_config(serv, instance, cluster, soa_dir)
+                if secret_name in config.get_env():
+                    secret_to_k8s_namespace.add(
+                        INSTANCE_TYPE_TO_K8S_NAMESPACE[instance_type]
+                    )
+
+    return secret_to_k8s_namespace
+
+
+def select_k8s_secret_namespace(namespaces: Set[str]) -> Optional[str]:
+    namespaces_count = len(namespaces)
+
+    if not namespaces_count:
+        return None
+
+    if namespaces_count == 1:
+        return namespaces.pop()
+
+    # prioritise paasta, tron namespaces when found
+    for namespace in namespaces:
+        if namespace.startswith("paasta"):
+            return namespace
+        if namespace == "tron":
+            return namespace
+
+    # only experimental k8s namespaces
+    return namespaces.pop()
+
+
+def extract_tags(paasta_tag: str) -> Mapping[str, str]:
     """Returns a dictionary containing information from a git tag"""
-    regex = r"^refs/tags/(?:paasta-){1,2}(?P<deploy_group>.*?)-(?P<tstamp>\d{8}T\d{6})-(?P<tag>.*?)$"
+    regex = r"^refs/tags/(?:paasta-){1,2}(?P<deploy_group>[a-zA-Z0-9._-]+)(?:\+(?P<image_version>.*)){0,1}-(?P<tstamp>\d{8}T\d{6})-(?P<tag>.*?)$"
     regex_match = re.match(regex, paasta_tag)
     return regex_match.groupdict() if regex_match else {}
 
 
 def list_deploy_groups(
     service: Optional[str], soa_dir: str = DEFAULT_SOA_DIR, parsed_args=None, **kwargs
 ) -> Set:
@@ -864,70 +947,66 @@
                 )
             },
         )
     )
 
 
 def validate_given_deploy_groups(
-    all_deploy_groups: Sequence[str], args_deploy_groups: Sequence[str]
+    all_deploy_groups: Collection[str], args_deploy_groups: Collection[str]
 ) -> Tuple[Set[str], Set[str]]:
     """Given two lists of deploy groups, return the intersection and difference between them.
 
     :param all_deploy_groups: instances actually belonging to a service
     :param args_deploy_groups: the desired instances
     :returns: a tuple with (common, difference) indicating deploy groups common in both
         lists and those only in args_deploy_groups
     """
     invalid_deploy_groups: Set[str]
-    if len(args_deploy_groups) == 0:
-        valid_deploy_groups = set(all_deploy_groups)
-        invalid_deploy_groups = set()
-    else:
-        valid_deploy_groups = set(args_deploy_groups).intersection(all_deploy_groups)
-        invalid_deploy_groups = set(args_deploy_groups).difference(all_deploy_groups)
+    valid_deploy_groups = set(args_deploy_groups).intersection(all_deploy_groups)
+    invalid_deploy_groups = set(args_deploy_groups).difference(all_deploy_groups)
 
     return valid_deploy_groups, invalid_deploy_groups
 
 
 def short_to_full_git_sha(short, refs):
-    """Converts a short git sha to a full sha
+    """Converts a short git sha to a full SHA
 
-    :param short: A short git sha represented as a string
-    :param refs: A list of refs in the git repository
-    :return: The full git sha or None if one can't be found
+    :param short: A short Git SHA represented as a string
+    :param refs: A list of refs in the Git repository
+    :return: The full Git SHA or None if one can't be found
     """
     return [sha for sha in set(refs.values()) if sha.startswith(short)]
 
 
 def validate_short_git_sha(value):
     pattern = re.compile("[a-f0-9]{4,40}")
     if not pattern.match(value):
-        raise argparse.ArgumentTypeError("%s is not a valid git sha" % value)
+        raise argparse.ArgumentTypeError("%s is not a valid Git SHA" % value)
     return value
 
 
-def validate_full_git_sha(value):
+def validate_full_git_sha(value: str) -> str:
     pattern = re.compile("[a-f0-9]{40}")
     if not pattern.match(value):
         raise argparse.ArgumentTypeError(
-            "%s is not a full git sha, and PaaSTA needs the full sha" % value
+            "%s is not a full Git SHA, and PaaSTA needs the full SHA" % value
         )
     return value
 
 
 def validate_git_sha(sha, git_url):
     try:
         validate_full_git_sha(sha)
         return sha
     except argparse.ArgumentTypeError:
         refs = remote_git.list_remote_refs(git_url)
         commits = short_to_full_git_sha(short=sha, refs=refs)
         if len(commits) != 1:
             raise ValueError(
-                "%s matched %d git shas (with refs pointing at them). Must match exactly 1."
+                "%s matched %d Git SHAs (with refs pointing at them). Must match exactly 1."
                 % (sha, len(commits))
             )
         return commits[0]
 
 
 def get_subparser(subparsers, function, command, help_text, description):
     new_parser = subparsers.add_parser(
@@ -945,15 +1024,15 @@
         help="The name of the service you wish to inspect",
         required=True,
     ).completer = lazy_choices_completer(list_services)
     new_parser.add_argument(
         "-c",
         "--cluster",
         help="Cluster on which the service is running"
-        "For example: --cluster norcal-prod",
+        "For example: --cluster pnw-prod",
         required=True,
     ).completer = lazy_choices_completer(list_clusters)
     new_parser.add_argument(
         "-i",
         "--instance",
         help="The instance that you wish to inspect" "For example: --instance main",
         required=True,
@@ -976,47 +1055,28 @@
         "running on the specified host. If not specified we "
         "will pick a task at random",
     )
     new_parser.set_defaults(command=function)
     return new_parser
 
 
-def get_status_for_instance(cluster, service, instance):
-    api = client.get_paasta_api_client(cluster=cluster)
-    if not api:
-        sys.exit(1)
-    status = api.service.status_instance(service=service, instance=instance).result()
-    if not status.marathon:
-        log.error("Not a marathon service, exiting")
-        sys.exit(1)
-    return status
-
-
-def pick_slave_from_status(status, host=None):
-    if host:
-        return host
-    else:
-        slaves = status.marathon.slaves
-        return slaves[0]
-
-
 def get_instance_configs_for_service(
     service: str,
     soa_dir: str,
     type_filter: Optional[Iterable[str]] = None,
     clusters: Optional[Sequence[str]] = None,
     instances: Optional[Sequence[str]] = None,
-) -> Iterable[InstanceConfig]:
+) -> Generator[InstanceConfig, None, None]:
     if not clusters:
         clusters = list_clusters(service=service, soa_dir=soa_dir)
 
     if type_filter is None:
         type_filter = INSTANCE_TYPE_HANDLERS.keys()
 
-    for cluster in list_clusters(service=service, soa_dir=soa_dir):
+    for cluster in clusters:
         for instance_type, instance_handlers in INSTANCE_TYPE_HANDLERS.items():
             if instance_type not in type_filter:
                 continue
 
             instance_lister, instance_loader = instance_handlers
 
             for _, instance in instance_lister(
@@ -1033,65 +1093,14 @@
                     instance=instance,
                     cluster=cluster,
                     soa_dir=soa_dir,
                     load_deployments=False,
                 )
 
 
-class PaastaTaskNotFound(Exception):
-    pass
-
-
-def get_task_from_instance(
-    cluster, service, instance, slave_hostname=None, task_id=None, verbose=True
-):
-    api = client.get_paasta_api_client(cluster=cluster)
-    if not api:
-        log.error(f"Could not get API client for cluster {cluster}")
-        raise PaastaTaskNotFound
-    if task_id:
-        log.warning("Specifying a task_id, so ignoring hostname if specified")
-        task = api.service.task_instance(
-            service=service, instance=instance, verbose=True, task_id=task_id
-        ).result()
-        return task
-    try:
-        if task_id:
-            log.warning("Specifying a task_id, so ignoring hostname if specified")
-            task = api.service.task_instance(
-                service=service, instance=instance, verbose=True, task_id=task_id
-            ).result()
-            return task
-        tasks = api.service.tasks_instance(
-            service=service,
-            instance=instance,
-            verbose=True,
-            slave_hostname=slave_hostname,
-        ).result()
-    except HTTPNotFound:
-        log.error(
-            "Cannot find instance {}, for service {}, in cluster {}".format(
-                instance, service, cluster
-            )
-        )
-        raise PaastaTaskNotFound
-    except HTTPError as e:
-        log.error("Problem with API call to find task details")
-        log.error(e.response.text)
-        raise PaastaTaskNotFound
-    if not tasks:
-        log.error(
-            "Cannot find any tasks on host: {} or with task_id: {}".format(
-                slave_hostname, task_id
-            )
-        )
-        raise PaastaTaskNotFound
-    return tasks[0]
-
-
 def get_container_name(task):
     container_name = "mesos-{}".format(task.executor["container"])
     return container_name
 
 
 def pick_random_port(service_name):
     """Return a random port.
@@ -1099,7 +1108,104 @@
     Tries to return the same port for the same service each time, when
     possible.
     """
     hash_key = f"{service_name},{getpass.getuser()}".encode("utf8")
     hash_number = int(hashlib.sha1(hash_key).hexdigest(), 16)
     preferred_port = 33000 + (hash_number % 25000)
     return ephemeral_port_reserve.reserve("0.0.0.0", preferred_port)
+
+
+def trigger_deploys(
+    service: str,
+    system_config: Optional["SystemPaastaConfig"] = None,
+) -> None:
+    """Connects to the deploymentsd watcher on sysgit, which is an extremely simple
+    service that listens for a service string and then generates a service deployment"""
+    logline = f"Notifying soa-configs primary to generate a deployment for {service}"
+    _log(service=service, line=logline, component="deploy", level="event")
+    if not system_config:
+        system_config = load_system_paasta_config()
+    server = system_config.get_git_repo_config("yelpsoa-configs").get(
+        "deploy_server",
+        DEFAULT_SOA_CONFIGS_GIT_URL,
+    )
+
+    client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+    try:
+        client.connect((server, 5049))
+        client.send(f"{service}\n".encode("utf-8"))
+    finally:
+        client.close()
+
+
+def verify_instances(
+    args_instances: str,
+    service: str,
+    clusters: Sequence[str],
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> Sequence[str]:
+    """Verify that a list of instances specified by user is correct for this service.
+
+    :param args_instances: a list of instances.
+    :param service: the service name
+    :param cluster: a list of clusters
+    :returns: a list of instances specified in args_instances without any exclusions.
+    """
+    unverified_instances = args_instances.split(",")
+    service_instances: Set[str] = list_all_instances_for_service(
+        service, clusters=clusters, soa_dir=soa_dir
+    )
+
+    misspelled_instances: Sequence[str] = [
+        i for i in unverified_instances if i not in service_instances
+    ]
+
+    if len(misspelled_instances) == 0:
+        return misspelled_instances
+
+    # Check for instances with suffixes other than Tron instances (i.e. Flink instances)
+    instances_without_suffixes = [x.split(".")[0] for x in unverified_instances]
+
+    misspelled_instances = [
+        i for i in instances_without_suffixes if i not in service_instances
+    ]
+
+    if misspelled_instances:
+        suggestions: List[str] = []
+        for instance in misspelled_instances:
+            matches = difflib.get_close_matches(
+                instance, service_instances, n=5, cutoff=0.5
+            )
+            suggestions.extend(matches)  # type: ignore
+        suggestions = list(set(suggestions))
+
+        if clusters:
+            message = "{} doesn't have any instances matching {} on {}.".format(
+                service,
+                ", ".join(sorted(misspelled_instances)),
+                ", ".join(sorted(clusters)),
+            )
+        else:
+            message = "{} doesn't have any instances matching {}.".format(
+                service, ", ".join(sorted(misspelled_instances))
+            )
+
+        print(PaastaColors.red(message))
+
+        if suggestions:
+            print("Did you mean any of these?")
+            for instance in sorted(suggestions):
+                print("  %s" % instance)
+
+    return misspelled_instances
+
+
+def get_paasta_oapi_api_clustername(cluster: str, is_eks: bool) -> str:
+    """
+    We'll be doing a tiny bit of lying while we have both EKS and non-EKS
+    clusters: these will generally share the same PaaSTA name (i.e., the
+    soaconfigs suffix will stay the same) - but we'll need a way to route API
+    requests to the correct place. To do so, we'll add "fake" entries to our
+    api_endpoints SystemPaastaConfig that are the PaaSTA clustername with an
+    "eks-" prefix
+    """
+    return f"eks-{cluster}" if is_eks else cluster
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/fsm_cmd.py` & `paasta-tools-1.0.0/paasta_tools/cli/fsm_cmd.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,15 +17,14 @@
 import shutil
 import sys
 
 from cookiecutter.main import cookiecutter
 
 from paasta_tools.cli.fsm.autosuggest import suggest_smartstack_proxy_port
 from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 
 
 @contextlib.contextmanager
 def make_copyfile_symlink_aware():
     """The reasoning behind this monkeypatch is that cookiecutter doesn't
     respect symlinks at all, and at Yelp we use symlinks to reduce duplication
@@ -78,15 +77,15 @@
 
 def get_paasta_config(yelpsoa_config_root):
     variables = {"proxy_port": suggest_smartstack_proxy_port(yelpsoa_config_root)}
     return variables
 
 
 def write_paasta_config(variables, template, destination):
-    paasta_print("Using cookiecutter template from %s" % template)
+    print("Using cookiecutter template from %s" % template)
     with make_copyfile_symlink_aware():
         cookiecutter(
             template=template,
             extra_context=variables,
             output_dir=destination,
             overwrite_if_exists=True,
             no_input=not sys.stdout.isatty(),
@@ -98,23 +97,23 @@
     destination = args.yelpsoa_config_root
 
     paasta_config = load_system_paasta_config()
     template = paasta_config.get_fsm_template()
 
     write_paasta_config(variables=variables, template=template, destination=destination)
 
-    paasta_print(PaastaColors.yellow("               _  _(o)_(o)_  _"))
-    paasta_print(PaastaColors.red(r"             ._\`:_ F S M _:' \_,"))
-    paasta_print(PaastaColors.green(r"                 / (`---'\ `-."))
-    paasta_print(PaastaColors.cyan("              ,-`  _)    (_,"))
-    paasta_print("With My Noodly Appendage I Have Written Configs!")
-    paasta_print()
-    paasta_print("Customize Them If It Makes You Happy -- http://y/paasta For Details")
-    paasta_print("Remember To Add, Commit, And Push When You're Done:")
-    paasta_print()
+    print(PaastaColors.yellow("               _  _(o)_(o)_  _"))
+    print(PaastaColors.red(r"             ._\`:_ F S M _:' \_,"))
+    print(PaastaColors.green(r"                 / (`---'\ `-."))
+    print(PaastaColors.cyan("              ,-`  _)    (_,"))
+    print("With My Noodly Appendage I Have Written Configs!")
+    print()
+    print("Customize Them If It Makes You Happy -- http://y/paasta For Details")
+    print("Remember To Add, Commit, And Push When You're Done:")
+    print()
 
 
 def main():
     args = parse_args()
     paasta_fsm(args)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/__init__.py` & `paasta-tools-1.0.0/paasta_tools/cli/fsm/__init__.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/schemas/adhoc_schema.json` & `paasta-tools-1.0.0/paasta_tools/cli/schemas/autotuned_defaults/kubernetes_schema.json`

 * *Files 11% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.8728669819078947%*

 * *Differences: {"'description'": "'Properties that can be set by automated processes for "*

 * *                  "http://paasta.readthedocs.io/en/latest/yelpsoa_configs.html#kubernetes-clustername-yaml'",*

 * * "'patternProperties'": "{'^([a-z0-9]|[a-z0-9][a-z0-9_-]*[a-z0-9])*$': {'properties': {'cpus': "*

 * *                        "{delete: ['default']}, 'mem': {'exclusiveMinimum': False, delete: "*

 * *                        "['default']}, 'disk': {'minimum': 128, 'exclusiveMinimum': False, delete: "*

 * *                        "['default' […]*

```diff
@@ -1,103 +1,89 @@
 {
     "$schema": "http://json-schema.org/draft-04/schema#",
     "additionalProperties": false,
-    "description": "http://paasta.readthedocs.io/en/latest/yelpsoa_configs.html#adhoc-clustername-yaml",
+    "description": "Properties that can be set by automated processes for http://paasta.readthedocs.io/en/latest/yelpsoa_configs.html#kubernetes-clustername-yaml",
     "minProperties": 1,
     "patternProperties": {
         "^([a-z0-9]|[a-z0-9][a-z0-9_-]*[a-z0-9])*$": {
             "additionalProperties": false,
             "minProperties": 1,
             "properties": {
-                "args": {
-                    "items": {
-                        "type": "string"
-                    },
-                    "type": "array"
-                },
-                "cap_add": {
-                    "items": {
-                        "type": "string"
-                    },
-                    "type": "array"
-                },
-                "cfs_period_us": {
-                    "exclusiveMinimum": false,
-                    "maximum": "1000000",
-                    "minimum": "1000",
-                    "type": "integer"
-                },
-                "cmd": {
-                    "type": "string"
-                },
                 "cpu_burst_add": {
                     "exclusiveMinimum": false,
                     "minimum": 0.0,
                     "type": "number"
                 },
                 "cpus": {
-                    "default": 1,
                     "exclusiveMinimum": true,
                     "minimum": 0,
                     "type": "number"
                 },
-                "deploy_group": {
-                    "type": "string"
-                },
                 "disk": {
-                    "default": 1024,
-                    "exclusiveMinimum": true,
-                    "minimum": 0,
+                    "exclusiveMinimum": false,
+                    "minimum": 128,
                     "type": "number"
                 },
-                "env": {
-                    "additionalProperties": false,
-                    "patternProperties": {
-                        "^[a-zA-Z_]+[a-zA-Z0-9_]*$": {
-                            "type": "string"
-                        }
-                    },
-                    "type": "object"
-                },
-                "extra_docker_args": {
-                    "additionalProperties": {
-                        "type": "string"
-                    },
-                    "type": "object"
-                },
-                "extra_volumes": {
-                    "items": {
-                        "type": "object"
-                    },
-                    "type": "array",
-                    "uniqueItems": true
-                },
-                "gpus": {
+                "max_instances": {
                     "exclusiveMinimum": false,
                     "minimum": 0,
                     "type": "integer"
                 },
                 "mem": {
-                    "default": 1024,
-                    "exclusiveMinimum": true,
+                    "exclusiveMinimum": false,
                     "minimum": 32,
                     "type": "number"
                 },
-                "net": {
-                    "type": "string"
-                },
-                "pool": {
-                    "type": "string"
+                "min_instances": {
+                    "exclusiveMinimum": false,
+                    "minimum": 0,
+                    "type": "integer"
                 },
-                "role": {
-                    "type": "string"
+                "sidecar_resource_requirements": {
+                    "additionalProperties": false,
+                    "properties": {
+                        "hacheck": {
+                            "properties": {
+                                "limits": {
+                                    "additionalProperties": false,
+                                    "properties": {
+                                        "cpu": {
+                                            "minimum": 0.0,
+                                            "type": "number"
+                                        },
+                                        "ephemeral-storage": {
+                                            "type": "string"
+                                        },
+                                        "memory": {
+                                            "type": "string"
+                                        }
+                                    },
+                                    "type": "object"
+                                },
+                                "requests": {
+                                    "additionalProperties": false,
+                                    "properties": {
+                                        "cpu": {
+                                            "minimum": 0.0,
+                                            "type": "number"
+                                        },
+                                        "ephemeral-storage": {
+                                            "type": "string"
+                                        },
+                                        "memory": {
+                                            "type": "string"
+                                        }
+                                    },
+                                    "type": "object"
+                                }
+                            },
+                            "type": "object"
+                        }
+                    },
+                    "type": "object"
                 }
             },
             "type": "object"
-        },
-        "^_.*$": {
-            "additionalProperties": true,
-            "type": "object"
         }
     },
     "type": "object"
 }
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/schemas/kubernetes_schema.json` & `paasta-tools-1.0.0/paasta_tools/cli/schemas/tron_schema.json`

 * *Files 26% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.4523809523809524%*

 * *Differences: {"'definitions'": "OrderedDict([('name', OrderedDict([('type', 'string'), ('pattern', "*

 * *                  "'^[A-Za-z_][\\\\w\\\\-]{0,254}$')])), ('time_delta', OrderedDict([('type', "*

 * *                  "'string'), ('pattern', '^\\\\d+\\\\s*[a-z]+$')])), ('action', "*

 * *                  "OrderedDict([('type', 'object'), ('additionalProperties', False), ('required', "*

 * *                  "['command']), ('properties', OrderedDict([('name', OrderedDict([('$ref', "*

 * *                  "'#definitions/name')])), ('comma […]*

```diff
@@ -1,529 +1,666 @@
 {
     "$schema": "http://json-schema.org/draft-04/schema#",
-    "additionalProperties": false,
-    "description": "http://paasta.readthedocs.io/en/latest/yelpsoa_configs.html#kubernetes-clustername-yaml",
-    "minProperties": 1,
-    "patternProperties": {
-        "^([a-z0-9]|[a-z0-9][a-z0-9_-]*[a-z0-9])*$": {
+    "definitions": {
+        "action": {
             "additionalProperties": false,
-            "allOf": [
-                {
-                    "oneOf": [
-                        {
-                            "properties": {
-                                "healthcheck_mode": {
-                                    "enum": [
-                                        "tcp",
-                                        "http",
-                                        "https"
-                                    ]
-                                }
-                            }
-                        },
-                        {
-                            "properties": {
-                                "healthcheck_cmd": {
-                                    "type": "string"
-                                },
-                                "healthcheck_mode": {
-                                    "enum": [
-                                        "cmd"
-                                    ]
-                                }
-                            },
-                            "required": [
-                                "healthcheck_cmd"
-                            ]
-                        }
-                    ]
-                },
-                {
-                    "oneOf": [
-                        {
-                            "properties": {
-                                "drain_method": {
-                                    "enum": [
-                                        "noop",
-                                        "hacheck",
-                                        "test"
-                                    ]
-                                }
-                            }
-                        },
-                        {
-                            "properties": {
-                                "drain_method": {
-                                    "enum": [
-                                        "http"
-                                    ]
-                                },
-                                "drain_method_params": {
-                                    "properties": {
-                                        "drain": {
-                                            "type": "object"
-                                        },
-                                        "is_draining": {
-                                            "type": "object"
-                                        },
-                                        "is_safe_to_kill": {
-                                            "type": "object"
-                                        },
-                                        "stop_draining": {
-                                            "type": "object"
-                                        }
-                                    },
-                                    "required": [
-                                        "drain",
-                                        "stop_draining",
-                                        "is_draining",
-                                        "is_safe_to_kill"
-                                    ],
-                                    "type": "object"
-                                }
-                            },
-                            "required": [
-                                "drain_method_params"
-                            ]
-                        }
-                    ]
-                }
-            ],
-            "minProperties": 1,
             "properties": {
-                "args": {
+                "aws_credentials": {
+                    "$comment": "we should eventually get rid of this once we move spark to pod identity - that said, this should be just the filename sans extension",
+                    "pattern": "[-a-zA-z0-9_]",
+                    "type": "string"
+                },
+                "boto_keys": {
                     "items": {
                         "type": "string"
                     },
                     "type": "array"
                 },
-                "autoscaling": {
-                    "type": "object"
-                },
-                "aws_ebs_volumes": {
-                    "items": {
-                        "type": "object"
-                    },
-                    "type": "array",
-                    "uniqueItems": true
-                },
-                "backoff_factor": {
-                    "default": 2,
-                    "type": "integer"
-                },
-                "bounce_health_params": {
-                    "properties": {
-                        "check_haproxy": {
-                            "default": true,
-                            "type": "boolean"
-                        },
-                        "haproxy_min_fraction_up": {
-                            "exclusiveMaximum": false,
-                            "exclusiveMinimum": true,
-                            "maximum": 1.0,
-                            "minimum": 0.0,
-                            "type": "number"
-                        },
-                        "min_task_uptime": {
-                            "type": "number"
-                        }
-                    },
-                    "type": "object"
-                },
-                "bounce_margin_factor": {
-                    "default": 1,
-                    "exclusiveMaximum": false,
-                    "exclusiveMinimum": true,
-                    "maximum": 1,
-                    "minimum": 0,
-                    "type": "number"
-                },
-                "bounce_method": {
-                    "enum": [
-                        "crossover",
-                        "brutal",
-                        "downthenup"
-                    ]
-                },
-                "bounce_priority": {
-                    "type": "integer"
-                },
                 "cap_add": {
                     "items": {
                         "type": "string"
                     },
                     "type": "array"
                 },
-                "cfs_period_us": {
-                    "exclusiveMinimum": false,
-                    "maximum": "1000000",
-                    "minimum": "1000",
-                    "type": "integer"
+                "cluster": {
+                    "type": "string"
                 },
-                "cmd": {
-                    "oneOf": [
-                        {
-                            "type": "string"
-                        },
-                        {
-                            "type": "array"
-                        }
-                    ]
+                "command": {
+                    "type": "string"
                 },
                 "constraints": {
                     "items": {
                         "type": "array"
                     },
                     "type": "array",
                     "uniqueItems": true
                 },
-                "container_port": {
-                    "type": "number"
-                },
                 "cpu_burst_add": {
                     "exclusiveMinimum": false,
                     "minimum": 0.0,
                     "type": "number"
                 },
                 "cpus": {
-                    "default": 0.25,
                     "exclusiveMinimum": true,
                     "minimum": 0,
                     "type": "number"
                 },
-                "dependencies_reference": {
-                    "type": "string"
-                },
-                "deploy_blacklist": {
-                    "type": "array"
+                "crypto_keys": {
+                    "additionalProperties": false,
+                    "properties": {
+                        "decrypt": {
+                            "items": {
+                                "pattern": "^[a-zA-Z0-9_.-]+$",
+                                "type": "string"
+                            },
+                            "type": "array",
+                            "uniqueItems": true
+                        },
+                        "encrypt": {
+                            "items": {
+                                "pattern": "^[a-zA-Z0-9_.-]+$",
+                                "type": "string"
+                            },
+                            "type": "array",
+                            "uniqueItems": true
+                        }
+                    },
+                    "type": "object"
                 },
                 "deploy_group": {
                     "type": "string"
                 },
-                "deploy_whitelist": {
-                    "type": "array"
-                },
                 "disk": {
-                    "default": 1024,
                     "exclusiveMinimum": true,
                     "minimum": 0,
                     "type": "number"
                 },
-                "drain_method": {
-                    "default": "noop",
-                    "enum": [
-                        "noop",
-                        "hacheck",
-                        "http",
-                        "test"
-                    ]
-                },
-                "drain_method_params": {
-                    "type": "object"
-                },
                 "env": {
                     "additionalProperties": false,
                     "patternProperties": {
                         "^[a-zA-Z_]+[a-zA-Z0-9_]*$": {
                             "type": "string"
                         }
                     },
                     "type": "object"
                 },
+                "executor": {
+                    "enum": [
+                        "ssh",
+                        "paasta",
+                        "spark"
+                    ]
+                },
+                "expected_runtime": {
+                    "$ref": "#definitions/time_delta"
+                },
                 "extra_constraints": {
                     "items": {
                         "type": "array"
                     },
                     "type": "array",
                     "uniqueItems": true
                 },
-                "extra_docker_args": {
-                    "additionalProperties": {
-                        "type": "string"
-                    },
-                    "type": "object"
-                },
                 "extra_volumes": {
                     "items": {
                         "type": "object"
                     },
                     "type": "array",
                     "uniqueItems": true
                 },
-                "gpus": {
-                    "exclusiveMinimum": false,
-                    "minimum": 0,
-                    "type": "integer"
+                "force_spark_resource_configs": {
+                    "type": "boolean"
                 },
-                "healthcheck_cmd": {
-                    "default": "/bin/true",
+                "iam_role": {
+                    "$comment": "This should be a valid AWS IAM role ARN, see https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html#reference_iam-quotas-names",
+                    "pattern": "^arn:aws:iam::[0-9]+:role/[a-zA-Z0-9+=,.@_-]+$",
                     "type": "string"
                 },
-                "healthcheck_grace_period_seconds": {
-                    "default": 60,
-                    "type": "number"
-                },
-                "healthcheck_interval_seconds": {
-                    "default": 10,
-                    "type": "number"
-                },
-                "healthcheck_max_consecutive_failures": {
-                    "default": 6,
-                    "type": "integer"
-                },
-                "healthcheck_mode": {
+                "iam_role_provider": {
                     "enum": [
-                        "cmd",
-                        "tcp",
-                        "http",
-                        "https"
+                        "aws"
                     ]
                 },
-                "healthcheck_timeout_seconds": {
-                    "default": 10,
+                "max_runtime": {
+                    "$ref": "#definitions/time_delta"
+                },
+                "mem": {
+                    "exclusiveMinimum": true,
+                    "minimum": 32,
                     "type": "number"
                 },
-                "healthcheck_uri": {
-                    "default": "/status",
-                    "type": "string"
+                "mrjob": {
+                    "type": "boolean"
+                },
+                "name": {
+                    "$ref": "#definitions/name"
                 },
-                "horizontal_autoscaling": {
+                "node": {
+                    "$ref": "#definitions/name"
+                },
+                "node_selectors": {
                     "additionalProperties": false,
                     "patternProperties": {
-                        "(?!(^cpu|memory|http|uwsgi)$)(^[a-z]([-a-z0-9]*[a-z0-9])?$)": {
-                            "additionalProperties": false,
-                            "properties": {
-                                "signalflow_metrics_query": {
+                        "^[a-zA-Z0-9]+[a-zA-Z0-9-_./]*[a-zA-Z0-9]+$": {
+                            "anyOf": [
+                                {
                                     "type": "string"
                                 },
-                                "target_value": {
-                                    "exclusiveMinimum": true,
-                                    "minimum": 0,
-                                    "type": "number"
+                                {
+                                    "items": {
+                                        "type": "string"
+                                    },
+                                    "type": "array",
+                                    "uniqueItems": true
+                                },
+                                {
+                                    "items": {
+                                        "anyOf": [
+                                            {
+                                                "additionalProperties": false,
+                                                "properties": {
+                                                    "operator": {
+                                                        "enum": [
+                                                            "In",
+                                                            "NotIn"
+                                                        ]
+                                                    },
+                                                    "values": {
+                                                        "items": {
+                                                            "type": "string"
+                                                        },
+                                                        "type": "array",
+                                                        "uniqueItems": true
+                                                    }
+                                                },
+                                                "required": [
+                                                    "operator",
+                                                    "values"
+                                                ],
+                                                "type": "object"
+                                            },
+                                            {
+                                                "additionalProperties": false,
+                                                "properties": {
+                                                    "operator": {
+                                                        "enum": [
+                                                            "Exists",
+                                                            "DoesNotExist"
+                                                        ]
+                                                    }
+                                                },
+                                                "required": [
+                                                    "operator"
+                                                ],
+                                                "type": "object"
+                                            },
+                                            {
+                                                "additionalProperties": false,
+                                                "properties": {
+                                                    "operator": {
+                                                        "enum": [
+                                                            "Gt",
+                                                            "Lt"
+                                                        ]
+                                                    },
+                                                    "value": {
+                                                        "type": "integer"
+                                                    }
+                                                },
+                                                "required": [
+                                                    "operator",
+                                                    "value"
+                                                ],
+                                                "type": "object"
+                                            }
+                                        ]
+                                    },
+                                    "type": "array"
                                 }
-                            },
-                            "required": [
-                                "signalflow_metrics_query",
-                                "target_value"
-                            ],
-                            "type": "object"
+                            ]
                         }
                     },
-                    "properties": {
-                        "cpu": {
-                            "additionalProperties": false,
-                            "properties": {
-                                "target_average_value": {
-                                    "default": 70,
-                                    "exclusiveMaximum": false,
-                                    "exclusiveMinimum": true,
-                                    "maximum": 100,
-                                    "minimum": 0,
-                                    "type": "integer"
-                                }
+                    "type": "object"
+                },
+                "on_upstream_rerun": {
+                    "type": "string"
+                },
+                "pool": {
+                    "type": "string"
+                },
+                "projected_sa_volumes": {
+                    "items": {
+                        "properties": {
+                            "audience": {
+                                "type": "string"
+                            },
+                            "container_path": {
+                                "type": "string"
+                            },
+                            "expiration_seconds": {
+                                "type": "integer"
+                            }
+                        },
+                        "required": [
+                            "container_path",
+                            "audience"
+                        ],
+                        "type": "object"
+                    },
+                    "type": "array",
+                    "uniqueItems": true
+                },
+                "requires": {
+                    "items": {
+                        "type": "string"
+                    },
+                    "type": "array"
+                },
+                "retries": {
+                    "exclusiveMinimum": false,
+                    "minimum": 0,
+                    "type": "integer"
+                },
+                "retries_delay": {
+                    "$ref": "#definitions/time_delta"
+                },
+                "secret_volumes": {
+                    "items": {
+                        "properties": {
+                            "container_path": {
+                                "type": "string"
                             },
-                            "required": [
-                                "target_average_value"
-                            ],
-                            "type": "object"
-                        },
-                        "http": {
-                            "additionalProperties": false,
-                            "properties": {
-                                "dimensions": {
-                                    "minProperties": 1,
-                                    "patternProperties": {
-                                        "^(?!(sf_|gcp_|azure_|aws_))[a-zA-Z]([-_a-zA-Z0-9])*$": {
+                            "default_mode": {
+                                "type": "string"
+                            },
+                            "items": {
+                                "items": {
+                                    "properties": {
+                                        "key": {
+                                            "type": "string"
+                                        },
+                                        "mode": {
+                                            "type": "string"
+                                        },
+                                        "path": {
                                             "type": "string"
                                         }
                                     },
+                                    "required": [
+                                        "key",
+                                        "path"
+                                    ],
                                     "type": "object"
                                 },
-                                "target_average_value": {
-                                    "default": 70,
-                                    "exclusiveMaximum": false,
+                                "maxItems": 1,
+                                "type": "array",
+                                "uniqueItems": true
+                            },
+                            "secret_name": {
+                                "type": "string"
+                            }
+                        },
+                        "required": [
+                            "container_path",
+                            "secret_name"
+                        ],
+                        "type": "object"
+                    },
+                    "type": "array",
+                    "uniqueItems": true
+                },
+                "service": {
+                    "type": "string"
+                },
+                "service_account_name": {
+                    "type": "string"
+                },
+                "spark_args": {
+                    "additionalProperties": true,
+                    "properties": {
+                        "spark.app.name": {
+                            "type": "boolean"
+                        },
+                        "spark.cores.max": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
+                            "type": "integer"
+                        },
+                        "spark.default.parallelism": {
+                            "type": "integer"
+                        },
+                        "spark.driver.cores": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
+                            "type": "integer"
+                        },
+                        "spark.driver.maxResultSize": {
+                            "pattern": "^[1-9]+[0-9]*[kmg]$",
+                            "type": "string"
+                        },
+                        "spark.driver.memory": {
+                            "pattern": "^[1-9]+[0-9]*[kmg]$",
+                            "type": "string"
+                        },
+                        "spark.driver.memoryOverhead": {
+                            "$comment": "we still need to validate this in code since there's a spark-enforced minimum of 384mb",
+                            "oneOf": [
+                                {
+                                    "pattern": "^[1-9]+[0-9]*[kmg]$",
+                                    "type": "string"
+                                },
+                                {
                                     "exclusiveMinimum": true,
-                                    "maximum": 100,
                                     "minimum": 0,
                                     "type": "number"
                                 }
-                            },
-                            "required": [
-                                "target_average_value"
-                            ],
-                            "type": "object"
-                        },
-                        "max_replicas": {
+                            ]
+                        },
+                        "spark.executor.cores": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
                             "type": "integer"
                         },
-                        "memory": {
-                            "additionalProperties": false,
-                            "properties": {
-                                "target_average_value": {
-                                    "default": 70,
-                                    "exclusiveMaximum": false,
+                        "spark.executor.instances": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
+                            "type": "integer"
+                        },
+                        "spark.executor.memory": {
+                            "pattern": "^[1-9]+[0-9]*[kmg]$",
+                            "type": "string"
+                        },
+                        "spark.executor.memoryOverhead": {
+                            "$comment": "we still need to validate this in code since there's a spark-enforced minimum of 384mb",
+                            "oneOf": [
+                                {
+                                    "pattern": "^[1-9]+[0-9]*[kmg]$",
+                                    "type": "string"
+                                },
+                                {
                                     "exclusiveMinimum": true,
-                                    "maximum": 100,
                                     "minimum": 0,
-                                    "type": "integer"
+                                    "type": "number"
                                 }
-                            },
-                            "required": [
-                                "target_average_value"
-                            ],
-                            "type": "object"
-                        },
-                        "min_replicas": {
-                            "default": 1,
-                            "type": "integer"
-                        },
-                        "uwsgi": {
-                            "additionalProperties": false,
-                            "properties": {
-                                "dimensions": {
-                                    "minProperties": 1,
-                                    "patternProperties": {
-                                        "^(?!(sf_|gcp_|azure_|aws_))[a-zA-Z]([-_a-zA-Z0-9])*$": {
-                                            "type": "string"
-                                        }
-                                    },
-                                    "type": "object"
+                            ]
+                        },
+                        "spark.hadoop.fs.s3a.multiobjectdelete.enable": {
+                            "type": "boolean"
+                        },
+                        "spark.kubernetes.allocation.batch.size": {
+                            "type": "integer"
+                        },
+                        "spark.kubernetes.driver.request.cores": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
+                            "type": "number"
+                        },
+                        "spark.kubernetes.executor.request.cores": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
+                            "type": "number"
+                        },
+                        "spark.kubernetes.memoryOverheadFactor": {
+                            "exclusiveMaximum": false,
+                            "exclusiveMinimum": false,
+                            "maximum": 1,
+                            "minimum": 0,
+                            "type": "number"
+                        },
+                        "spark.local.dir": {
+                            "type": "string"
+                        },
+                        "spark.scheduler.maxRegisteredResourcesWaitingTime": {
+                            "pattern": "^[1-9]+[0-9]*(s|min)$",
+                            "type": "string"
+                        },
+                        "spark.scheduler.minRegisteredResourcesRatio": {
+                            "exclusiveMinimum": true,
+                            "maximum": 1,
+                            "minimum": 0,
+                            "type": "number"
+                        },
+                        "spark.sql.autoBroadcastJoinThreshold": {
+                            "oneOf": [
+                                {
+                                    "$comment": "this is just a silly way to express that we want a non-zero integer or -1",
+                                    "enum": [
+                                        -1
+                                    ]
                                 },
-                                "target_average_value": {
-                                    "default": 70,
-                                    "exclusiveMaximum": false,
+                                {
                                     "exclusiveMinimum": true,
-                                    "maximum": 100,
                                     "minimum": 0,
                                     "type": "integer"
                                 }
-                            },
-                            "required": [
-                                "target_average_value"
-                            ],
-                            "type": "object"
+                            ]
+                        },
+                        "spark.sql.broadcastTimeout": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
+                            "type": "integer"
+                        },
+                        "spark.sql.parquet.enableVectorizedReader": {
+                            "type": "boolean"
+                        },
+                        "spark.sql.shuffle.partitions": {
+                            "exclusiveMinimum": true,
+                            "minimum": 0,
+                            "type": "integer"
+                        },
+                        "spark.stage.maxConsecutiveAttempts": {
+                            "type": "integer"
+                        },
+                        "spark.task.maxFailures": {
+                            "minimum": 1,
+                            "type": "integer"
                         }
                     },
-                    "required": [
-                        "max_replicas"
-                    ],
                     "type": "object"
                 },
-                "host_port": {
-                    "default": 0,
-                    "exclusiveMinimum": false,
-                    "maximum": 65535,
-                    "minimum": 0,
-                    "type": "integer"
+                "trigger_downstreams": {
+                    "additionalProperties": {
+                        "type": "string"
+                    },
+                    "type": [
+                        "object",
+                        "boolean"
+                    ]
                 },
-                "iam_role": {
-                    "type": "string"
+                "trigger_timeout": {
+                    "$ref": "#definitions/time_delta"
                 },
-                "instances": {
-                    "exclusiveMinimum": false,
-                    "minimum": 0,
-                    "type": "integer"
+                "triggered_by": {
+                    "items": {
+                        "type": "string"
+                    },
+                    "type": "array",
+                    "uniqueItems": true
+                }
+            },
+            "required": [
+                "command"
+            ],
+            "type": "object"
+        },
+        "job": {
+            "additionalProperties": false,
+            "properties": {
+                "actions": {
+                    "items": {
+                        "$ref": "#definitions/action"
+                    },
+                    "patternProperties": {
+                        ".+": {
+                            "$ref": "#definitions/action"
+                        }
+                    },
+                    "type": [
+                        "array",
+                        "object"
+                    ]
                 },
-                "marathon_shard": {
-                    "minimum": 0,
-                    "type": "integer"
+                "all_nodes": {
+                    "type": "boolean"
                 },
-                "max_instances": {
-                    "exclusiveMinimum": false,
-                    "minimum": 0,
-                    "type": "integer"
+                "allow_overlap": {
+                    "type": "boolean"
                 },
-                "max_launch_delay_seconds": {
-                    "default": 300,
-                    "type": "integer"
+                "cleanup_action": {
+                    "$ref": "#definitions/action"
                 },
-                "mem": {
-                    "default": 1024,
-                    "exclusiveMinimum": true,
-                    "minimum": 32,
-                    "type": "number"
+                "cluster": {
+                    "type": "string"
                 },
-                "min_instances": {
-                    "exclusiveMinimum": false,
-                    "minimum": 0,
-                    "type": "integer"
+                "deploy_group": {
+                    "type": "string"
+                },
+                "enabled": {
+                    "type": "boolean"
+                },
+                "expected_runtime": {
+                    "$ref": "#definitions/time_delta"
+                },
+                "max_runtime": {
+                    "$ref": "#definitions/time_delta"
                 },
                 "monitoring": {
-                    "additionalProperties": true,
+                    "additionalProperties": false,
                     "properties": {
+                        "alert_after": {
+                            "type": "string"
+                        },
+                        "check_oom_events": {
+                            "type": "boolean"
+                        },
+                        "check_that_every_day_has_a_successful_run": {
+                            "type": "boolean"
+                        },
+                        "component": {
+                            "type": [
+                                "string",
+                                "array"
+                            ]
+                        },
+                        "dependencies": {
+                            "items": {
+                                "type": "string"
+                            },
+                            "type": "array"
+                        },
+                        "description": {
+                            "type": "string"
+                        },
+                        "irc_channels": {
+                            "items": {
+                                "type": "string"
+                            },
+                            "type": "array"
+                        },
+                        "issuetype": {
+                            "type": "string"
+                        },
+                        "notification_email": {
+                            "type": [
+                                "string",
+                                "boolean",
+                                "null"
+                            ]
+                        },
                         "page": {
                             "type": "boolean"
                         },
-                        "team": {
+                        "page_for_expected_runtime": {
+                            "type": "boolean"
+                        },
+                        "priority": {
                             "type": "string"
-                        }
-                    },
-                    "type": "object"
-                },
-                "monitoring_blacklist": {
-                    "type": "array"
-                },
-                "net": {
-                    "type": "string"
-                },
-                "persistent_volumes": {
-                    "items": {
-                        "properties": {
-                            "container_path": {
+                        },
+                        "project": {
+                            "type": "string"
+                        },
+                        "realert_every": {
+                            "exclusiveMinimum": false,
+                            "minimum": -1,
+                            "type": "integer"
+                        },
+                        "runbook": {
+                            "type": "string"
+                        },
+                        "slack_channels": {
+                            "items": {
                                 "type": "string"
                             },
-                            "mode": {
+                            "type": "array"
+                        },
+                        "tags": {
+                            "items": {
                                 "type": "string"
                             },
-                            "size": {
-                                "type": "integer"
-                            }
+                            "type": "array"
                         },
-                        "type": "object"
+                        "team": {
+                            "type": "string"
+                        },
+                        "ticket": {
+                            "type": "boolean"
+                        },
+                        "tip": {
+                            "type": "string"
+                        }
                     },
-                    "type": "array",
-                    "uniqueItems": true
+                    "type": "object"
                 },
-                "pool": {
-                    "type": "string"
+                "name": {
+                    "$ref": "#definitions/name"
                 },
-                "previous_marathon_shards": {
-                    "type": "array"
+                "node": {
+                    "$ref": "#definitions/name"
                 },
-                "registrations": {
-                    "items": {
-                        "type": "string"
-                    },
-                    "type": "array",
-                    "uniqueItems": true
+                "queueing": {
+                    "type": "boolean"
                 },
-                "replication_threshold": {
+                "run_limit": {
+                    "exclusiveMinimum": true,
                     "minimum": 0,
                     "type": "integer"
                 },
-                "security": {
-                    "properties": {
-                        "outbound_firewall": {
-                            "enum": [
-                                "block",
-                                "monitor"
-                            ]
-                        }
-                    },
-                    "type": "object"
+                "schedule": {
+                    "type": [
+                        "string",
+                        "object"
+                    ]
                 },
-                "service_account_name": {
+                "service": {
                     "type": "string"
                 },
-                "sfn_autoscaling": {
-                    "type": "object"
+                "time_zone": {
+                    "type": "string"
                 }
             },
+            "required": [
+                "schedule",
+                "actions"
+            ],
             "type": "object"
         },
+        "name": {
+            "pattern": "^[A-Za-z_][\\w\\-]{0,254}$",
+            "type": "string"
+        },
+        "time_delta": {
+            "pattern": "^\\d+\\s*[a-z]+$",
+            "type": "string"
+        }
+    },
+    "description": "tron on paasta yaml (docs todo)",
+    "patternProperties": {
+        "^[^_].*$": {
+            "$ref": "#definitions/job"
+        },
         "^_.*$": {
             "additionalProperties": true,
             "type": "object"
         }
     },
     "type": "object"
 }
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/schemas/tron_schema.json` & `paasta-tools-1.0.0/paasta_tools/cli/schemas/adhoc_schema.json`

 * *Files 25% similar despite different names*

#### Pretty-printed

 * *Similarity: 0.4523809523809524%*

 * *Differences: {"'additionalProperties'": 'False',*

 * * "'description'": "'http://paasta.readthedocs.io/en/latest/yelpsoa_configs.html#adhoc-clustername-yaml'",*

 * * "'minProperties'": '1',*

 * * "'patternProperties'": "{'^([a-z0-9]|[a-z0-9][a-z0-9_-]*[a-z0-9])*$': OrderedDict([('type', "*

 * *                        "'object'), ('additionalProperties', False), ('minProperties', 1), "*

 * *                        "('properties', OrderedDict([('cpus', OrderedDict([('type', 'number'), "*

 * *                        "('minimum', 0), ('exclusiveMinimum' […]*

```diff
@@ -1,308 +1,184 @@
 {
     "$schema": "http://json-schema.org/draft-04/schema#",
-    "definitions": {
-        "action": {
+    "additionalProperties": false,
+    "description": "http://paasta.readthedocs.io/en/latest/yelpsoa_configs.html#adhoc-clustername-yaml",
+    "minProperties": 1,
+    "patternProperties": {
+        "^([a-z0-9]|[a-z0-9][a-z0-9_-]*[a-z0-9])*$": {
             "additionalProperties": false,
+            "minProperties": 1,
             "properties": {
-                "cap_add": {
+                "args": {
                     "items": {
                         "type": "string"
                     },
                     "type": "array"
                 },
-                "cluster": {
-                    "type": "string"
-                },
-                "command": {
-                    "type": "string"
+                "boto_keys": {
+                    "items": {
+                        "type": "string"
+                    },
+                    "type": "array"
                 },
-                "constraints": {
+                "cap_add": {
                     "items": {
-                        "type": "array"
+                        "type": "string"
                     },
-                    "type": "array",
-                    "uniqueItems": true
+                    "type": "array"
+                },
+                "cfs_period_us": {
+                    "exclusiveMinimum": false,
+                    "maximum": 1000000,
+                    "minimum": 1000,
+                    "type": "integer"
+                },
+                "cmd": {
+                    "type": "string"
                 },
                 "cpu_burst_add": {
                     "exclusiveMinimum": false,
                     "minimum": 0.0,
                     "type": "number"
                 },
                 "cpus": {
+                    "default": 1,
                     "exclusiveMinimum": true,
                     "minimum": 0,
                     "type": "number"
                 },
+                "crypto_keys": {
+                    "additionalProperties": false,
+                    "properties": {
+                        "decrypt": {
+                            "items": {
+                                "pattern": "^[a-zA-Z0-9_.-]+$",
+                                "type": "string"
+                            },
+                            "type": "array",
+                            "uniqueItems": true
+                        },
+                        "encrypt": {
+                            "items": {
+                                "pattern": "^[a-zA-Z0-9_.-]+$",
+                                "type": "string"
+                            },
+                            "type": "array",
+                            "uniqueItems": true
+                        }
+                    },
+                    "type": "object"
+                },
                 "deploy_group": {
                     "type": "string"
                 },
                 "disk": {
+                    "default": 1024,
                     "exclusiveMinimum": true,
                     "minimum": 0,
                     "type": "number"
                 },
                 "env": {
                     "additionalProperties": false,
                     "patternProperties": {
                         "^[a-zA-Z_]+[a-zA-Z0-9_]*$": {
                             "type": "string"
                         }
                     },
                     "type": "object"
                 },
-                "executor": {
-                    "enum": [
-                        "ssh",
-                        "paasta"
-                    ]
-                },
-                "expected_runtime": {
-                    "$ref": "#definitions/time_delta"
-                },
-                "extra_constraints": {
-                    "items": {
-                        "type": "array"
+                "extra_docker_args": {
+                    "additionalProperties": {
+                        "type": "string"
                     },
-                    "type": "array",
-                    "uniqueItems": true
+                    "type": "object"
                 },
                 "extra_volumes": {
                     "items": {
                         "type": "object"
                     },
                     "type": "array",
                     "uniqueItems": true
                 },
-                "mem": {
-                    "exclusiveMinimum": true,
-                    "minimum": 32,
-                    "type": "number"
-                },
-                "name": {
-                    "$ref": "#definitions/name"
-                },
-                "node": {
-                    "$ref": "#definitions/name"
-                },
-                "on_upstream_rerun": {
-                    "type": "string"
-                },
-                "pool": {
-                    "type": "string"
-                },
-                "requires": {
-                    "items": {
-                        "type": "string"
-                    },
-                    "type": "array"
-                },
-                "retries": {
+                "gpus": {
                     "exclusiveMinimum": false,
                     "minimum": 0,
                     "type": "integer"
                 },
-                "retries_delay": {
-                    "$ref": "#definitions/time_delta"
-                },
-                "service": {
+                "iam_role": {
                     "type": "string"
                 },
-                "trigger_downstreams": {
-                    "additionalProperties": {
-                        "type": "string"
-                    },
-                    "type": [
-                        "object",
-                        "boolean"
-                    ]
-                },
-                "trigger_timeout": {
-                    "$ref": "#definitions/time_delta"
-                },
-                "triggered_by": {
-                    "items": {
-                        "type": "string"
-                    },
-                    "type": "array",
-                    "uniqueItems": true
-                }
-            },
-            "required": [
-                "command"
-            ],
-            "type": "object"
-        },
-        "job": {
-            "additionalProperties": false,
-            "properties": {
-                "actions": {
-                    "items": {
-                        "$ref": "#definitions/action"
-                    },
-                    "patternProperties": {
-                        ".+": {
-                            "$ref": "#definitions/action"
-                        }
-                    },
-                    "type": [
-                        "array",
-                        "object"
+                "iam_role_provider": {
+                    "enum": [
+                        "aws"
                     ]
                 },
-                "all_nodes": {
-                    "type": "boolean"
-                },
-                "allow_overlap": {
-                    "type": "boolean"
-                },
-                "cleanup_action": {
-                    "$ref": "#definitions/action"
+                "mem": {
+                    "default": 1024,
+                    "exclusiveMinimum": true,
+                    "minimum": 32,
+                    "type": "number"
                 },
-                "cluster": {
+                "net": {
                     "type": "string"
                 },
-                "deploy_group": {
+                "pool": {
                     "type": "string"
                 },
-                "enabled": {
-                    "type": "boolean"
-                },
-                "expected_runtime": {
-                    "$ref": "#definitions/time_delta"
-                },
-                "max_runtime": {
-                    "$ref": "#definitions/time_delta"
+                "role": {
+                    "type": "string"
                 },
-                "monitoring": {
-                    "additionalProperties": false,
-                    "properties": {
-                        "alert_after": {
-                            "type": "string"
-                        },
-                        "check_that_every_day_has_a_successful_run": {
-                            "type": "boolean"
-                        },
-                        "component": {
-                            "type": [
-                                "string",
-                                "array"
-                            ]
-                        },
-                        "dependencies": {
-                            "items": {
+                "secret_volumes": {
+                    "items": {
+                        "properties": {
+                            "container_path": {
                                 "type": "string"
                             },
-                            "type": "array"
-                        },
-                        "description": {
-                            "type": "string"
-                        },
-                        "irc_channels": {
-                            "items": {
+                            "default_mode": {
                                 "type": "string"
                             },
-                            "type": "array"
-                        },
-                        "notification_email": {
-                            "type": [
-                                "string",
-                                "boolean",
-                                "null"
-                            ]
-                        },
-                        "page": {
-                            "type": "boolean"
-                        },
-                        "priority": {
-                            "type": "string"
-                        },
-                        "project": {
-                            "type": "string"
-                        },
-                        "realert_every": {
-                            "exclusiveMinimum": false,
-                            "minimum": -1,
-                            "type": "integer"
-                        },
-                        "runbook": {
-                            "type": "string"
-                        },
-                        "slack_channels": {
                             "items": {
-                                "type": "string"
+                                "items": {
+                                    "properties": {
+                                        "key": {
+                                            "type": "string"
+                                        },
+                                        "mode": {
+                                            "type": "string"
+                                        },
+                                        "path": {
+                                            "type": "string"
+                                        }
+                                    },
+                                    "required": [
+                                        "key",
+                                        "path"
+                                    ],
+                                    "type": "object"
+                                },
+                                "maxItems": 1,
+                                "type": "array",
+                                "uniqueItems": true
                             },
-                            "type": "array"
-                        },
-                        "tags": {
-                            "items": {
+                            "secret_name": {
                                 "type": "string"
-                            },
-                            "type": "array"
+                            }
                         },
-                        "team": {
-                            "type": "string"
-                        },
-                        "ticket": {
-                            "type": "boolean"
-                        },
-                        "tip": {
-                            "type": "string"
-                        }
+                        "required": [
+                            "container_path",
+                            "secret_name"
+                        ],
+                        "type": "object"
                     },
-                    "type": "object"
-                },
-                "name": {
-                    "$ref": "#definitions/name"
-                },
-                "node": {
-                    "$ref": "#definitions/name"
-                },
-                "queueing": {
-                    "type": "boolean"
-                },
-                "run_limit": {
-                    "exclusiveMinimum": true,
-                    "minimum": 0,
-                    "type": "integer"
-                },
-                "schedule": {
-                    "type": [
-                        "string",
-                        "object"
-                    ]
-                },
-                "service": {
-                    "type": "string"
-                },
-                "time_zone": {
-                    "type": "string"
+                    "type": "array",
+                    "uniqueItems": true
                 }
             },
-            "required": [
-                "node",
-                "schedule",
-                "actions"
-            ],
             "type": "object"
         },
-        "name": {
-            "pattern": "^[A-Za-z_][\\w\\-]{0,254}$",
-            "type": "string"
-        },
-        "time_delta": {
-            "pattern": "^\\d+\\s*[a-z]+$",
-            "type": "string"
-        }
-    },
-    "description": "tron on paasta yaml (docs todo)",
-    "patternProperties": {
-        "^[^_].*$": {
-            "$ref": "#definitions/job"
-        },
         "^_.*$": {
             "additionalProperties": true,
             "type": "object"
         }
     },
     "type": "object"
 }
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/fsm/autosuggest.py` & `paasta-tools-1.0.0/paasta_tools/cli/fsm/autosuggest.py`

 * *Files 0% similar despite different names*

```diff
@@ -53,17 +53,17 @@
             ports.add(port)
         except Exception:
             pass
     return ports
 
 
 def suggest_smartstack_proxy_port(
-    yelpsoa_config_root, range_min=20000, range_max=21000
+    yelpsoa_config_root, range_min=19000, range_max=21000
 ):
-    """Pick a random available port in the 20000-21000 block"""
+    """Pick a random available port in the 19000-21000 block"""
     available_proxy_ports = set(range(range_min, range_max + 1))
     for root, dirs, files in os.walk(yelpsoa_config_root):
         for f in files:
             if f.endswith("smartstack.yaml"):
                 try:
                     used_ports = _get_smartstack_proxy_ports_from_file(root, f)
                     for used_port in used_ports:
```

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/fsm/__init__.py` & `paasta-tools-1.0.0/paasta_tools/monitoring/__init__.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/monitoring.yaml` & `paasta-tools-1.0.0/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/monitoring.yaml`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/marathon-PROD.yaml` & `paasta-tools-1.0.0/paasta_tools/cli/fsm/template/{{cookiecutter.service}}/kubernetes-PROD.yaml`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 ---
-# Marathon is the PaaSTA component that takes a command and keeps copies of it running.
+# Kubernetes is the PaaSTA component that takes a command and keeps copies of it running.
 # You can see all the options for this file here:
-# http://paasta.readthedocs.org/en/latest/yelpsoa_configs.html#marathon-clustername-yaml
+# http://paasta.readthedocs.org/en/latest/yelpsoa_configs.html#kubernetes-clustername-yaml
 
 # In PaaSTA you can have different "instances" of the service that can run in different
 # modes. Out of Yelp convention, the "main" instance is the primary one and serves main
 # web traffic.
 main:
   # In Prod, it is important to have enough capacity to serve our users. Having 3 copies
   # of the service is a good start. Longer term the service might need more. Follow up
@@ -20,15 +20,15 @@
   # if they are available. `paasta status` can give a snapshot of how much
   # cpu a service is using or the signalfx graphs can give you a view of the cpu
   # over time. Be honest about your cpu needs to keep the scheduler informed
   # and neighbors happy.
   cpus: .1
   #
   # Mem is hard limit and at Yelp we don't allow swap. The service will OOM and die
-  # if it goes over the limit. Luckily Marathon will restart the service ASAP if that
+  # if it goes over the limit. Luckily Kubernetes will restart the service ASAP if that
   # happens. If it happens too much you will lose replication and get an alert. Again
   # use the Signalfx graphs to keep an eye on it and give yourself headroom.
   mem: 1000
 
   # All the monitoring options in monitoring.yaml can be set on a per-instance basis too.
   # You can even change the team responsible! Uncomment as necessary.
   #monitoring:
```

### Comparing `paasta-tools-0.92.1/paasta_tools/deployment_utils.py` & `paasta-tools-1.0.0/paasta_tools/deployment_utils.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,20 +8,37 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+from typing import Optional
+
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import DeploymentVersion
 from paasta_tools.utils import load_v2_deployments_json
 from paasta_tools.utils import NoDeploymentsAvailable
 
 
 def get_currently_deployed_sha(service, deploy_group, soa_dir=DEFAULT_SOA_DIR):
     """Tries to determine the currently deployed sha for a service and deploy_group,
     returns None if there isn't one ready yet"""
     try:
         deployments = load_v2_deployments_json(service=service, soa_dir=soa_dir)
         return deployments.get_git_sha_for_deploy_group(deploy_group=deploy_group)
     except NoDeploymentsAvailable:
         return None
+
+
+def get_currently_deployed_version(
+    service, deploy_group, soa_dir=DEFAULT_SOA_DIR
+) -> Optional[DeploymentVersion]:
+    """Tries to determine the currently deployed version for a service and deploy_group,
+    returns None if there isn't one ready yet"""
+    try:
+        deployments = load_v2_deployments_json(service=service, soa_dir=soa_dir)
+        return deployments.get_deployment_version_for_deploy_group(
+            deploy_group=deploy_group
+        )
+    except NoDeploymentsAvailable:
+        return None
```

### Comparing `paasta-tools-0.92.1/paasta_tools/generate_deployments_for_service.py` & `paasta-tools-1.0.0/paasta_tools/generate_deployments_for_service.py`

 * *Files 16% similar despite different names*

```diff
@@ -21,65 +21,74 @@
 is assumed to be paasta-{cluster}-{instance}, where cluster is the cluster
 the configuration is for and instance is the instance name.
 
 For example, if the service paasta_test has an instance called main with no
 deploy group in its configuration in the hab cluster, then this script
 will create a key/value pair of 'paasta_test:paasta-hab.main': 'services-paasta_test:paasta-SHA',
 where SHA is the current SHA at the tip of the branch named hab in
-git@git.yelpcorp.com:services/paasta_test.git. If main had a deploy_group key with
+git@github.yelpcorp.com:services/paasta_test.git. If main had a deploy_group key with
 a value of 'master', the key would be paasta_test:master instead, and the SHA
 would be the SHA at the tip of master.
 
 This is done for all services in the SOA configuration directory, across any
-service configuration files (filename is 'marathon-\*.yaml').
+service configuration files (filename is 'kubernetes-\*.yaml').
 
 Command line options:
 
 - -d <SOA_DIR>, --soa-dir <SOA_DIR>: Specify a SOA config dir to read from
 - -v, --verbose: Verbose output
 """
 import argparse
+import concurrent.futures
 import json
 import logging
 import os
 import re
 from typing import Any
 from typing import Dict
+from typing import List
+from typing import Optional
 from typing import Tuple
 
 from mypy_extensions import TypedDict
 
 from paasta_tools import remote_git
 from paasta_tools.cli.utils import get_instance_configs_for_service
 from paasta_tools.utils import atomic_file_write
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import get_git_url
+from paasta_tools.utils import get_latest_deployment_tag
 
 log = logging.getLogger(__name__)
 TARGET_FILE = "deployments.json"
 
 
 V1_Mapping = TypedDict(
     "V1_Mapping", {"docker_image": str, "desired_state": str, "force_bounce": str}
 )
-V2_Deployment = TypedDict("V2_Deployment", {"docker_image": str, "git_sha": str})
+V2_Deployment = TypedDict(
+    "V2_Deployment",
+    {"docker_image": str, "git_sha": str, "image_version": Optional[str]},
+)
 V2_Control = TypedDict("V2_Control", {"desired_state": str, "force_bounce": str})
 V2_Mappings = TypedDict(
     "V2_Mappings",
     {"deployments": Dict[str, V2_Deployment], "controls": Dict[str, V2_Control]},
 )
 
 
 DeploymentsDict = TypedDict(
     "DeploymentsDict", {"v1": Dict[str, V1_Mapping], "v2": V2_Mappings}
 )
 
 
 def parse_args() -> argparse.Namespace:
-    parser = argparse.ArgumentParser(description="Creates marathon jobs.")
+    parser = argparse.ArgumentParser(
+        description="Creates deployments.json for paasta services."
+    )
     parser.add_argument(
         "-d",
         "--soa-dir",
         dest="soa_dir",
         metavar="SOA_DIR",
         default=DEFAULT_SOA_DIR,
         help="define a different soa config directory",
@@ -93,46 +102,20 @@
         required=True,
         help="Service name to make the deployments.json for",
     )
     args = parser.parse_args()
     return args
 
 
-def get_latest_deployment_tag(
-    refs: Dict[str, str], deploy_group: str
-) -> Tuple[str, str]:
-    """Gets the latest deployment tag and sha for the specified deploy_group
-
-    :param refs: A dictionary mapping git refs to shas
-    :param deploy_group: The deployment group to return a deploy tag for
-
-    :returns: A tuple of the form (ref, sha) where ref is the actual deployment
-              tag (with the most recent timestamp)  and sha is the sha it points at
-    """
-    most_recent_dtime = None
-    most_recent_ref = None
-    most_recent_sha = None
-    pattern = re.compile(r"^refs/tags/paasta-%s-(\d{8}T\d{6})-deploy$" % deploy_group)
-
-    for ref_name, sha in refs.items():
-        match = pattern.match(ref_name)
-        if match:
-            dtime = match.groups()[0]
-            if most_recent_dtime is None or dtime > most_recent_dtime:
-                most_recent_dtime = dtime
-                most_recent_ref = ref_name
-                most_recent_sha = sha
-    return most_recent_ref, most_recent_sha
-
-
 def get_deploy_group_mappings(
     soa_dir: str, service: str
 ) -> Tuple[Dict[str, V1_Mapping], V2_Mappings]:
-    """Gets mappings from service:deploy_group to services-service:paasta-hash,
-    where hash is the current SHA at the HEAD of branch_name.
+    """Gets mappings from service:deploy_group to services-service:paasta-hash-image_version,
+    where hash is the current SHA at the HEAD of branch_name and image_version
+    can be used to provide additional version information for the Docker image.
     This is done for all services in soa_dir.
 
     :param soa_dir: The SOA configuration directory to read from
     :returns: A dictionary mapping service:deploy_group to a dictionary
       containing:
 
     - 'docker_image': something like "services-service:paasta-hash". This is
@@ -141,103 +124,101 @@
       should be running.
     - 'force_bounce': An arbitrary value, which may be None. A change in this
       value should trigger a bounce, even if the other properties of this app
       have not changed.
     """
     mappings: Dict[str, V1_Mapping] = {}
     v2_mappings: V2_Mappings = {"deployments": {}, "controls": {}}
+    git_url = get_git_url(service=service, soa_dir=soa_dir)
+
+    # Most of the time of this function is in two parts:
+    # 1. getting remote refs from git. (Mostly IO, just waiting for git to get back to us.)
+    # 2. loading instance configs. (Mostly CPU, copy.deepcopying yaml over and over again)
+    # Let's do these two things in parallel.
+
+    executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
+    remote_refs_future = executor.submit(remote_git.list_remote_refs, git_url)
 
     service_configs = get_instance_configs_for_service(soa_dir=soa_dir, service=service)
 
     deploy_group_branch_mappings = {
         config.get_branch(): config.get_deploy_group() for config in service_configs
     }
     if not deploy_group_branch_mappings:
         log.info("Service %s has no valid deploy groups. Skipping.", service)
         return mappings, v2_mappings
 
-    git_url = get_git_url(service=service, soa_dir=soa_dir)
-    remote_refs = remote_git.list_remote_refs(git_url)
+    remote_refs = remote_refs_future.result()
+
+    tag_by_deploy_group = {
+        dg: get_latest_deployment_tag(remote_refs, dg)
+        for dg in set(deploy_group_branch_mappings.values())
+    }
+    state_by_branch_and_sha = get_desired_state_by_branch_and_sha(remote_refs)
 
     for control_branch, deploy_group in deploy_group_branch_mappings.items():
-        (deploy_ref_name, _) = get_latest_deployment_tag(remote_refs, deploy_group)
+        (deploy_ref_name, deploy_ref_sha, image_version) = tag_by_deploy_group[
+            deploy_group
+        ]
         if deploy_ref_name in remote_refs:
             commit_sha = remote_refs[deploy_ref_name]
             control_branch_alias = f"{service}:paasta-{control_branch}"
             control_branch_alias_v2 = f"{service}:{control_branch}"
-            docker_image = build_docker_image_name(service, commit_sha)
-            desired_state, force_bounce = get_desired_state(
-                branch=control_branch,
-                remote_refs=remote_refs,
-                deploy_group=deploy_group,
+            docker_image = build_docker_image_name(service, commit_sha, image_version)
+            desired_state, force_bounce = state_by_branch_and_sha.get(
+                (control_branch, deploy_ref_sha), ("start", None)
             )
             log.info("Mapping %s to docker image %s", control_branch, docker_image)
 
             v2_mappings["deployments"][deploy_group] = {
                 "docker_image": docker_image,
                 "git_sha": commit_sha,
+                "image_version": image_version,
             }
             mappings[control_branch_alias] = {
                 "docker_image": docker_image,
                 "desired_state": desired_state,
                 "force_bounce": force_bounce,
             }
             v2_mappings["controls"][control_branch_alias_v2] = {
                 "desired_state": desired_state,
                 "force_bounce": force_bounce,
             }
     return mappings, v2_mappings
 
 
-def build_docker_image_name(service: str, sha: str) -> str:
-    return f"services-{service}:paasta-{sha}"
-
+def build_docker_image_name(
+    service: str, sha: str, image_version: Optional[str] = None
+) -> str:
+    image_name = f"services-{service}:paasta-{sha}"
+    if image_version is not None:
+        image_name += f"-{image_version}"
+
+    return image_name
+
+
+def get_desired_state_by_branch_and_sha(
+    remote_refs: Dict[str, str]
+) -> Dict[Tuple[str, str], Tuple[str, Any]]:
+    tag_pattern = r"^refs/tags/(?:paasta-){0,2}(?P<branch>[a-zA-Z0-9-_.]+)-(?P<force_bounce>[^-]+)-(?P<state>(start|stop))$"
 
-def get_service_from_docker_image(image_name: str) -> str:
-    """Does the opposite of build_docker_image_name and retrieves the
-    name of a service our of a provided docker image
-
-    An image name has the full path, including the registry. Like:
-    docker-paasta.yelpcorp.com:443/services-example_service:paasta-591ae8a7b3224e3b3322370b858377dd6ef335b6
-    """
-    matches = re.search(".*/services-(.*?):paasta-.*?", image_name)
-    return matches.group(1)
-
-
-def get_desired_state(
-    branch: str, remote_refs: Dict[str, str], deploy_group: str
-) -> Tuple[str, Any]:
-    """Gets the desired state (start or stop) from the given repo, as well as
-    an arbitrary value (which may be None) that will change when a restart is
-    desired.
-    """
-    # (?:paasta-){1,2} supports a previous mistake where some tags would be called
-    # paasta-paasta-cluster.instance
-    tag_pattern = (
-        r"^refs/tags/(?:paasta-){0,2}%s-(?P<force_bounce>[^-]+)-(?P<state>(start|stop))$"
-        % branch
-    )
-
-    states = []
-    (_, head_sha) = get_latest_deployment_tag(remote_refs, deploy_group)
+    states_by_branch_and_sha: Dict[Tuple[str, str], List[Tuple[str, Any]]] = {}
 
     for ref_name, sha in remote_refs.items():
-        if sha == head_sha:
-            match = re.match(tag_pattern, ref_name)
-            if match:
-                gd = match.groupdict()
-                states.append((gd["state"], gd["force_bounce"]))
-
-    if states:
-        # there may be more than one that matches, so take the one that sorts
-        # last by the force_bounce key.
-        sorted_states = sorted(states, key=lambda x: x[1])
-        return sorted_states[-1]
-    else:
-        return ("start", None)
+        match = re.match(tag_pattern, ref_name)
+        if match:
+            gd = match.groupdict()
+            states_by_branch_and_sha.setdefault((gd["branch"], sha), []).append(
+                (gd["state"], gd["force_bounce"])
+            )
+
+    return {
+        (branch, sha): sorted(states, key=lambda x: x[1])[-1]
+        for ((branch, sha), states) in states_by_branch_and_sha.items()
+    }
 
 
 def get_deployments_dict_from_deploy_group_mappings(
     deploy_group_mappings: Dict[str, V1_Mapping], v2_deploy_group_mappings: V2_Mappings
 ) -> DeploymentsDict:
     return {"v1": deploy_group_mappings, "v2": v2_deploy_group_mappings}
```

### Comparing `paasta-tools-0.92.1/paasta_tools/secret_providers/vault.py` & `paasta-tools-1.0.0/paasta_tools/secret_providers/vault.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,44 +1,45 @@
 import getpass
+import logging
 import os
 from typing import Any
 from typing import Dict
 from typing import List
 from typing import Mapping
 from typing import Optional
 
+from paasta_tools.secret_providers import CryptoKey
+
 try:
     from vault_tools.client.jsonsecret import get_plaintext
     from vault_tools.paasta_secret import get_vault_client
     from vault_tools.gpg import TempGpgKeyring
     from vault_tools.paasta_secret import encrypt_secret
-    from vault_tools.cert_tools import do_renew
     import hvac
 except ImportError:
 
     def get_plaintext(*args: Any, **kwargs: Any) -> bytes:
         return b"No plain text available without vault_tools"
 
     def get_vault_client(*args: Any, **kwargs: Any) -> None:
         return None
 
     TempGpgKeyring = None
 
     def encrypt_secret(*args: Any, **kwargs: Any) -> None:
         return None
 
-    def do_renew(*args: Any, **kwargs: Any) -> None:
-        return None
-
 
 from paasta_tools.secret_providers import BaseSecretProvider
-from paasta_tools.utils import paasta_print
 from paasta_tools.secret_tools import get_secret_name_from_ref
 
 
+log = logging.getLogger(__name__)
+
+
 class SecretProvider(BaseSecretProvider):
     def __init__(
         self,
         soa_dir: Optional[str],
         service_name: Optional[str],
         cluster_names: List[str],
         vault_cluster_config: Dict[str, str] = {},
@@ -83,92 +84,131 @@
                 client=client,
                 env=self.ecosystems[0],
                 path=secret_path,
                 cache_enabled=False,
                 cache_dir=None,
                 cache_key=None,
                 context=self.service_name,
+                rescue_failures=False,
             ).decode("utf-8")
             secret_environment[k] = secret
         return secret_environment
 
     def get_vault_ecosystems_for_clusters(self) -> List[str]:
         try:
             return list(
                 {
                     self.vault_cluster_config[cluster_name]
                     for cluster_name in self.cluster_names
                 }
             )
         except KeyError as e:
-            paasta_print(
+            print(
                 "Cannot find a vault cluster for the %s paasta cluster. A mapping must exist "
                 "in /etc/paasta so we contact the correct vault cluster to get/set secrets"
                 % e
             )
             raise
 
-    def write_secret(self, action: str, secret_name: str, plaintext: bytes) -> None:
+    def write_secret(
+        self,
+        action: str,
+        secret_name: str,
+        plaintext: bytes,
+        cross_environment_motivation: Optional[str] = None,
+    ) -> None:
         with TempGpgKeyring(overwrite=True):
             for ecosystem in self.ecosystems:
                 client = self.clients[ecosystem]
                 encrypt_secret(
                     client=client,
                     action=action,
                     ecosystem=ecosystem,
                     secret_name=secret_name,
                     soa_dir=self.soa_dir,
                     plaintext=plaintext,
                     service_name=self.service_name,
                     transit_key=self.encryption_key,
+                    cross_environment_motivation=cross_environment_motivation,
                 )
 
     def decrypt_secret(self, secret_name: str) -> str:
-        client = self.clients[self.ecosystems[0]]
-        secret_path = os.path.join(self.secret_dir, f"{secret_name}.json")
-        return get_plaintext(
-            client=client,
-            path=secret_path,
-            env=self.ecosystems[0],
-            cache_enabled=False,
-            cache_key=None,
-            cache_dir=None,
-            context=self.service_name,
-        ).decode("utf-8")
+        return self.decrypt_secret_raw(secret_name).decode("utf-8")
 
     def decrypt_secret_raw(self, secret_name: str) -> bytes:
         client = self.clients[self.ecosystems[0]]
         secret_path = os.path.join(self.secret_dir, f"{secret_name}.json")
         return get_plaintext(
             client=client,
             path=secret_path,
             env=self.ecosystems[0],
             cache_enabled=False,
             cache_key=None,
             cache_dir=None,
             context=self.service_name,
+            rescue_failures=False,
         )
 
     def get_secret_signature_from_data(self, data: Mapping[str, Any]) -> Optional[str]:
         ecosystem = self.ecosystems[0]
-        if data["environments"][ecosystem]:
+        if data["environments"].get(ecosystem):
             return data["environments"][ecosystem]["signature"]
         else:
             return None
 
-    def renew_issue_cert(self, pki_backend: str, ttl: str) -> None:
+    def get_data_from_vault_path(self, path: str) -> Optional[Dict[str, str]]:
+        # clients.read returns None if not set
+        # if it is set, it returns an object with { **metadata, data: {} }
+        # eg lease_id, request_id, etc. we only care about 'data' here
+
         client = self.clients[self.ecosystems[0]]
-        user = getpass.getuser()
-        pki_dir = os.path.expanduser("~/.paasta/pki")
-        do_renew(
-            client=client,
-            pki_backend=pki_backend,
-            role=user,
-            cn=f"{user}.{self.ecosystems[0]}.paasta.yelp",
-            cert_path=f"{pki_dir}/{self.ecosystems[0]}.crt",
-            key_path=f"{pki_dir}/{self.ecosystems[0]}.key",
-            ca_path=f"{pki_dir}/{self.ecosystems[0]}_ca.crt",
-            cert_owner=user,
-            cert_group="users",
-            cert_mode="0600",
-            ttl=ttl,
-        )
+
+        # there is a chance client is None (ie if the connection is invalid)
+        if client is None:
+            raise Exception(
+                "possible Vault misconfiguration as client is not available"
+            )
+
+        entry = client.read(path)
+
+        # returns one of 3 things:
+        #   entry -> could be None
+        #   entry["data"] -> could be an object with content
+        #   entry["data"] -> could be empty (ie no secrets)
+        # all are plausible and valid scenarios that give different information about the path
+
+        if entry is not None:
+            return entry.get("data", {})
+        return None
+
+    def get_key_versions(
+        self,
+        key_name: str,
+    ) -> List[CryptoKey]:
+        """
+        Retrieve all versions of Vault key based on its metadata
+        """
+        client = self.clients[self.ecosystems[0]]
+        crypto_keys: List[CryptoKey] = []
+        try:
+            meta_response = client.secrets.kv.read_secret_metadata(
+                path=key_name, mount_point="keystore"
+            )
+
+            for key_version in meta_response["data"]["versions"].keys():
+                key_response = client.secrets.kv.read_secret_version(
+                    path=key_name, version=key_version, mount_point="keystore"
+                )
+                crypto_keys.append(
+                    {
+                        "key_name": key_name,
+                        "key_version": key_response["data"]["metadata"]["version"],
+                        "key": key_response["data"]["data"]["key"],
+                    }
+                )
+        except hvac.exceptions.VaultError:
+            log.warning(
+                f"Could not fetch key versions for {key_name} on {self.ecosystems[0]}"
+            )
+            pass
+
+        return crypto_keys
```

### Comparing `paasta-tools-0.92.1/paasta_tools/kafkacluster_tools.py` & `paasta-tools-1.0.0/paasta_tools/kafkacluster_tools.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,22 +12,21 @@
 # limitations under the License.
 from typing import List
 from typing import Mapping
 from typing import Optional
 
 import service_configuration_lib
 
-from paasta_tools.kubernetes_tools import InvalidJobNameError
-from paasta_tools.kubernetes_tools import NoConfigurationForServiceError
 from paasta_tools.kubernetes_tools import sanitised_cr_name
 from paasta_tools.long_running_service_tools import LongRunningServiceConfig
 from paasta_tools.long_running_service_tools import LongRunningServiceConfigDict
 from paasta_tools.utils import BranchDictV2
 from paasta_tools.utils import deep_merge_dictionaries
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import load_service_instance_config
 from paasta_tools.utils import load_v2_deployments_json
 
 
 class KafkaClusterDeploymentConfigDict(LongRunningServiceConfigDict, total=False):
     replicas: int
 
 
@@ -97,30 +96,19 @@
     :param load_deployments: A boolean indicating if the corresponding deployments.json for this service
                              should also be loaded
     :param soa_dir: The SOA configuration directory to read from
     :returns: A dictionary of whatever was in the config for the service instance"""
     general_config = service_configuration_lib.read_service_configuration(
         service, soa_dir=soa_dir
     )
-    kafkacluster_conf_file = "kafkacluster-%s" % cluster
-    instance_configs = service_configuration_lib.read_extra_service_information(
-        service, kafkacluster_conf_file, soa_dir=soa_dir
+    instance_config = load_service_instance_config(
+        service, instance, "kafkacluster", cluster, soa_dir=soa_dir
     )
-
-    if instance.startswith("_"):
-        raise InvalidJobNameError(
-            f"Unable to load kubernetes job config for {service}.{instance} as instance name starts with '_'"
-        )
-    if instance not in instance_configs:
-        raise NoConfigurationForServiceError(
-            f"{instance} not found in config file {soa_dir}/{service}/{kafkacluster_conf_file}.yaml."
-        )
-
     general_config = deep_merge_dictionaries(
-        overrides=instance_configs[instance], defaults=general_config
+        overrides=instance_config, defaults=general_config
     )
 
     branch_dict: Optional[BranchDictV2] = None
     if load_deployments:
         deployments_json = load_v2_deployments_json(service, soa_dir=soa_dir)
         temp_instance_config = KafkaClusterDeploymentConfig(
             service=service,
```

### Comparing `paasta-tools-0.92.1/paasta_tools/slack.py` & `paasta-tools-1.0.0/paasta_tools/slack.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/setup_kubernetes_job.py` & `paasta-tools-1.0.0/paasta_tools/check_flink_services_health.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,195 +1,204 @@
 #!/usr/bin/env python
-# Copyright 2015-2018 Yelp Inc.
+# Copyright 2015-2019 Yelp Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Usage: ./setup_kubernetes_job.py <service.instance> [options]
-
-Command line options:
-
-- -d <SOA_DIR>, --soa-dir <SOA_DIR>: Specify a SOA config dir to read from
-- -v, --verbose: Verbose output
+Usage: ./check_flink_services_health.py [options]
 """
-import argparse
+import datetime
 import logging
-import sys
-from typing import Optional
 from typing import Sequence
 from typing import Tuple
 
-from kubernetes.client import V1Deployment
-from kubernetes.client import V1StatefulSet
+import pysensu_yelp
+
+from paasta_tools import flink_tools
+from paasta_tools import flinkeks_tools
+from paasta_tools.check_services_replication_tools import main
+from paasta_tools.check_services_replication_tools import parse_args
+from paasta_tools.flink_tools import FlinkDeploymentConfig
+from paasta_tools.kubernetes_tools import filter_pods_by_service_instance
+from paasta_tools.kubernetes_tools import is_pod_ready
+from paasta_tools.kubernetes_tools import V1Pod
+from paasta_tools.monitoring_tools import check_under_replication
+from paasta_tools.monitoring_tools import send_replication_event
+from paasta_tools.smartstack_tools import KubeSmartstackEnvoyReplicationChecker
+from paasta_tools.utils import is_under_replicated
 
-from paasta_tools.kubernetes.application.controller_wrappers import Application
-from paasta_tools.kubernetes.application.controller_wrappers import DeploymentWrapper
-from paasta_tools.kubernetes.application.controller_wrappers import StatefulSetWrapper
-from paasta_tools.kubernetes_tools import ensure_namespace
-from paasta_tools.kubernetes_tools import InvalidKubernetesConfig
-from paasta_tools.kubernetes_tools import KubeClient
-from paasta_tools.kubernetes_tools import list_all_deployments
-from paasta_tools.kubernetes_tools import load_kubernetes_service_config_no_cache
-from paasta_tools.utils import decompose_job_id
-from paasta_tools.utils import DEFAULT_SOA_DIR
-from paasta_tools.utils import InvalidJobNameError
-from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import NoConfigurationForServiceError
-from paasta_tools.utils import NoDeploymentsAvailable
-from paasta_tools.utils import SPACER
 
 log = logging.getLogger(__name__)
 
 
-def parse_args() -> argparse.Namespace:
-    parser = argparse.ArgumentParser(description="Creates marathon jobs.")
-    parser.add_argument(
-        "service_instance_list",
-        nargs="+",
-        help="The list of marathon service instances to create or update",
-        metavar="SERVICE%sINSTANCE" % SPACER,
-    )
-    parser.add_argument(
-        "-d",
-        "--soa-dir",
-        dest="soa_dir",
-        metavar="SOA_DIR",
-        default=DEFAULT_SOA_DIR,
-        help="define a different soa config directory",
-    )
-    parser.add_argument(
-        "-c", "--cluster", dest="cluster", help="paasta cluster",
-    )
-    parser.add_argument(
-        "-v", "--verbose", action="store_true", dest="verbose", default=False
+def container_lifetime(
+    pod: V1Pod,
+) -> datetime.timedelta:
+    """Return a time duration for how long the pod is alive"""
+    st = pod.status.start_time
+    return datetime.datetime.now(st.tzinfo) - st
+
+
+def healthy_flink_containers_cnt(si_pods: Sequence[V1Pod], container_type: str) -> int:
+    """Return count of healthy Flink containers with given type"""
+    return len(
+        [
+            pod
+            for pod in si_pods
+            if pod.metadata.labels["flink.yelp.com/container-type"] == container_type
+            and is_pod_ready(pod)
+            and container_lifetime(pod).total_seconds() > 60
+        ]
     )
-    args = parser.parse_args()
-    return args
 
 
-def main() -> None:
-    args = parse_args()
-    soa_dir = args.soa_dir
-    if args.verbose:
-        logging.basicConfig(level=logging.DEBUG)
+def check_under_registered_taskmanagers(
+    instance_config: FlinkDeploymentConfig,
+    expected_count: int,
+    cr_name: str,
+    is_eks: bool,
+) -> Tuple[bool, str, str]:
+    """Check if not enough taskmanagers have been registered to the jobmanager and
+    returns both the result of the check in the form of a boolean and a human-readable
+    text to be used in logging or monitoring events.
+    """
+    unhealthy = True
+    if cr_name != "":
+        try:
+            overview = flink_tools.get_flink_jobmanager_overview(
+                cr_name, instance_config.cluster, is_eks
+            )
+            num_reported = overview.get("taskmanagers", 0)
+            crit_threshold = instance_config.get_replication_crit_percentage()
+            output = (
+                f"{instance_config.job_id} has {num_reported}/{expected_count} "
+                f"taskmanagers reported by dashboard (threshold: {crit_threshold}%)"
+            )
+            unhealthy, _ = is_under_replicated(
+                num_reported, expected_count, crit_threshold
+            )
+        except ValueError as e:
+            output = (
+                f"Dashboard of service {instance_config.job_id} is not available ({e})"
+            )
     else:
-        logging.basicConfig(level=logging.WARNING)
+        output = f"Dashboard of service {instance_config.job_id} is not available"
+    if unhealthy:
+        description = f"""
+This alert means that the Flink dashboard is not reporting the expected
+number of taskmanagers.
 
-    # system_paasta_config = load_system_paasta_config()
-    kube_client = KubeClient()
+Reasons this might be happening:
 
-    ensure_namespace(kube_client, namespace="paasta")
-    setup_kube_succeeded = setup_kube_deployments(
-        kube_client=kube_client,
-        service_instances=args.service_instance_list,
-        soa_dir=soa_dir,
-        cluster=args.cluster or load_system_paasta_config().get_cluster(),
-    )
-    sys.exit(0 if setup_kube_succeeded else 1)
+  The service may simply be unhealthy. There also may not be enough resources
+  in the cluster to support the requested instance count.
 
+Things you can do:
 
-def validate_job_name(service_instance: str) -> bool:
-    try:
-        service, instance, _, __ = decompose_job_id(service_instance)
-    except InvalidJobNameError:
-        log.error(
-            "Invalid service instance specified. Format is service%sinstance." % SPACER
-        )
-        return False
-    return True
-
-
-def setup_kube_deployments(
-    kube_client: KubeClient,
-    service_instances: Sequence[str],
-    cluster: str,
-    soa_dir: str = DEFAULT_SOA_DIR,
-) -> bool:
-    if service_instances:
-        existing_kube_deployments = set(list_all_deployments(kube_client))
-        existing_apps = {
-            (deployment.service, deployment.instance)
-            for deployment in existing_kube_deployments
-        }
-    service_instances_with_valid_names = [
-        decompose_job_id(service_instance)
-        for service_instance in service_instances
-        if validate_job_name(service_instance)
-    ]
-    applications = [
-        create_application_object(
-            kube_client=kube_client,
-            service=service_instance[0],
-            instance=service_instance[1],
-            cluster=cluster,
-            soa_dir=soa_dir,
-        )
-        for service_instance in service_instances_with_valid_names
-    ]
+  * Fix the cause of the unhealthy service. Try running:
+
+     paasta status -s {instance_config.service} -i {instance_config.instance} -c {instance_config.cluster} -vv
 
-    for _, app in applications:
-        if (
-            app
-            and (app.kube_deployment.service, app.kube_deployment.instance)
-            not in existing_apps
-        ):
-            app.create(kube_client)
-        elif app and app.kube_deployment not in existing_kube_deployments:
-            app.update(kube_client)
-        else:
-            log.debug(f"{app} is up to date, no action taken")
-
-    return (False, None) not in applications and len(
-        service_instances_with_valid_names
-    ) == len(service_instances)
-
-
-def create_application_object(
-    kube_client: KubeClient, service: str, instance: str, cluster: str, soa_dir: str,
-) -> Tuple[bool, Optional[Application]]:
-    try:
-        service_instance_config = load_kubernetes_service_config_no_cache(
-            service, instance, cluster, soa_dir=soa_dir,
-        )
-    except NoDeploymentsAvailable:
-        log.debug(
-            "No deployments found for %s.%s in cluster %s. Skipping."
-            % (service, instance, cluster)
-        )
-        return True, None
-    except NoConfigurationForServiceError:
-        error_msg = (
-            "Could not read kubernetes configuration file for %s.%s in cluster %s"
-            % (service, instance, cluster)
-        )
-        log.error(error_msg)
-        return False, None
-
-    try:
-        formatted_application = service_instance_config.format_kubernetes_app()
-    except InvalidKubernetesConfig as e:
-        log.error(str(e))
-        return False, None
-
-    app: Optional[Application] = None
-    if isinstance(formatted_application, V1Deployment):
-        app = DeploymentWrapper(formatted_application)
-    elif isinstance(formatted_application, V1StatefulSet):
-        app = StatefulSetWrapper(formatted_application)
+"""
     else:
-        raise Exception("Unknown kubernetes object to update")
+        description = f"{instance_config.job_id} taskmanager is available"
+    return unhealthy, output, description
 
-    app.load_local_config(soa_dir, cluster)
-    return True, app
+
+def get_cr_name(si_pods: Sequence[V1Pod]) -> str:
+    """Returns the flink custom resource name based on the pod name.  We are randomly choosing jobmanager pod here.
+    This change is related to FLINK-3129
+    """
+    jobmanager_pod = [
+        pod
+        for pod in si_pods
+        if pod.metadata.labels["flink.yelp.com/container-type"] == "jobmanager"
+        and is_pod_ready(pod)
+        and container_lifetime(pod).total_seconds() > 60
+    ]
+    if len(jobmanager_pod) == 1:
+        return jobmanager_pod[0].metadata.name.split("-jobmanager-")[0]
+    else:
+        return ""
+
+
+def check_flink_service_health(
+    instance_config: FlinkDeploymentConfig,
+    all_pods: Sequence[V1Pod],
+    replication_checker: KubeSmartstackEnvoyReplicationChecker,
+    dry_run: bool = False,
+) -> None:
+    si_pods = filter_pods_by_service_instance(
+        pod_list=all_pods,
+        service=instance_config.service,
+        instance=instance_config.instance,
+    )
+    taskmanagers_expected_cnt = instance_config.config_dict.get(
+        "taskmanager", {"instances": 10}
+    ).get("instances", 10)
+    num_healthy_supervisors = healthy_flink_containers_cnt(si_pods, "supervisor")
+    num_healthy_jobmanagers = healthy_flink_containers_cnt(si_pods, "jobmanager")
+    num_healthy_taskmanagers = healthy_flink_containers_cnt(si_pods, "taskmanager")
+
+    service_cr_name = get_cr_name(si_pods)
+
+    results = [
+        check_under_replication(
+            instance_config=instance_config,
+            expected_count=1,
+            num_available=num_healthy_supervisors,
+            sub_component="supervisor",
+        ),
+        check_under_replication(
+            instance_config=instance_config,
+            expected_count=1,
+            num_available=num_healthy_jobmanagers,
+            sub_component="jobmanager",
+        ),
+        check_under_replication(
+            instance_config=instance_config,
+            expected_count=taskmanagers_expected_cnt,
+            num_available=num_healthy_taskmanagers,
+            sub_component="taskmanager",
+        ),
+        check_under_registered_taskmanagers(
+            instance_config=instance_config,
+            expected_count=taskmanagers_expected_cnt,
+            cr_name=service_cr_name,
+            is_eks=isinstance(instance_config, flinkeks_tools.FlinkEksDeploymentConfig),
+        ),
+    ]
+    output = ", ".join([r[1] for r in results])
+    description = "\n########\n".join([r[2] for r in results])
+    if any(r[0] for r in results):
+        log.error(output)
+        status = pysensu_yelp.Status.CRITICAL
+    else:
+        log.info(output)
+        status = pysensu_yelp.Status.OK
+    send_replication_event(
+        instance_config=instance_config,
+        status=status,
+        output=output,
+        description=description,
+        dry_run=dry_run,
+    )
 
 
 if __name__ == "__main__":
-    main()
+    args = parse_args()
+    main(
+        instance_type_class=flinkeks_tools.FlinkEksDeploymentConfig
+        if args.eks
+        else flink_tools.FlinkDeploymentConfig,
+        check_service_replication=check_flink_service_health,
+        namespace="paasta-flinks",
+    )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/clusterman.py` & `paasta-tools-1.0.0/paasta_tools/clusterman.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/iptables.py` & `paasta-tools-1.0.0/paasta_tools/iptables.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,15 +17,15 @@
     # all else defaults to '0'
     "LOG": 1,
     "REJECT": 2.0,
 }
 
 
 _RuleBase = collections.namedtuple(
-    "Rule", ("protocol", "src", "dst", "target", "matches", "target_parameters")
+    "_RuleBase", ("protocol", "src", "dst", "target", "matches", "target_parameters")
 )
 
 
 class Rule(_RuleBase):
     """Rule representation.
 
     Working with iptc's rule classes directly doesn't work well, since rules
@@ -169,16 +169,15 @@
 def _rule_sort_key(rule_tuple):
     old_index, rule = rule_tuple
     target_name = rule.target
     return (RULE_TARGET_SORT_ORDER.get(target_name, 0), old_index)
 
 
 def reorder_chain(chain_name):
-    """Ensure that any REJECT rules are last, and any LOG rules are second-to-last
-    """
+    """Ensure that any REJECT rules are last, and any LOG rules are second-to-last"""
 
     table = iptc.Table(iptc.Table.FILTER)
     with iptables_txn(table):
         rules = list_chain(chain_name)
         chain = iptc.Chain(table, chain_name)
 
         # sort the rules by rule_key, which uses (RULE_TARGET_SORT_ORDER, idx)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/adhoc_tools.py` & `paasta-tools-1.0.0/paasta_tools/adhoc_tools.py`

 * *Files 14% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 import service_configuration_lib
 
 from paasta_tools.long_running_service_tools import LongRunningServiceConfig
 from paasta_tools.long_running_service_tools import LongRunningServiceConfigDict
 from paasta_tools.utils import BranchDictV2
 from paasta_tools.utils import deep_merge_dictionaries
 from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import load_service_instance_config
 from paasta_tools.utils import load_v2_deployments_json
 from paasta_tools.utils import NoConfigurationForServiceError
 from paasta_tools.utils import NoDeploymentsAvailable
 from paasta_tools.utils import prompt_pick_one
 
 
 log = logging.getLogger(__name__)
@@ -31,26 +32,23 @@
 
 def load_adhoc_job_config(
     service, instance, cluster, load_deployments=True, soa_dir=DEFAULT_SOA_DIR
 ):
     general_config = service_configuration_lib.read_service_configuration(
         service, soa_dir=soa_dir
     )
-    adhoc_conf_file = "adhoc-%s" % cluster
-    instance_configs = service_configuration_lib.read_extra_service_information(
-        service_name=service, extra_info=adhoc_conf_file, soa_dir=soa_dir
+    instance_config = load_service_instance_config(
+        service=service,
+        instance=instance,
+        instance_type="adhoc",
+        cluster=cluster,
+        soa_dir=soa_dir,
     )
-
-    if instance not in instance_configs:
-        raise NoConfigurationForServiceError(
-            f"{instance} not found in config file {soa_dir}/{service}/{adhoc_conf_file}.yaml."
-        )
-
     general_config = deep_merge_dictionaries(
-        overrides=instance_configs[instance], defaults=general_config
+        overrides=instance_config, defaults=general_config
     )
 
     branch_dict = None
     if load_deployments:
         deployments_json = load_v2_deployments_json(service, soa_dir=soa_dir)
         temp_instance_config = AdhocJobConfig(
             service=service,
```

### Comparing `paasta-tools-0.92.1/paasta_tools/kubernetes/application/controller_wrappers.py` & `paasta-tools-1.0.0/paasta_tools/kubernetes/application/controller_wrappers.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 import logging
-import threading
 from abc import ABC
 from abc import abstractmethod
-from time import sleep
 from typing import Optional
 from typing import Union
 
 from kubernetes.client import V1beta1PodDisruptionBudget
 from kubernetes.client import V1DeleteOptions
 from kubernetes.client import V1Deployment
 from kubernetes.client import V1StatefulSet
 from kubernetes.client.rest import ApiException
 
+from paasta_tools.autoscaling.autoscaling_service_lib import autoscaling_is_paused
+from paasta_tools.eks_tools import load_eks_service_config_no_cache
 from paasta_tools.kubernetes_tools import create_deployment
 from paasta_tools.kubernetes_tools import create_pod_disruption_budget
 from paasta_tools.kubernetes_tools import create_stateful_set
-from paasta_tools.kubernetes_tools import force_delete_pods
 from paasta_tools.kubernetes_tools import KubeClient
 from paasta_tools.kubernetes_tools import KubeDeployment
 from paasta_tools.kubernetes_tools import KubernetesDeploymentConfig
-from paasta_tools.kubernetes_tools import list_all_deployments
 from paasta_tools.kubernetes_tools import load_kubernetes_service_config_no_cache
+from paasta_tools.kubernetes_tools import paasta_prefixed
 from paasta_tools.kubernetes_tools import pod_disruption_budget_for_service_instance
 from paasta_tools.kubernetes_tools import update_deployment
 from paasta_tools.kubernetes_tools import update_stateful_set
+from paasta_tools.utils import load_system_paasta_config
 
 
 class Application(ABC):
     def __init__(
         self,
         item: Union[V1Deployment, V1StatefulSet],
         logging=logging.getLogger(__name__),
@@ -36,35 +36,56 @@
         This Application wrapper is an interface for creating/deleting k8s deployments and statefulsets
         soa_config is KubernetesDeploymentConfig. It is not loaded in init because it is not always required.
         :param item: Kubernetes Object(V1Deployment/V1StatefulSet) that has already been filled up.
         :param logging: where logs go
         """
         if not item.metadata.namespace:
             item.metadata.namespace = "paasta"
+        attrs = {
+            attr: item.metadata.labels.get(paasta_prefixed(attr))
+            for attr in [
+                "service",
+                "instance",
+                "git_sha",
+                "image_version",
+                "config_sha",
+            ]
+        }
+
+        replicas = (
+            item.spec.replicas
+            if item.metadata.labels.get(paasta_prefixed("autoscaled"), "false")
+            == "false"
+            else None
+        )
         self.kube_deployment = KubeDeployment(
-            service=item.metadata.labels["paasta.yelp.com/service"],
-            instance=item.metadata.labels["paasta.yelp.com/instance"],
-            git_sha=item.metadata.labels["paasta.yelp.com/git_sha"],
-            config_sha=item.metadata.labels["paasta.yelp.com/config_sha"],
-            replicas=item.spec.replicas,
+            replicas=replicas, namespace=item.metadata.namespace, **attrs
         )
         self.item = item
         self.soa_config = None  # type: KubernetesDeploymentConfig
         self.logging = logging
 
     def load_local_config(
-        self, soa_dir: str, cluster: str
+        self, soa_dir: str, cluster: str, eks: bool = False
     ) -> Optional[KubernetesDeploymentConfig]:
         if not self.soa_config:
-            self.soa_config = load_kubernetes_service_config_no_cache(
-                service=self.kube_deployment.service,
-                instance=self.kube_deployment.instance,
-                cluster=cluster,
-                soa_dir=soa_dir,
-            )
+            if eks:
+                self.soa_config = load_eks_service_config_no_cache(
+                    service=self.kube_deployment.service,
+                    instance=self.kube_deployment.instance,
+                    cluster=cluster,
+                    soa_dir=soa_dir,
+                )
+            else:
+                self.soa_config = load_kubernetes_service_config_no_cache(
+                    service=self.kube_deployment.service,
+                    instance=self.kube_deployment.instance,
+                    cluster=cluster,
+                    soa_dir=soa_dir,
+                )
         return self.soa_config
 
     def __str__(self):
         service = self.kube_deployment.service
         instance = self.kube_deployment.instance
         git_sha = self.kube_deployment.git_sha
         config_sha = self.kube_deployment.config_sha
@@ -88,14 +109,21 @@
     def update(self, kube_client: KubeClient):
         """
         Update all controllers, HPA, and pod disruption budgets related to this application
         :param kube_client:
         """
         pass
 
+    def update_related_api_objects(self, kube_client: KubeClient) -> None:
+        """
+        Update related Kubernetes API objects such as HPAs and Pod Disruption Budgets
+        :param kube_client:
+        """
+        self.ensure_pod_disruption_budget(kube_client, self.soa_config.get_namespace())
+
     def delete_pod_disruption_budget(self, kube_client: KubeClient) -> None:
         try:
             kube_client.policy.delete_namespaced_pod_disruption_budget(
                 name=self.item.metadata.name,
                 namespace=self.item.metadata.namespace,
                 body=V1DeleteOptions(),
             )
@@ -114,60 +142,72 @@
             self.logging.info(
                 "deleted pod disruption budget/{} from namespace/{}".format(
                     self.item.metadata.name, self.item.metadata.namespace
                 )
             )
 
     def ensure_pod_disruption_budget(
-        self, kube_client: KubeClient
+        self, kube_client: KubeClient, namespace: str
     ) -> V1beta1PodDisruptionBudget:
+        max_unavailable: Union[str, int]
+        if "bounce_margin_factor" in self.soa_config.config_dict:
+            max_unavailable = (
+                f"{int((1 - self.soa_config.get_bounce_margin_factor()) * 100)}%"
+            )
+        else:
+            system_paasta_config = load_system_paasta_config()
+            max_unavailable = system_paasta_config.get_pdb_max_unavailable()
+
         pdr = pod_disruption_budget_for_service_instance(
             service=self.kube_deployment.service,
             instance=self.kube_deployment.instance,
-            max_unavailable="{}%".format(
-                int((1 - self.soa_config.get_bounce_margin_factor()) * 100)
-            ),
+            max_unavailable=max_unavailable,
+            namespace=namespace,
         )
         try:
             existing_pdr = kube_client.policy.read_namespaced_pod_disruption_budget(
                 name=pdr.metadata.name, namespace=pdr.metadata.namespace
             )
         except ApiException as e:
             if e.status == 404:
                 existing_pdr = None
             else:
                 raise
 
         if existing_pdr:
             if existing_pdr.spec.min_available is not None:
-                logging.debug(
+                logging.info(
                     "Not updating poddisruptionbudget: can't have both "
                     "min_available and max_unavailable"
                 )
             elif existing_pdr.spec.max_unavailable != pdr.spec.max_unavailable:
-                logging.debug(f"Updating poddisruptionbudget {pdr.metadata.name}")
+                logging.info(f"Updating poddisruptionbudget {pdr.metadata.name}")
                 return kube_client.policy.patch_namespaced_pod_disruption_budget(
                     name=pdr.metadata.name, namespace=pdr.metadata.namespace, body=pdr
                 )
             else:
-                logging.debug(f"poddisruptionbudget {pdr.metadata.name} up to date")
+                logging.info(f"poddisruptionbudget {pdr.metadata.name} up to date")
         else:
-            logging.debug(f"creating poddisruptionbudget {pdr.metadata.name}")
+            logging.info(f"creating poddisruptionbudget {pdr.metadata.name}")
             return create_pod_disruption_budget(
-                kube_client=kube_client, pod_disruption_budget=pdr
+                kube_client=kube_client,
+                pod_disruption_budget=pdr,
+                namespace=pdr.metadata.namespace,
             )
 
 
 class DeploymentWrapper(Application):
-    def deep_delete(self, kube_client: KubeClient) -> None:
+    def deep_delete(
+        self, kube_client: KubeClient, propagation_policy="Foreground"
+    ) -> None:
         """
         Remove all controllers, pods, and pod disruption budgets related to this application
         :param kube_client:
         """
-        delete_options = V1DeleteOptions(propagation_policy="Foreground")
+        delete_options = V1DeleteOptions(propagation_policy=propagation_policy)
         try:
             kube_client.deployments.delete_namespaced_deployment(
                 self.item.metadata.name,
                 self.item.metadata.namespace,
                 body=delete_options,
             )
         except ApiException as e:
@@ -192,124 +232,82 @@
 
     def get_existing_app(self, kube_client: KubeClient):
         return kube_client.deployments.read_namespaced_deployment(
             name=self.item.metadata.name, namespace=self.item.metadata.namespace
         )
 
     def create(self, kube_client: KubeClient) -> None:
-        create_deployment(kube_client=kube_client, formatted_deployment=self.item)
-        self.ensure_pod_disruption_budget(kube_client)
+        create_deployment(
+            kube_client=kube_client,
+            formatted_deployment=self.item,
+            namespace=self.soa_config.get_namespace(),
+        )
+        self.ensure_pod_disruption_budget(kube_client, self.soa_config.get_namespace())
         self.sync_horizontal_pod_autoscaler(kube_client)
 
-    def deep_delete_and_create(self, kube_client: KubeClient) -> None:
-        self.deep_delete(kube_client)
-        timer = 0
-        while (
-            self.kube_deployment in set(list_all_deployments(kube_client))
-            and timer < 60
-        ):
-            sleep(1)
-            timer += 1
-
-        if timer >= 60 and self.kube_deployment in set(
-            list_all_deployments(kube_client)
-        ):
-            try:
-                force_delete_pods(
-                    self.item.metadata.name,
-                    self.kube_deployment.service,
-                    self.kube_deployment.instance,
-                    self.item.metadata.namespace,
-                    kube_client,
-                )
-            except ApiException as e:
-                if e.status == 404:
-                    # Deployment does not exist, nothing to delete but
-                    # we can consider this a success.
-                    self.logging.debug(
-                        "not deleting nonexistent deploy/{} from namespace/{}".format(
-                            self.kube_deployment.service, self.item.metadata.namespace
-                        )
-                    )
-                else:
-                    raise
-        else:
-            self.logging.info(
-                "deleted deploy/{} from namespace/{}".format(
-                    self.kube_deployment.service, self.item.metadata.namespace
-                )
-            )
-        self.create(kube_client=kube_client)
-
     def update(self, kube_client: KubeClient) -> None:
         # If HPA is enabled, do not update replicas.
         # In all other cases, replica is set to max(instances, min_instances)
-        if self.soa_config.config_dict.get("bounce_method", "") == "brutal":
-            threading.Thread(
-                target=self.deep_delete_and_create, args=[KubeClient()]
-            ).start()
-            return
-        if self.should_have_hpa():
-            self.item.spec.replicas = self.get_existing_app(kube_client).spec.replicas
-        update_deployment(kube_client=kube_client, formatted_deployment=self.item)
-        self.ensure_pod_disruption_budget(kube_client)
-        self.sync_horizontal_pod_autoscaler(kube_client)
-
-    def should_have_hpa(self):
-        return (
-            (
-                self.soa_config.get_max_instances() is not None
-                or self.soa_config.config_dict.get("horizontal_autoscaling", False)
-            )
-            # with bespoke autoscaler, setup_kubernetes_job sets the number of instances directly; no HPA is required.
-            and self.soa_config.get_autoscaling_params()["decision_policy"] != "bespoke"
+        update_deployment(
+            kube_client=kube_client,
+            formatted_deployment=self.item,
+            namespace=self.soa_config.get_namespace(),
         )
 
+    def update_related_api_objects(self, kube_client: KubeClient) -> None:
+        super().update_related_api_objects(kube_client)
+        self.sync_horizontal_pod_autoscaler(kube_client)
+
     def sync_horizontal_pod_autoscaler(self, kube_client: KubeClient) -> None:
         """
         In order for autoscaling to work, there needs to be at least two configurations
         min_instnace, max_instance, and there cannot be instance.
         """
-        self.logging.info(
-            f"Syncing HPA setting for {self.item.metadata.name}/name in {self.item.metadata.namespace}"
+        desired_hpa_spec = self.soa_config.get_autoscaling_metric_spec(
+            name=self.item.metadata.name,
+            cluster=self.soa_config.cluster,
+            kube_client=kube_client,
+            namespace=self.item.metadata.namespace,
         )
+
         hpa_exists = self.exists_hpa(kube_client)
-        # NO autoscaling
-        if not self.should_have_hpa():
-            # Remove HPA if autoscaling is disabled
+        should_have_hpa = desired_hpa_spec and not autoscaling_is_paused()
+
+        if not should_have_hpa:
+            self.logging.info(
+                f"No HPA required for {self.item.metadata.name}/name in {self.item.metadata.namespace}"
+            )
             if hpa_exists:
+                self.logging.info(
+                    f"Deleting HPA for {self.item.metadata.name}/name in {self.item.metadata.namespace}"
+                )
                 self.delete_horizontal_pod_autoscaler(kube_client)
             return
 
-        body = self.soa_config.get_autoscaling_metric_spec(
-            name=self.item.metadata.name,
-            cluster=self.soa_config.cluster,
-            namespace=self.item.metadata.namespace,
+        self.logging.info(
+            f"Syncing HPA setting for {self.item.metadata.name}/name in {self.item.metadata.namespace}"
         )
-        if not body:
-            raise Exception(
-                f"CRIT: autoscaling misconfigured for {self.kube_deployment.service}."
-                + f"{self.kube_deployment.instance}.Please correct the configuration and update pre-commit hook."
-            )
-        self.logging.debug(body)
+        self.logging.debug(desired_hpa_spec)
         if not hpa_exists:
             self.logging.info(
                 f"Creating new HPA for {self.item.metadata.name}/name in {self.item.metadata.namespace}"
             )
             kube_client.autoscaling.create_namespaced_horizontal_pod_autoscaler(
-                namespace=self.item.metadata.namespace, body=body, pretty=True
+                namespace=self.item.metadata.namespace,
+                body=desired_hpa_spec,
+                pretty=True,
             )
         else:
             self.logging.info(
                 f"Updating new HPA for {self.item.metadata.name}/name in {self.item.metadata.namespace}/namespace"
             )
-            kube_client.autoscaling.patch_namespaced_horizontal_pod_autoscaler(
+            kube_client.autoscaling.replace_namespaced_horizontal_pod_autoscaler(
                 name=self.item.metadata.name,
                 namespace=self.item.metadata.namespace,
-                body=body,
+                body=desired_hpa_spec,
                 pretty=True,
             )
 
     def exists_hpa(self, kube_client: KubeClient) -> bool:
         return (
             len(
                 kube_client.autoscaling.list_namespaced_horizontal_pod_autoscaler(
@@ -373,13 +371,34 @@
                 "deleted statefulset/{} from namespace/{}".format(
                     self.item.metadata.name, self.item.metadata.namespace
                 )
             )
         self.delete_pod_disruption_budget(kube_client)
 
     def create(self, kube_client: KubeClient):
-        create_stateful_set(kube_client=kube_client, formatted_stateful_set=self.item)
-        self.ensure_pod_disruption_budget(kube_client)
+        create_stateful_set(
+            kube_client=kube_client,
+            formatted_stateful_set=self.item,
+            namespace=self.soa_config.get_namespace(),
+        )
+        self.ensure_pod_disruption_budget(kube_client, self.soa_config.get_namespace())
 
     def update(self, kube_client: KubeClient):
-        update_stateful_set(kube_client=kube_client, formatted_stateful_set=self.item)
-        self.ensure_pod_disruption_budget(kube_client)
+        update_stateful_set(
+            kube_client=kube_client,
+            formatted_stateful_set=self.item,
+            namespace=self.soa_config.get_namespace(),
+        )
+
+
+def get_application_wrapper(
+    formatted_application: Union[V1Deployment, V1StatefulSet]
+) -> Application:
+    app: Application
+    if isinstance(formatted_application, V1Deployment):
+        app = DeploymentWrapper(formatted_application)
+    elif isinstance(formatted_application, V1StatefulSet):
+        app = StatefulSetWrapper(formatted_application)
+    else:
+        raise Exception("Unknown kubernetes object to update")
+
+    return app
```

### Comparing `paasta-tools-0.92.1/paasta_tools/kubernetes/bin/kubernetes_remove_evicted_pods.py` & `paasta-tools-1.0.0/paasta_tools/kubernetes/bin/kubernetes_remove_evicted_pods.py`

 * *Files 4% similar despite different names*

```diff
@@ -66,19 +66,24 @@
         pod
         for pod in pods
         if pod.status.phase == "Failed" and pod.status.reason == "Evicted"
     ]
 
 
 def get_pod_service(pod: V1Pod) -> str:
-    return pod.metadata.labels.get("paasta.yelp.com/service")
+    if pod.metadata.labels is not None:
+        return pod.metadata.labels.get("paasta.yelp.com/service")
+    else:
+        return None
 
 
 def notify_service_owners(
-    services: Mapping[str, Sequence[EvictedPod]], soa_dir: str, dry_run: bool,
+    services: Mapping[str, Sequence[EvictedPod]],
+    soa_dir: str,
+    dry_run: bool,
 ) -> None:
     check_overrides = {
         "page": False,
         "alert_after": "0m",
         "realert_every": 1,
         "tip": "Pods can be Evicted if they go over the allowed quota for a given resource. Check the Eviction message to figure out which resource quota was breached",
     }
@@ -98,15 +103,17 @@
                 Status.CRITICAL,
                 check_output,
                 soa_dir,
             )
 
 
 def remove_pods(
-    client: KubeClient, services: Mapping[str, Sequence[EvictedPod]], dry_run: bool,
+    client: KubeClient,
+    services: Mapping[str, Sequence[EvictedPod]],
+    dry_run: bool,
 ) -> None:
     delete_options = V1DeleteOptions()
     for service in services:
         # Do not remove more than 2 pods per run
         for pod in services[service][0:2]:
             if dry_run:
                 log.info(f"Would have removed pod {pod.podname}")
@@ -117,15 +124,17 @@
                     body=delete_options,
                     grace_period_seconds=0,
                     propagation_policy="Background",
                 )
                 log.info(f"Removing pod {pod.podname}")
 
 
-def evicted_pods_per_service(client: KubeClient,) -> Mapping[str, Sequence[EvictedPod]]:
+def evicted_pods_per_service(
+    client: KubeClient,
+) -> Mapping[str, Sequence[EvictedPod]]:
     all_pods = get_all_pods(kube_client=client, namespace="")
     evicted_pods = get_evicted_pods(all_pods)
     log.info(f"Pods in evicted state: {[pod.metadata.name for pod in evicted_pods]}")
     evicted_pods_aggregated: Dict[str, List[EvictedPod]] = defaultdict(list)
     for pod in evicted_pods:
         service = get_pod_service(pod)
         if service:
@@ -144,13 +153,12 @@
     if args.verbose:
         logging.basicConfig(level=logging.DEBUG)
     else:
         logging.basicConfig(level=logging.INFO)
     kube_client = KubeClient()
 
     evicted_pods = evicted_pods_per_service(kube_client)
-    notify_service_owners(evicted_pods, args.soa_dir, args.dry_run)
     remove_pods(kube_client, evicted_pods, args.dry_run)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/kubernetes/bin/paasta_cleanup_stale_nodes.py` & `paasta-tools-1.0.0/paasta_tools/kubernetes/bin/paasta_cleanup_stale_nodes.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/smartstack_tools.py` & `paasta-tools-1.0.0/paasta_tools/smartstack_tools.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,66 +10,72 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import abc
 import collections
 import csv
-import socket
+import logging
+import random
+from typing import Any
 from typing import cast
 from typing import Collection
 from typing import DefaultDict
 from typing import Dict
 from typing import Iterable
 from typing import List
+from typing import MutableMapping
 from typing import NamedTuple
 from typing import Optional
 from typing import Sequence
 from typing import Tuple
 from typing import TypeVar
+from typing import Union
 
 import requests
 from kubernetes.client import V1Node
 from kubernetes.client import V1Pod
 from mypy_extensions import TypedDict
 
+from paasta_tools import envoy_tools
 from paasta_tools import kubernetes_tools
-from paasta_tools import marathon_tools
+from paasta_tools import long_running_service_tools
 from paasta_tools import mesos_tools
+from paasta_tools.long_running_service_tools import LongRunningServiceConfig
 from paasta_tools.mesos.exceptions import NoSlavesAvailableError
+from paasta_tools.monitoring_tools import ReplicationChecker
 from paasta_tools.utils import compose_job_id
 from paasta_tools.utils import DEFAULT_SOA_DIR
 from paasta_tools.utils import DeployBlacklist
 from paasta_tools.utils import get_user_agent
-from paasta_tools.utils import InstanceConfig
 from paasta_tools.utils import SystemPaastaConfig
 
 
-HaproxyBackend = TypedDict(
-    "HaproxyBackend",
-    {
-        "check_code": str,
-        "check_duration": str,
-        "check_status": str,
-        "lastchg": str,
-        "pxname": str,
-        "svname": str,
-        "status": str,
-    },
-    total=False,
-)
+class HaproxyBackend(TypedDict, total=False):
+    check_code: str
+    check_duration: str
+    check_status: str
+    lastchg: str
+    pxname: str
+    svname: str
+    status: str
+
+
+log = logging.getLogger(__name__)
 
 
 def retrieve_haproxy_csv(
     synapse_host: str, synapse_port: int, synapse_haproxy_url_format: str, scope: str
 ) -> Iterable[Dict[str, str]]:
     """Retrieves the haproxy csv from the haproxy web interface
 
-    :param synapse_host_port: A string in host:port format that this check
-                              should contact for replication information.
+    :param synapse_host: A host that this check should contact for replication information.
+    :param synapse_port: A integer that this check should contact for replication information.
+    :param synapse_haproxy_url_format: The format of the synapse haproxy URL.
+    :param scope: scope
     :returns reader: a csv.DictReader object
     """
     synapse_uri = synapse_haproxy_url_format.format(
         host=synapse_host, port=synapse_port, scope=scope
     )
 
     # timeout after 1 second and retry 3 times
@@ -87,16 +93,17 @@
     service: str, synapse_host: str, synapse_port: int, synapse_haproxy_url_format: str
 ) -> List[HaproxyBackend]:
     """Fetches the CSV from haproxy and returns a list of backends,
     regardless of their state.
 
     :param service: If None, return backends for all services, otherwise only return backends for this particular
                     service.
-    :param synapse_host_port: A string in host:port format that this check
-                              should contact for replication information.
+    :param synapse_host: A host that this check should contact for replication information.
+    :param synapse_port: A integer that this check should contact for replication information.
+    :param synapse_haproxy_url_format: The format of the synapse haproxy URL.
     :returns backends: A list of dicts representing the backends of all
                        services or the requested service
     """
     if service:
         services = [service]
     else:
         services = None
@@ -115,16 +122,17 @@
     synapse_haproxy_url_format: str,
 ) -> List[HaproxyBackend]:
     """Fetches the CSV from haproxy and returns a list of backends,
     regardless of their state.
 
     :param services: If None, return backends for all services, otherwise only return backends for these particular
                      services.
-    :param synapse_host_port: A string in host:port format that this check
-                              should contact for replication information.
+    :param synapse_host: A host that this check should contact for replication information.
+    :param synapse_port: A integer that this check should contact for replication information.
+    :param synapse_haproxy_url_format: The format of the synapse haproxy URL.
     :returns backends: A list of dicts representing the backends of all
                        services or the requested service
     """
 
     if services is not None and len(services) == 1:
         (scope,) = services
     else:
@@ -163,35 +171,36 @@
 def load_smartstack_info_for_service(
     service: str,
     namespace: str,
     blacklist: DeployBlacklist,
     system_paasta_config: SystemPaastaConfig,
     soa_dir: str = DEFAULT_SOA_DIR,
 ) -> Dict[str, Dict[str, int]]:
-    """Retrieves number of available backends for given services
+    """Retrieves number of available backends for given service
 
-    :param service_instances: A list of tuples of (service, instance)
-    :param namespaces: list of Smartstack namespaces
+    :param service: A service name
+    :param namespace: A Smartstack namespace
     :param blacklist: A list of blacklisted location tuples in the form (location, value)
     :param system_paasta_config: A SystemPaastaConfig object representing the system configuration.
+    :param soa_dir: SOA dir
     :returns: a dictionary of the form
 
     ::
 
         {
           'location_type': {
               'unique_location_name': {
                   'service.instance': <# ofavailable backends>
               },
               'other_unique_location_name': ...
           }
         }
 
     """
-    service_namespace_config = marathon_tools.load_service_namespace_config(
+    service_namespace_config = long_running_service_tools.load_service_namespace_config(
         service=service, namespace=namespace, soa_dir=soa_dir
     )
     discover_location_type = service_namespace_config.get_discover()
     return get_smartstack_replication_for_attribute(
         attribute=discover_location_type,
         service=service,
         namespace=namespace,
@@ -208,15 +217,14 @@
     system_paasta_config: SystemPaastaConfig,
 ) -> Dict[str, Dict[str, int]]:
     """Loads smartstack replication from a host with the specified attribute
 
     :param attribute: a Mesos attribute
     :param service: A service name, like 'example_service'
     :param namespace: A particular smartstack namespace to inspect, like 'main'
-    :param constraints: A list of Marathon constraints to restrict which synapse hosts to query
     :param blacklist: A list of blacklisted location tuples in the form of (location, value)
     :param system_paasta_config: A SystemPaastaConfig object representing the system configuration.
     :returns: a dictionary of the form {'<unique_attribute_value>': <smartstack replication hash>}
               (the dictionary will contain keys for unique all attribute values)
     """
     replication_info = {}
     filtered_slaves = mesos_tools.get_all_slaves_for_blacklist_whitelist(
@@ -247,15 +255,15 @@
 
 def get_replication_for_all_services(
     synapse_host: str, synapse_port: int, synapse_haproxy_url_format: str
 ) -> Dict[str, int]:
     """Returns the replication level for all services known to this synapse haproxy
 
     :param synapse_host: The host that this check should contact for replication information.
-    :param synapse_port: The port number that this check should contact for replication information.
+    :param synapse_port: The port that this check should contact for replication information.
     :param synapse_haproxy_url_format: The format of the synapse haproxy URL.
     :returns available_instance_counts: A dictionary mapping the service names
                                         to an integer number of available replicas.
     """
     backends = get_multiple_backends(
         services=None,
         synapse_host=synapse_host,
@@ -273,15 +281,15 @@
 ) -> Dict[str, int]:
     """Returns the replication level for the provided services
 
     This check is intended to be used with an haproxy load balancer, and
     relies on the implementation details of that choice.
 
     :param synapse_host: The host that this check should contact for replication information.
-    :param synapse_port: The port number that this check should contact for replication information.
+    :param synapse_port: The port that this check should contact for replication information.
     :param synapse_haproxy_url_format: The format of the synapse haproxy URL.
     :param services: A list of strings that are the service names
                           that should be checked for replication.
 
     :returns available_instance_counts: A dictionary mapping the service names
                                   to an integer number of available
                                   replicas
@@ -327,46 +335,14 @@
     # the one *not* in the list is the hostname
     hostname = parts.difference(ip_ports).pop()
 
     ip, port = ip_ports.pop().split(":")
     return ip, int(port), hostname
 
 
-def get_registered_marathon_tasks(
-    synapse_host: str,
-    synapse_port: int,
-    synapse_haproxy_url_format: str,
-    service: str,
-    marathon_tasks: Iterable[marathon_tools.MarathonTask],
-) -> List[marathon_tools.MarathonTask]:
-    """Returns the marathon tasks that are registered in haproxy under a given service (nerve_ns).
-
-    :param synapse_host: The host that this check should contact for replication information.
-    :param synapse_port: The port that this check should contact for replication information.
-    :param synapse_haproxy_url_format: The format of the synapse haproxy URL.
-    :param service: A list of strings that are the service names that should be checked for replication.
-    :param marathon_tasks: A list of MarathonTask objects, whose tasks we will check for in the HAProxy status.
-    """
-    backends = get_multiple_backends(
-        [service],
-        synapse_host=synapse_host,
-        synapse_port=synapse_port,
-        synapse_haproxy_url_format=synapse_haproxy_url_format,
-    )
-    healthy_tasks = []
-    for backend, task in match_backends_and_tasks(backends, marathon_tasks):
-        if (
-            backend is not None
-            and task is not None
-            and backend["status"].startswith("UP")
-        ):
-            healthy_tasks.append(task)
-    return healthy_tasks
-
-
 def are_services_up_on_ip_port(
     synapse_host: str,
     synapse_port: int,
     synapse_haproxy_url_format: str,
     services: Collection[str],
     host_ip: str,
     host_port: int,
@@ -393,50 +369,14 @@
     for service in services:
         for be in backends_on_ip:
             if be["pxname"] == service and backend_is_up(be):
                 services_with_atleast_one_backend_up[service] = True
     return all(services_with_atleast_one_backend_up.values())
 
 
-def match_backends_and_tasks(
-    backends: Iterable[HaproxyBackend], tasks: Iterable[marathon_tools.MarathonTask]
-) -> List[Tuple[Optional[HaproxyBackend], Optional[marathon_tools.MarathonTask]]]:
-    """Returns tuples of matching (backend, task) pairs, as matched by IP and port. Each backend will be listed exactly
-    once, and each task will be listed once per port. If a backend does not match with a task, (backend, None) will
-    be included. If a task's port does not match with any backends, (None, task) will be included.
-
-    :param backends: An iterable of haproxy backend dictionaries, e.g. the list returned by
-                     smartstack_tools.get_multiple_backends.
-    :param tasks: An iterable of MarathonTask objects.
-    """
-
-    # { (ip, port) : [backend1, backend2], ... }
-    backends_by_ip_port: DefaultDict[
-        Tuple[str, int], List[HaproxyBackend]
-    ] = collections.defaultdict(list)
-    backend_task_pairs = []
-
-    for backend in backends:
-        ip, port, _ = ip_port_hostname_from_svname(backend["svname"])
-        backends_by_ip_port[ip, port].append(backend)
-
-    for task in tasks:
-        ip = socket.gethostbyname(task.host)
-        for port in task.ports:
-            for backend in backends_by_ip_port.pop((ip, port), [None]):
-                backend_task_pairs.append((backend, task))
-
-    # we've been popping in the above loop, so anything left didn't match a marathon task.
-    for backends in backends_by_ip_port.values():
-        for backend in backends:
-            backend_task_pairs.append((backend, None))
-
-    return backend_task_pairs
-
-
 def match_backends_and_pods(
     backends: Iterable[HaproxyBackend], pods: Iterable[V1Pod]
 ) -> List[Tuple[Optional[HaproxyBackend], Optional[V1Pod]]]:
     """Returns tuples of matching (backend, pod) pairs, as matched by IP. Each backend will be listed exactly
     once. If a backend does not match with a pod, (backend, None) will be included.
     If a pod's IP does not match with any backends, (None, pod) will be included.
 
@@ -469,169 +409,268 @@
 
 
 _MesosSlaveDict = TypeVar(
     "_MesosSlaveDict", bound=Dict
 )  # no type has been defined in mesos_tools for these yet.
 
 
-class SmartstackHost(NamedTuple):
+class DiscoveredHost(NamedTuple):
     hostname: str
     pool: str
 
 
-class SmartstackReplicationChecker(abc.ABC):
-    """Base class for checking smartstack replication. Extendable for different frameworks.
+class ServiceDiscoveryProvider(abc.ABC):
 
-    Optimized for multiple queries. Gets the list of backends from synapse-`roxy
-    only once per location and reuse it in all subsequent calls of
-    SmartstackReplicationChecker.get_replication_for_instance().
+    NAME = "..."
 
-    get_allowed_locations_and_hosts must be implemented in sub class
-    """
+    @abc.abstractmethod
+    def get_replication_for_all_services(self, hostname: str) -> Dict[str, int]:
+        ...
+
+
+class SmartstackServiceDiscovery(ServiceDiscoveryProvider):
+
+    NAME = "Smartstack"
 
     def __init__(self, system_paasta_config: SystemPaastaConfig) -> None:
         self._synapse_port = system_paasta_config.get_synapse_port()
         self._synapse_haproxy_url_format = (
             system_paasta_config.get_synapse_haproxy_url_format()
         )
+
+    def get_replication_for_all_services(self, hostname: str) -> Dict[str, int]:
+        return get_replication_for_all_services(
+            synapse_host=hostname,
+            synapse_port=self._synapse_port,
+            synapse_haproxy_url_format=self._synapse_haproxy_url_format,
+        )
+
+
+class EnvoyServiceDiscovery(ServiceDiscoveryProvider):
+
+    NAME = "Envoy"
+
+    def __init__(self, system_paasta_config: SystemPaastaConfig) -> None:
+        self._envoy_admin_port = system_paasta_config.get_envoy_admin_port()
+        self._envoy_admin_endpoint_format = (
+            system_paasta_config.get_envoy_admin_endpoint_format()
+        )
+
+    def get_replication_for_all_services(self, hostname: str) -> Dict[str, int]:
+        return envoy_tools.get_replication_for_all_services(
+            envoy_host=hostname,
+            envoy_admin_port=self._envoy_admin_port,
+            envoy_admin_endpoint_format=self._envoy_admin_endpoint_format,
+        )
+
+
+def get_service_discovery_providers(
+    system_paasta_config: SystemPaastaConfig,
+) -> List[ServiceDiscoveryProvider]:
+    providers: List[ServiceDiscoveryProvider] = []
+    for name, _ in system_paasta_config.get_service_discovery_providers().items():
+        if name == "smartstack":
+            providers.append(SmartstackServiceDiscovery(system_paasta_config))
+        elif name == "envoy":
+            providers.append(EnvoyServiceDiscovery(system_paasta_config))
+        else:
+            log.warn("unknown provider")
+    return providers
+
+
+class BaseReplicationChecker(ReplicationChecker):
+    """Base class for checking replication. Extendable for different frameworks.
+
+    Optimized for multiple queries. Gets the list of backends from service
+    discovery provider only once per location and reuse it in all subsequent
+    calls of BaseReplicationChecker.get_replication_for_instance().
+
+    get_allowed_locations_and_hosts must be implemented in sub class
+
+    A list of service discovery providers to collect information about
+    instances and their status must be provided as
+    `service_discovery_providers`.
+    """
+
+    def __init__(
+        self,
+        system_paasta_config: SystemPaastaConfig,
+        service_discovery_providers: Iterable[ServiceDiscoveryProvider],
+    ) -> None:
         self._system_paasta_config = system_paasta_config
-        self._cache: Dict[str, Dict[str, int]] = {}
+        self._cache: Dict[Tuple[str, str], Dict[str, int]] = {}
+        self._service_discovery_providers = service_discovery_providers
 
     @abc.abstractmethod
     def get_allowed_locations_and_hosts(
-        self, instance_config: InstanceConfig
-    ) -> Dict[str, Sequence[SmartstackHost]]:
-        pass
+        self, instance_config: LongRunningServiceConfig
+    ) -> Dict[str, Sequence[DiscoveredHost]]:
+        ...
 
     def get_replication_for_instance(
-        self, instance_config: InstanceConfig
-    ) -> Dict[str, Dict[str, int]]:
-        """Returns the number of registered instances in each discoverable location.
+        self, instance_config: LongRunningServiceConfig
+    ) -> Dict[str, Dict[str, Dict[str, int]]]:
+        """Returns the number of registered instances in each discoverable
+        location for each service dicrovery provider.
 
-        :param instance_config: An instance of MarathonServiceConfig.
-        :returns: a dict {'location_type': {'service.instance': int}}
+        :param instance_config: An instance of LongRunningServiceConfig.
+        :returns: a dict {'service_discovery_provider': {'location_type': {'service.instance': int}}}
         """
-        replication_info = {}
-        attribute_host_dict = self.get_allowed_locations_and_hosts(instance_config)
-        instance_pool = instance_config.get_pool()
-        for location, hosts in attribute_host_dict.items():
-            hostname = self.get_first_host_in_pool(hosts, instance_pool)
-            replication_info[location] = self._get_replication_info(
-                location, hostname, instance_config
-            )
-        return replication_info
+        replication_infos = {}
+        for provider in self._service_discovery_providers:
+            replication_info = {}
+            attribute_host_dict = self.get_allowed_locations_and_hosts(instance_config)
+            instance_pool = instance_config.get_pool()
+            for location, hosts in attribute_host_dict.items():
+                # Try to get information from all available hosts in the pool before giving up
+                hostnames = self.get_hostnames_in_pool(hosts, instance_pool)
+                for hostname in hostnames:
+                    try:
+                        replication_info[location] = self._get_replication_info(
+                            location, hostname, instance_config, provider
+                        )
+                        break
+                    except Exception as e:
+                        log.warn(
+                            f"Error while getting replication info for {location} from {hostname}: {e}"
+                        )
+                        if hostname == hostnames[-1]:
+                            # Last hostname failed, giving up
+                            raise
+            replication_infos[provider.NAME] = replication_info
+        return replication_infos
 
-    def get_first_host_in_pool(self, hosts: Sequence[SmartstackHost], pool: str) -> str:
+    def get_first_host_in_pool(self, hosts: Sequence[DiscoveredHost], pool: str) -> str:
         for host in hosts:
             if host.pool == pool:
                 return host.hostname
         return hosts[0].hostname
 
+    def get_hostname_in_pool(self, hosts: Sequence[DiscoveredHost], pool: str) -> str:
+        return random.choice(self.get_hostnames_in_pool(hosts, pool))
+
+    def get_hostnames_in_pool(
+        self, hosts: Sequence[DiscoveredHost], pool: str
+    ) -> Sequence[str]:
+        hostnames = []
+        for host in hosts:
+            if host.pool == pool:
+                hostnames.append(host.hostname)
+        if len(hostnames) == 0:
+            hostnames.append(hosts[0].hostname)
+        return hostnames
+
     def _get_replication_info(
-        self, location: str, hostname: str, instance_config: InstanceConfig
+        self,
+        location: str,
+        hostname: str,
+        instance_config: LongRunningServiceConfig,
+        provider: ServiceDiscoveryProvider,
     ) -> Dict[str, int]:
         """Returns service.instance and the number of instances registered in smartstack
         at the location as a dict.
 
         :param location: A string that identifies a habitat, a region and etc.
         :param hostname: A mesos slave hostname to read replication information from.
-        :param instance_config: An instance of MarathonServiceConfig.
+        :param instance_config: An instance of LongRunningServiceConfig.
         :returns: A dict {"service.instance": number_of_instances}.
         """
         full_name = compose_job_id(instance_config.service, instance_config.instance)
-        if location not in self._cache:
-            self._cache[location] = get_replication_for_all_services(
-                synapse_host=hostname,
-                synapse_port=self._synapse_port,
-                synapse_haproxy_url_format=self._synapse_haproxy_url_format,
-            )
-        return {full_name: self._cache[location][full_name]}
-
-
-class MesosSmartstackReplicationChecker(SmartstackReplicationChecker):
-    """Retrieves the number of registered instances in each discoverable location.
-
-    Based on SmartstackReplicationChecker takes mesos slaves as an argument to filter
-    which services are allowed to run where.
-    :Example:
-
-    >>> from paasta_tools.mesos_tools import get_slaves
-    >>> from paasta_tools.utils import load_system_paasta_config
-    >>> from paasta_tools.marathon_tools import load_marathon_service_config
-    >>> from paasta_tools.smartstack_tools import SmartstackReplicationChecker
-    >>>
-    >>> mesos_slaves = get_slaves()
-    >>> system_paasta_config = load_system_paasta_config()
-    >>> instance_config = load_marathon_service_config(service='fake_service',
-    ...                       instance='fake_instance', cluster='norcal-stagef')
-    >>>
-    >>> c = SmartstackReplicationChecker(mesos_slaves, system_paasta_config)
-    >>> c.get_replication_for_instance(instance_config)
-    {'uswest1-stagef': {'fake_service.fake_instance': 2}}
-    >>>
-    """
+        key = (location, provider.NAME)
+        replication_info = self._cache.get(key)
+        if replication_info is None:
+            replication_info = provider.get_replication_for_all_services(hostname)
+            self._cache[key] = replication_info
+        return {full_name: replication_info[full_name]}
 
-    def __init__(
-        self,
-        mesos_slaves: List[_MesosSlaveDict],
-        system_paasta_config: SystemPaastaConfig,
-    ) -> None:
-        self._mesos_slaves = mesos_slaves
-        super().__init__(system_paasta_config=system_paasta_config)
-
-    def get_allowed_locations_and_hosts(
-        self, instance_config: InstanceConfig
-    ) -> Dict[str, Sequence[SmartstackHost]]:
-        """Returns a dict of locations and lists of corresponding mesos slaves
-        where deployment of the instance is allowed.
 
-        :param instance_config: An instance of MarathonServiceConfig
-        :returns: A dict {"uswest1-prod": [SmartstackHost(), SmartstackHost(), ...]}
-        """
-        discover_location_type = marathon_tools.load_service_namespace_config(
-            service=instance_config.service,
-            namespace=instance_config.instance,
-            soa_dir=instance_config.soa_dir,
-        ).get_discover()
-        attribute_to_slaves = mesos_tools.get_mesos_slaves_grouped_by_attribute(
-            slaves=self._mesos_slaves, attribute=discover_location_type
-        )
-        ret: Dict[str, Sequence[SmartstackHost]] = {}
-        for attr, slaves in attribute_to_slaves.items():
-            ret[attr] = [
-                SmartstackHost(
-                    hostname=slave["hostname"], pool=slave["attributes"]["pool"]
-                )
-                for slave in slaves
-            ]
-        return ret
-
-
-class KubeSmartstackReplicationChecker(SmartstackReplicationChecker):
+class KubeSmartstackEnvoyReplicationChecker(BaseReplicationChecker):
     def __init__(
         self, nodes: Sequence[V1Node], system_paasta_config: SystemPaastaConfig
     ) -> None:
         self.nodes = nodes
-        super().__init__(system_paasta_config=system_paasta_config)
+        super().__init__(
+            system_paasta_config=system_paasta_config,
+            service_discovery_providers=get_service_discovery_providers(
+                system_paasta_config
+            ),
+        )
 
     def get_allowed_locations_and_hosts(
-        self, instance_config: InstanceConfig
-    ) -> Dict[str, Sequence[SmartstackHost]]:
+        self, instance_config: LongRunningServiceConfig
+    ) -> Dict[str, Sequence[DiscoveredHost]]:
         discover_location_type = kubernetes_tools.load_service_namespace_config(
             service=instance_config.service,
-            namespace=instance_config.instance,
+            namespace=instance_config.get_nerve_namespace(),
             soa_dir=instance_config.soa_dir,
         ).get_discover()
 
         attribute_to_nodes = kubernetes_tools.get_nodes_grouped_by_attribute(
             nodes=self.nodes, attribute=discover_location_type
         )
-        ret: Dict[str, Sequence[SmartstackHost]] = {}
+        ret: Dict[str, Sequence[DiscoveredHost]] = {}
         for attr, nodes in attribute_to_nodes.items():
             ret[attr] = [
-                SmartstackHost(
+                DiscoveredHost(
                     hostname=node.metadata.labels["yelp.com/hostname"],
                     pool=node.metadata.labels["yelp.com/pool"],
                 )
                 for node in nodes
             ]
         return ret
+
+
+def build_smartstack_location_dict(
+    location: str,
+    matched_backends_and_tasks: List[
+        Tuple[
+            Optional[HaproxyBackend],
+            Optional[V1Pod],
+        ]
+    ],
+    should_return_individual_backends: bool = False,
+) -> MutableMapping[str, Any]:
+    running_backends_count = 0
+    backends = []
+    for backend, task in matched_backends_and_tasks:
+        if backend is None:
+            continue
+        if backend_is_up(backend):
+            running_backends_count += 1
+        if should_return_individual_backends:
+            backends.append(build_smartstack_backend_dict(backend, task))
+
+    return {
+        "name": location,
+        "running_backends_count": running_backends_count,
+        "backends": backends,
+    }
+
+
+def build_smartstack_backend_dict(
+    smartstack_backend: HaproxyBackend,
+    task: Union[V1Pod],
+) -> MutableMapping[str, Any]:
+    svname = smartstack_backend["svname"]
+    if isinstance(task, V1Pod):
+        node_hostname = svname.split("_")[0]
+        pod_ip = svname.split("_")[1].split(":")[0]
+        hostname = f"{node_hostname}:{pod_ip}"
+    else:
+        hostname = svname.split("_")[0]
+    port = svname.split("_")[-1].split(":")[-1]
+
+    smartstack_backend_dict = {
+        "hostname": hostname,
+        "port": int(port),
+        "status": smartstack_backend["status"],
+        "check_status": smartstack_backend["check_status"],
+        "check_code": smartstack_backend["check_code"],
+        "last_change": int(smartstack_backend["lastchg"]),
+        "has_associated_task": task is not None,
+    }
+
+    check_duration = smartstack_backend["check_duration"]
+    if check_duration:
+        smartstack_backend_dict["check_duration"] = int(check_duration)
+
+    return smartstack_backend_dict
```

### Comparing `paasta-tools-0.92.1/paasta_tools/drain_lib.py` & `paasta-tools-1.0.0/paasta_tools/drain_lib.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,16 +15,16 @@
 import time
 from typing import Any
 from typing import Awaitable
 from typing import Callable
 from typing import Dict
 from typing import List
 from typing import Optional
+from typing import Sequence
 from typing import Set
-from typing import Tuple
 from typing import Type
 from typing import TypeVar
 
 import aiohttp
 from mypy_extensions import TypedDict
 
 from paasta_tools.hacheck import get_spool
@@ -197,15 +197,15 @@
                 "registration": registration,
             }
             for registration in self.registrations
         ]
 
     async def for_each_registration(
         self, task: DrainTask, func: Callable[..., Awaitable[T]]
-    ) -> Tuple[T, ...]:
+    ) -> Sequence[T]:
         if task.ports == []:
             return None
         futures = [func(url) for url in self.spool_urls(task)]
         return await asyncio.gather(*futures)
 
     async def drain(self, task: DrainTask) -> None:
         await self.for_each_registration(task, self.down)
```

### Comparing `paasta-tools-0.92.1/paasta_tools/check_spark_jobs.py` & `paasta-tools-1.0.0/paasta_tools/check_spark_jobs.py`

 * *Files 2% similar despite different names*

```diff
@@ -162,15 +162,16 @@
 
 def email_user(framework_info, email_domain):
     guessed_user = None
     if framework_info["user"] != "root":
         guessed_user = framework_info["user"]
     elif framework_info["name"].startswith(JUPYTER_PREFIX):
         try:
-            guessed_user = framework_info["name"].split("_")[-2]
+            # the job format is now `<AppName>_<UserName>_<UIPort>_<StartTime>`
+            guessed_user = framework_info["name"].split("_")[-3]
         except IndexError:
             pass
 
     if guessed_user:
         print(
             f'Guessed {framework_info["name"]} belongs to {guessed_user}, sending email'
         )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/firewall_logging.py` & `paasta-tools-1.0.0/paasta_tools/firewall_logging.py`

 * *Files identical despite different names*

### Comparing `paasta-tools-0.92.1/paasta_tools/metrics/metastatus_lib.py` & `paasta-tools-1.0.0/paasta_tools/metrics/metastatus_lib.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,56 +11,60 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import copy
 import itertools
 import math
+import re
 from collections import Counter
 from collections import namedtuple
 from collections import OrderedDict
 from typing import Any
 from typing import Callable
 from typing import Mapping
 from typing import NamedTuple
 from typing import Sequence
 from typing import Tuple
 from typing import TypeVar
 
 import a_sync
 from humanize import naturalsize
 from kubernetes.client import V1Node
+from kubernetes.client import V1Pod
 from mypy_extensions import TypedDict
 from typing_extensions import Counter as _Counter
 
-from paasta_tools.kubernetes_tools import get_all_nodes
-from paasta_tools.kubernetes_tools import get_all_pods
+from paasta_tools.kubernetes_tools import get_all_nodes_cached
+from paasta_tools.kubernetes_tools import get_all_pods_cached
 from paasta_tools.kubernetes_tools import get_pod_status
 from paasta_tools.kubernetes_tools import is_node_ready
 from paasta_tools.kubernetes_tools import KubeClient
 from paasta_tools.kubernetes_tools import list_all_deployments
-from paasta_tools.kubernetes_tools import maybe_add_yelp_prefix
+from paasta_tools.kubernetes_tools import paasta_prefixed
 from paasta_tools.kubernetes_tools import PodStatus
-from paasta_tools.marathon_tools import get_all_marathon_apps
-from paasta_tools.marathon_tools import MarathonClient
 from paasta_tools.mesos.master import MesosMetrics
 from paasta_tools.mesos.master import MesosState
 from paasta_tools.mesos_maintenance import MAINTENANCE_ROLE
 from paasta_tools.mesos_tools import get_all_tasks_from_state
 from paasta_tools.mesos_tools import get_mesos_quorum
 from paasta_tools.mesos_tools import get_number_of_mesos_masters
 from paasta_tools.mesos_tools import get_zookeeper_host_path
 from paasta_tools.mesos_tools import is_task_terminal
 from paasta_tools.mesos_tools import MesosResources
 from paasta_tools.mesos_tools import MesosTask
-from paasta_tools.utils import paasta_print
 from paasta_tools.utils import PaastaColors
 from paasta_tools.utils import print_with_indent
 
 
+DEFAULT_KUBERNETES_CPU_REQUEST = "100m"
+DEFAULT_KUBERNETES_MEMORY_REQUEST = "200M"
+DEFAULT_KUBERNETES_DISK_REQUEST = "0"
+
+
 class ResourceInfo(namedtuple("ResourceInfo", ["cpus", "mem", "disk", "gpus"])):
     def __new__(cls, cpus, mem, disk, gpus=0):
         return super().__new__(cls, cpus, mem, disk, gpus)
 
 
 class HealthCheckResult(NamedTuple):
     message: str
@@ -70,15 +74,15 @@
 class ResourceUtilization(NamedTuple):
     metric: str
     total: int
     free: int
 
 
 def get_num_masters() -> int:
-    """ Gets the number of masters from mesos state """
+    """Gets the number of masters from mesos state"""
     zookeeper_host_path = get_zookeeper_host_path()
     return get_number_of_mesos_masters(
         zookeeper_host_path.host, zookeeper_host_path.path
     )
 
 
 def get_mesos_cpu_status(
@@ -97,23 +101,25 @@
     for slave in mesos_state["slaves"]:
         used += reserved_maintenence_resources(slave["reserved_resources"])["cpus"]
 
     available = total - used
     return total, used, available
 
 
-def get_kube_cpu_status(nodes: Sequence[V1Node],) -> Tuple[int, int, int]:
+def get_kube_cpu_status(
+    nodes: Sequence[V1Node],
+) -> Tuple[float, float, float]:
     """Takes in the list of Kubernetes nodes and analyzes them, returning the status.
 
     :param nodes: list of Kubernetes nodes.
     :returns: Tuple of total, used, and available CPUs.
     """
 
-    total = 0
-    available = 0
+    total = 0.0
+    available = 0.0
     for node in nodes:
         available += suffixed_number_value(node.status.allocatable["cpu"])
         total += suffixed_number_value(node.status.capacity["cpu"])
 
     used = total - available
     return total, used, available
 
@@ -134,22 +140,24 @@
         used += reserved_maintenence_resources(slave["reserved_resources"])["mem"]
 
     available = total - used
 
     return total, used, available
 
 
-def get_kube_memory_status(nodes: Sequence[V1Node],) -> Tuple[int, int, int]:
+def get_kube_memory_status(
+    nodes: Sequence[V1Node],
+) -> Tuple[float, float, float]:
     """Takes in the list of Kubernetes nodes and analyzes them, returning the status.
 
     :param nodes: list of Kubernetes nodes.
     :returns: Tuple of total, used, and available memory in Mi.
     """
-    total = 0
-    available = 0
+    total = 0.0
+    available = 0.0
     for node in nodes:
         available += suffixed_number_value(node.status.allocatable["memory"])
         total += suffixed_number_value(node.status.capacity["memory"])
 
     total //= 1024 * 1024
     available //= 1024 * 1024
     used = total - available
@@ -172,23 +180,25 @@
     for slave in mesos_state["slaves"]:
         used += reserved_maintenence_resources(slave["reserved_resources"])["disk"]
 
     available = total - used
     return total, used, available
 
 
-def get_kube_disk_status(nodes: Sequence[V1Node],) -> Tuple[int, int, int]:
+def get_kube_disk_status(
+    nodes: Sequence[V1Node],
+) -> Tuple[float, float, float]:
     """Takes in the list of Kubernetes nodes and analyzes them, returning the status.
 
     :param nodes: list of Kubernetes nodes.
     :returns: Tuple of total, used, and available disk space in Mi.
     """
 
-    total = 0
-    available = 0
+    total = 0.0
+    available = 0.0
     for node in nodes:
         available += suffixed_number_value(node.status.allocatable["ephemeral-storage"])
         total += suffixed_number_value(node.status.capacity["ephemeral-storage"])
 
     total //= 1024 * 1024
     available //= 1024 * 1024
     used = total - available
@@ -210,23 +220,25 @@
     for slave in mesos_state["slaves"]:
         used += reserved_maintenence_resources(slave["reserved_resources"])["gpus"]
 
     available = total - used
     return total, used, available
 
 
-def get_kube_gpu_status(nodes: Sequence[V1Node],) -> Tuple[int, int, int]:
+def get_kube_gpu_status(
+    nodes: Sequence[V1Node],
+) -> Tuple[float, float, float]:
     """Takes in the list of Kubernetes nodes and analyzes them, returning the status.
 
     :param nodes: list of Kubernetes nodes.
     :returns: Tuple of total, used, and available GPUs.
     """
 
-    total = 0
-    available = 0
+    total = 0.0
+    available = 0.0
     for node in nodes:
         available += suffixed_number_value(
             node.status.allocatable.get("nvidia.com/gpu", "0")
         )
         total += suffixed_number_value(node.status.capacity.get("nvidia.com/gpu", "0"))
 
     used = total - available
@@ -239,18 +251,56 @@
 
 
 def filter_kube_resources(dictionary: Mapping[str, str]) -> Mapping[str, str]:
     valid_keys = ["cpu", "memory", "ephemeral-storage", "nvidia.com/gpu"]
     return {key: value for (key, value) in dictionary.items() if key in valid_keys}
 
 
+class ResourceParser:
+    @staticmethod
+    def cpus(resources):
+        resources = resources or {}
+        return suffixed_number_value(
+            resources.get("cpu", DEFAULT_KUBERNETES_CPU_REQUEST)
+        )
+
+    @staticmethod
+    def mem(resources):
+        resources = resources or {}
+        return suffixed_number_value(
+            resources.get("memory", DEFAULT_KUBERNETES_MEMORY_REQUEST)
+        )
+
+    @staticmethod
+    def disk(resources):
+        resources = resources or {}
+        return suffixed_number_value(
+            resources.get("ephemeral-storage", DEFAULT_KUBERNETES_DISK_REQUEST)
+        )
+
+
+def allocated_node_resources(pods: Sequence[V1Pod]) -> Mapping[str, float]:
+    cpus = mem = disk = 0
+    for pod in pods:
+        cpus += sum(
+            ResourceParser.cpus(c.resources.requests) for c in pod.spec.containers
+        )
+        mem += sum(
+            ResourceParser.mem(c.resources.requests) for c in pod.spec.containers
+        )
+        disk += sum(
+            ResourceParser.disk(c.resources.requests) for c in pod.spec.containers
+        )
+    return {"cpu": cpus, "memory": mem, "ephemeral-storage": disk}
+
+
 def healthcheck_result_for_resource_utilization(
     resource_utilization: ResourceUtilization, threshold: int
 ) -> HealthCheckResult:
-    """ Given a resource data dict, assert that cpu
+    """Given a resource data dict, assert that cpu
     data is ok.
 
     :param resource_utilization: the resource_utilization tuple to check
     :returns: a HealthCheckResult
     """
     try:
         utilization = percent_used(
@@ -279,15 +329,15 @@
 
 
 def percent_used(total: float, used: float) -> float:
     return round(used / float(total) * 100.0, 2)
 
 
 def assert_cpu_health(
-    cpu_status: Tuple[int, int, int], threshold: int = 10
+    cpu_status: Tuple[float, float, float], threshold: int = 10
 ) -> HealthCheckResult:
     total, used, available = cpu_status
     try:
         perc_used = percent_used(total, used)
     except ZeroDivisionError:
         return HealthCheckResult(
             message="Error reading total available cpu from mesos!", healthy=False
@@ -304,15 +354,15 @@
             message="CRITICAL: Less than %d%% CPUs available. (Currently using %.2f%% of %d)"
             % (threshold, perc_used, total),
             healthy=False,
         )
 
 
 def assert_memory_health(
-    memory_status: Tuple[int, int, int], threshold: int = 10
+    memory_status: Tuple[float, float, float], threshold: int = 10
 ) -> HealthCheckResult:
     total: float
     used: float
     total, used, _ = memory_status
 
     total /= 1024
     used /= 1024
@@ -335,15 +385,15 @@
             message="CRITICAL: Less than %d%% memory available. (Currently using %.2f%% of %.2fGB)"
             % (threshold, perc_used, total),
             healthy=False,
         )
 
 
 def assert_disk_health(
-    disk_status: Tuple[int, int, int], threshold: int = 10
+    disk_status: Tuple[float, float, float], threshold: int = 10
 ) -> HealthCheckResult:
     total: float
     used: float
     total, used, _ = disk_status
 
     total /= 1024
     used /= 1024
@@ -366,15 +416,15 @@
             message="CRITICAL: Less than %d%% disk available. (Currently using %.2f%%)"
             % (threshold, perc_used),
             healthy=False,
         )
 
 
 def assert_gpu_health(
-    gpu_status: Tuple[int, int, int], threshold: int = 0
+    gpu_status: Tuple[float, float, float], threshold: int = 0
 ) -> HealthCheckResult:
     total, used, available = gpu_status
 
     if math.isclose(total, 0):
         # assume that no gpus is healthy since most machines don't have them
         return HealthCheckResult(message="No GPUs found!", healthy=True)
     else:
@@ -391,48 +441,54 @@
         return HealthCheckResult(
             message="CRITICAL: Less than %d%% GPUs available. (Currently using %.2f%% of %d)"
             % (threshold, perc_used, total),
             healthy=False,
         )
 
 
-def assert_mesos_tasks_running(metrics: MesosMetrics,) -> HealthCheckResult:
+def assert_mesos_tasks_running(
+    metrics: MesosMetrics,
+) -> HealthCheckResult:
     running = metrics["master/tasks_running"]
     staging = metrics["master/tasks_staging"]
     starting = metrics["master/tasks_starting"]
     return HealthCheckResult(
         message="Tasks: running: %d staging: %d starting: %d"
         % (running, staging, starting),
         healthy=True,
     )
 
 
-def assert_kube_pods_running(kube_client: KubeClient,) -> HealthCheckResult:
-    statuses = [get_pod_status(pod) for pod in get_all_pods(kube_client)]
+def assert_kube_pods_running(
+    kube_client: KubeClient, namespace: str
+) -> HealthCheckResult:
+    statuses = [
+        get_pod_status(pod) for pod in get_all_pods_cached(kube_client, namespace)
+    ]
     running = statuses.count(PodStatus.RUNNING)
     pending = statuses.count(PodStatus.PENDING)
     failed = statuses.count(PodStatus.FAILED)
     healthy = running > 0
     return HealthCheckResult(
         message=f"Pods: running: {running} pending: {pending} failed: {failed}",
         healthy=healthy,
     )
 
 
 def assert_no_duplicate_frameworks(
-    state: MesosState, framework_list: Sequence[str] = ["marathon"]
+    state: MesosState, framework_list: Sequence[str]
 ) -> HealthCheckResult:
     """A function which asserts that there are no duplicate frameworks running, where
     frameworks are identified by their name.
 
     Note the extra spaces in the output strings: this is to account for the extra indentation
     we add, so we can have:
 
         frameworks:
-          framework: marathon count: 1
+          framework: tron count: 1
 
     :param state: the state info from the Mesos master
     :returns: a tuple containing (output, ok): output is a log of the state of frameworks, ok a boolean
         indicating if there are any duplicate frameworks.
     """
     output = ["Frameworks:"]
     status = True
@@ -468,24 +524,30 @@
     else:
         return HealthCheckResult(
             message="CRITICAL: framework(s) %s not found" % ", ".join(not_found),
             healthy=ok,
         )
 
 
-def get_mesos_slaves_health_status(metrics: MesosMetrics,) -> Tuple[int, int]:
+def get_mesos_slaves_health_status(
+    metrics: MesosMetrics,
+) -> Tuple[int, int]:
     return metrics["master/slaves_active"], metrics["master/slaves_inactive"]
 
 
-def get_kube_nodes_health_status(nodes: Sequence[V1Node],) -> Tuple[int, int]:
+def get_kube_nodes_health_status(
+    nodes: Sequence[V1Node],
+) -> Tuple[int, int]:
     statuses = [is_node_ready(node) for node in nodes]
     return statuses.count(True), statuses.count(False)
 
 
-def assert_nodes_health(nodes_health_status: Tuple[int, int],) -> HealthCheckResult:
+def assert_nodes_health(
+    nodes_health_status: Tuple[int, int],
+) -> HealthCheckResult:
     active, inactive = nodes_health_status
     healthy = active > 0
     return HealthCheckResult(
         message="Nodes: active: %d inactive: %d" % (active, inactive), healthy=healthy
     )
 
 
@@ -519,16 +581,18 @@
 _GenericNodeGroupingFunctionT = Callable[[_GenericNodeT], _KeyFuncRetT]
 
 _GenericNodeFilterFunctionT = Callable[[_GenericNodeT], bool]
 
 _GenericNodeSortFunctionT = Callable[[Sequence[_GenericNodeT]], Sequence[_GenericNodeT]]
 
 
-def key_func_for_attribute(attribute: str,) -> Callable[[_SlaveT], str]:
-    """ Return a closure that given a slave, will return the value of a specific
+def key_func_for_attribute(
+    attribute: str,
+) -> Callable[[_SlaveT], str]:
+    """Return a closure that given a slave, will return the value of a specific
     attribute.
 
     :param attribute: the attribute to inspect in the slave
     :returns: a closure, which takes a slave and returns the value of an attribute
     """
 
     def key_func(slave):
@@ -536,15 +600,15 @@
 
     return key_func
 
 
 def key_func_for_attribute_multi(
     attributes: Sequence[str],
 ) -> _GenericNodeGroupingFunctionT:
-    """ Return a closure that given a slave, will return the value of a list of
+    """Return a closure that given a slave, will return the value of a list of
     attributes, compiled into a hashable tuple
 
     :param attributes: the attributes to inspect in the slave
     :returns: a closure, which takes a slave and returns the value of those attributes
     """
 
     def get_attribute(slave, attribute):
@@ -558,45 +622,47 @@
 
     return key_func
 
 
 def key_func_for_attribute_multi_kube(
     attributes: Sequence[str],
 ) -> Callable[[V1Node], _KeyFuncRetT]:
-    """ Return a closure that given a node, will return the value of a list of
+    """Return a closure that given a node, will return the value of a list of
     attributes, compiled into a hashable tuple
 
     :param attributes: the attributes to inspect in the slave
     :returns: a closure, which takes a node and returns the value of those attributes
     """
 
     def get_attribute(node, attribute):
-        return node.metadata.labels.get(maybe_add_yelp_prefix(attribute), "unknown")
+        return node.metadata.labels.get(paasta_prefixed(attribute), "unknown")
 
     def key_func(node):
         return tuple((a, get_attribute(node, a)) for a in attributes)
 
     return key_func
 
 
-def sort_func_for_attributes(attributes: Sequence[str],) -> _GenericNodeSortFunctionT:
+def sort_func_for_attributes(
+    attributes: Sequence[str],
+) -> _GenericNodeSortFunctionT:
     def sort(slaves):
         for attribute in attributes:
             slaves = sorted(slaves, key=key_func_for_attribute(attribute))
         return slaves
 
     return sort
 
 
 def group_slaves_by_key_func(
     key_func: _GenericNodeGroupingFunctionT,
     slaves: Sequence[_GenericNodeT],
     sort_func: _GenericNodeSortFunctionT = None,
 ) -> Mapping[_KeyFuncRetT, Sequence[_GenericNodeT]]:
-    """ Given a function for grouping slaves, return a
+    """Given a function for grouping slaves, return a
     dict where keys are the unique values returned by
     the key_func and the values are all those slaves which
     have that specific value.
 
     :param key_func: a function which consumes a slave and returns a value
     :param slaves: a list of slaves
     :returns: a dict of key: [slaves]
@@ -615,15 +681,15 @@
     total: ResourceInfo
     slave_count: int
 
 
 def calculate_resource_utilization_for_slaves(
     slaves: Sequence[_SlaveT], tasks: Sequence[MesosTask]
 ) -> ResourceUtilizationDict:
-    """ Given a list of slaves and a list of tasks, calculate the total available
+    """Given a list of slaves and a list of tasks, calculate the total available
     resource available in that list of slaves, and the resources consumed by tasks
     running on those slaves.
 
     :param slaves: a list of slaves to calculate resource usage for
     :param tasks: the list of tasks running in the mesos cluster
     :returns: a dict, containing keys for "free" and "total" resources. Each of these keys
     is a ResourceInfo tuple, exposing a number for cpu, disk and mem.
@@ -655,77 +721,94 @@
             gpus=resource_total_dict.get("gpus", 0),
         ),
         "slave_count": len(slaves),
     }
 
 
 _IEC_NUMBER_SUFFIXES = {
+    "k": 1000,
+    "m": 1000**-1,
+    "M": 1000**2,
+    "G": 1000**3,
+    "T": 1000**4,
+    "P": 1000**5,
     "Ki": 1024,
-    "Mi": 1024 ** 2,
-    "Gi": 1024 ** 3,
-    "Ti": 1024 ** 4,
-    "Pi": 1024 ** 5,
+    "Mi": 1024**2,
+    "Gi": 1024**3,
+    "Ti": 1024**4,
+    "Pi": 1024**5,
 }
 
 
-def suffixed_number_value(s: str) -> int:
-    suff = s[-2:]
+def suffixed_number_value(s: str) -> float:
+    pattern = r"(?P<number>\d+)(?P<suff>\w*)"
+    match = re.match(pattern, s)
+    number, suff = match.groups()
+
     if suff in _IEC_NUMBER_SUFFIXES:
-        return int(s[:-2]) * _IEC_NUMBER_SUFFIXES[suff]
+        return float(number) * _IEC_NUMBER_SUFFIXES[suff]
     else:
-        return int(s)
+        return float(number)
 
 
-def suffixed_number_dict_values(d: Mapping[Any, str]) -> Mapping[Any, int]:
+def suffixed_number_dict_values(d: Mapping[Any, str]) -> Mapping[Any, float]:
     return {k: suffixed_number_value(v) for k, v in d.items()}
 
 
 def calculate_resource_utilization_for_kube_nodes(
     nodes: Sequence[V1Node],
+    pods_by_node: Mapping[str, Sequence[V1Pod]],
 ) -> ResourceUtilizationDict:
-    """ Given a list of Kubernetes nodes, calculate the total available
+    """Given a list of Kubernetes nodes, calculate the total available
     resource available and the resources consumed in that list of nodes.
 
     :param nodes: a list of Kubernetes nodes to calculate resource usage for
     :returns: a dict, containing keys for "free" and "total" resources. Each of these keys
     is a ResourceInfo tuple, exposing a number for cpu, disk and mem.
     """
     resource_total_dict: _Counter[str] = Counter()
-    for node in nodes:
-        filtered_resources = filter_kube_resources(node.status.capacity)
-        resource_total_dict.update(
-            Counter(suffixed_number_dict_values(filtered_resources))
-        )
     resource_free_dict: _Counter[str] = Counter()
     for node in nodes:
-        filtered_resources = filter_kube_resources(node.status.allocatable)
+        allocatable_resources = suffixed_number_dict_values(
+            filter_kube_resources(node.status.allocatable)
+        )
+        resource_total_dict.update(Counter(allocatable_resources))
+        allocated_resources = allocated_node_resources(pods_by_node[node.metadata.name])
         resource_free_dict.update(
-            Counter(suffixed_number_dict_values(filtered_resources))
+            Counter(
+                {
+                    "cpu": allocatable_resources["cpu"] - allocated_resources["cpu"],
+                    "ephemeral-storage": allocatable_resources["ephemeral-storage"]
+                    - allocated_resources["ephemeral-storage"],
+                    "memory": allocatable_resources["memory"]
+                    - allocated_resources["memory"],
+                }
+            )
         )
     return {
         "free": ResourceInfo(
             cpus=resource_free_dict["cpu"],
-            disk=resource_free_dict["ephemeral-storage"] / (1024 ** 2),
-            mem=resource_free_dict["memory"] / (1024 ** 2),
+            disk=resource_free_dict["ephemeral-storage"] / (1024**2),
+            mem=resource_free_dict["memory"] / (1024**2),
             gpus=resource_free_dict.get("nvidia.com/gpu", 0),
         ),
         "total": ResourceInfo(
             cpus=resource_total_dict["cpu"],
-            disk=resource_total_dict["ephemeral-storage"] / (1024 ** 2),
-            mem=resource_total_dict["memory"] / (1024 ** 2),
+            disk=resource_total_dict["ephemeral-storage"] / (1024**2),
+            mem=resource_total_dict["memory"] / (1024**2),
             gpus=resource_total_dict.get("nvidia.com/gpu", 0),
         ),
         "slave_count": len(nodes),
     }
 
 
 def filter_tasks_for_slaves(
     slaves: Sequence[_SlaveT], tasks: Sequence[MesosTask]
 ) -> Sequence[MesosTask]:
-    """ Given a list of slaves and a list of tasks, return a filtered
+    """Given a list of slaves and a list of tasks, return a filtered
     list of tasks, where those returned belong to slaves in the list of
     slaves
 
     :param slaves: the list of slaves which the tasks provided should be
     running on.
     :param tasks: the tasks to filter :returns: a list of tasks,
     identical to that provided by the tasks param, but with only those where
@@ -743,15 +826,15 @@
 
     return filter_func
 
 
 def filter_slaves(
     slaves: Sequence[_GenericNodeT], filters: Sequence[_GenericNodeFilterFunctionT]
 ) -> Sequence[_GenericNodeT]:
-    """ Filter slaves by attributes
+    """Filter slaves by attributes
 
     :param slaves: list of slaves to filter
     :param filters: list of functions that take a slave and return whether the
     slave should be included
     :returns: list of slaves that return true for all the filters
     """
     if filters is None:
@@ -761,15 +844,15 @@
 
 def get_resource_utilization_by_grouping(
     grouping_func: _GenericNodeGroupingFunctionT,
     mesos_state: MesosState,
     filters: Sequence[_GenericNodeFilterFunctionT] = [],
     sort_func: _GenericNodeSortFunctionT = None,
 ) -> Mapping[_KeyFuncRetT, ResourceUtilizationDict]:
-    """ Given a function used to group slaves and mesos state, calculate
+    """Given a function used to group slaves and mesos state, calculate
     resource utilization for each value of a given attribute.
 
     :grouping_func: a function that given a slave, will return the value of an
     attribute to group by.
     :param mesos_state: the mesos state
     :param filters: filters to apply to the slaves in the calculation, with
     filtering preformed by filter_slaves
@@ -795,40 +878,51 @@
         for attribute_value, slaves in slave_groupings.items()
     }
 
 
 def get_resource_utilization_by_grouping_kube(
     grouping_func: _GenericNodeGroupingFunctionT,
     kube_client: KubeClient,
+    *,
+    namespace: str,
     filters: Sequence[_GenericNodeFilterFunctionT] = [],
     sort_func: _GenericNodeSortFunctionT = None,
 ) -> Mapping[_KeyFuncRetT, ResourceUtilizationDict]:
-    """ Given a function used to group nodes, calculate resource utilization
+    """Given a function used to group nodes, calculate resource utilization
     for each value of a given attribute.
 
     :grouping_func: a function that given a node, will return the value of an
     attribute to group by.
     :param kube_client: the Kubernetes client
     :param filters: filters to apply to the nodes in the calculation, with
     filtering preformed by filter_slaves
     :param sort_func: a function that given a list of nodes, will return the
     sorted list of nodes.
     :returns: a dict of {attribute_value: resource_usage}, where resource usage
     is the dict returned by ``calculate_resource_utilization_for_kube_nodes`` for
     nodes grouped by attribute value.
     """
-    nodes: Sequence[V1Node] = get_all_nodes(kube_client)
+    nodes = get_all_nodes_cached(kube_client)
     nodes = filter_slaves(nodes, filters)
     if len(nodes) == 0:
         raise ValueError("There are no nodes registered in the Kubernetes.")
 
     node_groupings = group_slaves_by_key_func(grouping_func, nodes, sort_func)
 
+    pods = get_all_pods_cached(kube_client, namespace)
+
+    pods_by_node = {}
+    for node in nodes:
+        pods_by_node[node.metadata.name] = [
+            pod for pod in pods if pod.spec.node_name == node.metadata.name
+        ]
     return {
-        attribute_value: calculate_resource_utilization_for_kube_nodes(nodes)
+        attribute_value: calculate_resource_utilization_for_kube_nodes(
+            nodes, pods_by_node
+        )
         for attribute_value, nodes in node_groupings.items()
     }
 
 
 def resource_utillizations_from_resource_info(
     total: ResourceInfo, free: ResourceInfo
 ) -> Sequence[ResourceUtilization]:
@@ -841,16 +935,18 @@
     """
     return [
         ResourceUtilization(metric=field, total=total[index], free=free[index])
         for index, field in enumerate(ResourceInfo._fields)
     ]
 
 
-def has_registered_slaves(mesos_state: MesosState,) -> bool:
-    """ Return a boolean indicating if there are any slaves registered
+def has_registered_slaves(
+    mesos_state: MesosState,
+) -> bool:
+    """Return a boolean indicating if there are any slaves registered
     to the master according to the mesos state.
     :param mesos_state: the mesos state from the master
     :returns: a boolean, indicating if there are > 0 slaves
     """
     return len(mesos_state.get("slaves", [])) > 0
 
 
@@ -876,106 +972,71 @@
     kube_client: KubeClient,
 ) -> Sequence[HealthCheckResult]:
     """Perform healthchecks against Kubernetes.
     :param kube_client: the KUbernetes client
     :returns: a list of HealthCheckResult tuples
     """
 
-    nodes = get_all_nodes(kube_client)
+    nodes = get_all_nodes_cached(kube_client)
 
     return [
         assert_cpu_health(get_kube_cpu_status(nodes)),
         assert_memory_health(get_kube_memory_status(nodes)),
         assert_disk_health(get_kube_disk_status(nodes)),
         assert_gpu_health(get_kube_gpu_status(nodes)),
         assert_nodes_health(get_kube_nodes_health_status(nodes)),
     ]
 
 
-def get_mesos_state_status(mesos_state: MesosState,) -> Sequence[HealthCheckResult]:
+def get_mesos_state_status(
+    mesos_state: MesosState,
+) -> Sequence[HealthCheckResult]:
     """Perform healthchecks against mesos state.
     :param mesos_state: a dict exposing the mesos state described in
     https://mesos.apache.org/documentation/latest/endpoints/master/state.json/
     :returns: a list of HealthCheckResult tuples
     """
     return [
         assert_quorum_size(),
-        assert_no_duplicate_frameworks(state=mesos_state, framework_list=["marathon"]),
     ]
 
 
 def run_healthchecks_with_param(
     param: Any,
     healthcheck_functions: Sequence[Callable[..., HealthCheckResult]],
     format_options: Mapping[str, Any] = {},
 ) -> Sequence[HealthCheckResult]:
     return [
         healthcheck(param, **format_options) for healthcheck in healthcheck_functions
     ]
 
 
-def assert_marathon_apps(clients: Sequence[MarathonClient],) -> HealthCheckResult:
-    num_apps = [len(get_all_marathon_apps(c)) for c in clients]
-    if sum(num_apps) < 1:
-        return HealthCheckResult(
-            message="CRITICAL: No marathon apps running", healthy=False
-        )
-    else:
-        return HealthCheckResult(
-            message="marathon apps: %10d" % sum(num_apps), healthy=True
-        )
-
-
-def assert_marathon_tasks(clients: Sequence[MarathonClient],) -> HealthCheckResult:
-    num_tasks = [len(c.list_tasks()) for c in clients]
-    return HealthCheckResult(
-        message="marathon tasks: %9d" % sum(num_tasks), healthy=True
-    )
-
-
-def assert_marathon_deployments(
-    clients: Sequence[MarathonClient],
+def assert_kube_deployments(
+    kube_client: KubeClient, namespace: str
 ) -> HealthCheckResult:
-    num_deployments = [len(c.list_deployments()) for c in clients]
-    return HealthCheckResult(
-        message="marathon deployments: %3d" % sum(num_deployments), healthy=True
-    )
-
-
-def assert_kube_deployments(kube_client: KubeClient,) -> HealthCheckResult:
-    num_deployments = len(list_all_deployments(kube_client))
+    num_deployments = len(list_all_deployments(kube_client, namespace))
     return HealthCheckResult(
         message=f"Kubernetes deployments: {num_deployments:>3}", healthy=True
     )
 
 
-def get_marathon_status(
-    clients: Sequence[MarathonClient],
+def get_kube_status(
+    kube_client: KubeClient, namespace: str
 ) -> Sequence[HealthCheckResult]:
-    """ Gathers information about marathon.
-    :return: string containing the status.  """
-    return run_healthchecks_with_param(
-        clients,
-        [assert_marathon_apps, assert_marathon_tasks, assert_marathon_deployments],
-    )
-
-
-def get_kube_status(kube_client: KubeClient,) -> Sequence[HealthCheckResult]:
     """Gather information about Kubernetes.
     :param kube_client: the KUbernetes client
     :return: string containing the status
     """
     return run_healthchecks_with_param(
-        kube_client, [assert_kube_deployments, assert_kube_pods_running]
+        [kube_client, namespace], [assert_kube_deployments, assert_kube_pods_running]
     )
 
 
 def critical_events_in_outputs(healthcheck_outputs):
-    """Given a list of HealthCheckResults return those which are unhealthy.
-    """
+    """Given a list of HealthCheckResults return those which are unhealthy."""
     return [
         healthcheck
         for healthcheck in healthcheck_outputs
         if healthcheck.healthy is False
     ]
 
 
@@ -994,15 +1055,15 @@
     :param healthcheck_results: a list of HealthCheckResult tuples
     :returns: a list of booleans.
     """
     return [result.healthy for result in healthcheck_results]
 
 
 def print_results_for_healthchecks(summary, ok, results, verbose, indent=2):
-    paasta_print(summary)
+    print(summary)
     if verbose >= 1:
         for health_check_result in results:
             if health_check_result.healthy:
                 print_with_indent(health_check_result.message, indent)
             else:
                 print_with_indent(PaastaColors.red(health_check_result.message), indent)
     elif not ok:
@@ -1080,23 +1141,25 @@
         for pair in healthcheck_utilization_pairs
     ]
 
 
 def get_table_rows_for_resource_info_dict(
     attribute_values, healthcheck_utilization_pairs
 ):
-    """ A wrapper method to join together
+    """A wrapper method to join together
 
     :param attribute: The attribute value and formatted columns to be shown in
     a single row.  :param attribute_value: The value of the attribute
     associated with the row. This becomes index 0 in the array returned.
     :param healthcheck_utilization_pairs: a list of 2-tuples, where each tuple has the elements
     (HealthCheckResult, ResourceUtilization)
     :returns: a list of strings, representing a row in a table to be formatted.
     """
     return attribute_values + format_row_for_resource_utilization_healthchecks(
         healthcheck_utilization_pairs
     )
 
 
-def reserved_maintenence_resources(resources: MesosResources,):
+def reserved_maintenence_resources(
+    resources: MesosResources,
+):
     return resources.get(MAINTENANCE_ROLE, {"cpus": 0, "mem": 0, "disk": 0, "gpus": 0})
```

### Comparing `paasta-tools-0.92.1/paasta_tools/setup_tron_namespace.py` & `paasta-tools-1.0.0/paasta_tools/setup_tron_namespace.py`

 * *Files 6% similar despite different names*

```diff
@@ -20,14 +20,16 @@
 The script will load the service configuration file, generate a Tron configuration
 file for it, and send the updated file to Tron.
 """
 import argparse
 import logging
 import sys
 
+import ruamel.yaml as yaml
+
 from paasta_tools import tron_tools
 from paasta_tools.tron_tools import MASTER_NAMESPACE
 
 log = logging.getLogger(__name__)
 
 
 def parse_args():
@@ -74,16 +76,16 @@
             log.error("Do not pass service names with --all flag")
             sys.exit(1)
 
         try:
             services = tron_tools.get_tron_namespaces(
                 cluster=args.cluster, soa_dir=args.soa_dir
             )
-        except Exception as e:
-            log.error("Failed to list tron namespaces: {error}".format(error=str(e)))
+        except Exception:
+            log.exception("Failed to list tron namespaces:")
             sys.exit(1)
     else:
         services = args.services
 
     if not services:
         log.warning("No namespaces found")
         sys.exit(0)
@@ -99,40 +101,50 @@
         cluster=args.cluster, soa_dir=args.soa_dir
     )
     if args.dry_run:
         log.info(f"Would update {MASTER_NAMESPACE} to:")
         log.info(f"{master_config}")
         updated.append(MASTER_NAMESPACE)
     else:
-        if client.update_namespace(MASTER_NAMESPACE, master_config):
-            updated.append(MASTER_NAMESPACE)
-            log.debug(f"Updated {MASTER_NAMESPACE}")
-        else:
-            skipped.append(MASTER_NAMESPACE)
-            log.debug(f"Skipped {MASTER_NAMESPACE}")
+        try:
+            if client.update_namespace(MASTER_NAMESPACE, master_config):
+                updated.append(MASTER_NAMESPACE)
+                log.debug(f"Updated {MASTER_NAMESPACE}")
+            else:
+                skipped.append(MASTER_NAMESPACE)
+                log.debug(f"Skipped {MASTER_NAMESPACE}")
+        except Exception:
+            failed.append(MASTER_NAMESPACE)
+            log.exception(f"Error while updating {MASTER_NAMESPACE}:")
 
+    k8s_enabled_for_cluster = (
+        yaml.safe_load(master_config).get("k8s_options", {}).get("enabled", False)
+    )
     for service in sorted(services):
         try:
             new_config = tron_tools.create_complete_config(
-                cluster=args.cluster, service=service, soa_dir=args.soa_dir
+                cluster=args.cluster,
+                service=service,
+                soa_dir=args.soa_dir,
+                k8s_enabled=k8s_enabled_for_cluster,
+                dry_run=args.dry_run,
             )
             if args.dry_run:
                 log.info(f"Would update {service} to:")
                 log.info(f"{new_config}")
                 updated.append(service)
             else:
                 if client.update_namespace(service, new_config):
                     updated.append(service)
                     log.debug(f"Updated {service}")
                 else:
                     skipped.append(service)
                     log.debug(f"Skipped {service}")
-        except Exception as e:
-            log.error(f"Update for {service} failed: {str(e)}")
-            log.debug(f"Exception while updating {service}", exc_info=1)
+        except Exception:
+            log.exception(f"Update for {service} failed:")
             failed.append(service)
 
     skipped_report = skipped if args.verbose else len(skipped)
     log.info(
         f"Updated following namespaces: {updated}, "
         f"failed: {failed}, skipped: {skipped_report}"
     )
```

### Comparing `paasta-tools-0.92.1/paasta_tools/list_tron_namespaces.py` & `paasta-tools-1.0.0/paasta_tools/list_tron_namespaces.py`

 * *Files 19% similar despite different names*

```diff
@@ -10,15 +10,14 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import argparse
 
 from paasta_tools import tron_tools
-from paasta_tools.utils import paasta_print
 
 
 def parse_args():
     parser = argparse.ArgumentParser(
         description="Lists Tron namespaces for a cluster, excluding MASTER"
     )
     parser.add_argument(
@@ -31,21 +30,31 @@
     parser.add_argument(
         "-d",
         "--soa-dir",
         dest="soa_dir",
         default=tron_tools.DEFAULT_SOA_DIR,
         help="Use a different soa config directory",
     )
+    parser.add_argument(
+        "-e",
+        "--executors",
+        default=tron_tools.EXECUTOR_TYPES,
+        nargs="+",
+        help="tron executor types to list namespaces for",
+        choices=tron_tools.EXECUTOR_TYPES,
+    )
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
     namespaces = tron_tools.get_tron_namespaces(
-        cluster=args.cluster, soa_dir=args.soa_dir
+        cluster=args.cluster,
+        soa_dir=args.soa_dir,
+        tron_executors=args.executors,
     )
-    paasta_print("\n".join(namespaces))
+    print("\n".join(namespaces))
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `paasta-tools-0.92.1/paasta_tools/graceful_app_drain.py` & `paasta-tools-1.0.0/paasta_tools/nrtsearchservice_tools.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,127 +1,143 @@
-#!/usr/bin/env python
-import argparse
-import sys
-import time
-
-from paasta_tools import bounce_lib
-from paasta_tools import drain_lib
-from paasta_tools import marathon_tools
-from paasta_tools.setup_marathon_job import do_bounce
-from paasta_tools.setup_marathon_job import get_tasks_by_state
-from paasta_tools.utils import decompose_job_id
-from paasta_tools.utils import load_system_paasta_config
-from paasta_tools.utils import paasta_print
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(
-        description="""This script attempts to gracefully drain and kill a marathon app.
-        It is intended for use in emergencies when the regular bounce script can't proceed,
-        and needs to kill a specific app to get going."""
-    )
-    parser.add_argument("appname", help="the app that will be drained")
-    parser.add_argument(
-        "-d",
-        "--soa-dir",
-        dest="soa_dir",
-        metavar="SOA_DIR",
-        default=marathon_tools.DEFAULT_SOA_DIR,
-        help="define a different soa config directory",
-    )
-    return parser.parse_args()
-
-
-def kill_marathon_app(full_appid, cluster, client, soa_dir):
-    service, instance, _, __ = (
-        s.replace("--", "_") for s in decompose_job_id(full_appid)
-    )
-    service_instance_config = marathon_tools.load_marathon_service_config(
-        service=service, instance=instance, cluster=cluster, soa_dir=soa_dir
-    )
-    complete_config = service_instance_config.format_marathon_app_dict()
-    registrations = service_instance_config.get_registrations()
-    service_namespace_config = marathon_tools.load_service_namespace_config(
-        service=service, namespace=registrations[0]
-    )
-    drain_method = drain_lib.get_drain_method(
-        service_instance_config.get_drain_method(service_namespace_config),
-        service=service,
-        instance=instance,
-        registrations=registrations,
-        drain_method_params=service_instance_config.get_drain_method_params(
-            service_namespace_config
-        ),
-    )
-
-    bounce_func = bounce_lib.get_bounce_method_func("down")
+# Copyright 2015-2019 Yelp Inc.
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+from typing import List
+from typing import Mapping
+from typing import Optional
+
+import service_configuration_lib
+
+from paasta_tools.kubernetes_tools import sanitised_cr_name
+from paasta_tools.long_running_service_tools import LongRunningServiceConfig
+from paasta_tools.long_running_service_tools import LongRunningServiceConfigDict
+from paasta_tools.utils import BranchDictV2
+from paasta_tools.utils import deep_merge_dictionaries
+from paasta_tools.utils import DEFAULT_SOA_DIR
+from paasta_tools.utils import load_service_instance_config
+from paasta_tools.utils import load_v2_deployments_json
+
+
+class NrtsearchServiceDeploymentConfigDict(LongRunningServiceConfigDict, total=False):
+    replicas: int
+
+
+class NrtsearchServiceDeploymentConfig(LongRunningServiceConfig):
+    config_dict: NrtsearchServiceDeploymentConfigDict
+
+    config_filename_prefix = "nrtsearchservice"
+
+    def __init__(
+        self,
+        service: str,
+        cluster: str,
+        instance: str,
+        config_dict: NrtsearchServiceDeploymentConfigDict,
+        branch_dict: Optional[BranchDictV2],
+        soa_dir: str = DEFAULT_SOA_DIR,
+    ) -> None:
 
-    while marathon_tools.is_app_id_running(app_id=full_appid, client=client):
-        app_to_kill = client.get_app(full_appid)
-        (
-            old_app_live_happy_tasks,
-            old_app_live_unhappy_tasks,
-            old_app_draining_tasks,
-            old_app_at_risk_tasks,
-        ) = get_tasks_by_state(
-            other_apps=[app_to_kill],
-            drain_method=drain_method,
+        super().__init__(
+            cluster=cluster,
+            instance=instance,
             service=service,
-            nerve_ns=registrations[0],
-            bounce_health_params=service_instance_config.get_bounce_health_params(
-                service_namespace_config
-            ),
+            soa_dir=soa_dir,
+            config_dict=config_dict,
+            branch_dict=branch_dict,
         )
-        do_bounce(
-            bounce_func=bounce_func,
-            drain_method=drain_method,
-            config=complete_config,
-            new_app_running="",
-            happy_new_tasks=[],
-            old_app_live_happy_tasks=old_app_live_happy_tasks,
-            old_app_live_unhappy_tasks=old_app_live_unhappy_tasks,
-            old_app_draining_tasks=old_app_draining_tasks,
-            old_app_at_risk_tasks=old_app_at_risk_tasks,
-            serviceinstance=f"{service}.{instance}",
-            bounce_method="down",
+
+    def get_instances(self, with_limit: bool = True) -> int:
+        return self.config_dict.get("replicas", 1)
+
+    def validate(
+        self,
+        params: List[str] = [
+            "cpus",
+            "security",
+            "dependencies_reference",
+            "deploy_group",
+        ],
+    ) -> List[str]:
+        # Use InstanceConfig to validate shared config keys like cpus and mem
+        # TODO: add mem back to this list once we fix PAASTA-15582 and
+        # move to using the same units as flink/marathon etc.
+        error_msgs = super().validate(params=params)
+
+        if error_msgs:
+            name = self.get_instance()
+            return [f"{name}: {msg}" for msg in error_msgs]
+        else:
+            return []
+
+
+def load_nrtsearchservice_instance_config(
+    service: str,
+    instance: str,
+    cluster: str,
+    load_deployments: bool = True,
+    soa_dir: str = DEFAULT_SOA_DIR,
+) -> NrtsearchServiceDeploymentConfig:
+    """Read a service instance's configuration for Nrtsearch.
+
+    If a branch isn't specified for a config, the 'branch' key defaults to
+    paasta-${cluster}.${instance}.
+
+    :param service: The service name
+    :param instance: The instance of the service to retrieve
+    :param cluster: The cluster to read the configuration for
+    :param load_deployments: A boolean indicating if the corresponding deployments.json for this service
+                             should also be loaded
+    :param soa_dir: The SOA configuration directory to read from
+    :returns: A dictionary of whatever was in the config for the service instance"""
+    general_config = service_configuration_lib.read_service_configuration(
+        service, soa_dir=soa_dir
+    )
+    instance_config = load_service_instance_config(
+        service, instance, "nrtsearchservice", cluster, soa_dir=soa_dir
+    )
+    general_config = deep_merge_dictionaries(
+        overrides=instance_config, defaults=general_config
+    )
+
+    branch_dict: Optional[BranchDictV2] = None
+    if load_deployments:
+        deployments_json = load_v2_deployments_json(service, soa_dir=soa_dir)
+        temp_instance_config = NrtsearchServiceDeploymentConfig(
             service=service,
             cluster=cluster,
             instance=instance,
-            marathon_jobid=full_appid,
-            client=client,
+            config_dict=general_config,
+            branch_dict=None,
             soa_dir=soa_dir,
         )
+        branch = temp_instance_config.get_branch()
+        deploy_group = temp_instance_config.get_deploy_group()
+        branch_dict = deployments_json.get_branch_dict(service, branch, deploy_group)
 
-        paasta_print("Sleeping for 10 seconds to give the tasks time to drain")
-        time.sleep(10)
-
-    paasta_print(f"Successfully killed {full_appid}")
-
-
-def main():
-    exit_code = 1
-    args = parse_args()
-    full_appid = args.appname.lstrip("/")
-
-    system_paasta_config = load_system_paasta_config()
-    cluster = system_paasta_config.get_cluster()
-    clients = marathon_tools.get_list_of_marathon_clients(
-        system_paasta_config=system_paasta_config
+    return NrtsearchServiceDeploymentConfig(
+        service=service,
+        cluster=cluster,
+        instance=instance,
+        config_dict=general_config,
+        branch_dict=branch_dict,
+        soa_dir=soa_dir,
     )
 
-    for client in clients:
-        if marathon_tools.is_app_id_running(app_id=full_appid, client=client):
-            kill_marathon_app(
-                full_appid=full_appid,
-                cluster=cluster,
-                client=client,
-                soa_dir=args.soa_dir,
-            )
-            exit_code = 0
-
-    if exit_code:
-        paasta_print(f"Couldn't find an app named {full_appid}")
-    return exit_code
-
 
-if __name__ == "__main__":
-    sys.exit(main())
+# TODO: read this from CRD in service configs
+def cr_id(service: str, instance: str) -> Mapping[str, str]:
+    return dict(
+        group="yelp.com",
+        version="v1alpha1",
+        namespace="paasta-nrtsearchservices",
+        plural="nrtsearchservices",
+        name=sanitised_cr_name(service, instance),
+    )
```

### Comparing `paasta-tools-0.92.1/setup.py` & `paasta-tools-1.0.0/setup.py`

 * *Files 7% similar despite different names*

```diff
@@ -28,82 +28,82 @@
     return minimal_reqs
 
 
 setup(
     name="paasta-tools",
     version=__version__,
     provides=["paasta_tools"],
-    author="Kyle Anderson",
-    author_email="kwa@yelp.com",
+    author="Compute Infrastructure @ Yelp",
+    author_email="compute-infra@yelp.com",
     description="Tools for Yelps SOA infrastructure",
     packages=find_packages(exclude=("tests*", "scripts*")),
     include_package_data=True,
+    python_requires=">=3.8.0",
     install_requires=get_install_requires(),
     scripts=[
         "paasta_tools/am_i_mesos_leader.py",
-        "paasta_tools/autoscale_all_services.py",
-        "paasta_tools/check_flink_services_health.py",
+        "paasta_tools/apply_external_resources.py",
+        "paasta_tools/check_autoscaler_max_instances.py",
         "paasta_tools/check_cassandracluster_services_replication.py",
-        "paasta_tools/check_marathon_services_replication.py",
+        "paasta_tools/check_flink_services_health.py",
         "paasta_tools/check_kubernetes_api.py",
         "paasta_tools/check_kubernetes_services_replication.py",
         "paasta_tools/check_oom_events.py",
         "paasta_tools/check_spark_jobs.py",
-        "paasta_tools/cleanup_marathon_jobs.py",
         "paasta_tools/cleanup_kubernetes_cr.py",
         "paasta_tools/cleanup_kubernetes_crd.py",
         "paasta_tools/cleanup_kubernetes_jobs.py",
-        "paasta_tools/deploy_marathon_services",
-        "paasta_tools/paasta_deploy_tron_jobs",
+        "paasta_tools/cli/paasta_tabcomplete.sh",
+        "paasta_tools/delete_kubernetes_deployments.py",
         "paasta_tools/generate_all_deployments",
         "paasta_tools/generate_deployments_for_service.py",
         "paasta_tools/generate_services_file.py",
         "paasta_tools/generate_services_yaml.py",
         "paasta_tools/get_mesos_leader.py",
-        "paasta_tools/kubernetes/bin/paasta_secrets_sync.py",
-        "paasta_tools/kubernetes/bin/paasta_cleanup_stale_nodes.py",
         "paasta_tools/kubernetes/bin/kubernetes_remove_evicted_pods.py",
-        "paasta_tools/list_marathon_service_instances.py",
-        "paasta_tools/marathon_dashboard.py",
+        "paasta_tools/kubernetes/bin/paasta_cleanup_stale_nodes.py",
+        "paasta_tools/kubernetes/bin/paasta_secrets_sync.py",
+        "paasta_tools/log_task_lifecycle_events.py",
         "paasta_tools/monitoring/check_capacity.py",
-        "paasta_tools/monitoring/check_marathon_has_apps.py",
         "paasta_tools/monitoring/check_mesos_active_frameworks.py",
         "paasta_tools/monitoring/check_mesos_duplicate_frameworks.py",
         "paasta_tools/monitoring/check_mesos_quorum.py",
-        "paasta_tools/monitoring/check_mesos_outdated_tasks.py",
-        "paasta_tools/monitoring/check_synapse_replication.py",
         "paasta_tools/monitoring/kill_orphaned_docker_containers.py",
-        "paasta_tools/cli/paasta_tabcomplete.sh",
         "paasta_tools/paasta_cluster_boost.py",
+        "paasta_tools/paasta_deploy_tron_jobs",
         "paasta_tools/paasta_execute_docker_command.py",
         "paasta_tools/paasta_maintenance.py",
         "paasta_tools/paasta_metastatus.py",
         "paasta_tools/paasta_remote_run.py",
-        "paasta_tools/setup_kubernetes_job.py",
-        "paasta_tools/setup_kubernetes_crd.py",
+        "paasta_tools/setup_istio_mesh.py",
         "paasta_tools/setup_kubernetes_cr.py",
-        "paasta_tools/setup_marathon_job.py",
+        "paasta_tools/setup_kubernetes_crd.py",
+        "paasta_tools/setup_kubernetes_internal_crd.py",
+        "paasta_tools/setup_kubernetes_job.py",
+        "paasta_tools/setup_prometheus_adapter_config.py",
         "paasta_tools/synapse_srv_namespaces_fact.py",
     ]
-    + glob.glob("paasta_tools/contrib/*"),
+    + glob.glob("paasta_tools/contrib/*.sh")
+    + glob.glob("paasta_tools/contrib/[!_]*.py"),
     entry_points={
         "console_scripts": [
             "paasta=paasta_tools.cli.cli:main",
             "paasta-api=paasta_tools.api.api:main",
             "paasta-deployd=paasta_tools.deployd.master:main",
             "paasta-fsm=paasta_tools.cli.fsm_cmd:main",
-            "paasta_autoscale_cluster=paasta_tools.autoscale_cluster:main",
+            "paasta_prune_completed_pods=paasta_tools.prune_completed_pods:main",
             "paasta_cleanup_tron_namespaces=paasta_tools.cleanup_tron_namespaces:main",
             "paasta_list_kubernetes_service_instances=paasta_tools.list_kubernetes_service_instances:main",
             "paasta_list_tron_namespaces=paasta_tools.list_tron_namespaces:main",
             "paasta_setup_tron_namespace=paasta_tools.setup_tron_namespace:main",
             "paasta_cleanup_maintenance=paasta_tools.cleanup_maintenance:main",
             "paasta_docker_wrapper=paasta_tools.docker_wrapper:main",
             "paasta_firewall_update=paasta_tools.firewall_update:main",
             "paasta_firewall_logging=paasta_tools.firewall_logging:main",
             "paasta_oom_logger=paasta_tools.oom_logger:main",
-            "paasta_broadcast_log=paasta_tools.marathon_tools:broadcast_log_all_services_running_here_from_stdin",
+            "paasta_broadcast_log=paasta_tools.broadcast_log_to_services:main",
             "paasta_dump_locally_running_services=paasta_tools.dump_locally_running_services:main",
+            "paasta_habitat_fixer=paasta_tools.contrib.habitat_fixer:main",
         ],
         "paste.app_factory": ["paasta-api-config=paasta_tools.api.api:make_app"],
     },
 )
```

### Comparing `paasta-tools-0.92.1/paasta_tools.egg-info/requires.txt` & `paasta-tools-1.0.0/paasta_tools.egg-info/requires.txt`

 * *Files 24% similar despite different names*

```diff
@@ -2,56 +2,60 @@
 aiohttp>=3.5.4
 argcomplete>=0.8.1
 boto
 boto3
 boto3-type-annotations
 botocore
 bravado>=10.2.0
+certifi
 choice>=0.1
 cookiecutter>=1.4.0
 croniter
 docker-py>=1.2.3
 dulwich>=0.17.3
 ephemeral-port-reserve>=1.0.1
 graphviz
 gunicorn
+humanfriendly
 humanize>=0.5.1
 inotify>=0.2.8
 ipaddress>=1.0.22
 isodate>=0.5.0
 jsonschema[format]
 kazoo>=2.0.0
-kubernetes
+kubernetes<22.0.0,>=18.20.0
+ldap3
 manhole
-marathon>=0.12.0
 mypy-extensions>=0.3.0
+nulltype
 objgraph
 ply
 progressbar2>=3.10.0
+prometheus-client
 pymesos>=0.2.0
 pyramid>=1.8
 pyramid-swagger>=2.3.0
 pysensu-yelp>=0.3.4
 PyStaticConfiguration
 python-crontab>=2.1.1
 python-dateutil>=2.4.0
 python-iptables
 pytimeparse>=1.1.0
 pytz>=2014.10
 requests>=2.18.4
-requests-cache<=0.5.0,>=0.4.10
+requests-cache>=0.4.10
 retry
 ruamel.yaml
 sensu-plugin
-service-configuration-lib>=2.1.0
+service-configuration-lib>=2.18.17
 signalfx
 slackclient>=1.2.1
 sticht>=1.1.0
 syslogmp
 task-processing
-thriftpy
+thriftpy2
 transitions
 typing-extensions
 tzlocal
-ujson>=1.35
+urllib3
 utaw>=0.2.0
 wsgicors
```

