# Comparing `tmp/spark_rapids_ml-23.8.0-158_5dab107-py3-none-any.whl.zip` & `tmp/spark_rapids_ml-24.2.0-327_e0f644d-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,23 @@
-Zip file size: 86594 bytes, number of entries: 21
--rw-r--r--  2.0 unx      615 b- defN 23-Sep-13 02:37 spark_rapids_ml/__init__.py
--rw-r--r--  2.0 unx    37391 b- defN 23-Sep-13 02:37 spark_rapids_ml/classification.py
--rw-r--r--  2.0 unx    14009 b- defN 23-Sep-13 02:37 spark_rapids_ml/clustering.py
--rw-r--r--  2.0 unx    46609 b- defN 23-Sep-13 02:37 spark_rapids_ml/core.py
--rw-r--r--  2.0 unx    13694 b- defN 23-Sep-13 02:37 spark_rapids_ml/feature.py
--rw-r--r--  2.0 unx    24753 b- defN 23-Sep-13 02:37 spark_rapids_ml/knn.py
--rw-r--r--  2.0 unx    16996 b- defN 23-Sep-13 02:37 spark_rapids_ml/params.py
--rw-r--r--  2.0 unx    37634 b- defN 23-Sep-13 02:37 spark_rapids_ml/regression.py
--rw-r--r--  2.0 unx    20969 b- defN 23-Sep-13 02:37 spark_rapids_ml/tree.py
--rw-r--r--  2.0 unx     6731 b- defN 23-Sep-13 02:37 spark_rapids_ml/tuning.py
--rw-r--r--  2.0 unx    46825 b- defN 23-Sep-13 02:37 spark_rapids_ml/umap.py
--rw-r--r--  2.0 unx    14879 b- defN 23-Sep-13 02:37 spark_rapids_ml/utils.py
--rw-r--r--  2.0 unx      592 b- defN 23-Sep-13 02:37 spark_rapids_ml/common/__init__.py
--rw-r--r--  2.0 unx     6545 b- defN 23-Sep-13 02:37 spark_rapids_ml/common/cuml_context.py
--rw-r--r--  2.0 unx     6144 b- defN 23-Sep-13 02:37 spark_rapids_ml/metrics/MulticlassMetrics.py
--rw-r--r--  2.0 unx     9495 b- defN 23-Sep-13 02:37 spark_rapids_ml/metrics/RegressionMetrics.py
--rw-r--r--  2.0 unx      592 b- defN 23-Sep-13 02:37 spark_rapids_ml/metrics/__init__.py
--rw-r--r--  2.0 unx     8918 b- defN 23-Sep-13 05:01 spark_rapids_ml-23.8.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Sep-13 05:01 spark_rapids_ml-23.8.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       16 b- defN 23-Sep-13 05:01 spark_rapids_ml-23.8.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1814 b- defN 23-Sep-13 05:01 spark_rapids_ml-23.8.0.dist-info/RECORD
-21 files, 315313 bytes uncompressed, 83640 bytes compressed:  73.5%
+Zip file size: 97250 bytes, number of entries: 21
+-rw-r--r--  2.0 unx      616 b- defN 24-Mar-12 06:43 spark_rapids_ml/__init__.py
+-rw-r--r--  2.0 unx    57609 b- defN 24-Mar-12 06:43 spark_rapids_ml/classification.py
+-rw-r--r--  2.0 unx    16416 b- defN 24-Mar-12 06:43 spark_rapids_ml/clustering.py
+-rw-r--r--  2.0 unx    59050 b- defN 24-Mar-12 06:43 spark_rapids_ml/core.py
+-rw-r--r--  2.0 unx    15078 b- defN 24-Mar-12 06:43 spark_rapids_ml/feature.py
+-rw-r--r--  2.0 unx    25979 b- defN 24-Mar-12 06:43 spark_rapids_ml/knn.py
+-rw-r--r--  2.0 unx    18712 b- defN 24-Mar-12 06:43 spark_rapids_ml/params.py
+-rw-r--r--  2.0 unx    38290 b- defN 24-Mar-12 06:43 spark_rapids_ml/regression.py
+-rw-r--r--  2.0 unx    21051 b- defN 24-Mar-12 06:43 spark_rapids_ml/tree.py
+-rw-r--r--  2.0 unx     6731 b- defN 24-Mar-12 06:43 spark_rapids_ml/tuning.py
+-rw-r--r--  2.0 unx    48385 b- defN 24-Mar-12 06:43 spark_rapids_ml/umap.py
+-rw-r--r--  2.0 unx    16252 b- defN 24-Mar-12 06:43 spark_rapids_ml/utils.py
+-rw-r--r--  2.0 unx      592 b- defN 24-Mar-12 06:43 spark_rapids_ml/common/__init__.py
+-rw-r--r--  2.0 unx     6683 b- defN 24-Mar-12 06:43 spark_rapids_ml/common/cuml_context.py
+-rw-r--r--  2.0 unx     7032 b- defN 24-Mar-12 06:43 spark_rapids_ml/metrics/MulticlassMetrics.py
+-rw-r--r--  2.0 unx     9496 b- defN 24-Mar-12 06:43 spark_rapids_ml/metrics/RegressionMetrics.py
+-rw-r--r--  2.0 unx     1313 b- defN 24-Mar-12 06:43 spark_rapids_ml/metrics/__init__.py
+-rw-r--r--  2.0 unx     8807 b- defN 24-Mar-12 11:56 spark_rapids_ml-24.2.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Mar-12 11:56 spark_rapids_ml-24.2.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       16 b- defN 24-Mar-12 11:56 spark_rapids_ml-24.2.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1815 b- defN 24-Mar-12 11:56 spark_rapids_ml-24.2.0.dist-info/RECORD
+21 files, 360015 bytes uncompressed, 94296 bytes compressed:  73.8%
```

## zipnote {}

```diff
@@ -45,20 +45,20 @@
 
 Filename: spark_rapids_ml/metrics/RegressionMetrics.py
 Comment: 
 
 Filename: spark_rapids_ml/metrics/__init__.py
 Comment: 
 
-Filename: spark_rapids_ml-23.8.0.dist-info/METADATA
+Filename: spark_rapids_ml-24.2.0.dist-info/METADATA
 Comment: 
 
-Filename: spark_rapids_ml-23.8.0.dist-info/WHEEL
+Filename: spark_rapids_ml-24.2.0.dist-info/WHEEL
 Comment: 
 
-Filename: spark_rapids_ml-23.8.0.dist-info/top_level.txt
+Filename: spark_rapids_ml-24.2.0.dist-info/top_level.txt
 Comment: 
 
-Filename: spark_rapids_ml-23.8.0.dist-info/RECORD
+Filename: spark_rapids_ml-24.2.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## spark_rapids_ml/__init__.py

```diff
@@ -9,8 +9,8 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
-__version__ = "23.8.0"
+__version__ = "24.02.0"
```

## spark_rapids_ml/classification.py

```diff
@@ -1,47 +1,52 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
+from abc import ABCMeta
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     List,
     Optional,
     Tuple,
     Type,
     TypeVar,
     Union,
     cast,
 )
 
+import pyspark
 from pyspark.ml.common import _py2java
 from pyspark.ml.evaluation import Evaluator, MulticlassClassificationEvaluator
 
+from .metrics import EvalMetricInfo, transform_evaluate_metric
 from .metrics.MulticlassMetrics import MulticlassMetrics
 
 if TYPE_CHECKING:
+    import cupy as cp
     from pyspark.ml._typing import ParamMap
 
 import numpy as np
 import pandas as pd
-from pyspark import Row, keyword_only
+import scipy
+from pyspark import Row, TaskContext, keyword_only
 from pyspark.ml.classification import BinaryRandomForestClassificationSummary
 from pyspark.ml.classification import (
     LogisticRegressionModel as SparkLogisticRegressionModel,
 )
 from pyspark.ml.classification import (
     LogisticRegressionSummary,
     LogisticRegressionTrainingSummary,
@@ -50,16 +55,16 @@
     RandomForestClassificationModel as SparkRandomForestClassificationModel,
 )
 from pyspark.ml.classification import (
     RandomForestClassificationSummary,
     _LogisticRegressionParams,
     _RandomForestClassifierParams,
 )
-from pyspark.ml.linalg import DenseMatrix, Vector, Vectors
-from pyspark.ml.param.shared import HasProbabilityCol, HasRawPredictionCol
+from pyspark.ml.linalg import DenseMatrix, Matrix, Vector, Vectors
+from pyspark.ml.param.shared import HasLabelCol, HasProbabilityCol, HasRawPredictionCol
 from pyspark.sql import Column, DataFrame
 from pyspark.sql.functions import col
 from pyspark.sql.types import (
     ArrayType,
     DoubleType,
     FloatType,
     IntegerType,
@@ -77,17 +82,16 @@
     _CumlEstimatorSupervised,
     _CumlModelWithPredictionCol,
     _EvaluateFunc,
     _TransformFunc,
     alias,
     param_alias,
     pred,
-    transform_evaluate,
 )
-from .params import HasFeaturesCols, _CumlClass, _CumlParams
+from .params import HasEnableSparseDataOptim, HasFeaturesCols, _CumlClass, _CumlParams
 from .tree import (
     _RandomForestClass,
     _RandomForestCumlParams,
     _RandomForestEstimator,
     _RandomForestModel,
 )
 from .utils import (
@@ -98,14 +102,180 @@
     get_logger,
     java_uid,
 )
 
 T = TypeVar("T")
 
 
+class _ClassificationModelEvaluationMixIn:
+    # https://github.com/python/mypy/issues/5868#issuecomment-437690894 to bypass mypy checking
+    _this_model: Union["RandomForestClassificationModel", "LogisticRegressionModel"]
+
+    def _get_evaluate_fn(self, eval_metric_info: EvalMetricInfo) -> _EvaluateFunc:
+        def _evaluate(
+            input: TransformInputType,
+            transformed: TransformInputType,
+        ) -> pd.DataFrame:
+            # calculate the count of (label, prediction)
+            # TBD: keep all intermediate transform output on gpu as long as possible to avoid copies
+
+            if eval_metric_info.eval_metric == transform_evaluate_metric.accuracy_like:
+                comb = pd.DataFrame(
+                    {
+                        "label": input[alias.label],
+                        "prediction": transformed[pred.prediction],
+                    }
+                )
+                confusion = (
+                    comb.groupby(["label", "prediction"])
+                    .size()
+                    .reset_index(name="total")
+                )
+
+                return confusion
+            else:
+                # once data is maintained on gpu replace with cuml.metrics.log_loss
+                from spark_rapids_ml.metrics.MulticlassMetrics import log_loss
+
+                _log_loss = log_loss(
+                    np.array(input[alias.label]),
+                    np.array(list(transformed[pred.probability])),
+                    eval_metric_info.eps,
+                )
+
+                _log_loss_pdf = pd.DataFrame(
+                    {"total": [len(input[alias.label])], "log_loss": [_log_loss]}
+                )
+
+                return _log_loss_pdf
+
+        return _evaluate
+
+    def _transformEvaluate(
+        self,
+        dataset: DataFrame,
+        evaluator: Evaluator,
+        params: Optional["ParamMap"] = None,
+    ) -> List[float]:
+        """
+        Transforms and evaluates the input dataset with optional parameters in a single pass.
+
+        Parameters
+        ----------
+        dataset : :py:class:`pyspark.sql.DataFrame`
+            a dataset that contains labels/observations and predictions
+        evaluator: :py:class:`pyspark.ml.evaluation.Evaluator`
+            an evaluator user intends to use
+        params : dict, optional
+            an optional param map that overrides embedded params
+
+        Returns
+        -------
+        list of float
+            metrics
+        """
+
+        if not isinstance(evaluator, MulticlassClassificationEvaluator):
+            raise NotImplementedError(f"{evaluator} is unsupported yet.")
+
+        if (
+            evaluator.getMetricName()
+            not in MulticlassMetrics.SUPPORTED_MULTI_CLASS_METRIC_NAMES
+        ):
+            raise NotImplementedError(
+                f"{evaluator.getMetricName()} is not supported yet."
+            )
+
+        if self._this_model.getLabelCol() not in dataset.schema.names:
+            raise RuntimeError("Label column is not existing.")
+
+        dataset = dataset.withColumnRenamed(self._this_model.getLabelCol(), alias.label)
+
+        if evaluator.getMetricName() == "logLoss":
+            schema = StructType(
+                [
+                    StructField(pred.model_index, IntegerType()),
+                    StructField("total", FloatType()),
+                    StructField("log_loss", FloatType()),
+                ]
+            )
+
+            eval_metric_info = EvalMetricInfo(
+                eval_metric=transform_evaluate_metric.log_loss, eps=evaluator.getEps()
+            )
+        else:
+            schema = StructType(
+                [
+                    StructField(pred.model_index, IntegerType()),
+                    StructField("label", FloatType()),
+                    StructField("prediction", FloatType()),
+                    StructField("total", FloatType()),
+                ]
+            )
+            eval_metric_info = EvalMetricInfo(
+                eval_metric=transform_evaluate_metric.accuracy_like
+            )
+        # TBD: use toPandas and pandas df operations below
+        rows = self._this_model._transform_evaluate_internal(
+            dataset, schema, eval_metric_info
+        ).collect()
+
+        num_models = self._this_model._get_num_models()
+
+        if eval_metric_info.eval_metric == transform_evaluate_metric.accuracy_like:
+            tp_by_class: List[Dict[float, float]] = [{} for _ in range(num_models)]
+            fp_by_class: List[Dict[float, float]] = [{} for _ in range(num_models)]
+            label_count_by_class: List[Dict[float, float]] = [
+                {} for _ in range(num_models)
+            ]
+            label_count = [0 for _ in range(num_models)]
+
+            for i in range(num_models):
+                for j in range(self._this_model._num_classes):
+                    tp_by_class[i][float(j)] = 0.0
+                    label_count_by_class[i][float(j)] = 0.0
+                    fp_by_class[i][float(j)] = 0.0
+
+            for row in rows:
+                label_count[row.model_index] += row.total
+                label_count_by_class[row.model_index][row.label] += row.total
+
+                if row.label == row.prediction:
+                    tp_by_class[row.model_index][row.label] += row.total
+                else:
+                    fp_by_class[row.model_index][row.prediction] += row.total
+
+            scores = []
+            for i in range(num_models):
+                metrics = MulticlassMetrics(
+                    tp=tp_by_class[i],
+                    fp=fp_by_class[i],
+                    label=label_count_by_class[i],
+                    label_count=label_count[i],
+                )
+                scores.append(metrics.evaluate(evaluator))
+        else:
+            # logLoss metric
+            label_count = [0 for _ in range(num_models)]
+            log_loss = [0.0 for _ in range(num_models)]
+            for row in rows:
+                label_count[row.model_index] += row.total
+                log_loss[row.model_index] += row.log_loss
+
+            scores = []
+            for i in range(num_models):
+                metrics = MulticlassMetrics(
+                    label_count=label_count[i],
+                    log_loss=log_loss[i],
+                )
+                scores.append(metrics.evaluate(evaluator))
+
+        return scores
+
+
 class _RFClassifierParams(
     _RandomForestClassifierParams, HasProbabilityCol, HasRawPredictionCol
 ):
     def __init__(self, *args: Any):
         super().__init__(*args)
 
     def setProbabilityCol(
@@ -125,17 +295,19 @@
         return self._set(rawPredictionCol=value)
 
 
 class _RandomForestClassifierClass(_RandomForestClass):
     @classmethod
     def _param_mapping(cls) -> Dict[str, Optional[str]]:
         mapping = super()._param_mapping()
-        mapping["rawPredictionCol"] = ""
         return mapping
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return pyspark.ml.classification.RandomForestClassifier
+
 
 class RandomForestClassifier(
     _RandomForestClassifierClass,
     _RandomForestEstimator,
     _RandomForestCumlParams,
     _RFClassifierParams,
 ):
@@ -159,24 +331,26 @@
     and :py:class:`cuml.ensemble.RandomForestClassifier`. And it can automatically
     map pyspark parameters to cuML parameters.
 
 
     Parameters
     ----------
 
-    featuresCol:
+    featuresCol: str or List[str]
         The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
             * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
             * When the value is a list of strings, the feature columns must be numeric types.
     labelCol:
         The label column name.
     predictionCol:
         The prediction column name.
-    probabilityCol
+    probabilityCol:
         The column name for predicted class conditional probabilities.
+    rawPredictionCol:
+        The column name for class raw predictions - this is currently set equal to probabilityCol values.
     maxDepth:
         Maximum tree depth. Must be greater than 0.
     maxBins:
         Maximum number of bins used by the split algorithm per feature.
     minInstancesPerNode:
         The minimum number of samples (rows) in each leaf node.
     impurity: str = "gini",
@@ -282,14 +456,15 @@
     def __init__(
         self,
         *,
         featuresCol: Union[str, List[str]] = "features",
         labelCol: str = "label",
         predictionCol: str = "prediction",
         probabilityCol: str = "probability",
+        rawPredictionCol: str = "rawPrediction",
         maxDepth: int = 5,
         maxBins: int = 32,
         minInstancesPerNode: int = 1,
         impurity: str = "gini",
         numTrees: int = 20,
         featureSubsetStrategy: str = "auto",
         seed: Optional[int] = None,
@@ -318,15 +493,15 @@
             raise ValueError(
                 "Label column must be integral types or float/double types."
             )
 
         return label_col
 
     def _create_pyspark_model(self, result: Row) -> "RandomForestClassificationModel":
-        return RandomForestClassificationModel.from_row(result)
+        return RandomForestClassificationModel._from_row(result)
 
     def _is_classification(self) -> bool:
         return True
 
     def _supportsTransformEvaluate(self, evaluator: Evaluator) -> bool:
         if (
             isinstance(evaluator, MulticlassClassificationEvaluator)
@@ -335,14 +510,15 @@
         ):
             return True
 
         return False
 
 
 class RandomForestClassificationModel(
+    _ClassificationModelEvaluationMixIn,
     _RandomForestClassifierClass,
     _RandomForestModel,
     _RandomForestCumlParams,
     _RFClassifierParams,
 ):
     """
     Model fitted by :class:`RandomForestClassifier`.
@@ -362,14 +538,15 @@
             treelite_model=treelite_model,
             model_json=model_json,
             num_classes=num_classes,
         )
         self._num_classes = num_classes
         self._model_json = model_json
         self._rf_spark_model: Optional[SparkRandomForestClassificationModel] = None
+        self._this_model = self
 
     def cpu(self) -> SparkRandomForestClassificationModel:
         """Return the PySpark ML RandomForestClassificationModel"""
 
         if self._rf_spark_model is None:
             sc = _get_spark_session().sparkContext
             assert sc._jvm is not None
@@ -383,17 +560,25 @@
                 self.numFeatures,
                 self._num_classes,
             )
             self._rf_spark_model = SparkRandomForestClassificationModel(java_rf_model)
             self._copyValues(self._rf_spark_model)
         return self._rf_spark_model
 
+    def _get_num_models(self) -> int:
+        return (
+            len(self._treelite_model) if isinstance(self._treelite_model, list) else 1
+        )
+
     def _is_classification(self) -> bool:
         return True
 
+    def _use_prob_as_raw_pred_col(self) -> bool:
+        return True
+
     @property
     def hasSummary(self) -> bool:
         """Indicates whether a training summary exists for this model instance."""
         return False
 
     @property
     def numClasses(self) -> int:
@@ -424,195 +609,124 @@
         ----------
         dataset : :py:class:`pyspark.sql.DataFrame`
             Test dataset to evaluate model on.
         """
         return self.cpu().evaluate(dataset)
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         _construct_rf, _, _ = super()._get_cuml_transform_func(dataset)
 
         def _predict(rf: CumlT, pdf: TransformInputType) -> pd.Series:
             data = {}
             rf.update_labels = False
             data[pred.prediction] = rf.predict(pdf)
 
-            if category == transform_evaluate.transform:
-                # transform_evaluate doesn't need probs for f1 score.
+            # non log-loss metric doesn't need probs.
+            if (
+                not eval_metric_info
+                or eval_metric_info.eval_metric == transform_evaluate_metric.log_loss
+            ):
                 probs = rf.predict_proba(pdf)
                 if isinstance(probs, pd.DataFrame):
                     # For 2302, when input is multi-cols, the output will be DataFrame
                     data[pred.probability] = pd.Series(probs.values.tolist())
                 else:
                     # should be np.ndarray
                     data[pred.probability] = pd.Series(list(probs))
 
             return pd.DataFrame(data)
 
-        def _evaluate(
-            input: TransformInputType,
-            transformed: TransformInputType,
-        ) -> pd.DataFrame:
-            # calculate the count of (label, prediction)
-            comb = pd.DataFrame(
-                {
-                    "label": input[alias.label],
-                    "prediction": transformed[pred.prediction],
-                }
-            )
-            confusion = (
-                comb.groupby(["label", "prediction"]).size().reset_index(name="total")
-            )
-            return confusion
-
-        return _construct_rf, _predict, _evaluate
-
-    def _transformEvaluate(
-        self,
-        dataset: DataFrame,
-        evaluator: Evaluator,
-        params: Optional["ParamMap"] = None,
-    ) -> List[float]:
-        """
-        Transforms and evaluates the input dataset with optional parameters in a single pass.
-
-        Parameters
-        ----------
-        dataset : :py:class:`pyspark.sql.DataFrame`
-            a dataset that contains labels/observations and predictions
-        evaluator: :py:class:`pyspark.ml.evaluation.Evaluator`
-            an evaluator user intends to use
-        params : dict, optional
-            an optional param map that overrides embedded params
-
-        Returns
-        -------
-        list of float
-            metrics
-        """
-
-        if not isinstance(evaluator, MulticlassClassificationEvaluator):
-            raise NotImplementedError(f"{evaluator} is unsupported yet.")
-
-        if (
-            evaluator.getMetricName()
-            not in MulticlassMetrics.SUPPORTED_MULTI_CLASS_METRIC_NAMES
-        ):
-            raise NotImplementedError(
-                f"{evaluator.getMetricName()} is not supported yet."
-            )
-
-        if self.getLabelCol() not in dataset.schema.names:
-            raise RuntimeError("Label column is not existing.")
-
-        dataset = dataset.withColumnRenamed(self.getLabelCol(), alias.label)
-
-        schema = StructType(
-            [
-                StructField(pred.model_index, IntegerType()),
-                StructField("label", FloatType()),
-                StructField("prediction", FloatType()),
-                StructField("total", FloatType()),
-            ]
-        )
-
-        rows = super()._transform_evaluate_internal(dataset, schema).collect()
-
-        num_models = (
-            len(self._treelite_model) if isinstance(self._treelite_model, list) else 1
+        _evaluate = (
+            self._get_evaluate_fn(eval_metric_info) if eval_metric_info else None
         )
 
-        tp_by_class: List[Dict[float, float]] = [{} for _ in range(num_models)]
-        fp_by_class: List[Dict[float, float]] = [{} for _ in range(num_models)]
-        label_count_by_class: List[Dict[float, float]] = [{} for _ in range(num_models)]
-        label_count = [0 for _ in range(num_models)]
-
-        for i in range(num_models):
-            for j in range(self._num_classes):
-                tp_by_class[i][float(j)] = 0.0
-                label_count_by_class[i][float(j)] = 0.0
-                fp_by_class[i][float(j)] = 0.0
-
-        for row in rows:
-            label_count[row.model_index] += row.total
-            label_count_by_class[row.model_index][row.label] += row.total
-
-            if row.label == row.prediction:
-                tp_by_class[row.model_index][row.label] += row.total
-            else:
-                fp_by_class[row.model_index][row.prediction] += row.total
-
-        scores = []
-        for i in range(num_models):
-            metrics = MulticlassMetrics(
-                tp=tp_by_class[i],
-                fp=fp_by_class[i],
-                label=label_count_by_class[i],
-                label_count=label_count[i],
-            )
-            scores.append(metrics.evaluate(evaluator))
-        return scores
+        return _construct_rf, _predict, _evaluate
 
 
 class LogisticRegressionClass(_CumlClass):
     @classmethod
     def _param_mapping(cls) -> Dict[str, Optional[str]]:
         return {
             "maxIter": "max_iter",
-            "regParam": "C",  # regParam = 1/C
+            "regParam": "C",
+            "elasticNetParam": "l1_ratio",
             "tol": "tol",
             "fitIntercept": "fit_intercept",
-            "elasticNetParam": None,
             "threshold": None,
             "thresholds": None,
-            "standardization": "",  # Set to "" instead of None because cuml defaults to standardization = False
+            "standardization": "standardization",
             "weightCol": None,
             "aggregationDepth": None,
             "family": "",  # family can be 'auto', 'binomial' or 'multinomial', cuml automatically detects num_classes
             "lowerBoundsOnCoefficients": None,
             "upperBoundsOnCoefficients": None,
             "lowerBoundsOnIntercepts": None,
             "upperBoundsOnIntercepts": None,
             "maxBlockSizeInMB": None,
-            "rawPredictionCol": "",
         }
 
     @classmethod
     def _param_value_mapping(
         cls,
     ) -> Dict[str, Callable[[Any], Union[None, str, float, int]]]:
-        def regParam_value_mapper(x: float) -> float:
-            # TODO: remove this checking and set regParam to 0.0 once no regularization is supported
-            if x == 0.0:
-                logger = get_logger(cls)
-                logger.warning(
-                    "no regularization is not supported yet. if regParam is set to 0,"
-                    + "it will be mapped to smallest positive float, i.e. numpy.finfo('float32').tiny"
-                )
-
-                return 1.0 / np.finfo("float32").tiny.item()
-            else:
-                return 1.0 / x
-
-        return {"C": lambda x: regParam_value_mapper(x)}
+        return {"C": lambda x: 1 / x if x > 0.0 else (0.0 if x == 0.0 else None)}
 
     def _get_cuml_params_default(self) -> Dict[str, Any]:
         return {
             "fit_intercept": True,
+            "standardization": False,
             "verbose": False,
             "C": 1.0,
+            "penalty": "l2",
+            "l1_ratio": None,
             "max_iter": 1000,
             "tol": 0.0001,
         }
 
+    # Given Spark params: regParam, elasticNetParam,
+    # return cuml params: penalty, C, l1_ratio
+    @classmethod
+    def _reg_params_value_mapping(
+        cls, reg_param: float, elasticNet_param: float
+    ) -> Tuple[str, float, float]:
+        # Note cuml ignores l1_ratio when penalty is "none", "l2", and "l1"
+        # Spark Rapids ML sets it to elasticNet_param to be compatible with Spark
+        if reg_param == 0.0:
+            penalty = "none"
+            C = 0.0
+            l1_ratio = elasticNet_param
+        elif elasticNet_param == 0.0:
+            penalty = "l2"
+            C = 1.0 / reg_param
+            l1_ratio = elasticNet_param
+        elif elasticNet_param == 1.0:
+            penalty = "l1"
+            C = 1.0 / reg_param
+            l1_ratio = elasticNet_param
+        else:
+            penalty = "elasticnet"
+            C = 1.0 / reg_param
+            l1_ratio = elasticNet_param
+
+        return (penalty, C, l1_ratio)
+
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return pyspark.ml.classification.LogisticRegression
+
 
 class _LogisticRegressionCumlParams(
     _CumlParams,
     _LogisticRegressionParams,
+    HasEnableSparseDataOptim,
     HasFeaturesCols,
     HasProbabilityCol,
     HasRawPredictionCol,
 ):
     def getFeaturesCol(self) -> Union[str, List[str]]:  # type:ignore
         """
         Gets the value of :py:attr:`featuresCol` or :py:attr:`featuresCols`
@@ -627,50 +741,50 @@
     def setFeaturesCol(
         self: "_LogisticRegressionCumlParams", value: Union[str, List[str]]
     ) -> "_LogisticRegressionCumlParams":
         """
         Sets the value of :py:attr:`featuresCol` or :py:attr:`featureCols`.
         """
         if isinstance(value, str):
-            self.set_params(featuresCol=value)
+            self._set_params(featuresCol=value)
         else:
-            self.set_params(featuresCols=value)
+            self._set_params(featuresCols=value)
         return self
 
     def setFeaturesCols(
         self: "_LogisticRegressionCumlParams", value: List[str]
     ) -> "_LogisticRegressionCumlParams":
         """
         Sets the value of :py:attr:`featuresCols`.
         """
-        return self.set_params(featuresCols=value)
+        return self._set_params(featuresCols=value)
 
     def setLabelCol(
         self: "_LogisticRegressionCumlParams", value: str
     ) -> "_LogisticRegressionCumlParams":
         """
         Sets the value of :py:attr:`labelCol`.
         """
-        return self.set_params(labelCol=value)
+        return self._set_params(labelCol=value)
 
     def setPredictionCol(
         self: "_LogisticRegressionCumlParams", value: str
     ) -> "_LogisticRegressionCumlParams":
         """
         Sets the value of :py:attr:`predictionCol`.
         """
-        return self.set_params(predictionCol=value)
+        return self._set_params(predictionCol=value)
 
     def setProbabilityCol(
         self: "_LogisticRegressionCumlParams", value: str
     ) -> "_LogisticRegressionCumlParams":
         """
         Sets the value of :py:attr:`probabilityCol`.
         """
-        return self.set_params(probabilityCol=value)
+        return self._set_params(probabilityCol=value)
 
     def setRawPredictionCol(
         self: "_LogisticRegressionCumlParams", value: str
     ) -> "_LogisticRegressionCumlParams":
         """
         Sets the value of :py:attr:`rawPredictionCol`.
         """
@@ -687,46 +801,61 @@
     combination of the features in X. It implements cuML's GPU accelerated
     LogisticRegression algorithm based on cuML python library, and it can be used in
     PySpark Pipeline and PySpark ML meta algorithms like
     :py:class:`~pyspark.ml.tuning.CrossValidator`/
     :py:class:`~pyspark.ml.tuning.TrainValidationSplit`/
     :py:class:`~pyspark.ml.classification.OneVsRest`
 
-    This currently supports the regularization options:
+    This supports multiple types of regularization:
 
     * none
     * L2 (ridge regression)
-
-    and two classes.
+    * L1 (lasso)
+    * L2 + L1 (elastic net)
 
     LogisticRegression automatically supports most of the parameters from both
     :py:class:`~pyspark.ml.classification.LogisticRegression`.
     And it will automatically map pyspark parameters
     to cuML parameters.
 
     Parameters
     ----------
-    featuresCol:
+    featuresCol: str or List[str]
         The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
             * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
             * When the value is a list of strings, the feature columns must be numeric types.
     labelCol:
         The label column name.
     predictionCol:
         The class prediction column name.
     probabilityCol:
         The probability prediction column name.
+    rawPredictionCol:
+        The column name for class raw predictions - this is currently set equal to probabilityCol values.
     maxIter:
         The maximum number of iterations of the underlying L-BFGS algorithm.
     regParam:
         The regularization parameter.
+    elasticNetParam:
+        The ElasticNet mixing parameter, in range [0, 1]. For alpha = 0,
+        the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.
     tol:
         The convergence tolerance.
+    enable_sparse_data_optim: None or boolean, optional (default=None)
+        If features column is VectorUDT type, Spark rapids ml relies on this parameter to decide whether to use dense array or sparse array in cuml.
+        If None, use dense array if the first VectorUDT of a dataframe is DenseVector. Use sparse array if it is SparseVector.
+        If False, always uses dense array. This is favorable if the majority of VectorUDT vectors are DenseVector.
+        If True, always uses sparse array. This is favorable if the majority of the VectorUDT vectors are SparseVector.
+        Note this is only supported in spark >= 3.4.
     fitIntercept:
         Whether to fit an intercept term.
+    standardization:
+        Whether to standardize the training data. If true, spark rapids ml sets enable_sparse_data_optim=False
+        to densify sparse vectors into dense vectors for fitting. Currently there is no support for sparse vectors
+        standardization in cuml yet.
     num_workers:
         Number of cuML workers, where each cuML worker corresponds to one Spark task
         running on one GPU. If not set, spark-rapids-ml tries to infer the number of
         cuML workers (i.e. GPUs in cluster) from the Spark environment.
     verbose:
     Logging level.
             * ``0`` - Disables all log messages.
@@ -774,84 +903,259 @@
     def __init__(
         self,
         *,
         featuresCol: Union[str, List[str]] = "features",
         labelCol: str = "label",
         predictionCol: str = "prediction",
         probabilityCol: str = "probability",
+        rawPredictionCol: str = "rawPrediction",
         maxIter: int = 100,
-        regParam: float = 0.0,  # NOTE: the default value of regParam is actually mapped to sys.float_info.min on GPU
+        regParam: float = 0.0,
+        elasticNetParam: float = 0.0,
         tol: float = 1e-6,
         fitIntercept: bool = True,
+        standardization: bool = True,
+        enable_sparse_data_optim: Optional[bool] = None,
         num_workers: Optional[int] = None,
         verbose: Union[int, bool] = False,
         **kwargs: Any,
     ):
         if not self._input_kwargs.get("float32_inputs", True):
             get_logger(self.__class__).warning(
                 "This estimator does not support double precision inputs. Setting float32_inputs to False will be ignored."
             )
             self._input_kwargs.pop("float32_inputs")
+
         super().__init__()
-        self.set_params(**self._input_kwargs)
+        self._set_cuml_reg_params()
+        self._set_params(**self._input_kwargs)
 
     def _fit_array_order(self) -> _ArrayOrder:
         return "C"
 
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         array_order = self._fit_array_order()
+        standardization = self.getStandardization()
+        fit_intercept = self.getFitIntercept()
+
+        logger = get_logger(self.__class__)
+        if (
+            self.getStandardization() is True
+            and self.getOrDefault("enable_sparse_data_optim") is not False
+        ):
+            logger.warning(
+                (
+                    "when standardization is True, spark rapids ml forces densifying sparse vectors to dense vectors for training."
+                )
+            )
 
         def _logistic_regression_fit(
             dfs: FitInputType,
             params: Dict[str, Any],
         ) -> Dict[str, Any]:
-            init_parameters = params[param_alias.cuml_init]
-
+            import cupyx
             from cuml.linear_model.logistic_regression_mg import LogisticRegressionMG
 
-            logistic_regression = LogisticRegressionMG(
-                handle=params[param_alias.handle],
-                **init_parameters,
-            )
-
-            logistic_regression.penalty_normalized = False
-            logistic_regression.lbfgs_memory = 10
-
             X_list = [x for (x, _, _) in dfs]
             y_list = [y for (_, y, _) in dfs]
+
             if isinstance(X_list[0], pd.DataFrame):
                 concated = pd.concat(X_list)
                 concated_y = pd.concat(y_list)
             else:
-                # features are either cp or np arrays here
+                # features are either cp, np, scipy csr or cupyx csr arrays here
                 concated = _concat_and_free(X_list, order=array_order)
                 concated_y = _concat_and_free(y_list, order=array_order)
 
+            is_sparse = isinstance(concated, scipy.sparse.csr_matrix) or isinstance(
+                concated, cupyx.scipy.sparse.csr_matrix
+            )
+
+            # densifying sparse vectors into dense to use standardization
+            if standardization is True and is_sparse is True:
+                concated = concated.toarray()
+
             pdesc = PartitionDescriptor.build(
-                [concated.shape[0]], params[param_alias.num_cols]
+                [concated.shape[0]],
+                params[param_alias.num_cols],
             )
 
-            logistic_regression.fit(
-                [(concated, concated_y)],
-                pdesc.m,
-                pdesc.n,
-                pdesc.parts_rank_size,
-                pdesc.rank,
-            )
-
-            return {
-                "coef_": [logistic_regression.coef_.tolist()],
-                "intercept_": [logistic_regression.intercept_.tolist()],
-                "n_cols": [logistic_regression.n_cols],
-                "dtype": [logistic_regression.dtype.name],
-            }
+            # Use cupy to standardize dataset as a workaround to gain better numeric stability
+            standarization_with_cupy = standardization
+            if standarization_with_cupy is True:
+                import cupy as cp
+
+                if isinstance(concated, np.ndarray):
+                    concated = cp.array(concated)
+                elif isinstance(concated, pd.DataFrame):
+                    concated = cp.array(concated.values)
+                else:
+                    assert isinstance(
+                        concated, cp.ndarray
+                    ), "only numpy array, cupy array, and pandas dataframe are supported when standardization_with_cupy is on"
+
+                mean_partial = concated.sum(axis=0) / pdesc.m
+
+                import json
+
+                from pyspark import BarrierTaskContext
+
+                context = BarrierTaskContext.get()
+
+                def all_gather_then_sum(
+                    cp_array: cp.ndarray, dtype: Union[np.float32, np.float64]
+                ) -> cp.ndarray:
+                    msgs = context.allGather(json.dumps(cp_array.tolist()))
+                    arrays = [json.loads(p) for p in msgs]
+                    array_sum = np.sum(arrays, axis=0).astype(dtype)
+                    return cp.array(array_sum)
+
+                mean = all_gather_then_sum(mean_partial, concated.dtype)
+                concated -= mean
+
+                l2 = cp.linalg.norm(concated, ord=2, axis=0)
+
+                var_partial = l2 * l2 / (pdesc.m - 1)
+                var = all_gather_then_sum(var_partial, concated.dtype)
+
+                assert cp.all(
+                    var >= 0
+                ), "numeric instable detected when calculating variance. Got negative variance"
+
+                stddev = cp.sqrt(var)
+
+                stddev_inv = cp.where(stddev != 0, 1.0 / stddev, 1.0)
+
+                if fit_intercept is False:
+                    concated += mean
+
+                concated *= stddev_inv
+
+            def _single_fit(init_parameters: Dict[str, Any]) -> Dict[str, Any]:
+                if standarization_with_cupy is True:
+                    init_parameters["standardization"] = False
+
+                if init_parameters["C"] == 0.0:
+                    init_parameters["penalty"] = "none"
+
+                elif init_parameters["l1_ratio"] == 0.0:
+                    init_parameters["penalty"] = "l2"
+
+                elif init_parameters["l1_ratio"] == 1.0:
+                    init_parameters["penalty"] = "l1"
+
+                else:
+                    init_parameters["penalty"] = "elasticnet"
+
+                logistic_regression = LogisticRegressionMG(
+                    handle=params[param_alias.handle],
+                    **init_parameters,
+                )
+
+                logistic_regression.penalty_normalized = False
+                logistic_regression.lbfgs_memory = 10
+
+                logistic_regression.fit(
+                    [(concated, concated_y)],
+                    pdesc.m,
+                    pdesc.n,
+                    pdesc.parts_rank_size,
+                    pdesc.rank,
+                )
+
+                coef_ = logistic_regression.coef_
+                intercept_ = logistic_regression.intercept_
+                if standarization_with_cupy is True:
+                    import cupy as cp
+
+                    coef_ = cp.where(stddev > 0, coef_ / stddev, coef_)
+                    if init_parameters["fit_intercept"] is True:
+                        intercept_ = intercept_ - cp.dot(coef_, mean)
+
+                intercept_array = intercept_
+                # follow Spark to center the intercepts for multinomial classification
+                if (
+                    init_parameters["fit_intercept"] is True
+                    and len(intercept_array) > 1
+                ):
+                    import cupy as cp
+
+                    intercept_mean = (
+                        np.mean(intercept_array)
+                        if isinstance(intercept_array, np.ndarray)
+                        else cp.mean(intercept_array)
+                    )
+                    intercept_array -= intercept_mean
+
+                n_cols = logistic_regression.n_cols
+
+                model = {
+                    "coef_": coef_[:, :n_cols].tolist(),
+                    "intercept_": intercept_.tolist(),
+                    "classes_": logistic_regression.classes_.tolist(),
+                    "n_cols": n_cols,
+                    "dtype": logistic_regression.dtype.name,
+                    "num_iters": logistic_regression.solver_model.num_iters,
+                    "objective": logistic_regression.solver_model.objective,
+                }
+
+                # check if invalid label exists
+                for class_val in model["classes_"]:
+                    if class_val < 0:
+                        raise RuntimeError(
+                            f"Labels MUST be in [0, 2147483647), but got {class_val}"
+                        )
+                    elif not class_val.is_integer():
+                        raise RuntimeError(
+                            f"Labels MUST be Integers, but got {class_val}"
+                        )
+
+                if len(logistic_regression.classes_) == 1:
+                    class_val = logistic_regression.classes_[0]
+                    # TODO: match Spark to use max(class_list) to calculate the number of classes
+                    # Cuml currently uses unique(class_list)
+                    if class_val != 1.0 and class_val != 0.0:
+                        raise RuntimeError(
+                            "class value must be either 1. or 0. when dataset has one label"
+                        )
+
+                    if init_parameters["fit_intercept"] is True:
+                        model["coef_"] = [[0.0] * n_cols]
+                        model["intercept_"] = [
+                            float("inf") if class_val == 1.0 else float("-inf")
+                        ]
+
+                del logistic_regression
+                return model
+
+            init_parameters = params[param_alias.cuml_init]
+            fit_multiple_params = params[param_alias.fit_multiple_params]
+            if len(fit_multiple_params) == 0:
+                fit_multiple_params.append({})
+
+            models = []
+            for i in range(len(fit_multiple_params)):
+                tmp_params = init_parameters.copy()
+                tmp_params.update(fit_multiple_params[i])
+                models.append(_single_fit(tmp_params))
+
+            models_dict = {}
+            tc = TaskContext.get()
+            assert tc is not None
+            if tc.partitionId() == 0:
+                for k in models[0].keys():
+                    models_dict[k] = [m[k] for m in models]
+            return models_dict
 
         return _logistic_regression_fit
 
     def _pre_process_data(
         self, dataset: DataFrame
     ) -> Tuple[
         List[Column], Optional[List[str]], int, Union[Type[FloatType], Type[DoubleType]]
@@ -866,155 +1170,357 @@
         return select_cols, multi_col_names, dimension, feature_type
 
     def _out_schema(self) -> Union[StructType, str]:
         return StructType(
             [
                 StructField("coef_", ArrayType(ArrayType(DoubleType()), False), False),
                 StructField("intercept_", ArrayType(DoubleType()), False),
+                StructField("classes_", ArrayType(DoubleType()), False),
                 StructField("n_cols", IntegerType(), False),
                 StructField("dtype", StringType(), False),
+                StructField("num_iters", IntegerType(), False),
+                StructField("objective", DoubleType(), False),
             ]
         )
 
     def _create_pyspark_model(self, result: Row) -> "LogisticRegressionModel":
-        return LogisticRegressionModel.from_row(result)
+        logger = get_logger(self.__class__)
+        if len(result["classes_"]) == 1:
+            if self.getFitIntercept() is False:
+                logger.warning(
+                    "All labels belong to a single class and fitIntercept=false. It's a dangerous ground, so the algorithm may not converge."
+                )
+            else:
+                logger.warning(
+                    "All labels are the same value and fitIntercept=true, so the coefficients will be zeros. Training is not needed."
+                )
+
+        return LogisticRegressionModel._from_row(result)
+
+    def _set_cuml_reg_params(self) -> "LogisticRegression":
+        penalty, C, l1_ratio = self._reg_params_value_mapping(
+            self.getRegParam(), self.getElasticNetParam()
+        )
+        self._cuml_params["penalty"] = penalty
+        self._cuml_params["C"] = C
+        self._cuml_params["l1_ratio"] = l1_ratio
+        return self
+
+    def _set_params(self, **kwargs: Any) -> "LogisticRegression":
+        super()._set_params(**kwargs)
+        if "regParam" in kwargs or "elasticNetParam" in kwargs:
+            self._set_cuml_reg_params()
+        return self
 
     def setMaxIter(self, value: int) -> "LogisticRegression":
         """
         Sets the value of :py:attr:`maxIter`.
         """
-        return self.set_params(maxIter=value)
+        return self._set_params(maxIter=value)
 
     def setRegParam(self, value: float) -> "LogisticRegression":
         """
         Sets the value of :py:attr:`regParam`.
         """
-        return self.set_params(regParam=value)
+        return self._set_params(regParam=value)
+
+    def setElasticNetParam(self, value: float) -> "LogisticRegression":
+        """
+        Sets the value of :py:attr:`regParam`.
+        """
+        return self._set_params(elasticNetParam=value)
 
     def setTol(self, value: float) -> "LogisticRegression":
         """
         Sets the value of :py:attr:`tol`.
         """
-        return self.set_params(tol=value)
+        return self._set_params(tol=value)
 
     def setFitIntercept(self, value: bool) -> "LogisticRegression":
         """
         Sets the value of :py:attr:`fitIntercept`.
         """
-        return self.set_params(fitIntercept=value)
+        return self._set_params(fitIntercept=value)
+
+    def _enable_fit_multiple_in_single_pass(self) -> bool:
+        return True
+
+    def _supportsTransformEvaluate(self, evaluator: Evaluator) -> bool:
+        if (
+            isinstance(evaluator, MulticlassClassificationEvaluator)
+            and evaluator.getMetricName()
+            in MulticlassMetrics.SUPPORTED_MULTI_CLASS_METRIC_NAMES
+        ):
+            return True
+
+        return False
 
 
 class LogisticRegressionModel(
     LogisticRegressionClass,
+    _ClassificationModelEvaluationMixIn,
     _CumlModelWithPredictionCol,
     _LogisticRegressionCumlParams,
 ):
     """Model fitted by :class:`LogisticRegression`."""
 
     def __init__(
         self,
-        coef_: List[List[float]],
-        intercept_: List[float],
+        coef_: Union[List[List[float]], List[List[List[float]]]],
+        intercept_: Union[List[float], List[List[float]]],
+        classes_: List[float],
         n_cols: int,
         dtype: str,
+        num_iters: int,
+        objective: float,
     ) -> None:
-        super().__init__(dtype=dtype, n_cols=n_cols, coef_=coef_, intercept_=intercept_)
+        super().__init__(
+            dtype=dtype,
+            n_cols=n_cols,
+            coef_=coef_,
+            intercept_=intercept_,
+            classes_=classes_,
+            num_iters=num_iters,
+            objective=objective,
+        )
         self.coef_ = coef_
         self.intercept_ = intercept_
+        self.classes_ = classes_
         self._lr_spark_model: Optional[SparkLogisticRegressionModel] = None
+        self._num_classes = len(self.classes_)
+        self.num_iters = num_iters
+        self.objective = objective
+        self._this_model = self
 
     def cpu(self) -> SparkLogisticRegressionModel:
         """Return the PySpark ML LogisticRegressionModel"""
         if self._lr_spark_model is None:
             sc = _get_spark_session().sparkContext
             assert sc._jvm is not None
 
-            # TODO Multinomial is not supported yet.
-            num_classes = 2
-            is_multinomial = False
-            num_coefficient_sets = 1
-            coefficients = self.coef_[0]
+            is_multinomial = False if len(self.classes_) == 2 else True
 
             assert self.n_cols is not None
-            coefficients_dmatrix = DenseMatrix(
-                num_coefficient_sets, self.n_cols, list(coefficients), True
-            )
-            intercepts = Vectors.dense(self.intercept)
 
             java_model = (
                 sc._jvm.org.apache.spark.ml.classification.LogisticRegressionModel(
                     java_uid(sc, "logreg"),
-                    _py2java(sc, coefficients_dmatrix),
-                    _py2java(sc, intercepts),
-                    num_classes,
+                    _py2java(sc, self.coefficientMatrix),
+                    _py2java(sc, self.interceptVector),
+                    self._num_classes,
                     is_multinomial,
                 )
             )
             self._lr_spark_model = SparkLogisticRegressionModel(java_model)
             self._copyValues(self._lr_spark_model)
 
         return self._lr_spark_model
 
+    def _get_num_models(self) -> int:
+        return 1 if isinstance(self.intercept_[0], float) else len(self.intercept_)
+
     @property
     def coefficients(self) -> Vector:
         """
         Model coefficients.
         """
-        assert len(self.coef_) == 1, "multi classes not supported yet"
-        return Vectors.dense(cast(list, self.coef_[0]))
+        if isinstance(self.coef_[0][0], float):
+            if len(self.coef_) == 1:
+                return Vectors.dense(cast(list, self.coef_[0]))
+            else:
+                raise Exception(
+                    "Multinomial models contain a matrix of coefficients, use coefficientMatrix instead."
+                )
+        else:
+            raise Exception("coefficients not defined for multi-model instance")
 
     @property
     def intercept(self) -> float:
         """
         Model intercept.
         """
-        assert len(self.intercept_) == 1, "multi classes not supported yet"
-        return self.intercept_[0]
+        if isinstance(self.intercept_[0], float):
+            if len(self.intercept_) == 1:
+                return self.intercept_[0]
+            else:
+                raise Exception(
+                    "Multinomial models contain a vector of intercepts, use interceptVector instead."
+                )
+        else:
+            raise Exception("intercept not defined for multi-model instance")
+
+    @property
+    def coefficientMatrix(self) -> Matrix:
+        """
+        Model coefficients.
+        Note Spark CPU uses denseCoefficientMatrix.compressed that may return a sparse vector
+        if there are many zero values. Since the compressed function is not available in pyspark,
+        Spark Rapids ML always returns a dense vector.
+        """
+
+        if isinstance(self.coef_[0][0], float):
+            n_rows = len(self.coef_)
+            n_cols = len(self.coef_[0])
+            flat_coef = [cast(float, c) for row in self.coef_ for c in row]
+            return DenseMatrix(
+                numRows=n_rows, numCols=n_cols, values=flat_coef, isTransposed=True
+            )
+        else:
+            raise Exception("coefficientMatrix not defined for multi-model instance")
+
+    @property
+    def interceptVector(self) -> Vector:
+        """
+        Model intercept.
+        """
+
+        if isinstance(self.intercept_[0], float):
+            nnz = np.count_nonzero(self.intercept_)
+
+            # spark returns interceptVec.compressed
+            # According spark doc, a dense vector needs 8 * size + 8 bytes, while a sparse vector needs 12 * nnz + 20 bytes.
+            if 1.5 * (nnz + 1.0) < len(self.intercept_):
+                size = len(self.intercept_)
+                data_m = {p[0]: cast(float, p[1]) for p in enumerate(self.intercept_)}
+                return Vectors.sparse(size, data_m)
+            else:
+                return Vectors.dense(cast(list, self.intercept_))
+        else:
+            raise Exception("interceptVector not defined for multi-model instance")
+
+    @property
+    def numClasses(self) -> int:
+        return self._num_classes
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         coef_ = self.coef_
         intercept_ = self.intercept_
+        classes_ = self.classes_
         n_cols = self.n_cols
         dtype = self.dtype
 
+        num_models = self._get_num_models()
+
+        # from cuml logistic_regression.pyx
+        def _predict_proba(scores: "cp.ndarray", _num_classes: int) -> "cp.ndarray":
+            import cupy as cp
+
+            if _num_classes == 2:
+                proba = cp.zeros((scores.shape[0], 2))
+                proba[:, 1] = 1 / (1 + cp.exp(-scores.ravel()))
+                proba[:, 0] = 1 - proba[:, 1]
+            elif _num_classes > 2:
+                max_scores = cp.max(scores, axis=1).reshape((-1, 1))
+                scores_shifted = scores - max_scores
+                proba = cp.exp(scores_shifted)
+                row_sum = cp.sum(proba, axis=1).reshape((-1, 1))
+                proba /= row_sum
+            return proba
+
+        def _predict_labels(scores: "cp.ndarray", _num_classes: int) -> "cp.ndarray":
+            import cupy as cp
+
+            _num_classes = max(scores.shape[1] if len(scores.shape) == 2 else 2, 2)
+            if _num_classes == 2:
+                predictions = (scores.ravel() > 0).astype("float32")
+            else:
+                predictions = cp.argmax(scores, axis=1)
+            return predictions
+
         def _construct_lr() -> CumlT:
+            import cupy as cp
             import numpy as np
             from cuml.internals.input_utils import input_to_cuml_array
             from cuml.linear_model.logistic_regression_mg import LogisticRegressionMG
 
-            lr = LogisticRegressionMG(output_type="numpy")
-            lr.n_cols = n_cols
-            lr.dtype = np.dtype(dtype)
-            lr.intercept_ = input_to_cuml_array(
-                np.array(intercept_, order="C").astype(dtype)
-            ).array
-            lr.coef_ = input_to_cuml_array(
-                np.array(coef_, order="C").astype(dtype)
-            ).array
-            # TBD: infer class indices from data for > 2 classes
-            # needed for predict_proba
-            lr.classes_ = input_to_cuml_array(
-                np.array([0, 1], order="F").astype(dtype)
-            ).array
-            return lr
+            _intercepts, _coefs = (
+                (intercept_, coef_) if num_models > 1 else ([intercept_], [coef_])
+            )
+            lrs = []
+
+            for i in range(num_models):
+                lr = LogisticRegressionMG(output_type="cupy")
+                lr.n_cols = n_cols
+                lr.dtype = np.dtype(dtype)
+
+                gpu_intercept_ = cp.array(_intercepts[i], order="C", dtype=dtype)
+
+                gpu_coef_ = cp.array(_coefs[i], order="F", dtype=dtype).T
+                gpu_stacked = cp.vstack([gpu_coef_, gpu_intercept_])
+                lr.solver_model._coef_ = input_to_cuml_array(
+                    gpu_stacked, order="C"
+                ).array
+
+                lr.classes_ = input_to_cuml_array(
+                    np.array(classes_, order="F").astype(dtype)
+                ).array
+                lr._num_classes = len(lr.classes_)
+
+                lr.loss = "sigmoid" if lr._num_classes <= 2 else "softmax"
+                lr.solver_model.qnparams = lr.create_qnparams()
+                lrs.append(lr)
+
+            return lrs
+
+        _evaluate = (
+            self._get_evaluate_fn(eval_metric_info) if eval_metric_info else None
+        )
 
         def _predict(lr: CumlT, pdf: TransformInputType) -> pd.DataFrame:
+            import cupy as cp
+
             data = {}
-            data[pred.prediction] = lr.predict(pdf)
-            probs = lr.predict_proba(pdf)
-            if isinstance(probs, pd.DataFrame):
-                data[pred.probability] = pd.Series(probs.values.tolist())
-            else:
-                # should be np.ndarray
-                data[pred.probability] = pd.Series(list(probs))
+            scores = lr.decision_function(pdf).T
+            assert isinstance(scores, cp.ndarray)
+            _num_classes = max(scores.shape[1] if len(scores.shape) == 2 else 2, 2)
+            data[pred.prediction] = pd.Series(
+                list(_predict_labels(scores, _num_classes).get())
+            )
+            # non log-loss metric doesn't need probs.
+            if (
+                not eval_metric_info
+                or eval_metric_info.eval_metric == transform_evaluate_metric.log_loss
+            ):
+                data[pred.probability] = pd.Series(
+                    list(_predict_proba(scores, _num_classes).get())
+                )
+                if _num_classes == 2:
+                    raw_prediction = cp.zeros((scores.shape[0], 2))
+                    raw_prediction[:, 1] = scores.ravel()
+                    raw_prediction[:, 0] = -raw_prediction[:, 1]
+                elif _num_classes > 2:
+                    raw_prediction = scores
+                data[pred.raw_prediction] = pd.Series(list(cp.asnumpy(raw_prediction)))
+
             return pd.DataFrame(data)
 
-        return _construct_lr, _predict, None
+        return _construct_lr, _predict, _evaluate
+
+    @classmethod
+    def _combine(
+        cls: Type["LogisticRegressionModel"], models: List["LogisticRegressionModel"]  # type: ignore
+    ) -> "LogisticRegressionModel":
+        assert len(models) > 0 and all(isinstance(model, cls) for model in models)
+        first_model = models[0]
+        intercepts = [model.intercept_ for model in models]
+        coefs = [model.coef_ for model in models]
+        attrs = first_model._get_model_attributes()
+        assert attrs is not None
+        attrs["coef_"] = coefs
+        attrs["intercept_"] = intercepts
+        lr_model = cls(**attrs)
+        first_model._copyValues(lr_model)
+        first_model._copy_cuml_params(lr_model)
+        return lr_model
 
     @property
     def hasSummary(self) -> bool:
         """
         Indicates whether a training summary exists for this model
         instance.
         """
```

## spark_rapids_ml/clustering.py

```diff
@@ -1,27 +1,30 @@
 #
-# Copyright (c) 2022-2023, NVIDIA CORPORATION.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
+from abc import ABCMeta
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union, cast
 
 import numpy as np
 import pandas as pd
+import pyspark
+from pyspark import keyword_only
 from pyspark.ml.clustering import KMeansModel as SparkKMeansModel
 from pyspark.ml.clustering import _KMeansParams
 from pyspark.ml.linalg import Vector
 from pyspark.sql.dataframe import DataFrame
 from pyspark.sql.types import (
     ArrayType,
     DoubleType,
@@ -37,53 +40,85 @@
     FitInputType,
     _ConstructFunc,
     _CumlEstimator,
     _CumlModelWithPredictionCol,
     _EvaluateFunc,
     _TransformFunc,
     param_alias,
-    transform_evaluate,
 )
+from .metrics import EvalMetricInfo
 from .params import HasFeaturesCols, P, _CumlClass, _CumlParams
 from .utils import (
     _ArrayOrder,
     _concat_and_free,
     _get_spark_session,
     get_logger,
     java_uid,
 )
 
 
 class KMeansClass(_CumlClass):
     @classmethod
     def _param_mapping(cls) -> Dict[str, Optional[str]]:
-        return {
+        param_map = {
             "distanceMeasure": None,
             "initMode": "init",
             "k": "n_clusters",
             "initSteps": "",
             "maxIter": "max_iter",
             "seed": "random_state",
             "tol": "tol",
             "weightCol": None,
+            "solver": "",
+            "maxBlockSizeInMB": "",
         }
 
+        import pyspark
+        from packaging import version
+
+        if version.parse(pyspark.__version__) < version.parse("3.4.0"):
+            param_map.pop("solver")
+            param_map.pop("maxBlockSizeInMB")
+
+        return param_map
+
+    @classmethod
+    def _param_value_mapping(
+        cls,
+    ) -> Dict[str, Callable[[Any], Union[None, str, float, int]]]:
+        def tol_value_mapper(x: float) -> float:
+            if x == 0.0:
+                logger = get_logger(cls)
+                logger.warn(
+                    "tol=0 is not supported in cuml yet. "
+                    + "It will be mapped to smallest positive float, i.e. numpy.finfo('float32').tiny."
+                )
+
+                return np.finfo("float32").tiny.item()
+            else:
+                return x
+
+        return {"tol": lambda x: tol_value_mapper(x)}
+
     def _get_cuml_params_default(self) -> Dict[str, Any]:
         return {
             "n_clusters": 8,
             "max_iter": 300,
             "tol": 0.0001,
             "verbose": False,
             "random_state": 1,
             "init": "scalable-k-means++",
             "n_init": 1,
             "oversampling_factor": 2.0,
             "max_samples_per_batch": 32768,
         }
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return pyspark.ml.clustering.KMeans
+
 
 class _KMeansCumlParams(_CumlParams, _KMeansParams, HasFeaturesCols):
     """
     Shared Spark Params for KMeans and KMeansModel.
     """
 
     def __init__(self) -> None:
@@ -100,33 +135,33 @@
         elif self.isDefined(self.featuresCol):
             return self.getOrDefault("featuresCol")
         else:
             raise RuntimeError("featuresCol is not set")
 
     def setFeaturesCol(self: P, value: Union[str, List[str]]) -> P:
         """
-        Sets the value of :py:attr:`featuresCol` or :py:attr:`featuresCols`. Used when input vectors are stored in a single column.
+        Sets the value of :py:attr:`featuresCol` or :py:attr:`featuresCols`.
         """
         if isinstance(value, str):
-            self.set_params(featuresCol=value)
+            self._set_params(featuresCol=value)
         else:
-            self.set_params(featuresCols=value)
+            self._set_params(featuresCols=value)
         return self
 
     def setFeaturesCols(self: P, value: List[str]) -> P:
         """
         Sets the value of :py:attr:`featuresCols`. Used when input vectors are stored as multiple feature columns.
         """
-        return self.set_params(featuresCols=value)
+        return self._set_params(featuresCols=value)
 
     def setPredictionCol(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`predictionCol`.
         """
-        self.set_params(predictionCol=value)
+        self._set_params(predictionCol=value)
         return self
 
 
 class KMeans(KMeansClass, _CumlEstimator, _KMeansCumlParams):
     """
     KMeans algorithm partitions data points into a fixed number (denoted as k) of clusters.
     The algorithm initializes a set of k random centers then runs in iterations.
@@ -135,33 +170,50 @@
     This class provides GPU acceleration for pyspark distributed KMeans.
 
     Parameters
     ----------
     k: int (default = 8)
         the number of centers. Set this parameter to enable KMeans to learn k centers from input vectors.
 
+    initMode: str (default = "k-means||")
+        the algorithm to select initial centroids. It can be "k-means||" or "random".
+
     maxIter: int (default = 300)
         the maximum iterations the algorithm will run to learn the k centers.
         More iterations help generate more accurate centers.
 
     seed: int (default = 1)
         the random seed used by the algorithm to initialize a set of k random centers to start with.
 
     tol: float (default = 1e-4)
         early stopping criterion if centers do not change much after an iteration.
 
-    featuresCol: str
-        the name of the column that contains input vectors. featuresCol should be set when input vectors are stored in a single column of a dataframe.
-
-    featuresCols: List[str]
-        the names of feature columns that form input vectors. featureCols should be set when input vectors are stored as multiple feature columns of a dataframe.
+    featuresCol: str or List[str]
+        The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
+            * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
+            * When the value is a list of strings, the feature columns must be numeric types.
 
     predictionCol: str
         the name of the column that stores cluster indices of input vectors. predictionCol should be set when users expect to apply the transform function of a learned model.
 
+    num_workers:
+        Number of cuML workers, where each cuML worker corresponds to one Spark task
+        running on one GPU. If not set, spark-rapids-ml tries to infer the number of
+        cuML workers (i.e. GPUs in cluster) from the Spark environment.
+
+    verbose:
+    Logging level.
+            * ``0`` - Disables all log messages.
+            * ``1`` - Enables only critical messages.
+            * ``2`` - Enables all messages up to and including errors.
+            * ``3`` - Enables all messages up to and including warnings.
+            * ``4 or False`` - Enables all messages up to and including information messages.
+            * ``5 or True`` - Enables all messages up to and including debug messages.
+            * ``6`` - Enables all messages up to and including trace messages.
+
     Examples
     --------
     >>> from spark_rapids_ml.clustering import KMeans
     >>> data = [([0.0, 0.0],),
     ...         ([1.0, 1.0],),
     ...         ([9.0, 8.0],),
     ...         ([8.0, 9.0],),]
@@ -216,58 +268,75 @@
     >>> df = spark.createDataFrame(data, ["f1", "f2"])
     >>> gpu_kmeans = KMeans(k=2).setFeaturesCols(["f1", "f2"])
     >>> gpu_kmeans.getFeaturesCols()
     ['f1', 'f2']
     >>> gpu_kmeans = gpu_kmeans.fit(df)
     """
 
-    def __init__(self, **kwargs: Any) -> None:
+    @keyword_only
+    def __init__(
+        self,
+        *,
+        featuresCol: str = "features",
+        predictionCol: str = "prediction",
+        k: int = 2,
+        initMode: str = "k-means||",
+        tol: float = 0.0001,
+        maxIter: int = 20,
+        seed: Optional[int] = None,
+        num_workers: Optional[int] = None,
+        verbose: Union[int, bool] = False,
+        **kwargs: Any,
+    ) -> None:
         super().__init__()
-        self.set_params(**kwargs)
+        self._set_params(**self._input_kwargs)
 
     def setK(self, value: int) -> "KMeans":
         """
         Sets the value of :py:attr:`k`.
         """
-        return self.set_params(k=value)
+        return self._set_params(k=value)
 
     def setMaxIter(self, value: int) -> "KMeans":
         """
         Sets the value of :py:attr:`maxIter`.
         """
-        return self.set_params(maxIter=value)
+        return self._set_params(maxIter=value)
 
     def setSeed(self, value: int) -> "KMeans":
         """
         Sets the value of :py:attr:`seed`.
         """
         if value > 0x07FFFFFFF:
             raise ValueError("cuML seed value must be a 32-bit integer.")
-        return self.set_params(seed=value)
+        return self._set_params(seed=value)
 
     def setTol(self, value: float) -> "KMeans":
         """
         Sets the value of :py:attr:`tol`.
         """
-        return self.set_params(tol=value)
+        return self._set_params(tol=value)
 
     def setWeightCol(self, value: str) -> "KMeans":
         """
         Sets the value of :py:attr:`weightCol`.
         """
         raise ValueError("'weightCol' is not supported by cuML.")
 
     def _fit_array_order(self) -> _ArrayOrder:
         return "C"
 
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         cls = self.__class__
 
         array_order = self._fit_array_order()
 
         def _cuml_fit(
             dfs: FitInputType,
             params: Dict[str, Any],
@@ -316,15 +385,15 @@
                 ),
                 StructField("n_cols", IntegerType(), False),
                 StructField("dtype", StringType(), False),
             ]
         )
 
     def _create_pyspark_model(self, result: Row) -> "KMeansModel":
-        return KMeansModel.from_row(result)
+        return KMeansModel._from_row(result)
 
 
 class KMeansModel(KMeansClass, _CumlModelWithPredictionCol, _KMeansCumlParams):
     """
     KMeans gpu model for clustering input vectors to learned k centers.
     Refer to the KMeans class for learning the k centers.
     """
@@ -383,16 +452,20 @@
         ret_schema = "int"
         return ret_schema
 
     def _transform_array_order(self) -> _ArrayOrder:
         return "C"
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         cuml_alg_params = self.cuml_params.copy()
 
         cluster_centers_ = self.cluster_centers_
         dtype = self.dtype
         n_cols = self.n_cols
         array_order = self._transform_array_order()
```

## spark_rapids_ml/core.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023, NVIDIA CORPORATION.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,15 +12,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 import json
 import os
 import threading
-from abc import abstractmethod
+from abc import ABCMeta, abstractmethod
 from collections import namedtuple
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     Generic,
@@ -32,46 +32,50 @@
     Type,
     TypeVar,
     Union,
 )
 
 import numpy as np
 import pandas as pd
-from pyspark import RDD, TaskContext
+from pyspark import RDD, SparkConf, TaskContext
 from pyspark.ml import Estimator, Model
 from pyspark.ml.evaluation import Evaluator
 from pyspark.ml.functions import array_to_vector, vector_to_array
-from pyspark.ml.linalg import VectorUDT
+from pyspark.ml.linalg import DenseVector, SparseVector, VectorUDT
 from pyspark.ml.param.shared import (
     HasLabelCol,
     HasOutputCol,
     HasPredictionCol,
     HasProbabilityCol,
+    HasRawPredictionCol,
 )
 from pyspark.ml.util import (
     DefaultParamsReader,
     DefaultParamsWriter,
     MLReadable,
     MLReader,
     MLWritable,
     MLWriter,
 )
+from pyspark.ml.wrapper import JavaParams
 from pyspark.sql import Column, DataFrame
 from pyspark.sql.functions import col, struct
 from pyspark.sql.pandas.functions import pandas_udf
 from pyspark.sql.types import (
     ArrayType,
     DoubleType,
     FloatType,
     IntegralType,
     Row,
     StructType,
 )
+from scipy.sparse import csr_matrix
 
 from .common.cuml_context import CumlContext
+from .metrics import EvalMetricInfo
 from .params import _CumlParams
 from .utils import (
     _ArrayOrder,
     _get_gpu_id,
     _get_spark_session,
     _is_local,
     _is_standalone_or_localcluster,
@@ -108,35 +112,137 @@
         TransformInputType,  # input dataset with label column
         TransformInputType,  # inferred dataset with prediction column
     ],
     pd.DataFrame,
 ]
 
 # Global constant for defining column alias
-Alias = namedtuple("Alias", ("data", "label", "row_number"))
-alias = Alias("cuml_values", "cuml_label", "unique_id")
+Alias = namedtuple(
+    "Alias",
+    (
+        "featureVectorType",
+        "featureVectorSize",
+        "featureVectorIndices",
+        "data",
+        "label",
+        "row_number",
+    ),
+)
+alias = Alias(
+    "vector_type_c3BhcmstcmFwaWRzLW1sCg==",
+    "vector_size_c3BhcmstcmFwaWRzLW1sCg==",
+    "vector_indices_c3BhcmstcmFwaWRzLW1sCg==",
+    "cuml_values_c3BhcmstcmFwaWRzLW1sCg==",
+    "cuml_label",
+    "unique_id",
+)
 
 # Global prediction names
-Pred = namedtuple("Pred", ("prediction", "probability", "model_index"))
-pred = Pred("prediction", "probability", "model_index")
+Pred = namedtuple(
+    "Pred", ("prediction", "probability", "model_index", "raw_prediction")
+)
+pred = Pred("prediction", "probability", "model_index", "raw_prediction")
 
 # Global parameter alias used by core and subclasses.
 ParamAlias = namedtuple(
     "ParamAlias",
     ("cuml_init", "handle", "num_cols", "part_sizes", "loop", "fit_multiple_params"),
 )
 param_alias = ParamAlias(
     "cuml_init", "handle", "num_cols", "part_sizes", "loop", "fit_multiple_params"
 )
 
 CumlModel = TypeVar("CumlModel", bound="_CumlModel")
 
-# Global parameter used by core and subclasses.
-TransformEvaluate = namedtuple("TransformEvaluate", ("transform", "transform_evaluate"))
-transform_evaluate = TransformEvaluate("transform", "transform_evaluate")
+from .utils import _get_unwrap_udt_fn
+
+
+# similar to the XGBOOST _get_unwrapped_vec_cols in https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/spark/core.py
+def _get_unwrapped_vec_cols(feature_col: Column) -> List[Column]:
+    unwrap_udt = _get_unwrap_udt_fn()
+    features_unwrapped_vec_col = unwrap_udt(feature_col)
+
+    # After a `pyspark.ml.linalg.VectorUDT` type column being unwrapped, it becomes
+    # a pyspark struct type column, the struct fields are:
+    #  - `type`: byte
+    #  - `size`: int
+    #  - `indices`: array<int>
+    #  - `values`: array<double>
+    # For sparse vector, `type` field is 0, `size` field means vector dimension,
+    # `indices` field is the array of active element indices, `values` field
+    # is the array of active element values.
+    # For dense vector, `type` field is 1, `size` and `indices` fields are None,
+    # `values` field is the array of the vector element values.
+    return [
+        features_unwrapped_vec_col.type.alias(alias.featureVectorType),
+        features_unwrapped_vec_col.size.alias(alias.featureVectorSize),
+        features_unwrapped_vec_col.indices.alias(alias.featureVectorIndices),
+        # Note: the value field is double array type, cast it to float32 array type
+        # for speedup following repartitioning.
+        features_unwrapped_vec_col.values.cast(ArrayType(FloatType())).alias(
+            alias.data
+        ),
+    ]
+
+
+def _use_sparse_in_cuml(dataset: DataFrame) -> bool:
+    return (
+        alias.featureVectorType in dataset.schema.fieldNames()
+        and alias.featureVectorSize in dataset.schema.fieldNames()
+        and alias.featureVectorIndices in dataset.schema.fieldNames()
+    )  # use sparse array in cuml only if features vectorudt column was unwrapped
+
+
+# similar to the XGBOOST _read_csr_matrix_from_unwrapped_spark_vec in https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/spark/data.py
+def _read_csr_matrix_from_unwrapped_spark_vec(part: pd.DataFrame) -> csr_matrix:
+    # variables for constructing csr_matrix
+    csr_indices_list, csr_indptr_list, csr_values_list = [], [0], []
+
+    n_features = 0
+
+    # TBD: investigate if there is a more efficient 'vectorized' approach to doing this. Iterating in python can be slow
+    for vec_type, vec_size_, vec_indices, vec_values in zip(
+        part[alias.featureVectorType],
+        part[alias.featureVectorSize],
+        part[alias.featureVectorIndices],
+        part[alias.data],
+    ):
+        if vec_type == 0:
+            # sparse vector
+            vec_size = int(vec_size_)
+            csr_indices = vec_indices
+            csr_values = vec_values
+        else:
+            # dense vector
+            # Note: According to spark ML VectorUDT format,
+            # when type field is 1, the size field is also empty.
+            # we need to check the values field to get vector length.
+            vec_size = len(vec_values)
+            csr_indices = np.arange(vec_size, dtype=np.int32)
+            csr_values = vec_values
+
+        if n_features == 0:
+            n_features = vec_size
+        assert n_features == vec_size, "all vectors must be of the same dimension"
+
+        csr_indices_list.append(csr_indices)
+        csr_indptr_list.append(csr_indptr_list[-1] + len(csr_indices))
+        assert len(csr_indptr_list) == 1 + len(csr_indices_list)
+
+        csr_values_list.append(csr_values)
+
+    assert len(csr_indptr_list) == 1 + len(part)
+
+    csr_indptr_arr = np.array(csr_indptr_list)
+    csr_indices_arr = np.concatenate(csr_indices_list)
+    csr_values_arr = np.concatenate(csr_values_list)
+
+    return csr_matrix(
+        (csr_values_arr, csr_indices_arr, csr_indptr_arr), shape=(len(part), n_features)
+    )
 
 
 class _CumlEstimatorWriter(MLWriter):
     """
     Write the parameters of _CumlEstimator to the file
     """
 
@@ -194,15 +300,15 @@
             extraMetadata={
                 "_cuml_params": self.instance._cuml_params,
                 "_num_workers": self.instance._num_workers,
                 "_float32_inputs": self.instance._float32_inputs,
             },
         )
         data_path = os.path.join(path, "data")
-        model_attributes = self.instance.get_model_attributes()
+        model_attributes = self.instance._get_model_attributes()
         model_attributes_str = json.dumps(model_attributes)
         self.sc.parallelize([model_attributes_str], 1).saveAsTextFile(data_path)
 
 
 class _CumlModelReader(MLReader):
     """
     Instantiate the _CumlModel from the file.
@@ -226,19 +332,19 @@
 
 
 class _CumlCommon(MLWritable, MLReadable):
     def __init__(self) -> None:
         super().__init__()
 
     @staticmethod
-    def set_gpu_device(
+    def _get_gpu_device(
         context: Optional[TaskContext], is_local: bool, is_transform: bool = False
-    ) -> None:
+    ) -> int:
         """
-        Set gpu device according to the spark task resources.
+        Get gpu device according to the spark task resources.
 
         If it is local mode, we use partition id as gpu id for training
         and (partition id ) % gpus for transform.
         """
         # Get the GPU ID from resources
         assert context is not None
 
@@ -251,18 +357,37 @@
                 total_gpus = cupy.cuda.runtime.getDeviceCount()
                 gpu_id = partition_id % total_gpus
             else:
                 gpu_id = partition_id
         else:
             gpu_id = _get_gpu_id(context)
 
+        return gpu_id
+
+    @staticmethod
+    def _set_gpu_device(
+        context: Optional[TaskContext], is_local: bool, is_transform: bool = False
+    ) -> None:
+        """
+        Set gpu device according to the spark task resources.
+
+        If it is local mode, we use partition id as gpu id for training
+        and (partition id ) % gpus for transform.
+        """
+        # Get the GPU ID from resources
+        assert context is not None
+
+        import cupy
+
+        gpu_id = _CumlCommon._get_gpu_device(context, is_local, is_transform)
+
         cupy.cuda.Device(gpu_id).use()
 
     @staticmethod
-    def initialize_cuml_logging(verbose: Optional[Union[bool, int]]) -> None:
+    def _initialize_cuml_logging(verbose: Optional[Union[bool, int]]) -> None:
         """Initializes the logger for cuML.
 
         Parameters
         ----------
         verbose : Optional[Union[bool, int]]
             If True, sets the log_level to 5.  If integer value, sets the log_level to the value.
         """
@@ -278,28 +403,38 @@
             elif isinstance(verbose, int):
                 log_level = verbose
             else:
                 raise ValueError(f"invalid value for verbose parameter: {verbose}")
 
             cuml_logger.set_level(log_level)
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        """
+        Subclass should override to return corresponding pyspark.ml class
+        Ex. logistic regression should return pyspark.ml.classification.LogisticRegression
+        Return None if no corresponding class in pyspark, e.g. knn
+        """
+        raise NotImplementedError(
+            "pyspark.ml class corresponding to estimator not specified."
+        )
+
 
 class _CumlCaller(_CumlParams, _CumlCommon):
     """
     This class is responsible for calling cuml function (e.g. fit or kneighbor) on pyspark dataframe,
     to run a multi-node multi-gpu algorithm on the dataframe. A function usually comes from a multi-gpu cuml class,
     such as cuml.decomposition.pca_mg.PCAMG or cuml.neighbors.nearest_neighbors_mg.NearestNeighborsMG.
     This class converts dataframe into cuml input type, and leverages NCCL or UCX for communicator. To use this class,
     developers can override the key methods including _out_schema(...) and _get_cuml_fit_func(...). Examples can be found in
     spark_rapids_ml.clustering.KMeans and spark_rapids_ml.regression.LinearRegression.
     """
 
     def __init__(self) -> None:
         super().__init__()
-        self.initialize_cuml_params()
+        self._initialize_cuml_params()
 
     @abstractmethod
     def _out_schema(self) -> Union[StructType, str]:
         """
         The output schema of the estimator, which will be used to
         construct the returning pandas dataframe
         """
@@ -307,17 +442,15 @@
 
     def _repartition_dataset(self, dataset: DataFrame) -> DataFrame:
         """
         Repartition the dataset to the desired number of workers.
         """
         return dataset.repartition(self.num_workers)
 
-    def _pre_process_data(
-        self, dataset: DataFrame
-    ) -> Tuple[
+    def _pre_process_data(self, dataset: DataFrame) -> Tuple[
         List[Column],
         Optional[List[str]],
         int,
         Union[Type[FloatType], Type[DoubleType]],
     ]:
         select_cols = []
 
@@ -325,14 +458,15 @@
         feature_type: Union[Type[FloatType], Type[DoubleType]] = FloatType
 
         input_col, input_cols = self._get_input_columns()
 
         if input_col is not None:
             # Single Column
             input_datatype = dataset.schema[input_col].dataType
+            first_record = dataset.first()
 
             if isinstance(input_datatype, ArrayType):
                 # Array type
                 if (
                     isinstance(input_datatype.elementType, DoubleType)
                     and not self._float32_inputs
                 ):
@@ -345,25 +479,45 @@
                     select_cols.append(
                         col(input_col).cast(ArrayType(feature_type())).alias(alias.data)
                     )
                 else:
                     # FloatType array
                     select_cols.append(col(input_col).alias(alias.data))
             elif isinstance(input_datatype, VectorUDT):
-                # Vector type
                 vector_element_type = "float32" if self._float32_inputs else "float64"
-                select_cols.append(
-                    vector_to_array(col(input_col), vector_element_type).alias(alias.data)  # type: ignore
+                first_vectorudt_type = (
+                    DenseVector
+                    if first_record is None
+                    or type(first_record[input_col]) is DenseVector
+                    else SparseVector
                 )
+                use_sparse = self.hasParam(
+                    "enable_sparse_data_optim"
+                ) and self.getOrDefault("enable_sparse_data_optim")
+
+                if use_sparse is True or (
+                    use_sparse is None and first_vectorudt_type is SparseVector
+                ):
+                    # Sparse Vector type
+                    select_cols += _get_unwrapped_vec_cols(col(input_col))
+                else:
+                    # Dense Vector type
+                    assert use_sparse is False or (
+                        use_sparse is None and first_vectorudt_type is DenseVector
+                    )
+                    select_cols.append(
+                        vector_to_array(col(input_col), vector_element_type).alias(alias.data)  # type: ignore
+                    )
+
                 if not self._float32_inputs:
                     feature_type = DoubleType
             else:
                 raise ValueError("Unsupported input type.")
 
-            dimension = len(dataset.first()[input_col])  # type: ignore
+            dimension = len(first_record[input_col])  # type: ignore
 
         elif input_cols is not None:
             # if self._float32_inputs is False and if any columns are double type, convert all to double type
             # otherwise convert all to float type
             any_double_types = any(
                 [isinstance(dataset.schema[c].dataType, DoubleType) for c in input_cols]
             )
@@ -396,20 +550,48 @@
         If enable or disable communication layer (NCCL or UCX).
         Return (False, False) if no communication layer is required.
         Return (True, False) if only NCCL is required.
         Return (True, True) if UCX is required. Cuml UCX backend currently also requires NCCL.
         """
         return (True, False)
 
+    def _validate_parameters(self) -> None:
+        cls_name = self._pyspark_class()
+
+        if cls_name is not None:
+            pyspark_est = cls_name()
+            # Both pyspark and cuml may have a parameter with the same name,
+            # but cuml might have additional optional values that can be set.
+            # If we transfer these cuml-specific values to the Spark JVM,
+            # it would result in an exception.
+            # To avoid this issue, we skip transferring these parameters
+            # since the mapped parameters have been validated in _get_cuml_mapping_value.
+            cuml_est = self.copy()
+            cuml_params = cuml_est._param_value_mapping().keys()
+            param_mapping = cuml_est._param_mapping()
+            pyspark_params = [k for k, v in param_mapping.items() if v in cuml_params]
+            for p in pyspark_params:
+                cuml_est.clear(cuml_est.getParam(p))
+
+            cuml_est._copyValues(pyspark_est)
+            # validate the parameters
+            pyspark_est._transfer_params_to_java()
+
+            del pyspark_est
+            del cuml_est
+
     @abstractmethod
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         """
         Subclass must implement this function to return a cuml fit function that will be
         sent to executor to run.
 
         Eg,
 
         def _get_cuml_fit_func(self, dataset: DataFrame, extra_params: Optional[List[Dict[str, Any]]] = None):
@@ -445,14 +627,15 @@
             input dataset
 
         Returns
         -------
         :class:`Transformer`
             fitted model
         """
+        self._validate_parameters()
 
         cls = self.__class__
 
         select_cols, multi_col_names, dimension, _ = self._pre_process_data(dataset)
 
         num_workers = self.num_workers
 
@@ -493,71 +676,83 @@
             dataset, None if len(fit_multiple_params) == 0 else fit_multiple_params
         )
 
         array_order = self._fit_array_order()
 
         cuml_verbose = self.cuml_params.get("verbose", False)
 
+        use_sparse_array = _use_sparse_in_cuml(dataset)
+
         (enable_nccl, require_ucx) = self._require_nccl_ucx()
 
         def _train_udf(pdf_iter: Iterator[pd.DataFrame]) -> pd.DataFrame:
+            import cupy as cp
+            import cupyx
             from pyspark import BarrierTaskContext
 
+            context = BarrierTaskContext.get()
+            partition_id = context.partitionId()
             logger = get_logger(cls)
-            logger.info("Initializing cuml context")
 
-            import cupy as cp
+            # set gpu device
+            _CumlCommon._set_gpu_device(context, is_local)
 
             if cuda_managed_mem_enabled:
                 import rmm
                 from rmm.allocators.cupy import rmm_cupy_allocator
 
-                rmm.reinitialize(managed_memory=True)
+                rmm.reinitialize(
+                    managed_memory=True,
+                    devices=_CumlCommon._get_gpu_device(context, is_local),
+                )
                 cp.cuda.set_allocator(rmm_cupy_allocator)
 
-            _CumlCommon.initialize_cuml_logging(cuml_verbose)
+            _CumlCommon._initialize_cuml_logging(cuml_verbose)
 
-            context = BarrierTaskContext.get()
-            partition_id = context.partitionId()
+            # handle the input
+            # inputs = [(X, Optional(y)), (X, Optional(y))]
+            logger.info("Loading data into python worker memory")
+            inputs = []
+            sizes = []
 
-            # set gpu device
-            _CumlCommon.set_gpu_device(context, is_local)
+            for pdf in pdf_iter:
+                sizes.append(pdf.shape[0])
+                if multi_col_names:
+                    features = np.array(pdf[multi_col_names], order=array_order)
+                elif use_sparse_array:
+                    # sparse vector
+                    features = _read_csr_matrix_from_unwrapped_spark_vec(pdf)
+                else:
+                    # dense vector
+                    features = np.array(list(pdf[alias.data]), order=array_order)
 
-            with CumlContext(
-                partition_id, num_workers, context, enable_nccl, require_ucx
-            ) as cc:
-                # handle the input
-                # inputs = [(X, Optional(y)), (X, Optional(y))]
-                logger.info("Loading data into python worker memory")
-                inputs = []
-                sizes = []
-                for pdf in pdf_iter:
-                    sizes.append(pdf.shape[0])
-                    if multi_col_names:
-                        features = np.array(pdf[multi_col_names], order=array_order)
-                    else:
-                        features = np.array(list(pdf[alias.data]), order=array_order)
-                    # experiments indicate it is faster to convert to numpy array and then to cupy array than directly
-                    # invoking cupy array on the list
-                    if cuda_managed_mem_enabled:
-                        features = cp.array(features)
-
-                    label = pdf[alias.label] if alias.label in pdf.columns else None
-                    row_number = (
-                        pdf[alias.row_number]
-                        if alias.row_number in pdf.columns
-                        else None
+                # experiments indicate it is faster to convert to numpy array and then to cupy array than directly
+                # invoking cupy array on the list
+                if cuda_managed_mem_enabled:
+                    features = (
+                        cp.array(features)
+                        if use_sparse_array is False
+                        else cupyx.scipy.sparse.csr_matrix(features)
                     )
-                    inputs.append((features, label, row_number))
 
-                if len(sizes) == 0 or all(sz == 0 for sz in sizes):
-                    raise RuntimeError(
-                        "A python worker received no data.  Please increase amount of data or use fewer workers."
-                    )
+                label = pdf[alias.label] if alias.label in pdf.columns else None
+                row_number = (
+                    pdf[alias.row_number] if alias.row_number in pdf.columns else None
+                )
+                inputs.append((features, label, row_number))
+
+            if len(sizes) == 0 or all(sz == 0 for sz in sizes):
+                raise RuntimeError(
+                    "A python worker received no data.  Please increase amount of data or use fewer workers."
+                )
 
+            logger.info("Initializing cuml context")
+            with CumlContext(
+                partition_id, num_workers, context, enable_nccl, require_ucx
+            ) as cc:
                 params[param_alias.handle] = cc.handle
                 params[param_alias.part_sizes] = sizes
                 params[param_alias.num_cols] = dimension
                 params[param_alias.loop] = cc._loop
 
                 logger.info("Invoking cuml fit")
 
@@ -692,85 +887,101 @@
             def fitMultipleModels() -> List["_CumlModel"]:
                 return estimator._fit_internal(dataset, paramMaps)
 
             return _FitMultipleIterator(fitMultipleModels, len(paramMaps))
         else:
             return super().fitMultiple(dataset, paramMaps)
 
-    def _try_stage_level_scheduling(self, rdd: RDD) -> RDD:
-        ss = _get_spark_session()
-        sc = ss.sparkContext
-
-        if ss.version < "3.4.0":
-            self.logger.warning(
-                "Stage level scheduling in spark-rapids-ml requires spark version 3.4.0+"
+    def _skip_stage_level_scheduling(self, spark_version: str, conf: SparkConf) -> bool:
+        """Check if stage-level scheduling is not needed,
+        return true to skip stage-level scheduling"""
+
+        if spark_version < "3.4.0":
+            self.logger.info(
+                "Stage-level scheduling in spark-rapids-ml requires spark version 3.4.0+"
             )
-            return rdd
-        elif not _is_standalone_or_localcluster(sc):
-            # Only standalone or local-cluster supports stage-level scheduling with dynamic
-            # allocation disabled.
-            self.logger.warning(
-                "Stage level scheduling in spark-rapids-ml only works on spark standalone or "
-                "local cluster mode"
+            return True
+
+        if not _is_standalone_or_localcluster(conf):
+            self.logger.info(
+                "Stage-level scheduling in spark-rapids-ml requires spark standalone or "
+                "local-cluster mode"
             )
-            return rdd
+            return True
 
-        executor_cores = sc.getConf().get("spark.executor.cores")
-        executor_gpu_amount = sc.getConf().get("spark.executor.resource.gpu.amount")
+        executor_cores = conf.get("spark.executor.cores")
+        executor_gpus = conf.get("spark.executor.resource.gpu.amount")
+        if executor_cores is None or executor_gpus is None:
+            self.logger.info(
+                "Stage-level scheduling in spark-rapids-ml requires spark.executor.cores, "
+                "spark.executor.resource.gpu.amount to be set."
+            )
+            return True
 
-        if executor_cores is None or executor_gpu_amount is None:
-            self.logger.warning(
-                "Stage level scheduling in spark-rapids-ml requires spark.executor.cores, "
-                "spark.executor.resource.gpu.amount to be set "
+        if int(executor_cores) == 1:
+            # there will be only 1 task running at any time.
+            self.logger.info(
+                "Stage-level scheduling in spark-rapids-ml requires spark.executor.cores > 1 "
             )
-            return rdd
+            return True
 
-        if int(executor_gpu_amount) > 1:
-            # For spark.executor.resource.gpu.amount>1, we suppose user knows how to configure
+        if int(executor_gpus) > 1:
+            # For spark.executor.resource.gpu.amount > 1, we suppose user knows how to configure
             # to make spark-rapids-ml run successfully.
-            #
-            self.logger.warning(
-                "Stage level scheduling in spark-rapids-ml will not work "
+            self.logger.info(
+                "Stage-level scheduling in spark-rapids-ml will not work "
                 "when spark.executor.resource.gpu.amount>1"
             )
-            return rdd
+            return True
 
-        task_gpu_amount = sc.getConf().get("spark.task.resource.gpu.amount")
+        task_gpu_amount = conf.get("spark.task.resource.gpu.amount")
 
         if task_gpu_amount is None:
-            # if spark.task.resource.gpu.amount is not set, the default concurrent tasks
-            # with gpu requirement will be 1, which means 2 training tasks will never
-            # be scheduled into the same executor.
-            return rdd
+            # The ETL tasks will not grab a gpu when spark.task.resource.gpu.amount is not set,
+            # but with stage-level scheduling, we can make training task grab the gpu.
+            return False
+
+        if float(task_gpu_amount) == float(executor_gpus):
+            # spark.executor.resource.gpu.amount=spark.task.resource.gpu.amount "
+            # results in only 1 task running at a time, which may cause perf issue.
+            return True
 
-        if float(task_gpu_amount) == float(executor_gpu_amount):
-            self.logger.warning(
-                f"The configuration of cores (exec = {executor_gpu_amount} task = {task_gpu_amount}, "
-                f"runnable tasks = 1) will result in wasted resources due to resource gpu limiting"
-                f"the number of runnable tasks per executor to: 1. Please adjust your configuration."
-            )
+        # We can enable stage-level scheduling
+        return False
+
+    def _try_stage_level_scheduling(self, rdd: RDD) -> RDD:
+        ss = _get_spark_session()
+        sc = ss.sparkContext
+
+        if self._skip_stage_level_scheduling(ss.version, sc.getConf()):
             return rdd
 
+        # executor_cores will not be None
+        executor_cores = ss.sparkContext.getConf().get("spark.executor.cores")
+        assert executor_cores is not None
+
         from pyspark.resource.profile import ResourceProfileBuilder
         from pyspark.resource.requests import TaskResourceRequests
 
         # each training task requires cpu cores > total executor cores/2 which can
         # ensure each training task be sent to different executor.
         #
         # Please note that we can't set task_cores to the value which is smaller than total executor cores/2
         # because only task_gpus can't ensure the tasks be sent to different executor even task_gpus=1.0
         #
         # If spark-rapids enabled. we don't allow other ETL task running alongside training task to avoid OOM
         spark_plugins = ss.conf.get("spark.plugins", " ")
         assert spark_plugins is not None
         spark_rapids_sql_enabled = ss.conf.get("spark.rapids.sql.enabled", "true")
+        assert spark_rapids_sql_enabled is not None
+
         task_cores = (
             int(executor_cores)
             if "com.nvidia.spark.SQLPlugin" in spark_plugins
-            and "true" == spark_rapids_sql_enabled
+            and "true" == spark_rapids_sql_enabled.lower()
             else (int(executor_cores) // 2) + 1
         )
         # task_gpus means how many slots per gpu address the task requires,
         # it does mean how many gpus it would like to require, so it can be any value of (0, 0.5] or 1.
         task_gpus = 1.0
 
         treqs = TaskResourceRequests().cpus(task_cores).resource("gpu", task_gpus)
@@ -893,44 +1104,46 @@
         n_cols: Optional[int] = None,
         **model_attributes: Any,
     ) -> None:
         """
         Subclass must pass the model attributes which will be saved in model persistence.
         """
         super().__init__()
-        self.initialize_cuml_params()
+        self._initialize_cuml_params()
 
         # model_data is the native data which will be saved for model persistence
         self._model_attributes = model_attributes
         self._model_attributes["dtype"] = dtype
         self._model_attributes["n_cols"] = n_cols
         self.dtype = dtype
         self.n_cols = n_cols
 
     def cpu(self) -> Model:
         """Return the equivalent PySpark CPU model"""
         raise NotImplementedError()
 
-    def get_model_attributes(self) -> Optional[Dict[str, Any]]:
+    def _get_model_attributes(self) -> Optional[Dict[str, Any]]:
         """Return model attributes as a dictionary."""
         return self._model_attributes
 
     @classmethod
-    def from_row(cls, model_attributes: Row):  # type: ignore
+    def _from_row(cls, model_attributes: Row):  # type: ignore
         """
         Default to pass all the attributes of the model to the model constructor,
         So please make sure if the constructor can accept all of them.
         """
         attr_dict = model_attributes.asDict()
         return cls(**attr_dict)
 
     @abstractmethod
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self,
+        dataset: DataFrame,
+        eval_metric_info: Optional[EvalMetricInfo] = None,
+    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc]]:
         """
         Subclass must implement this function to return three functions,
         1. a function to construct cuml counterpart instance
         2. a function to transform the dataset
         3. an optional function to evaluate.
 
         Eg,
@@ -941,15 +1154,15 @@
                 ...
             def _cuml_transform(cuml_obj: CumlT, df: Union[pd.DataFrame, np.ndarray]) ->pd.DataFrame:
                 ...
             def _evaluate(input_df: Union[pd.DataFrame, np.ndarray], transformed_df: Union[pd.DataFrame, np.ndarray]) -> pd.DataFrame:
                 ...
             ...
 
-            # please note that if category is transform, the evaluate function will be ignored.
+            # please note that if eval_metric_info is None, the evaluate function will be None.
             return _construct_cuml_object, _cuml_transform, _evaluate
 
         _get_cuml_transform_func itself runs on the driver side, while the returned
         _construct_cuml_object and _cuml_transform, _evaluate will run on the executor side.
         """
         raise NotImplementedError()
 
@@ -981,39 +1194,78 @@
         tmp_cols = []
         input_is_multi_cols = True
 
         input_col, input_cols = self._get_input_columns()
 
         if input_col is not None:
             input_col_type = dataset.schema[input_col].dataType
-            tmp_input_col = f"{alias.data}_c3BhcmstcmFwaWRzLW1sCg=="
             if isinstance(input_col_type, VectorUDT):
                 # Vector type
-                # Avoid same naming. `echo spark-rapids-ml | base64` = c3BhcmstcmFwaWRzLW1sCg==
                 vector_element_type = "float32" if self._float32_inputs else "float64"
-                dataset = dataset.withColumn(
-                    tmp_input_col, vector_to_array(col(input_col), vector_element_type)
-                )
-                select_cols.append(tmp_input_col)
-                tmp_cols.append(tmp_input_col)
+
+                if self.hasParam("enable_sparse_data_optim") is False:
+                    use_cuml_sparse = False
+                elif self.getOrDefault("enable_sparse_data_optim") is None:
+                    first_record = dataset.first()
+                    first_vectorudt_type = (
+                        DenseVector
+                        if first_record is None
+                        or type(first_record[input_col]) is DenseVector
+                        else SparseVector
+                    )
+                    use_cuml_sparse = first_vectorudt_type is SparseVector
+                else:
+                    use_cuml_sparse = self.getOrDefault("enable_sparse_data_optim")
+
+                if use_cuml_sparse:
+                    type_col, size_col, indices_col, data_col = _get_unwrapped_vec_cols(
+                        col(input_col)
+                    )
+
+                    dataset = dataset.withColumn(alias.featureVectorType, type_col)
+
+                    dataset = dataset.withColumn(alias.featureVectorSize, size_col)
+
+                    dataset = dataset.withColumn(
+                        alias.featureVectorIndices, indices_col
+                    )
+
+                    dataset = dataset.withColumn(alias.data, data_col.alias(alias.data))
+
+                    for col_name in [
+                        alias.featureVectorType,
+                        alias.featureVectorSize,
+                        alias.featureVectorIndices,
+                        alias.data,
+                    ]:
+                        select_cols.append(col_name)
+                        tmp_cols.append(col_name)
+                else:
+                    # Avoid same naming. `echo spark-rapids-ml | base64` = c3BhcmstcmFwaWRzLW1sCg==
+                    dataset = dataset.withColumn(
+                        alias.data,
+                        vector_to_array(col(input_col), vector_element_type),
+                    )
+                    select_cols.append(alias.data)
+                    tmp_cols.append(alias.data)
             elif isinstance(input_col_type, ArrayType):
                 if (
                     isinstance(input_col_type.elementType, DoubleType)
                     and not self._float32_inputs
                 ):
                     select_cols.append(input_col)
                 elif (
                     isinstance(input_col_type.elementType, DoubleType)
                     and self._float32_inputs
                 ):
                     dataset = dataset.withColumn(
-                        tmp_input_col, col(input_col).cast(ArrayType(FloatType()))
+                        alias.data, col(input_col).cast(ArrayType(FloatType()))
                     )
-                    select_cols.append(tmp_input_col)
-                    tmp_cols.append(tmp_input_col)
+                    select_cols.append(alias.data)
+                    tmp_cols.append(alias.data)
                 else:
                     # FloatType array
                     select_cols.append(input_col)
             elif not isinstance(input_col_type, ArrayType):
                 # not Array type
                 raise ValueError("Unsupported input type.")
             input_is_multi_cols = False
@@ -1047,54 +1299,62 @@
         else:
             # should never get here
             raise Exception("Unable to determine input column(s).")
 
         return dataset, select_cols, input_is_multi_cols, tmp_cols
 
     def _transform_evaluate_internal(
-        self, dataset: DataFrame, schema: Union[StructType, str]
+        self,
+        dataset: DataFrame,
+        schema: Union[StructType, str],
+        eval_metric_info: Optional[EvalMetricInfo] = None,
     ) -> DataFrame:
         """Internal API to support transform and evaluation in a single pass"""
         dataset, select_cols, input_is_multi_cols, _ = self._pre_process_data(dataset)
 
         is_local = _is_local(_get_spark_session().sparkContext)
 
         # Get the functions which will be passed into executor to run.
         (
             construct_cuml_object_func,
             cuml_transform_func,
             evaluate_func,
-        ) = self._get_cuml_transform_func(
-            dataset, transform_evaluate.transform_evaluate
-        )
+        ) = self._get_cuml_transform_func(dataset, eval_metric_info)
         if evaluate_func:
             dataset = dataset.select(alias.label, *select_cols)
         else:
             dataset = dataset.select(*select_cols)
 
         array_order = self._transform_array_order()
 
+        use_sparse_array = _use_sparse_in_cuml(dataset)
+
         def _transform_udf(pdf_iter: Iterator[pd.DataFrame]) -> pd.DataFrame:
             from pyspark import TaskContext
 
             context = TaskContext.get()
 
-            _CumlCommon.set_gpu_device(context, is_local, True)
+            _CumlCommon._set_gpu_device(context, is_local, True)
 
             # Construct the cuml counterpart object
             cuml_instance = construct_cuml_object_func()
             cuml_objects = (
                 cuml_instance if isinstance(cuml_instance, list) else [cuml_instance]
             )
 
             # TODO try to concatenate all the data and do the transform.
             for pdf in pdf_iter:
                 for index, cuml_object in enumerate(cuml_objects):
                     # Transform the dataset
-                    if input_is_multi_cols:
+                    if use_sparse_array:
+                        features = _read_csr_matrix_from_unwrapped_spark_vec(
+                            pdf[select_cols]
+                        )
+                        data = cuml_transform_func(cuml_object, features)
+                    elif input_is_multi_cols:
                         data = cuml_transform_func(cuml_object, pdf[select_cols])
                     else:
                         nparray = np.array(list(pdf[select_cols[0]]), order=array_order)
                         data = cuml_transform_func(cuml_object, nparray)
                     # Evaluate the dataset if necessary.
                     if evaluate_func is not None:
                         data = evaluate_func(pdf, data)
@@ -1172,14 +1432,28 @@
 
     def _has_probability_col(self) -> bool:
         """This API is needed and can be overwritten by subclass which
         hasn't implemented predict probability yet"""
 
         return True if isinstance(self, HasProbabilityCol) else False
 
+    def _has_raw_pred_col(self) -> bool:
+        """This API is needed and can be overwritten by subclass which
+        hasn't implemented predict raw yet"""
+
+        return True if isinstance(self, HasRawPredictionCol) else False
+
+    def _use_prob_as_raw_pred_col(self) -> bool:
+        """This API is needed and can be overwritten by subclass which
+        doesn't support raw predictions in cuml to use copy of probability
+        column instead.
+        """
+
+        return False
+
     def _get_prediction_name(self) -> str:
         """Different algos have different prediction names,
         eg, PCA: value of outputCol param, RF/LR/Kmeans: value of predictionCol name"""
         if isinstance(self, HasPredictionCol):
             return self.getPredictionCol()
         elif isinstance(self, HasOutputCol):
             return self.getOutputCol()
@@ -1199,39 +1473,60 @@
             construct_cuml_object_func,
             cuml_transform_func,
             _,
         ) = self._get_cuml_transform_func(dataset)
 
         array_order = self._transform_array_order()
 
-        @pandas_udf(self._out_schema(dataset.schema))  # type: ignore
+        use_sparse_array = _use_sparse_in_cuml(dataset)
+
+        output_schema = self._out_schema(dataset.schema)
+
+        @pandas_udf(output_schema)  # type: ignore
         def predict_udf(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.Series]:
             from pyspark import TaskContext
 
             context = TaskContext.get()
-            _CumlCommon.set_gpu_device(context, is_local, True)
+            _CumlCommon._set_gpu_device(context, is_local, True)
             cuml_objects = construct_cuml_object_func()
             cuml_object = (
                 cuml_objects[0] if isinstance(cuml_objects, list) else cuml_objects
             )
             for pdf in iterator:
-                if not input_is_multi_cols:
+                if use_sparse_array:
+                    data = _read_csr_matrix_from_unwrapped_spark_vec(pdf[select_cols])
+                elif not input_is_multi_cols:
                     data = np.array(list(pdf[select_cols[0]]), order=array_order)
                 else:
                     data = pdf[select_cols]
                 # for normal transform, we don't allow multiple models.
                 res = cuml_transform_func(cuml_object, data)
                 del data
                 yield res
 
         pred_name = self._get_prediction_name()
         pred_col = predict_udf(struct(*select_cols))
 
         if self._is_single_pred(dataset.schema):
-            return dataset.withColumn(pred_name, pred_col)
+            output_schema_str = (
+                output_schema
+                if isinstance(output_schema, str)
+                else output_schema.simpleString()
+            )
+            if (
+                "array<float>" in output_schema_str
+                or "array<double>" in output_schema_str
+            ):
+                input_col, input_cols = self._get_input_columns()
+                if input_col is not None:
+                    input_datatype = dataset.schema[input_col].dataType
+                    if isinstance(input_datatype, VectorUDT):
+                        pred_col = array_to_vector(pred_col)
+
+            return dataset.withColumn(pred_name, pred_col).drop(*tmp_cols)
         else:
             # Avoid same naming. `echo sparkcuml | base64` = c3BhcmtjdW1sCg==
             pred_struct_col_name = "_prediction_struct_c3BhcmtjdW1sCg=="
             dataset = dataset.withColumn(pred_struct_col_name, pred_col)
 
             # 1. Add prediction in the base class
             dataset = dataset.withColumn(
@@ -1243,26 +1538,47 @@
                 probability_col = self.getOrDefault("probabilityCol")
                 dataset = dataset.withColumn(
                     probability_col,
                     array_to_vector(
                         getattr(col(pred_struct_col_name), pred.probability)
                     ),
                 )
+            # 2a. Handle raw prediction - for algos that have it in spark but not yet supported in cuml,
+            # we duplicate probability col for interop with default raw prediction col
+            # in spark evaluators. i.e. auc works equivalently with probabilities.
+            # TBD replace with rawPredictions in individual algos as support is added
+            if self._has_raw_pred_col():
+                raw_pred_col = self.getOrDefault("rawPredictionCol")
+                if self._use_prob_as_raw_pred_col():
+                    dataset = dataset.withColumn(
+                        raw_pred_col,
+                        col(probability_col),
+                    )
+                else:
+                    # class supports raw predictions from cuml layer
+                    dataset = dataset.withColumn(
+                        raw_pred_col,
+                        array_to_vector(
+                            getattr(col(pred_struct_col_name), pred.raw_prediction)
+                        ),
+                    )
 
             # 3. Drop the unused column
             dataset = dataset.drop(pred_struct_col_name)
 
             return dataset.drop(*tmp_cols)
 
     def _out_schema(self, input_schema: StructType) -> Union[StructType, str]:
         assert self.dtype is not None
 
         schema = f"{pred.prediction} double"
         if self._has_probability_col():
             schema = f"{schema}, {pred.probability} array<double>"
+            if self._has_raw_pred_col() and not self._use_prob_as_raw_pred_col():
+                schema = f"{schema}, {pred.raw_prediction} array<double>"
         else:
             schema = f"double"
 
         return schema
 
 
 class _CumlModelWithPredictionCol(_CumlModelWithColumns, HasPredictionCol):
```

## spark_rapids_ml/feature.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023, NVIDIA CORPORATION.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,21 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 import itertools
+from abc import ABCMeta
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
 import numpy as np
 import pandas as pd
+import pyspark
+from pyspark import keyword_only
 from pyspark.ml.common import _py2java
 from pyspark.ml.feature import PCAModel as SparkPCAModel
 from pyspark.ml.feature import _PCAParams
 from pyspark.ml.linalg import DenseMatrix, DenseVector
 from pyspark.ml.param.shared import HasInputCols
 from pyspark.sql.dataframe import DataFrame
 from pyspark.sql.types import (
@@ -40,16 +43,16 @@
     FitInputType,
     _ConstructFunc,
     _CumlEstimator,
     _CumlModelWithColumns,
     _EvaluateFunc,
     _TransformFunc,
     param_alias,
-    transform_evaluate,
 )
+from .metrics import EvalMetricInfo
 from .params import P, _CumlClass, _CumlParams
 from .utils import (
     PartitionDescriptor,
     _get_spark_session,
     dtype_to_pyspark_type,
     java_uid,
 )
@@ -64,66 +67,83 @@
         return {
             "n_components": None,
             "svd_solver": "auto",
             "verbose": False,
             "whiten": False,
         }
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return pyspark.ml.feature.PCA
+
 
 class _PCACumlParams(_CumlParams, _PCAParams, HasInputCols):
     """
     Shared Spark Params for PCA and PCAModel.
     """
 
     def setInputCol(self: P, value: Union[str, List[str]]) -> P:
         """
-        Sets the value of :py:attr:`inputCol` or :py:attr:`inputCols`. Used when input vectors are stored in a single column.
+        Sets the value of :py:attr:`inputCol` or :py:attr:`inputCols`.
         """
         if isinstance(value, str):
-            self.set_params(inputCol=value)
+            self._set_params(inputCol=value)
         else:
-            self.set_params(inputCols=value)
+            self._set_params(inputCols=value)
         return self
 
     def setInputCols(self: P, value: List[str]) -> P:
         """
         Sets the value of :py:attr:`inputCols`. Used when input vectors are stored as multiple feature columns.
         """
-        return self.set_params(inputCols=value)
+        return self._set_params(inputCols=value)
 
     def setOutputCol(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`outputCol`
         """
-        return self.set_params(outputCol=value)
+        return self._set_params(outputCol=value)
 
 
 class PCA(PCAClass, _CumlEstimator, _PCACumlParams):
     """
     PCA algorithm learns principal component vectors to project high-dimensional vectors
     into low-dimensional vectors, while preserving the similarity of the vectors. PCA
     has been used in dimensionality reduction, clustering, and data visualization on large
     datasets. This class provides GPU acceleration for pyspark distributed PCA.
 
     Parameters
     ----------
     k: int
         the number of components, or equivalently the dimension that all vectors will be projected to.
-    inputCol: str
-        the name of the column that contains input vectors. inputCol should be set when input vectors
-        are stored in a single column of a dataframe.
-
-    inputCols: List[str]
-        the names of feature columns that form input vectors. inputCols should be set when input vectors
-        are stored as multiple feature columns of a dataframe.
+
+    inputCol: str or List[str]
+        The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
+            * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
+            * When the value is a list of strings, the feature columns must be numeric types.
 
     outputCol: str
         the name of the column that stores output vectors. outputCol should be set when users expect to
         store output vectors in a single column.
 
+    num_workers:
+        Number of cuML workers, where each cuML worker corresponds to one Spark task
+        running on one GPU. If not set, spark-rapids-ml tries to infer the number of
+        cuML workers (i.e. GPUs in cluster) from the Spark environment.
+
+    verbose:
+    Logging level.
+            * ``0`` - Disables all log messages.
+            * ``1`` - Enables only critical messages.
+            * ``2`` - Enables all messages up to and including errors.
+            * ``3`` - Enables all messages up to and including warnings.
+            * ``4 or False`` - Enables all messages up to and including information messages.
+            * ``5 or True`` - Enables all messages up to and including debug messages.
+            * ``6`` - Enables all messages up to and including trace messages.
+
+
 
     Examples
     --------
     >>> from spark_rapids_ml.feature import PCA
     >>> data = [([1.0, 1.0],),
     ...         ([2.0, 2.0],),
     ...         ([3.0, 3.0],),]
@@ -161,29 +181,42 @@
     >>> df = spark.createDataFrame(data, ["f1", "f2"])
     >>> gpu_pca = PCA(k=1).setInputCols(["f1", "f2"])
     >>> gpu_pca.getInputCols()
     ['f1', 'f2']
     >>> gpu_model = gpu_pca.fit(df)
     """
 
-    def __init__(self, **kwargs: Any) -> None:
+    @keyword_only
+    def __init__(
+        self,
+        *,
+        k: Optional[int] = None,
+        inputCol: Optional[Union[str, List[str]]] = None,
+        outputCol: Optional[str] = None,
+        num_workers: Optional[int] = None,
+        verbose: Union[int, bool] = False,
+        **kwargs: Any,
+    ) -> None:
         super().__init__()
-        self.set_params(**kwargs)
+        self._set_params(**self._input_kwargs)
 
     def setK(self, value: int) -> "PCA":
         """
         Sets the value of :py:attr:`k`.
         """
-        return self.set_params(k=value)
+        return self._set_params(k=value)
 
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         def _cuml_fit(
             dfs: FitInputType,
             params: Dict[str, Any],
         ) -> Dict[str, Any]:
             from cuml.decomposition.pca_mg import PCAMG as CumlPCAMG
 
             pca_object = CumlPCAMG(
@@ -238,25 +271,28 @@
                 StructField("singular_values_", ArrayType(DoubleType(), False), False),
                 StructField("n_cols", IntegerType(), False),
                 StructField("dtype", StringType(), False),
             ]
         )
 
     def _create_pyspark_model(self, result: Row) -> "PCAModel":
-        return PCAModel.from_row(result)
+        return PCAModel._from_row(result)
 
 
 class PCAModel(PCAClass, _CumlModelWithColumns, _PCACumlParams):
     """Applies dimensionality reduction on an input DataFrame.
 
     Note: Input vectors must be zero-centered to ensure PCA work properly.
     Spark PCA does not automatically remove the mean of the input data, so use the
     :py:class::`~pyspark.ml.feature.StandardScaler` to center the input data before
     invoking transform.
 
+    The input vectors can be stored in three different formats: a column of vector,
+    a column of array, or multiple scalar columns.
+
     Examples
     --------
     >>> from spark_rapids_ml.feature import PCA
     >>> data = [([-1.0, -1.0],),
     ...         ([0.0, 0.0],),
     ...         ([1.0, 1.0],),]
     >>> df = spark.createDataFrame(data, ["features"])
@@ -293,15 +329,15 @@
 
         self.mean_ = mean_
         self.components_ = components_
         self.explained_variance_ratio_ = explained_variance_ratio_
         self.singular_values_ = singular_values_
         self._pca_ml_model: Optional[SparkPCAModel] = None
 
-        self.set_params(n_components=len(components_))
+        self._set_params(n_components=len(components_))
 
     @property
     def mean(self) -> List[float]:
         """
         Returns the mean of the input vectors.
         """
         return self.mean_
@@ -339,16 +375,20 @@
             )
             self._pca_ml_model = SparkPCAModel(java_model)
             self._copyValues(self._pca_ml_model)
 
         return self._pca_ml_model
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         cuml_alg_params = self.cuml_params.copy()
 
         n_cols = self.n_cols
         dype = self.dtype
         components = self.components_
         mean = self.mean_
         singular_values = self.singular_values_
```

## spark_rapids_ml/knn.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023, NVIDIA CORPORATION.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,18 +11,20 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 import asyncio
+from abc import ABCMeta
 from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
 
 import numpy as np
 import pandas as pd
+from pyspark import keyword_only
 from pyspark.ml.functions import vector_to_array
 from pyspark.ml.linalg import VectorUDT
 from pyspark.ml.param.shared import (
     HasInputCol,
     HasInputCols,
     HasLabelCol,
     Param,
@@ -49,102 +51,105 @@
     _CumlCaller,
     _CumlEstimatorSupervised,
     _CumlModel,
     _EvaluateFunc,
     _TransformFunc,
     alias,
     param_alias,
-    transform_evaluate,
 )
+from .metrics import EvalMetricInfo
 from .params import P, _CumlClass, _CumlParams
 from .utils import _concat_and_free, get_logger
 
 
 class NearestNeighborsClass(_CumlClass):
     @classmethod
     def _param_mapping(cls) -> Dict[str, Optional[str]]:
         return {"k": "n_neighbors"}
 
     def _get_cuml_params_default(self) -> Dict[str, Any]:
         return {"n_neighbors": 5, "verbose": False, "batch_size": 2000000}
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return None
+
 
 class _NearestNeighborsCumlParams(_CumlParams, HasInputCol, HasLabelCol, HasInputCols):
     """
     Shared Spark Params for NearestNeighbor and NearestNeighborModel.
     """
 
     def __init__(self) -> None:
         super().__init__()
-        self._setDefault(id_col=alias.row_number)
+        self._setDefault(idCol=alias.row_number)
 
     k = Param(
         Params._dummy(),
         "k",
         "The number nearest neighbors to retrieve. Must be >= 1.",
         typeConverter=TypeConverters.toInt,
     )
 
-    id_col = Param(
+    idCol = Param(
         Params._dummy(),
-        "id_col",
+        "idCol",
         "id column name.",
         typeConverter=TypeConverters.toString,
     )
 
     def setK(self: P, value: int) -> P:
         """
         Sets the value of `k`.
         """
-        self.set_params(k=value)
+        self._set_params(k=value)
         return self
 
     def setInputCol(self: P, value: Union[str, List[str]]) -> P:
         """
-        Sets the value of :py:attr:`inputCol` or :py:attr:`inputCols`. Used when input vectors are stored in a single column.
+        Sets the value of :py:attr:`inputCol` or :py:attr:`inputCols`.
         """
         if isinstance(value, str):
-            self.set_params(inputCol=value)
+            self._set_params(inputCol=value)
         else:
-            self.set_params(inputCols=value)
+            self._set_params(inputCols=value)
         return self
 
     def setInputCols(self: P, value: List[str]) -> P:
         """
         Sets the value of :py:attr:`inputCols`. Used when input vectors are stored as multiple feature columns.
         """
-        return self.set_params(inputCols=value)
+        return self._set_params(inputCols=value)
 
     def setIdCol(self: P, value: str) -> P:
         """
-        Sets the value of `id_col`. If not set, an id column will be added with column name `unique_id`. The id column is used to specify nearest neighbor vectors by associated id value.
+        Sets the value of `idCol`. If not set, an id column will be added with column name `unique_id`. The id column is used to specify nearest neighbor vectors by associated id value.
         """
-        self.set_params(id_col=value)
+        self._set_params(idCol=value)
         return self
 
     def getIdCol(self) -> str:
         """
-        Gets the value of `id_col`.
+        Gets the value of `idCol`.
         """
-        return self.getOrDefault(self.id_col)
+        return self.getOrDefault(self.idCol)
 
     def _ensureIdCol(self, df: DataFrame) -> DataFrame:
         """
         Ensure an id column exists in the input dataframe. Add the column if not exists.
         """
-        if not self.isSet("id_col") and self.getIdCol() in df.columns:
+        if not self.isSet("idCol") and self.getIdCol() in df.columns:
             raise ValueError(
                 f"Cannot create a default id column since a column with the default name '{self.getIdCol()}' already exists."
                 + "Please specify an id column"
             )
 
         id_col_name = self.getIdCol()
         df_withid = (
             df
-            if self.isSet("id_col")
+            if self.isSet("idCol")
             else df.select(monotonically_increasing_id().alias(id_col_name), "*")
         )
         return df_withid
 
 
 class NearestNeighbors(
     NearestNeighborsClass, _CumlEstimatorSupervised, _NearestNeighborsCumlParams
@@ -159,27 +164,39 @@
     be converted into float during computation.
 
     Parameters
     ----------
     k: int (default = 5)
         the default number nearest neighbors to retrieve for each query.
 
-    inputCol: str
-        the name of the column that contains input vectors. inputCol should be set when feature vectors
-        are stored in a single column of a dataframe.
-
-    inputCols: List[str]
-        the names of feature columns that form input vectors. inputCols should be set when input vectors
-        are stored as multiple feature columns of a dataframe.
+    inputCol: str or List[str]
+        The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
+            * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
+            * When the value is a list of strings, the feature columns must be numeric types.
 
     idCol: str
         the name of the column in a dataframe that uniquely identifies each vector. idCol should be set
         if such a column exists in the dataframe. If idCol is not set, a column with the name `unique_id`
         will be automatically added to the dataframe and used as unique identifier for each vector.
 
+    num_workers:
+        Number of cuML workers, where each cuML worker corresponds to one Spark task
+        running on one GPU. If not set, spark-rapids-ml tries to infer the number of
+        cuML workers (i.e. GPUs in cluster) from the Spark environment.
+
+    verbose:
+    Logging level.
+            * ``0`` - Disables all log messages.
+            * ``1`` - Enables only critical messages.
+            * ``2`` - Enables all messages up to and including errors.
+            * ``3`` - Enables all messages up to and including warnings.
+            * ``4 or False`` - Enables all messages up to and including information messages.
+            * ``5 or True`` - Enables all messages up to and including debug messages.
+            * ``6`` - Enables all messages up to and including trace messages.
+
     Examples
     --------
     >>> from spark_rapids_ml.knn import NearestNeighbors
     >>> data = [(0, [1.0, 1.0]),
     ...         (1, [2.0, 2.0]),
     ...         (2, [3.0, 3.0]),]
     >>> data_df = spark.createDataFrame(data, schema="id int, features array<float>")
@@ -247,29 +264,39 @@
     ...          (4, 3.0, 3.0),]
     >>> query_df = spark.createDataFrame(query, schema="id int, f1 float, f2 float")
     >>> topk = 2
     >>> gpu_knn = NearestNeighbors().setInputCols(["f1", "f2"]).setIdCol("id").setK(topk)
     >>> gpu_model = gpu_knn.fit(data_df)
     """
 
-    def __init__(self, **kwargs: Any) -> None:
-        if not kwargs.get("float32_inputs", True):
+    @keyword_only
+    def __init__(
+        self,
+        *,
+        k: Optional[int] = None,
+        inputCol: Optional[Union[str, List[str]]] = None,
+        idCol: Optional[str] = None,
+        num_workers: Optional[int] = None,
+        verbose: Union[int, bool] = False,
+        **kwargs: Any,
+    ) -> None:
+        if not self._input_kwargs.get("float32_inputs", True):
             get_logger(self.__class__).warning(
                 "This estimator does not support double precision inputs. Setting float32_inputs to False will be ignored."
             )
-            kwargs.pop("float32_inputs")
+            self._input_kwargs.pop("float32_inputs")
 
         super().__init__()
-        self.set_params(**kwargs)
+        self._set_params(**self._input_kwargs)
         self._label_isdata = 0
         self._label_isquery = 1
-        self.set_params(labelCol=alias.label)
+        self._set_params(labelCol=alias.label)
 
     def _create_pyspark_model(self, result: Row) -> "NearestNeighborsModel":
-        return NearestNeighborsModel.from_row(result)
+        return NearestNeighborsModel._from_row(result)
 
     def _fit(self, item_df: DataFrame) -> "NearestNeighborsModel":
         self._item_df_withid = self._ensureIdCol(item_df)
 
         self._processed_item_df = self._item_df_withid.withColumn(
             alias.label, lit(self._label_isdata)
         )
@@ -291,17 +318,18 @@
 
     def _out_schema(self) -> Union[StructType, str]:  # type: ignore
         """
         This class overrides _fit and will not call _out_schema.
         """
         pass
 
-    def _get_cuml_fit_func(  # type: ignore
-        self, dataset: DataFrame
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    def _get_cuml_fit_func(self, dataset: DataFrame) -> Callable[  # type: ignore
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         """
         This class overrides _fit and will not call _get_cuml_fit_func.
         """
         pass
 
     def write(self) -> MLWriter:
         raise NotImplementedError(
@@ -369,16 +397,16 @@
         ):
             select_cols[0] = vector_to_array(col(input_col), dtype="float32").alias(
                 alias.data
             )
 
         select_cols.append(col(alias.label))
 
-        if self.hasParam("id_col") and self.isDefined("id_col"):
-            id_col_name = self.getOrDefault("id_col")
+        if self.hasParam("idCol") and self.isDefined("idCol"):
+            id_col_name = self.getOrDefault("idCol")
             select_cols.append(col(id_col_name).alias(alias.row_number))
         else:
             select_cols.append(col(alias.row_number))
 
         return select_cols, multi_col_names, dimension, feature_type
 
     def kneighbors(self, query_df: DataFrame) -> Tuple[DataFrame, DataFrame, DataFrame]:
@@ -438,15 +466,18 @@
 
         return (self._item_df_withid, query_df_withid, knn_df)
 
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         label_isdata = self._label_isdata
         label_isquery = self._label_isquery
         id_col_name = self.getIdCol()
 
         def _cuml_fit(
             dfs: FitInputType,
             params: Dict[str, Any],
@@ -564,16 +595,20 @@
 
     def _transform(self, dataset: DataFrame) -> DataFrame:
         raise NotImplementedError(
             "NearestNeighborsModel does not provide a transform function. Use 'kneighbors' instead."
         )
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         raise NotImplementedError(
             "'_CumlModel._get_cuml_transform_func' method is not implemented. Use 'kneighbors' instead."
         )
 
     def exactNearestNeighborsJoin(
         self,
         query_df: DataFrame,
@@ -629,15 +664,15 @@
         )
         knnjoin_df = knnjoin_df.join(
             query_df_struct,
             knnjoin_df[f"query_{id_col_name}"]
             == query_df_struct[f"query_df.{id_col_name}"],
         )
 
-        if self.isSet(self.id_col):
+        if self.isSet(self.idCol):
             knnjoin_df = knnjoin_df.select("item_df", "query_df", distCol)
         else:
             knnjoin_df = knnjoin_df.select(
                 knnjoin_df["item_df"].dropFields(id_col_name).alias("item_df"),
                 knnjoin_df["query_df"].dropFields(id_col_name).alias("query_df"),
                 distCol,
             )
```

## spark_rapids_ml/params.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -23,25 +23,50 @@
     List,
     Optional,
     Tuple,
     TypeVar,
     Union,
 )
 
+from pyspark import SparkContext
 from pyspark.ml.param import Param, Params, TypeConverters
 from pyspark.sql import SparkSession
 
 from .utils import _get_spark_session, _is_local, get_logger
 
 if TYPE_CHECKING:
     from pyspark.ml._typing import ParamMap
 
 P = TypeVar("P", bound="_CumlParams")
 
 
+class HasEnableSparseDataOptim(Params):
+    """
+    This is a Params based class inherited from XGBOOST: https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/spark/params.py.
+    It holds the variable to store the boolean config for enabling sparse data optimization.
+    """
+
+    enable_sparse_data_optim = Param(
+        Params._dummy(),
+        "enable_sparse_data_optim",
+        "This param activates sparse data optimization for VectorUDT features column. "
+        "If the param is not included in an Estimator class, "
+        "Spark rapids ml always converts VectorUDT features column into dense arrays when calling cuml backend. "
+        "If included, Spark rapids ml will determine whether to create sparse arrays based on the param value: "
+        "(1) If None, create dense arrays if the first VectorUDT of a dataframe is DenseVector. Create sparse arrays if it is SparseVector."
+        "(2) If False, create dense arrays. This is favorable if the majority of vectors are DenseVector."
+        "(3) If True, create sparse arrays. This is favorable if the majority of the VectorUDT vectors are SparseVector.",
+        typeConverter=TypeConverters.toBoolean,
+    )
+
+    def __init__(self) -> None:
+        super().__init__()
+        self._setDefault(enable_sparse_data_optim=None)
+
+
 class HasFeaturesCols(Params):
     """
     Mixin for param featuresCols: features column names for multi-column input.
     """
 
     featuresCols = Param(
         Params._dummy(),  # type: ignore
@@ -215,29 +240,29 @@
                         "Expecting a valid instance of Param, but received: {}".format(
                             param
                         )
                     )
         instance._cuml_params = cuml_params
         return instance
 
-    def initialize_cuml_params(self) -> None:
+    def _initialize_cuml_params(self) -> None:
         """
         Set the default values of cuML parameters to match their Spark equivalents.
         """
         # initialize cuml_params with defaults from cuML
         self._cuml_params = self._get_cuml_params_default()
 
         # update default values from Spark ML Param equivalents
         param_map = self._param_mapping()
 
         for spark_param in param_map.keys():
             if self.hasDefault(spark_param):
                 self._set_cuml_param(spark_param, self.getOrDefault(spark_param))
 
-    def set_params(self: P, **kwargs: Any) -> P:
+    def _set_params(self: P, **kwargs: Any) -> P:
         """
         Set the kwargs as Spark ML Params and/or cuML parameters, while maintaining parameter
         and value mappings defined by the _CumlClass.
         """
         param_map = self._param_mapping()
 
         # raise error if setting both sides of a param mapping
@@ -351,15 +376,15 @@
 
     def _infer_num_workers(self) -> int:
         """
         Try to infer the number of cuML workers (i.e. GPUs in cluster) from the Spark environment.
         """
         num_workers = 1
         try:
-            spark = SparkSession.getActiveSession()
+            spark = _get_spark_session()
             if spark:
                 sc = spark.sparkContext
                 if _is_local(sc):
                     # assume using all local GPUs for Spark local mode
                     # TODO suggest using more CPUs (e.g. local[*]) if number of GPUs > number of CPUs
                     import cupy
 
@@ -424,15 +449,24 @@
             If the Spark Param is explictly not supported.
         """
 
         cuml_param = self._get_cuml_param(spark_param, silent)
 
         if cuml_param is not None:
             # if Spark Param is mapped to cuML parameter, set cuml_params
-            self._set_cuml_value(cuml_param, spark_value)
+            try:
+                self._set_cuml_value(cuml_param, spark_value)
+            except ValueError:
+                # create more informative message
+                param_ref_str = (
+                    cuml_param + " or " + spark_param
+                    if cuml_param != spark_param
+                    else spark_param
+                )
+                raise ValueError(f"{param_ref_str} given invalid value {spark_value}")
 
     def _get_cuml_mapping_value(self, k: str, v: Any) -> Any:
         value_map = self._param_value_mapping()
         if k not in value_map:
             # no value mapping required
             return v
         else:
```

## spark_rapids_ml/regression.py

```diff
@@ -1,21 +1,22 @@
-# Copyright (c) 2022-2023, NVIDIA CORPORATION.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
+from abc import ABCMeta
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     List,
     Optional,
@@ -24,14 +25,15 @@
     TypeVar,
     Union,
     cast,
 )
 
 import numpy as np
 import pandas as pd
+import pyspark
 from pyspark import Row, TaskContext, keyword_only
 from pyspark.ml.common import _py2java
 from pyspark.ml.evaluation import Evaluator, RegressionEvaluator
 from pyspark.ml.linalg import Vector, Vectors, _convert_to_vector
 from pyspark.ml.regression import LinearRegressionModel as SparkLinearRegressionModel
 from pyspark.ml.regression import LinearRegressionSummary
 from pyspark.ml.regression import (
@@ -58,16 +60,16 @@
     _CumlModel,
     _CumlModelWithPredictionCol,
     _EvaluateFunc,
     _TransformFunc,
     alias,
     param_alias,
     pred,
-    transform_evaluate,
 )
+from .metrics import EvalMetricInfo, transform_evaluate_metric
 from .metrics.RegressionMetrics import RegressionMetrics, reg_metrics
 from .params import HasFeaturesCols, P, _CumlClass, _CumlParams
 from .tree import (
     _RandomForestClass,
     _RandomForestCumlParams,
     _RandomForestEstimator,
     _RandomForestModel,
@@ -125,21 +127,25 @@
                 StructField(reg_metrics.m2n, ArrayType(FloatType())),
                 StructField(reg_metrics.m2, ArrayType(FloatType())),
                 StructField(reg_metrics.l1, ArrayType(FloatType())),
                 StructField(reg_metrics.total_count, IntegerType()),
             ]
         )
 
-        rows = self._this_model._transform_evaluate_internal(dataset, schema).collect()
+        rows = self._this_model._transform_evaluate_internal(
+            dataset,
+            schema,
+            EvalMetricInfo(eval_metric=transform_evaluate_metric.regression),
+        ).collect()
 
-        metrics = RegressionMetrics.from_rows(num_models, rows)
+        metrics = RegressionMetrics._from_rows(num_models, rows)
         return [metric.evaluate(evaluator) for metric in metrics]
 
     @staticmethod
-    def calculate_regression_metrics(
+    def _calculate_regression_metrics(
         input: TransformInputType,
         transformed: TransformInputType,
     ) -> pd.DataFrame:
         """calculate the metrics: mean/m2n/m2/l1 ...
 
         input must have `alias.label` column"""
 
@@ -210,14 +216,17 @@
             "loss": "squared_loss",
             "l1_ratio": 0.15,
             "max_iter": 1000,
             "tol": 0.001,
             "shuffle": True,
         }
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return pyspark.ml.regression.LinearRegression
+
 
 class _LinearRegressionCumlParams(
     _CumlParams, _LinearRegressionParams, HasFeaturesCols
 ):
     """
     Shared Spark Params for LinearRegression and LinearRegressionModel.
     """
@@ -234,36 +243,36 @@
             raise RuntimeError("featuresCol is not set")
 
     def setFeaturesCol(self: P, value: Union[str, List[str]]) -> P:
         """
         Sets the value of :py:attr:`featuresCol` or :py:attr:`featureCols`.
         """
         if isinstance(value, str):
-            self.set_params(featuresCol=value)
+            self._set_params(featuresCol=value)
         else:
-            self.set_params(featuresCols=value)
+            self._set_params(featuresCols=value)
         return self
 
     def setFeaturesCols(self: P, value: List[str]) -> P:
         """
         Sets the value of :py:attr:`featuresCols`.
         """
-        return self.set_params(featuresCols=value)
+        return self._set_params(featuresCols=value)
 
     def setLabelCol(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`labelCol`.
         """
-        return self.set_params(labelCol=value)
+        return self._set_params(labelCol=value)
 
     def setPredictionCol(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`predictionCol`.
         """
-        return self.set_params(predictionCol=value)
+        return self._set_params(predictionCol=value)
 
 
 class LinearRegression(
     LinearRegressionClass,
     _CumlEstimatorSupervised,
     _LinearRegressionCumlParams,
 ):
@@ -293,15 +302,15 @@
         Results for spark ML and spark rapids ml fit() will currently match in all regularization
         cases only if features and labels are standardized in the input dataframe.  Otherwise,
         they will match only if regParam = 0 or elastNetParam = 1.0 (aka Lasso).
 
     Parameters
     ----------
 
-    featuresCol:
+    featuresCol: str or List[str]
         The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
             * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
             * When the value is a list of strings, the feature columns must be numeric types.
     labelCol:
         The label column name.
     predictionCol:
         The prediction column name.
@@ -409,51 +418,51 @@
         solver: str = "auto",
         loss: str = "squaredError",
         num_workers: Optional[int] = None,
         verbose: Union[int, bool] = False,
         **kwargs: Any,
     ):
         super().__init__()
-        self.set_params(**self._input_kwargs)
+        self._set_params(**self._input_kwargs)
 
     def setMaxIter(self, value: int) -> "LinearRegression":
         """
         Sets the value of :py:attr:`maxIter`.
         """
-        return self.set_params(maxIter=value)
+        return self._set_params(maxIter=value)
 
     def setRegParam(self, value: float) -> "LinearRegression":
         """
         Sets the value of :py:attr:`regParam`.
         """
-        return self.set_params(regParam=value)
+        return self._set_params(regParam=value)
 
     def setElasticNetParam(self, value: float) -> "LinearRegression":
         """
         Sets the value of :py:attr:`elasticNetParam`.
         """
-        return self.set_params(elasticNetParam=value)
+        return self._set_params(elasticNetParam=value)
 
     def setLoss(self, value: str) -> "LinearRegression":
         """
         Sets the value of :py:attr:`loss`.
         """
-        return self.set_params(loss=value)
+        return self._set_params(loss=value)
 
     def setStandardization(self, value: bool) -> "LinearRegression":
         """
         Sets the value of :py:attr:`standardization`.
         """
-        return self.set_params(standardization=value)
+        return self._set_params(standardization=value)
 
     def setTol(self, value: float) -> "LinearRegression":
         """
         Sets the value of :py:attr:`tol`.
         """
-        return self.set_params(tol=value)
+        return self._set_params(tol=value)
 
     def _pre_process_data(
         self, dataset: DataFrame
     ) -> Tuple[
         List[Column], Optional[List[str]], int, Union[Type[FloatType], Type[DoubleType]]
     ]:
         (
@@ -473,15 +482,18 @@
 
         return select_cols, multi_col_names, dimension, feature_type
 
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         def _linear_regression_fit(
             dfs: FitInputType,
             params: Dict[str, Any],
         ) -> Dict[str, Any]:
             # Step 1, get the PartitionDescriptor
             pdesc = PartitionDescriptor.build(
                 params[param_alias.part_sizes], params[param_alias.num_cols]
@@ -604,15 +616,15 @@
                 StructField("intercept_", DoubleType(), False),
                 StructField("n_cols", IntegerType(), False),
                 StructField("dtype", StringType(), False),
             ]
         )
 
     def _create_pyspark_model(self, result: Row) -> "LinearRegressionModel":
-        return LinearRegressionModel.from_row(result)
+        return LinearRegressionModel._from_row(result)
 
     def _enable_fit_multiple_in_single_pass(self) -> bool:
         return True
 
     def _supportsTransformEvaluate(self, evaluator: Evaluator) -> bool:
         return True if isinstance(evaluator, RegressionEvaluator) else False
 
@@ -682,26 +694,32 @@
     def scale(self) -> float:
         """
         Since "huber" loss is not supported by cuML, just returns the value 1.0 for API compatibility.
         """
         return 1.0
 
     def predict(self, value: T) -> float:
-        """cuML doesn't support predicting 1 single sample.
-        Fall back to PySpark ML LinearRegressionModel"""
+        """predict a single sample"""
+        # cuML doesn't support predicting 1 single sample.
+        # Fall back to PySpark ML LinearRegressionModel
         return self.cpu().predict(value)
 
     def evaluate(self, dataset: DataFrame) -> LinearRegressionSummary:
-        """cuML doesn't support evaluating.
-        Fall back to PySpark ML LinearRegressionModel"""
+        """evaluate model on dataset"""
+        # cuML doesn't support evaluating.
+        # Fall back to PySpark ML LinearRegressionModel
         return self.cpu().evaluate(dataset)
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         coef_ = self.coef_
         intercept_ = self.intercept_
         n_cols = self.n_cols
         dtype = self.dtype
 
         def _construct_lr() -> CumlT:
             from cuml.linear_model.linear_regression_mg import LinearRegressionMG
@@ -723,15 +741,15 @@
 
             return lrs
 
         def _predict(lr: CumlT, pdf: TransformInputType) -> pd.Series:
             ret = lr.predict(pdf)
             return pd.Series(ret)
 
-        return _construct_lr, _predict, self.calculate_regression_metrics
+        return _construct_lr, _predict, self._calculate_regression_metrics
 
     @classmethod
     def _combine(
         cls: Type["LinearRegressionModel"], models: List["LinearRegressionModel"]  # type: ignore
     ) -> "LinearRegressionModel":
         assert len(models) > 0 and all(isinstance(model, cls) for model in models)
         first_model = models[0]
@@ -773,14 +791,17 @@
     ) -> Dict[str, Callable[[Any], Union[None, str, float, int]]]:
         mapping = super()._param_value_mapping()
         mapping["split_criterion"] = lambda x: {"variance": "mse", "mse": "mse"}.get(
             x, None
         )
         return mapping
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return pyspark.ml.regression.RandomForestRegressor
+
 
 class RandomForestRegressor(
     _RandomForestRegressorClass,
     _RandomForestEstimator,
     _RandomForestCumlParams,
     _RandomForestRegressorParams,
 ):
@@ -803,15 +824,15 @@
     :py:class:`cuml.ensemble.RandomForestRegressor`. And it can automatically map
     pyspark parameters to cuML parameters.
 
 
     Parameters
     ----------
 
-    featuresCol:
+    featuresCol: str or List[str]
         The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
             * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
             * When the value is a list of strings, the feature columns must be numeric types.
     labelCol:
         The label column name.
     predictionCol:
         The prediction column name.
@@ -942,15 +963,15 @@
     ):
         super().__init__(**self._input_kwargs)
 
     def _is_classification(self) -> bool:
         return False
 
     def _create_pyspark_model(self, result: Row) -> "RandomForestRegressionModel":
-        return RandomForestRegressionModel.from_row(result)
+        return RandomForestRegressionModel._from_row(result)
 
     def _supportsTransformEvaluate(self, evaluator: Evaluator) -> bool:
         return True if isinstance(evaluator, RegressionEvaluator) else False
 
 
 class RandomForestRegressionModel(
     _RandomForestRegressorClass,
@@ -1000,18 +1021,24 @@
             self._copyValues(self._rf_spark_model)
         return self._rf_spark_model
 
     def _is_classification(self) -> bool:
         return False
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
-        _construct_rf, _predict, _ = super()._get_cuml_transform_func(dataset, category)
-        return _construct_rf, _predict, self.calculate_regression_metrics
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
+        _construct_rf, _predict, _ = super()._get_cuml_transform_func(
+            dataset, eval_metric_info
+        )
+        return _construct_rf, _predict, self._calculate_regression_metrics
 
     def _transformEvaluate(
         self,
         dataset: DataFrame,
         evaluator: Evaluator,
         params: Optional["ParamMap"] = None,
     ) -> List[float]:
```

## spark_rapids_ml/tree.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -47,16 +47,16 @@
     TransformInputType,
     _ConstructFunc,
     _CumlEstimatorSupervised,
     _CumlModelWithPredictionCol,
     _EvaluateFunc,
     _TransformFunc,
     param_alias,
-    transform_evaluate,
 )
+from .metrics import EvalMetricInfo
 from .params import HasFeaturesCols, P, _CumlClass, _CumlParams
 from .utils import (
     _concat_and_free,
     _get_spark_session,
     _str_or_numerical,
     java_uid,
     translate_trees,
@@ -151,24 +151,24 @@
             raise RuntimeError("featuresCol is not set")
 
     def setFeaturesCol(self: P, value: Union[str, List[str]]) -> P:
         """
         Sets the value of :py:attr:`featuresCol` or :py:attr:`featureCols`.
         """
         if isinstance(value, str):
-            self.set_params(featuresCol=value)
+            self._set_params(featuresCol=value)
         else:
-            self.set_params(featuresCols=value)
+            self._set_params(featuresCols=value)
         return self
 
     def setFeaturesCols(self: P, value: List[str]) -> P:
         """
         Sets the value of :py:attr:`featuresCols`.
         """
-        return self.set_params(featuresCols=value)
+        return self._set_params(featuresCols=value)
 
     def setLabelCol(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`labelCol`.
         """
         self._set(labelCol=value)  # type: ignore
         return self
@@ -186,68 +186,68 @@
         super().__init__()
 
     # include setters used only in estimator classes (classifier and regressor) here
     def setBootstrap(self: P, value: bool) -> P:
         """
         Sets the value of :py:attr:`bootstrap`.
         """
-        return self.set_params(bootstrap=value)
+        return self._set_params(bootstrap=value)
 
     def setFeatureSubsetStrategy(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`featureSubsetStrategy`.
         """
-        return self.set_params(featureSubsetStrategy=value)
+        return self._set_params(featureSubsetStrategy=value)
 
     def setImpurity(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`impurity`.
         """
-        return self.set_params(impurity=value)  # type: ignore
+        return self._set_params(impurity=value)  # type: ignore
 
     def setMaxBins(self: P, value: int) -> P:
         """
         Sets the value of :py:attr:`maxBins`.
         """
-        return self.set_params(maxBins=value)
+        return self._set_params(maxBins=value)
 
     def setMaxDepth(self: P, value: int) -> P:
         """
         Sets the value of :py:attr:`maxDepth`.
         """
-        return self.set_params(maxDepth=value)
+        return self._set_params(maxDepth=value)
 
     def setMinInstancesPerNode(self: P, value: int) -> P:
         """
         Sets the value of :py:attr:`minInstancesPerNode`.
         """
-        return self.set_params(minInstancesPerNode=value)
+        return self._set_params(minInstancesPerNode=value)
 
     def setNumTrees(self: P, value: int) -> P:
         """
         Sets the value of :py:attr:`numTrees`.
         """
-        return self.set_params(numTrees=value)
+        return self._set_params(numTrees=value)
 
     def setSeed(self: P, value: int) -> P:
         """
         Sets the value of :py:attr:`seed`.
         """
         if value > 0x07FFFFFFF:
             raise ValueError("cuML seed value must be a 32-bit integer.")
-        return self.set_params(seed=value)
+        return self._set_params(seed=value)
 
 
 class _RandomForestEstimator(
     _CumlEstimatorSupervised,
     _RandomForestEstimatorParams,
 ):
     def __init__(self, **kwargs: Any):
         super().__init__()
-        self.set_params(**kwargs)
+        self._set_params(**kwargs)
         if "n_streams" not in kwargs:
             # cuML will throw exception when running on a node with multi-gpus when n_streams > 0
             self._set_cuml_value("n_streams", 1)
 
     @abstractmethod
     def _is_classification(self) -> bool:
         """Indicate if it is regression or classification estimator"""
@@ -266,15 +266,18 @@
             n_estimators_per_worker[i] = n_estimators_per_worker[i] + 1
         return n_estimators_per_worker
 
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         # Each element of n_estimators_of_all_params is a list value which
         # is composed of n_estimators per worker.
         n_estimators_of_all_params: List[List[int]] = []
         total_trees: List[int] = []
 
         all_params = [{}] if extra_params is None else extra_params
 
@@ -548,16 +551,20 @@
         )
         for i in range(len(decision_trees)):
             java_trees[i] = decision_trees[i]
 
         return uid, java_trees
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         treelite_model = self._treelite_model
         is_classification = self._is_classification()
 
         def _construct_rf() -> CumlT:
             if is_classification:
                 from cuml import RandomForestClassifier as cuRf
             else:
@@ -593,15 +600,15 @@
     def _combine(
         cls: Type["_RandomForestModel"], models: List["_RandomForestModel"]  # type: ignore
     ) -> "_RandomForestModel":
         assert len(models) > 0 and all(isinstance(model, cls) for model in models)
         first_model = models[0]
         treelite_models = [model._treelite_model for model in models]
         model_jsons = [model._model_json for model in models]
-        attrs = first_model.get_model_attributes()
+        attrs = first_model._get_model_attributes()
         assert attrs is not None
         attrs["treelite_model"] = treelite_models
         attrs["model_json"] = model_jsons
         rf_model = cls(**attrs)
         first_model._copyValues(rf_model)
         first_model._copy_cuml_params(rf_model)
         return rf_model
```

## spark_rapids_ml/tuning.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

## spark_rapids_ml/umap.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,14 +12,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 import json
 import os
+from abc import ABCMeta
 from typing import (
     TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     Iterable,
     List,
@@ -66,16 +67,16 @@
     _CumlModel,
     _CumlModelReader,
     _CumlModelWriter,
     _EvaluateFunc,
     _TransformFunc,
     alias,
     param_alias,
-    transform_evaluate,
 )
+from .metrics import EvalMetricInfo
 from .params import HasFeaturesCols, P, _CumlClass, _CumlParams
 from .utils import (
     _ArrayOrder,
     _concat_and_free,
     _get_spark_session,
     _is_local,
     get_logger,
@@ -109,14 +110,17 @@
             "a": None,
             "b": None,
             "precomputed_knn": None,
             "random_state": None,
             "verbose": False,
         }
 
+    def _pyspark_class(self) -> Optional[ABCMeta]:
+        return None
+
 
 class _UMAPCumlParams(
     _CumlParams, HasFeaturesCol, HasFeaturesCols, HasLabelCol, HasOutputCol
 ):
     def __init__(self) -> None:
         super().__init__()
         self._setDefault(
@@ -342,219 +346,219 @@
         """
         return self.getOrDefault("n_neighbors")
 
     def setNNeighbors(self: P, value: float) -> P:
         """
         Sets the value of `n_neighbors`.
         """
-        return self.set_params(n_neighbors=value)
+        return self._set_params(n_neighbors=value)
 
     def getNComponents(self) -> int:
         """
         Gets the value of `n_components`.
         """
         return self.getOrDefault("n_components")
 
     def setNComponents(self: P, value: int) -> P:
         """
         Sets the value of `n_components`.
         """
-        return self.set_params(n_components=value)
+        return self._set_params(n_components=value)
 
     def getMetric(self) -> str:
         """
         Gets the value of `metric`.
         """
         return self.getOrDefault("metric")
 
     def setMetric(self: P, value: str) -> P:
         """
         Sets the value of `metric`.
         """
-        return self.set_params(metric=value)
+        return self._set_params(metric=value)
 
     def getNEpochs(self) -> int:
         """
         Gets the value of `n_epochs`.
         """
         return self.getOrDefault("n_epochs")
 
     def setNEpochs(self: P, value: int) -> P:
         """
         Sets the value of `n_epochs`.
         """
-        return self.set_params(n_epochs=value)
+        return self._set_params(n_epochs=value)
 
     def getLearningRate(self) -> float:
         """
         Gets the value of `learning_rate`.
         """
         return self.getOrDefault("learning_rate")
 
     def setLearningRate(self: P, value: float) -> P:
         """
         Sets the value of `learning_rate`.
         """
-        return self.set_params(learning_rate=value)
+        return self._set_params(learning_rate=value)
 
     def getInit(self) -> str:
         """
         Gets the value of `init`.
         """
         return self.getOrDefault("init")
 
     def setInit(self: P, value: str) -> P:
         """
         Sets the value of `init`.
         """
-        return self.set_params(init=value)
+        return self._set_params(init=value)
 
     def getMinDist(self) -> float:
         """
         Gets the value of `min_dist`.
         """
         return self.getOrDefault("min_dist")
 
     def setMinDist(self: P, value: float) -> P:
         """
         Sets the value of `min_dist`.
         """
-        return self.set_params(min_dist=value)
+        return self._set_params(min_dist=value)
 
     def getSpread(self) -> float:
         """
         Gets the value of `spread`.
         """
         return self.getOrDefault("spread")
 
     def setSpread(self: P, value: float) -> P:
         """
         Sets the value of `spread`.
         """
-        return self.set_params(spread=value)
+        return self._set_params(spread=value)
 
     def getSetOpMixRatio(self) -> float:
         """
         Gets the value of `set_op_mix_ratio`.
         """
         return self.getOrDefault("set_op_mix_ratio")
 
     def setSetOpMixRatio(self: P, value: float) -> P:
         """
         Sets the value of `set_op_mix_ratio`.
         """
-        return self.set_params(set_op_mix_ratio=value)
+        return self._set_params(set_op_mix_ratio=value)
 
     def getLocalConnectivity(self) -> float:
         """
         Gets the value of `local_connectivity`.
         """
         return self.getOrDefault("local_connectivity")
 
     def setLocalConnectivity(self: P, value: float) -> P:
         """
         Sets the value of `local_connectivity`.
         """
-        return self.set_params(local_connectivity=value)
+        return self._set_params(local_connectivity=value)
 
     def getRepulsionStrength(self) -> float:
         """
         Gets the value of `repulsion_strength`.
         """
         return self.getOrDefault("repulsion_strength")
 
     def setRepulsionStrength(self: P, value: float) -> P:
         """
         Sets the value of `repulsion_strength`.
         """
-        return self.set_params(repulsion_strength=value)
+        return self._set_params(repulsion_strength=value)
 
     def getNegativeSampleRate(self) -> int:
         """
         Gets the value of `negative_sample_rate`.
         """
         return self.getOrDefault("negative_sample_rate")
 
     def setNegativeSampleRate(self: P, value: int) -> P:
         """
         Sets the value of `negative_sample_rate`.
         """
-        return self.set_params(negative_sample_rate=value)
+        return self._set_params(negative_sample_rate=value)
 
     def getTransformQueueSize(self) -> float:
         """
         Gets the value of `transform_queue_size`.
         """
         return self.getOrDefault("transform_queue_size")
 
     def setTransformQueueSize(self: P, value: float) -> P:
         """
         Sets the value of `transform_queue_size`.
         """
-        return self.set_params(transform_queue_size=value)
+        return self._set_params(transform_queue_size=value)
 
     def getA(self) -> float:
         """
         Gets the value of `a`.
         """
         return self.getOrDefault("a")
 
     def setA(self: P, value: float) -> P:
         """
         Sets the value of `a`.
         """
-        return self.set_params(a=value)
+        return self._set_params(a=value)
 
     def getB(self) -> float:
         """
         Gets the value of `b`.
         """
         return self.getOrDefault("b")
 
     def setB(self: P, value: float) -> P:
         """
         Sets the value of `b`.
         """
-        return self.set_params(b=value)
+        return self._set_params(b=value)
 
     def getPrecomputedKNN(self) -> List[List[float]]:
         """
         Gets the value of `precomputed_knn`.
         """
         return self.getOrDefault("precomputed_knn")
 
     def setPrecomputedKNN(self: P, value: List[List[float]]) -> P:
         """
         Sets the value of `precomputed_knn`.
         """
-        return self.set_params(precomputed_knn=value)
+        return self._set_params(precomputed_knn=value)
 
     def getRandomState(self) -> int:
         """
         Gets the value of `random_state`.
         """
         return self.getOrDefault("random_state")
 
     def setRandomState(self: P, value: int) -> P:
         """
         Sets the value of `random_state`.
         """
-        return self.set_params(random_state=value)
+        return self._set_params(random_state=value)
 
     def getSampleFraction(self) -> float:
         """
         Gets the value of `sample_fraction`.
         """
         return self.getOrDefault("sample_fraction")
 
     def setSampleFraction(self: P, value: float) -> P:
         """
         Sets the value of `sample_fraction`.
         """
-        return self.set_params(sample_fraction=value)
+        return self._set_params(sample_fraction=value)
 
     def getFeaturesCol(self) -> Union[str, List[str]]:  # type: ignore
         """
         Gets the value of :py:attr:`featuresCol` or :py:attr:`featuresCols`
         """
 
         if self.isDefined(self.featuresCols):
@@ -565,46 +569,45 @@
             raise RuntimeError("featuresCol is not set")
 
     def setFeaturesCol(self: P, value: Union[str, List[str]]) -> P:
         """
         Sets the value of :py:attr:`featuresCol` or :py:attr:`featuresCols`. Used when input vectors are stored in a single column.
         """
         if isinstance(value, str):
-            self.set_params(featuresCol=value)
+            self._set_params(featuresCol=value)
         else:
-            self.set_params(featuresCols=value)
+            self._set_params(featuresCols=value)
         return self
 
     def setFeaturesCols(self: P, value: List[str]) -> P:
         """
         Sets the value of :py:attr:`featuresCols`. Used when input vectors are stored as multiple feature columns.
         """
-        return self.set_params(featuresCols=value)
+        return self._set_params(featuresCols=value)
 
     def setLabelCol(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`labelCol`.
         """
-        return self.set_params(labelCol=value)
+        return self._set_params(labelCol=value)
 
     def getOutputCol(self) -> str:
         """
         Gets the value of :py:attr:`outputCol`. Contains the embeddings of the input data.
         """
         return self.getOrDefault("outputCol")
 
     def setOutputCol(self: P, value: str) -> P:
         """
         Sets the value of :py:attr:`outputCol`. Contains the embeddings of the input data.
         """
-        return self.set_params(outputCol=value)
+        return self._set_params(outputCol=value)
 
 
 class UMAP(UMAPClass, _CumlEstimatorSupervised, _UMAPCumlParams):
-
     """
     Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique
     used for low-dimensional data visualization and general non-linear dimension reduction.
     The algorithm finds a low dimensional embedding of the data that approximates an underlying manifold.
     The fit() method constructs a KNN-graph representation of an input dataset and then optimizes a
     low dimensional embedding, and is performed on a single node. The transform() method transforms an input dataset
     into the optimized embedding space, and is performed distributedly.
@@ -706,29 +709,31 @@
             * ``6`` - Enables all messages up to and including trace messages.
 
     sample_fraction : float (optional, default=1.0)
         The fraction of the dataset to be used for fitting the model. Since fitting is done on a single node, very large
         datasets must be subsampled to fit within the node's memory and execute in a reasonable time. Smaller fractions
         will result in faster training, but may result in sub-optimal embeddings.
 
-    featuresCol: str
-        The name of the column that contains input vectors. featuresCol should be set when input vectors are stored
-        in a single column of a dataframe.
-
-    featuresCols: List[str]
-        The names of the columns that contain input vectors. featuresCols should be set when input vectors are stored
-        in multiple columns of a dataframe.
+    featuresCol: str or List[str]
+        The feature column names, spark-rapids-ml supports vector, array and columnar as the input.\n
+            * When the value is a string, the feature columns must be assembled into 1 column with vector or array type.
+            * When the value is a list of strings, the feature columns must be numeric types.
 
     labelCol: str (optional)
         The name of the column that contains labels. If provided, supervised fitting will be performed, where labels
         will be taken into account when optimizing the embedding.
 
     outputCol: str (optional)
         The name of the column that contains embeddings. If not provided, the default name of "embedding" will be used.
 
+    num_workers:
+        Number of cuML workers, where each cuML worker corresponds to one Spark task
+        running on one GPU. If not set, spark-rapids-ml tries to infer the number of
+        cuML workers (i.e. GPUs in cluster) from the Spark environment.
+
     Examples
     --------
     >>> from spark_rapids_ml.umap import UMAP
     >>> from cuml.datasets import make_blobs
     >>> import cupy as cp
 
     >>> X, _ = make_blobs(500, 5, centers=42, cluster_std=0.1, dtype=np.float32, random_state=10)
@@ -772,22 +777,50 @@
     [  0.6557462  17.965862 ]
     [-16.220764   -6.4817486]
     [ 12.476492   13.80965  ]
     [  6.823325  -16.71719  ]]
 
     """
 
-    def __init__(self, **kwargs: Any) -> None:
+    @pyspark.keyword_only
+    def __init__(
+        self,
+        *,
+        n_neighbors: Optional[float] = 15,
+        n_components: Optional[int] = 15,
+        metric: str = "euclidean",
+        n_epochs: Optional[int] = None,
+        learning_rate: Optional[float] = 1.0,
+        init: Optional[str] = "spectral",
+        min_dist: Optional[float] = 0.1,
+        spread: Optional[float] = 1.0,
+        set_op_mix_ratio: Optional[float] = 1.0,
+        local_connectivity: Optional[float] = 1.0,
+        repulsion_strength: Optional[float] = 1.0,
+        negative_sample_rate: Optional[int] = 5,
+        transform_queue_size: Optional[float] = 1.0,
+        a: Optional[float] = None,
+        b: Optional[float] = None,
+        precomputed_knn: Optional[List[List[float]]] = None,
+        random_state: Optional[int] = None,
+        sample_fraction: Optional[float] = 1.0,
+        featuresCol: Optional[Union[str, List[str]]] = None,
+        labelCol: Optional[str] = None,
+        outputCol: Optional[str] = None,
+        num_workers: Optional[int] = None,
+        verbose: Union[int, bool] = False,
+        **kwargs: Any,
+    ) -> None:
         super().__init__()
-        if not kwargs.get("float32_inputs", True):
+        if not self._input_kwargs.get("float32_inputs", True):
             get_logger(self.__class__).warning(
                 "This estimator does not support double precision inputs. Setting float32_inputs to False will be ignored."
             )
-            kwargs.pop("float32_inputs")
-        self.set_params(**kwargs)
+            self._input_kwargs.pop("float32_inputs")
+        self._set_params(**self._input_kwargs)
         max_records_per_batch_str = _get_spark_session().conf.get(
             "spark.sql.execution.arrow.maxRecordsPerBatch", "10000"
         )
         assert max_records_per_batch_str is not None
         self.max_records_per_batch = int(max_records_per_batch_str)
         self.BROADCAST_LIMIT = 8 << 30
 
@@ -878,15 +911,18 @@
     def _fit_array_order(self) -> _ArrayOrder:
         return "C"
 
     def _get_cuml_fit_func(
         self,
         dataset: DataFrame,
         extra_params: Optional[List[Dict[str, Any]]] = None,
-    ) -> Callable[[FitInputType, Dict[str, Any]], Dict[str, Any],]:
+    ) -> Callable[
+        [FitInputType, Dict[str, Any]],
+        Dict[str, Any],
+    ]:
         array_order = self._fit_array_order()
 
         def _cuml_fit(
             dfs: FitInputType,
             params: Dict[str, Any],
         ) -> Dict[str, Any]:
             from cuml.manifold import UMAP as CumlUMAP
@@ -982,20 +1018,20 @@
             if cuda_managed_mem_enabled:
                 import rmm
                 from rmm.allocators.cupy import rmm_cupy_allocator
 
                 rmm.reinitialize(managed_memory=True)
                 cp.cuda.set_allocator(rmm_cupy_allocator)
 
-            _CumlCommon.initialize_cuml_logging(cuml_verbose)
+            _CumlCommon._initialize_cuml_logging(cuml_verbose)
 
             context = TaskContext.get()
 
             # set gpu device
-            _CumlCommon.set_gpu_device(context, is_local)
+            _CumlCommon._set_gpu_device(context, is_local)
 
             # handle the input
             # inputs = [(X, Optional(y)), (X, Optional(y))]
             logger.info("Loading data into python worker memory")
             inputs = []
             sizes = []
             for pdf in pdf_iter:
@@ -1107,16 +1143,20 @@
     def raw_data(self) -> List[List[float]]:
         res = []
         for chunk in self.raw_data_:
             res.extend(chunk.value.tolist())
         return res
 
     def _get_cuml_transform_func(
-        self, dataset: DataFrame, category: str = transform_evaluate.transform
-    ) -> Tuple[_ConstructFunc, _TransformFunc, Optional[_EvaluateFunc],]:
+        self, dataset: DataFrame, eval_metric_info: Optional[EvalMetricInfo] = None
+    ) -> Tuple[
+        _ConstructFunc,
+        _TransformFunc,
+        Optional[_EvaluateFunc],
+    ]:
         cuml_alg_params = self.cuml_params
         driver_embedding = self.embedding_
         driver_raw_data = self.raw_data_
         outputCol = self.getOutputCol()
 
         def _construct_umap() -> CumlT:
             import cupy as cp
@@ -1196,15 +1236,15 @@
         return StructType(
             [
                 StructField("features", ArrayType(FloatType(), False), False),
                 StructField(self.getOutputCol(), ArrayType(FloatType(), False), False),
             ]
         )
 
-    def get_model_attributes(self) -> Optional[Dict[str, Any]]:
+    def _get_model_attributes(self) -> Optional[Dict[str, Any]]:
         """
         Override parent method to bring broadcast variables to driver before JSON serialization.
         """
 
         self._model_attributes["embedding_"] = [
             chunk.value for chunk in self.embedding_
         ]
@@ -1232,15 +1272,15 @@
             extraMetadata={
                 "_cuml_params": self.instance._cuml_params,
                 "_num_workers": self.instance._num_workers,
                 "_float32_inputs": self.instance._float32_inputs,
             },
         )
         data_path = os.path.join(path, "data")
-        model_attributes = self.instance.get_model_attributes()
+        model_attributes = self.instance._get_model_attributes()
 
         if not os.path.exists(data_path):
             os.makedirs(data_path)
         assert model_attributes is not None
         for key, value in model_attributes.items():
             if isinstance(value, list) and isinstance(value[0], np.ndarray):
                 paths = []
```

## spark_rapids_ml/utils.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023, NVIDIA CORPORATION.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,18 +17,22 @@
 import logging
 import sys
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Literal, Set, Tuple, Union
 
 if TYPE_CHECKING:
     import cudf
     import cupy as cp
+    import cupyx
 
 import numpy as np
-from pyspark import BarrierTaskContext, SparkContext, TaskContext
-from pyspark.sql import SparkSession
+import pandas as pd
+import scipy
+from pyspark import BarrierTaskContext, SparkConf, SparkContext, TaskContext
+from pyspark.sql import Column, SparkSession
+from pyspark.sql.types import ArrayType, FloatType
 
 _ArrayOrder = Literal["C", "F"]
 
 
 def _method_names_from_param(spark_param_name: str) -> List[str]:
     """
     Returns getter and setter method names, per Spark ML conventions, for passed in attribute.
@@ -69,16 +73,16 @@
 
 
 def _is_local(sc: SparkContext) -> bool:
     """Whether it is Spark local mode"""
     return sc._jsc.sc().isLocal()  # type: ignore
 
 
-def _is_standalone_or_localcluster(sc: SparkContext) -> bool:
-    master = sc.getConf().get("spark.master")
+def _is_standalone_or_localcluster(conf: SparkConf) -> bool:
+    master = conf.get("spark.master")
     return master is not None and (
         master.startswith("spark://") or master.startswith("local-cluster")
     )
 
 
 def _str_or_numerical(x: str) -> Union[str, float, int]:
     """
@@ -193,34 +197,51 @@
         parts_rank_size = [item for pair in messages for item in json.loads(pair)]
         total_rows = sum(pair[1] for pair in parts_rank_size)
 
         return cls(total_rows, total_cols, rank, parts_rank_size)
 
 
 def _concat_and_free(
-    array_list: Union[List["cp.ndarray"], List[np.ndarray]], order: _ArrayOrder = "F"
-) -> Union["cp.ndarray", np.ndarray]:
+    array_list: Union[
+        List["cp.ndarray"],
+        List[np.ndarray],
+        List[scipy.sparse.csr_matrix],
+        List["cupyx.scipy.sparse.csr_matrix"],
+    ],
+    order: _ArrayOrder = "F",
+) -> Union[
+    "cp.ndarray", np.ndarray, scipy.sparse.csr_matrix, "cupyx.scipy.sparse.csr_matrix"
+]:
     """
     concatenates a list of compatible numpy arrays into a 'order' ordered output array,
     in a memory efficient way.
     Note: frees list elements so do not reuse after calling.
-    """
-    import cupy as cp
 
-    array_module = cp if isinstance(array_list[0], cp.ndarray) else np
+    if the type of input arrays is scipy or cupyx csr_matrix, 'order' parameter will not be used.
+    """
+    import cupyx
 
-    rows = sum(arr.shape[0] for arr in array_list)
-    if len(array_list[0].shape) > 1:
-        cols = array_list[0].shape[1]
-        concat_shape: Tuple[int, ...] = (rows, cols)
+    if isinstance(array_list[0], scipy.sparse.csr_matrix):
+        concated = scipy.sparse.vstack(array_list)
+    elif isinstance(array_list[0], cupyx.scipy.sparse.csr_matrix):
+        concated = cupyx.scipy.sparse.vstack(array_list)
     else:
-        concat_shape = (rows,)
-    d_type = array_list[0].dtype
-    concated = array_module.empty(shape=concat_shape, order=order, dtype=d_type)
-    array_module.concatenate(array_list, out=concated)
+        import cupy as cp
+
+        array_module = cp if isinstance(array_list[0], cp.ndarray) else np
+
+        rows = sum(arr.shape[0] for arr in array_list)
+        if len(array_list[0].shape) > 1:
+            cols = array_list[0].shape[1]
+            concat_shape: Tuple[int, ...] = (rows, cols)
+        else:
+            concat_shape = (rows,)
+        d_type = array_list[0].dtype
+        concated = array_module.empty(shape=concat_shape, order=order, dtype=d_type)
+        array_module.concatenate(array_list, out=concated)
     del array_list[:]
     return concated
 
 
 def cudf_to_cuml_array(gdf: Union["cudf.DataFrame", "cudf.Series"], order: str = "F"):  # type: ignore
     try:
         # Compatible with older cuml version (before 23.02)
@@ -440,7 +461,27 @@
             impurity,
             model,
             translate_trees(sc, impurity, left_child),
             translate_trees(sc, impurity, right_child),
         )
     elif "leaf_value" in model:
         return _create_leaf_node(sc, impurity, model)
+
+
+# to the XGBOOST _get_unwrap_udt_fn in https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/spark/core.py
+def _get_unwrap_udt_fn() -> Callable[[Union[Column, str]], Column]:
+    try:
+        from pyspark.sql.functions import unwrap_udt  # type: ignore
+
+        return unwrap_udt
+    except ImportError:
+        pass
+
+    try:
+        from pyspark.databricks.sql.functions import unwrap_udt as databricks_unwrap_udt
+
+        return databricks_unwrap_udt
+    except ImportError as exc:
+        raise RuntimeError(
+            "Cannot import pyspark `unwrap_udt` function. Please install pyspark>=3.4 "
+            "or run on Databricks Runtime."
+        ) from exc
```

## spark_rapids_ml/common/cuml_context.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2022-2023, NVIDIA CORPORATION.
+# Copyright (c) 2022-2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -146,15 +146,21 @@
             )
         return self
 
     def __exit__(self, *args: Any) -> None:
         if not self.enable:
             return
         assert self._nccl_comm is not None
-        self._nccl_comm.destroy()
+
+        # if no exception cleanup nicely, otherwise abort
+        if not args[0]:
+            self._nccl_comm.destroy()
+        else:
+            self._nccl_comm.abort()
+
         del self._nccl_comm
 
         del self._handle
 
         if self._loop is not None:
             asyncio.get_event_loop().stop()
             asyncio.get_event_loop().close()
```

## spark_rapids_ml/metrics/MulticlassMetrics.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,17 +12,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 from typing import Dict
 
+import numpy as np
 from pyspark.ml.evaluation import MulticlassClassificationEvaluator
 
 
+# sklearn's version will not support fixed eps starting v1.5
+def log_loss(labels: np.ndarray, probs: np.ndarray, eps: float) -> float:
+    if np.any(labels < 0) or np.any(labels > probs.shape[1] - 1):
+        raise ValueError(f"labels must be in the range [0,{probs.shape[1]-1}]")
+    if np.any(probs < 0) or np.any(probs > 1.0):
+        raise ValueError("probs must be in the range [0.0, 1.0]")
+    probs_for_labels = probs[np.arange(probs.shape[0]), labels.astype(np.int32)]
+    probs_for_labels = np.maximum(probs_for_labels, eps)
+    return sum(-np.log(probs_for_labels))
+
+
 class MulticlassMetrics:
     """Metrics for multiclass classification."""
 
     SUPPORTED_MULTI_CLASS_METRIC_NAMES = [
         "f1",
         "accuracy",
         "weightedPrecision",
@@ -32,29 +44,32 @@
         "weightedFMeasure",
         "truePositiveRateByLabel",
         "falsePositiveRateByLabel",
         "precisionByLabel",
         "recallByLabel",
         "fMeasureByLabel",
         "hammingLoss",
+        "logLoss",
     ]
 
     # This class is aligning with MulticlassMetrics scala version.
 
     def __init__(
         self,
-        tp: Dict[float, float],
-        fp: Dict[float, float],
-        label: Dict[float, float],
-        label_count: int,
+        tp: Dict[float, float] = {},
+        fp: Dict[float, float] = {},
+        label: Dict[float, float] = {},
+        label_count: int = 0,
+        log_loss: float = -1,
     ) -> None:
         self._tp_by_class = tp
         self._fp_by_class = fp
         self._label_count_by_class = label
         self._label_count = label_count
+        self._log_loss = log_loss
 
     def _precision(self, label: float) -> float:
         """Returns precision for a given label (category)"""
         tp = self._tp_by_class[label]
         fp = self._fp_by_class[label]
         return 0.0 if (tp + fp == 0) else tp / (tp + fp)
 
@@ -123,14 +138,18 @@
 
     def hamming_loss(self) -> float:
         """Returns Hamming-loss"""
         numerator = sum(self._fp_by_class.values())
         denominator = self._label_count
         return numerator / denominator
 
+    def log_loss(self) -> float:
+        """Returns log loss"""
+        return self._log_loss / self._label_count
+
     def evaluate(self, evaluator: MulticlassClassificationEvaluator) -> float:
         metric_name = evaluator.getMetricName()
         if metric_name == "f1":
             return self.weighted_fmeasure()
         elif metric_name == "accuracy":
             return self.accuracy()
         elif metric_name == "weightedPrecision":
@@ -151,9 +170,11 @@
             return self._precision(evaluator.getMetricLabel())
         elif metric_name == "recallByLabel":
             return self._recall(evaluator.getMetricLabel())
         elif metric_name == "fMeasureByLabel":
             return self._f_measure(evaluator.getMetricLabel(), evaluator.getBeta())
         elif metric_name == "hammingLoss":
             return self.hamming_loss()
+        elif metric_name == "logLoss":
+            return self.log_loss()
         else:
             raise ValueError(f"Unsupported metric name, found {metric_name}")
```

## spark_rapids_ml/metrics/RegressionMetrics.py

```diff
@@ -1,9 +1,9 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -163,15 +163,15 @@
         m2: List[float],
         l1: List[float],
         total_cnt: int,
     ) -> "RegressionMetrics":
         return RegressionMetrics(_SummarizerBuffer(mean, m2n, m2, l1, total_cnt))
 
     @classmethod
-    def from_rows(cls, num_models: int, rows: List[Row]) -> List["RegressionMetrics"]:
+    def _from_rows(cls, num_models: int, rows: List[Row]) -> List["RegressionMetrics"]:
         """The rows must contain pred.model_index, and mean/m2n/m2/l1/total_count"""
         metrics: List[Optional["RegressionMetrics"]] = [None] * num_models
 
         for row in rows:
             index = row[pred.model_index]
             metric = RegressionMetrics.create(
                 mean=row[reg_metrics.mean],
```

## spark_rapids_ml/metrics/__init__.py

```diff
@@ -1,15 +1,40 @@
 #
-# Copyright (c) 2023, NVIDIA CORPORATION.
+# Copyright (c) 2024, NVIDIA CORPORATION.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
+
+from collections import namedtuple
+from dataclasses import dataclass
+from typing import Optional
+
+# Global parameter used by core and subclasses.
+TransformEvaluateMetric = namedtuple(
+    "TransformEvaluateMetric", ("accuracy_like", "log_loss", "regression")
+)
+transform_evaluate_metric = TransformEvaluateMetric(
+    "accuracy_like", "log_loss", "regression"
+)
+
+
+@dataclass
+class EvalMetricInfo:
+    """Class for holding info about
+    Spark evaluators to be passed in to transform_evaluate local computations"""
+
+    # MulticlassClassificationEvaluator
+    eps: float = 1.0e-15  # logLoss
+    # BinaryClassificationEvaluator - placeholder till we support
+    numBins: int = 1000
+
+    eval_metric: Optional[str] = None
```

## Comparing `spark_rapids_ml-23.8.0.dist-info/METADATA` & `spark_rapids_ml-24.2.0.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: spark-rapids-ml
-Version: 23.8.0
+Version: 24.2.0
 Summary: Apache Spark integration with RAPIDS and cuML
 Author-email: Jinfeng Li <jinfeng@nvidia.com>, Bobby Wang <bobwang@nvidia.com>, Erik Ordentlich <eordentlich@nvidia.com>, Lee Yang <leey@nvidia.com>
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Classifier: License :: OSI Approved :: Apache Software License
@@ -20,26 +20,26 @@
 
 ## Installation
 
 For simplicity, the following instructions just use Spark local mode, assuming a server with at least one GPU.
 
 First, install RAPIDS cuML per [these instructions](https://rapids.ai/start.html).   Example for CUDA Toolkit 11.8:
 ```bash
-conda create -n rapids-23.08 \
+conda create -n rapids-24.02 \
     -c rapidsai -c conda-forge -c nvidia \
-    cuml=23.08 python=3.9 cuda-version=11.8
+    cuml=24.02 python=3.9 cuda-version=11.8
 ```
 
 **Note**: while testing, we recommend using conda or docker to simplify installation and isolate your environment while experimenting.  Once you have a working environment, you can then try installing directly, if necessary.
 
 **Note**: you can select the latest version compatible with your environment from [rapids.ai](https://rapids.ai/start.html#get-rapids).
 
 Once you have the conda environment, activate it and install the required packages.
 ```bash
-conda activate rapids-23.08
+conda activate rapids-24.02
 
 ## for development access to notebooks, tests, and benchmarks
 git clone --branch main https://github.com/NVIDIA/spark-rapids-ml.git
 cd spark-rapids-ml/python
 # install additional non-RAPIDS python dependencies for dev
 pip install -r requirements_dev.txt
 pip install -e .
@@ -60,16 +60,16 @@
 ```python
 ## pyspark --master local[*]
 # from pyspark.ml.regression import LinearRegression
 from spark_rapids_ml.regression import LinearRegression
 from pyspark.ml.linalg import Vectors
 
 df = spark.createDataFrame([
-     (1.0, 2.0, Vectors.dense(1.0, 0.0)),
-     (0.0, 2.0, Vectors.dense(0.0, 1.0))], ["label", "weight", "features"])
+     (1.0, Vectors.dense(1.0, 0.0)),
+     (0.0, Vectors.dense(0.0, 1.0))], ["label", "features"])
 
 # number of partitions should match number of GPUs in Spark cluster
 df = df.repartition(1)
 
 lr = LinearRegression(regParam=0.0, solver="normal")
 lr.setMaxIter(5)
 lr.setRegParam(0.0)
@@ -84,17 +84,17 @@
 
 #### K-Means
 ```python
 ## pyspark --master local[*]
 # from pyspark.ml.clustering import KMeans
 from spark_rapids_ml.clustering import KMeans
 from pyspark.ml.linalg import Vectors
-data = [(Vectors.dense([0.0, 0.0]), 2.0), (Vectors.dense([1.0, 1.0]), 2.0),
-        (Vectors.dense([9.0, 8.0]), 2.0), (Vectors.dense([8.0, 9.0]), 2.0)]
-df = spark.createDataFrame(data, ["features", "weighCol"])
+data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),
+        (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]
+df = spark.createDataFrame(data, ["features"])
 
 # number of partitions should match number of GPUs in Spark cluster
 df = df.repartition(1)
 
 kmeans = KMeans(k=2)
 kmeans.setSeed(1)
 kmeans.setMaxIter(20)
@@ -105,21 +105,21 @@
 centers = model.clusterCenters()
 print(centers)
 # [array([0.5, 0.5]), array([8.5, 8.5])]
 
 model.setPredictionCol("newPrediction")
 transformed = model.transform(df)
 transformed.show()
-# +--------+----------+-------------+
-# |weighCol|  features|newPrediction|
-# +--------+----------+-------------+
-# |     2.0|[0.0, 0.0]|            1|
-# |     2.0|[1.0, 1.0]|            1|
-# |     2.0|[9.0, 8.0]|            0|
-# |     2.0|[8.0, 9.0]|            0|
+# +----------+-------------+
+# |  features|newPrediction|
+# +----------+-------------+
+# |[0.0, 0.0]|            1|
+# |[1.0, 1.0]|            1|
+# |[9.0, 8.0]|            0|
+# |[8.0, 9.0]|            0|
 # +--------+----------+-------------+
 rows[0].newPrediction == rows[1].newPrediction
 # True
 rows[2].newPrediction == rows[3].newPrediction
 # True
 ```
```

